<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 21 Feb 24  to  Thu 22 Feb 24, announced Fri, 23 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item379">Cross-lists</a></li>
<li><a href="#item419">Replacements</a></li>
</ul>
<small>[ total of 662 entries:  <b>1-662</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 23 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14021" title="Abstract">arXiv:2402.14021</a> [<a href="/pdf/2402.14021" title="Download PDF">pdf</a>, <a href="/ps/2402.14021" title="Download PostScript">ps</a>, <a href="/format/2402.14021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Betting on what is neither verifiable nor falsifiable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudhir%2C+A+P">Abhimanyu Pallavi Sudhir</a>, 
<a href="/search/cs?searchtype=author&query=Tran-Thanh%2C+L">Long Tran-Thanh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Prediction markets are useful for estimating probabilities of claims whose
truth will be revealed at some fixed time -- this includes questions about the
values of real-world events (i.e. statistical uncertainty), and questions about
the values of primitive recursive functions (i.e. logical or algorithmic
uncertainty). However, they cannot be directly applied to questions without a
fixed resolution criterion, and real-world applications of prediction markets
to such questions often amount to predicting not whether a sentence is true,
but whether it will be proven. Such questions could be represented by countable
unions or intersections of more basic events, or as First-Order-Logic sentences
on the Arithmetical Hierarchy (or even beyond FOL, as hyperarithmetical
sentences). In this paper, we propose an approach to betting on such events via
options, or equivalently as bets on the outcome of a
"verification-falsification game". Our work thus acts as an alternative to the
existing framework of Garrabrant induction for logical uncertainty, and relates
to the stance known as constructivism in the philosophy of mathematics;
furthermore it has broader implications for philosophy and mathematical logic.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14027" title="Abstract">arXiv:2402.14027</a> [<a href="/pdf/2402.14027" title="Download PDF">pdf</a>, <a href="/ps/2402.14027" title="Download PostScript">ps</a>, <a href="/format/2402.14027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning causation event conjunction sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Portegys%2C+T+E">Thomas E. Portegys</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This is an examination of some methods that learn causations in event
sequences. A causation is defined as a conjunction of one or more cause events
occurring in an arbitrary order, with possible intervening non-causal events,
that lead to an effect. The methods include recurrent and non-recurrent
artificial neural networks (ANNs), as well as a histogram-based algorithm. An
attention recurrent ANN performed the best of the ANNs, while the histogram
algorithm was significantly superior to all the ANNs.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14029" title="Abstract">arXiv:2402.14029</a> [<a href="/pdf/2402.14029" title="Download PDF">pdf</a>, <a href="/format/2402.14029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Search in a Frozen Network is Enough to Find a Strong Lottery  Ticket
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Otsuka%2C+H">Hikari Otsuka</a>, 
<a href="/search/cs?searchtype=author&query=Chijiwa%2C+D">Daiki Chijiwa</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Arias%2C+%C3%81+L">&#xc1;ngel L&#xf3;pez Garc&#xed;a-Arias</a>, 
<a href="/search/cs?searchtype=author&query=Okoshi%2C+Y">Yasuyuki Okoshi</a>, 
<a href="/search/cs?searchtype=author&query=Kawamura%2C+K">Kazushi Kawamura</a>, 
<a href="/search/cs?searchtype=author&query=Van+Chu%2C+T">Thiem Van Chu</a>, 
<a href="/search/cs?searchtype=author&query=Fujiki%2C+D">Daichi Fujiki</a>, 
<a href="/search/cs?searchtype=author&query=Takeuchi%2C+S">Susumu Takeuchi</a>, 
<a href="/search/cs?searchtype=author&query=Motomura%2C+M">Masato Motomura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages; comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Randomly initialized dense networks contain subnetworks that achieve high
accuracy without weight learning -- strong lottery tickets (SLTs). Recently,
Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs
can also be found within a randomly pruned source network, thus reducing the
SLT search space. However, this limits the search to SLTs that are even sparser
than the source, leading to worse accuracy due to unintentionally high
sparsity. This paper proposes a method that reduces the SLT search space by an
arbitrary ratio that is independent of the desired SLT sparsity. A random
subset of the initial weights is excluded from the search space by freezing it
-- i.e., by either permanently pruning them or locking them as a fixed part of
the SLT. Indeed, the SLT existence in such a reduced search space is
theoretically guaranteed by our subset-sum approximation with randomly frozen
variables. In addition to reducing search space, the random freezing pattern
can also be exploited to reduce model size in inference. Furthermore,
experimental results show that the proposed method finds SLTs with better
accuracy and model size trade-off than the SLTs obtained from dense or randomly
pruned source networks. In particular, the SLT found in a frozen graph neural
network achieves higher accuracy than its weight trained counterpart while
reducing model size by $40.3\times$.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14031" title="Abstract">arXiv:2402.14031</a> [<a href="/pdf/2402.14031" title="Download PDF">pdf</a>, <a href="/ps/2402.14031" title="Download PostScript">ps</a>, <a href="/format/2402.14031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoencoder with Ordered Variance for Nonlinear Model Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Augustine%2C+M+T">Midhun T. Augustine</a>, 
<a href="/search/eess?searchtype=author&query=Patil%2C+P">Parag Patil</a>, 
<a href="/search/eess?searchtype=author&query=Bhushan%2C+M">Mani Bhushan</a>, 
<a href="/search/eess?searchtype=author&query=Bhartiya%2C+S">Sharad Bhartiya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a novel autoencoder with ordered variance (AEO) in which
the loss function is modified with a variance regularization term to enforce
order in the latent space. Further, the autoencoder is modified using ResNets,
which results in a ResNet AEO (RAEO). The paper also illustrates the
effectiveness of AEO and RAEO in extracting nonlinear relationships among input
variables in an unsupervised setting.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14033" title="Abstract">arXiv:2402.14033</a> [<a href="/pdf/2402.14033" title="Download PDF">pdf</a>, <a href="/format/2402.14033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VN Network: Embedding Newly Emerging Entities with Virtual Neighbors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongquan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CIKM (2020) 505-514
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Embedding entities and relations into continuous vector spaces has attracted
a surge of interest in recent years. Most embedding methods assume that all
test entities are available during training, which makes it time-consuming to
retrain embeddings for newly emerging entities. To address this issue, recent
works apply the graph neural network on the existing neighbors of the unseen
entities. In this paper, we propose a novel framework, namely Virtual Neighbor
(VN) network, to address three key challenges. Firstly, to reduce the neighbor
sparsity problem, we introduce the concept of the virtual neighbors inferred by
rules. And we assign soft labels to these neighbors by solving a
rule-constrained problem, rather than simply regarding them as unquestionably
true. Secondly, many existing methods only use one-hop or two-hop neighbors for
aggregation and ignore the distant information that may be helpful. Instead, we
identify both logic and symmetric path rules to capture complex patterns.
Finally, instead of one-time injection of rules, we employ an iterative
learning scheme between the embedding method and virtual neighbor prediction to
capture the interactions within. Experimental results on two knowledge graph
completion tasks demonstrate that our VN network significantly outperforms
state-of-the-art baselines. Furthermore, results on Subject/Object-R show that
our proposed VN network is highly robust to the neighbor sparsity problem.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14034" title="Abstract">arXiv:2402.14034</a> [<a href="/pdf/2402.14034" title="Download PDF">pdf</a>, <a href="/format/2402.14034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentScope: A Flexible yet Robust Multi-Agent Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+D">Dawei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+W">Weirui Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuchen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhijian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+B">Bingchen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hongzhu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have released code on <a href="https://github.com/modelscope/agentscope">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rapid advancement of Large Language Models (LLMs), significant
progress has been made in multi-agent applications. However, the complexities
in coordinating agents' cooperation and LLMs' erratic performance pose notable
challenges in developing robust and efficient multi-agent applications. To
tackle these challenges, we propose AgentScope, a developer-centric multi-agent
platform with message exchange as its core communication mechanism. Together
with abundant syntactic tools, built-in resources, and user-friendly
interactions, our communication mechanism significantly reduces the barriers to
both development and understanding. Towards robust and flexible multi-agent
application, AgentScope provides both built-in and customizable fault tolerance
mechanisms while it is also armed with system-level supports for multi-modal
data generation, storage and transmission. Additionally, we design an
actor-based distribution framework, enabling easy conversion between local and
distributed deployments and automatic parallel optimization without extra
effort. With these features, AgentScope empowers developers to build
applications that fully realize the potential of intelligent agents. We have
released AgentScope at https://github.com/modelscope/agentscope, and hope
AgentScope invites wider participation and innovation in this fast-moving
field.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14035" title="Abstract">arXiv:2402.14035</a> [<a href="/pdf/2402.14035" title="Download PDF">pdf</a>, <a href="/format/2402.14035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wisdom of Committee: Distilling from Foundation Model to  SpecializedApplication Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuening Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Shuchao Bi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhe Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in foundation models have yielded impressive performance
across a wide range of tasks. Meanwhile, for specific applications,
practitioners have been developing specialized application models. To enjoy the
benefits of both kinds of models, one natural path is to transfer the knowledge
in foundation models into specialized application models, which are generally
more efficient for serving. Techniques from knowledge distillation may be
applied here, where the application model learns to mimic the foundation model.
However, specialized application models and foundation models have substantial
gaps in capacity, employing distinct architectures, using different input
features from different modalities, and being optimized on different
distributions. These differences in model characteristics lead to significant
challenges for distillation methods. In this work, we propose creating a
teaching committee comprising both foundation model teachers and complementary
teachers. Complementary teachers possess model characteristics akin to the
student's, aiming to bridge the gap between the foundation model and
specialized application models for a smoother knowledge transfer. Further, to
accommodate the dissimilarity among the teachers in the committee, we introduce
DiverseDistill, which allows the student to understand the expertise of each
teacher and extract task knowledge. Our evaluations demonstrate that adding
complementary teachers enhances student performance. Finally, DiverseDistill
consistently outperforms baseline distillation methods, regardless of the
teacher choices, resulting in significantly improved student performance.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14037" title="Abstract">arXiv:2402.14037</a> [<a href="/pdf/2402.14037" title="Download PDF">pdf</a>, <a href="/ps/2402.14037" title="Download PostScript">ps</a>, <a href="/format/2402.14037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Effective Networks Intrusion Detection Approach Based on Hybrid  Harris Hawks and Multi-Layer Perceptron
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alazab%2C+M">Moutaz Alazab</a>, 
<a href="/search/cs?searchtype=author&query=Khurma%2C+R+A">Ruba Abu Khurma</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Abu-Salih%2C+B">Bilal Abu-Salih</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+A">Alejandro Martin</a>, 
<a href="/search/cs?searchtype=author&query=Camacho%2C+D">David Camacho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes an Intrusion Detection System (IDS) employing the Harris
Hawks Optimization algorithm (HHO) to optimize Multilayer Perceptron learning
by optimizing bias and weight parameters. HHO-MLP aims to select optimal
parameters in its learning process to minimize intrusion detection errors in
networks. HHO-MLP has been implemented using EvoloPy NN framework, an
open-source Python tool specialized for training MLPs using evolutionary
algorithms. For purposes of comparing the HHO model against other evolutionary
methodologies currently available, specificity and sensitivity measures,
accuracy measures, and mse and rmse measures have been calculated using KDD
datasets. Experiments have demonstrated the HHO MLP method is effective at
identifying malicious patterns. HHO-MLP has been tested against evolutionary
algorithms like Butterfly Optimization Algorithm (BOA), Grasshopper
Optimization Algorithms (GOA), and Black Widow Optimizations (BOW), with
validation by Random Forest (RF), XG-Boost. HHO-MLP showed superior performance
by attaining top scores with accuracy rate of 93.17%, sensitivity level of
89.25%, and specificity percentage of 95.41%.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14039" title="Abstract">arXiv:2402.14039</a> [<a href="/pdf/2402.14039" title="Download PDF">pdf</a>, <a href="/ps/2402.14039" title="Download PostScript">ps</a>, <a href="/format/2402.14039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specialty detection in the context of telemedicine in a highly  imbalanced multi-class distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alomari%2C+A">Alaa Alomari</a>, 
<a href="/search/cs?searchtype=author&query=Faris%2C+H">Hossam Faris</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Covid-19 pandemic has led to an increase in the awareness of and demand
for telemedicine services, resulting in a need for automating the process and
relying on machine learning (ML) to reduce the operational load. This research
proposes a specialty detection classifier based on a machine learning model to
automate the process of detecting the correct specialty for each question and
routing it to the correct doctor. The study focuses on handling multiclass and
highly imbalanced datasets for Arabic medical questions, comparing some
oversampling techniques, developing a Deep Neural Network (DNN) model for
specialty detection, and exploring the hidden business areas that rely on
specialty detection such as customizing and personalizing the consultation flow
for different specialties. The proposed module is deployed in both synchronous
and asynchronous medical consultations to provide more real-time
classification, minimize the doctor effort in addressing the correct specialty,
and give the system more flexibility in customizing the medical consultation
flow. The evaluation and assessment are based on accuracy, precision, recall,
and F1-score. The experimental results suggest that combining multiple
techniques, such as SMOTE and reweighing with keyword identification, is
necessary to achieve improved performance in detecting rare classes in
imbalanced multiclass datasets. By using these techniques, specialty detection
models can more accurately detect rare classes in real-world scenarios where
imbalanced data is common.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14041" title="Abstract">arXiv:2402.14041</a> [<a href="/pdf/2402.14041" title="Download PDF">pdf</a>, <a href="/ps/2402.14041" title="Download PostScript">ps</a>, <a href="/format/2402.14041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E2USD: Efficient-yet-effective Unsupervised State Detection for  Multivariate Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhichen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dalin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weizhu Qian</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+C+S">Christian S. Jensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by The Web Conference 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">We propose E2USD that enables efficient-yet-accurate unsupervised MTS state
detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor
(FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together
encode input MTSs at low computational overhead. Additionally, we propose a
False Negative Cancellation Contrastive Learning method (FNCCLearning) to
counteract the effects of false negatives and to achieve more cluster-friendly
embedding spaces. To reduce computational overhead further in streaming
settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive
experiments with six baselines and six datasets offer evidence that E2USD is
capable of SOTA accuracy at significantly reduced computational overhead. Our
code is available at https://github.com/AI4CTS/E2Usd.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14042" title="Abstract">arXiv:2402.14042</a> [<a href="/pdf/2402.14042" title="Download PDF">pdf</a>, <a href="/format/2402.14042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protect and Extend -- Using GANs for Synthetic Data Generation of  Time-Series Medical Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashrafi%2C+N">Navid Ashrafi</a>, 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+V">Vera Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Spang%2C+R+P">Robert P. Spang</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+S">Sebastian M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Voigt-Antons%2C+J">Jan-Niklas Voigt-Antons</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Preservation of private user data is of paramount importance for high Quality
of Experience (QoE) and acceptability, particularly with services treating
sensitive data, such as IT-based health services. Whereas anonymization
techniques were shown to be prone to data re-identification, synthetic data
generation has gradually replaced anonymization since it is relatively less
time and resource-consuming and more robust to data leakage. Generative
Adversarial Networks (GANs) have been used for generating synthetic datasets,
especially GAN frameworks adhering to the differential privacy phenomena. This
research compares state-of-the-art GAN-based models for synthetic data
generation to generate time-series synthetic medical records of dementia
patients which can be distributed without privacy concerns. Predictive
modeling, autocorrelation, and distribution analysis are used to assess the
Quality of Generating (QoG) of the generated data. The privacy preservation of
the respective models is assessed by applying membership inference attacks to
determine potential data leakage risks. Our experiments indicate the
superiority of the privacy-preserving GAN (PPGAN) model over other models
regarding privacy preservation while maintaining an acceptable level of QoG.
The presented results can support better data protection for medical use cases
in the future.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14044" title="Abstract">arXiv:2402.14044</a> [<a href="/pdf/2402.14044" title="Download PDF">pdf</a>, <a href="/ps/2402.14044" title="Download PostScript">ps</a>, <a href="/format/2402.14044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new approach for solving global optimization and engineering problems  based on modified Sea Horse Optimizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashim%2C+F+A">Fatma A. Hashim</a>, 
<a href="/search/cs?searchtype=author&query=Mostafa%2C+R+R">Reham R. Mostafa</a>, 
<a href="/search/cs?searchtype=author&query=Khurma%2C+R+A">Ruba Abu Khurma</a>, 
<a href="/search/cs?searchtype=author&query=Qaddoura%2C+R">Raneem Qaddoura</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">P.A. Castillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sea Horse Optimizer (SHO) is a noteworthy metaheuristic algorithm that
emulates various intelligent behaviors exhibited by sea horses, encompassing
feeding patterns, male reproductive strategies, and intricate movement
patterns. To mimic the nuanced locomotion of sea horses, SHO integrates the
logarithmic helical equation and Levy flight, effectively incorporating both
random movements with substantial step sizes and refined local exploitation.
Additionally, the utilization of Brownian motion facilitates a more
comprehensive exploration of the search space. This study introduces a robust
and high-performance variant of the SHO algorithm named mSHO. The enhancement
primarily focuses on bolstering SHO's exploitation capabilities by replacing
its original method with an innovative local search strategy encompassing three
distinct steps: a neighborhood-based local search, a global non-neighbor-based
search, and a method involving circumnavigation of the existing search region.
These techniques improve mSHO algorithm's search capabilities, allowing it to
navigate the search space and converge toward optimal solutions efficiently.
The comprehensive results distinctly establish the supremacy and efficiency of
the mSHO method as an exemplary tool for tackling an array of optimization
quandaries. The results show that the proposed mSHO algorithm has a total rank
of 1 for CEC'2020 test functions. In contrast, the mSHO achieved the best value
for the engineering problems, recording a value of 0.012665, 2993.634, 0.01266,
1.724967, 263.8915, 0.032255, 58507.14, 1.339956, and 0.23524 for the pressure
vessel design, speed reducer design, tension/compression spring, welded beam
design, three-bar truss engineering design, industrial refrigeration system,
multi-Product batch plant, cantilever beam problem, multiple disc clutch brake
problems, respectively.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14047" title="Abstract">arXiv:2402.14047</a> [<a href="/pdf/2402.14047" title="Download PDF">pdf</a>, <a href="/format/2402.14047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Effective Transfer Learning for Neuro-Symbolic Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daniele%2C+A">Alessandro Daniele</a>, 
<a href="/search/cs?searchtype=author&query=Campari%2C+T">Tommaso Campari</a>, 
<a href="/search/cs?searchtype=author&query=Malhotra%2C+S">Sagar Malhotra</a>, 
<a href="/search/cs?searchtype=author&query=Serafini%2C+L">Luciano Serafini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep Learning (DL) techniques have achieved remarkable successes in recent
years. However, their ability to generalize and execute reasoning tasks remains
a challenge. A potential solution to this issue is Neuro-Symbolic Integration
(NeSy), where neural approaches are combined with symbolic reasoning. Most of
these methods exploit a neural network to map perceptions to symbols and a
logical reasoner to predict the output of the downstream task. These methods
exhibit superior generalization capacity compared to fully neural
architectures. However, they suffer from several issues, including slow
convergence, learning difficulties with complex perception tasks, and
convergence to local minima. This paper proposes a simple yet effective method
to ameliorate these problems. The key idea involves pretraining a neural model
on the downstream task. Then, a NeSy model is trained on the same task via
transfer learning, where the weights of the perceptual part are injected from
the pretrained network. The key observation of our work is that the neural
network fails to generalize only at the level of the symbolic part while being
perfectly capable of learning the mapping from perceptions to symbols. We have
tested our training strategy on various SOTA NeSy methods and datasets,
demonstrating consistent improvements in the aforementioned problems.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14048" title="Abstract">arXiv:2402.14048</a> [<a href="/pdf/2402.14048" title="Download PDF">pdf</a>, <a href="/format/2402.14048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hottung%2C+A">Andr&#xe9; Hottung</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+M">Mridul Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Tierney%2C+K">Kevin Tierney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning-based methods for constructing solutions to
combinatorial optimization problems are rapidly approaching the performance of
human-designed algorithms. To further narrow the gap, learning-based approaches
must efficiently explore the solution space during the search process. Recent
approaches artificially increase exploration by enforcing diverse solution
generation through handcrafted rules, however, these rules can impair solution
quality and are difficult to design for more complex problems. In this paper,
we introduce PolyNet, an approach for improving exploration of the solution
space by learning complementary solution strategies. In contrast to other
works, PolyNet uses only a single-decoder and a training schema that does not
enforce diverse solution generation through handcrafted rules. We evaluate
PolyNet on four combinatorial optimization problems and observe that the
implicit diversity mechanism allows PolyNet to find better solutions than
approaches the explicitly enforce diverse solution generation.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14049" title="Abstract">arXiv:2402.14049</a> [<a href="/pdf/2402.14049" title="Download PDF">pdf</a>, <a href="/format/2402.14049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Models for Extreme Downscaling of Climate  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guiye Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+G">Guofeng Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Addressing the challenges of climate change requires accurate and
high-resolution mapping of climate and weather variables. However, many
existing climate datasets, such as the gridded outputs of the state-of-the-art
numerical climate models (e.g., general circulation models), are only available
at very coarse spatial resolutions due to the model complexity and extremely
high computational demand. Deep-learning-based methods, particularly generative
adversarial networks (GANs) and their variants, have proved effective for
refining natural images, and have shown great promise in improving scientific
datasets. In this paper, we describe a conditional GAN-based geospatial
downscaling method for extreme downscaling of gridded climate datasets.
Compared to most existing methods, the method can generate high-resolution
accurate climate datasets from very low-resolution inputs. More importantly,
the method explicitly considers the uncertainty inherent to the downscaling
process that tends to be ignored in existing methods. Given an input, the
method can produce a multitude of plausible high-resolution samples instead of
one single deterministic result. These samples allow for an empirical
exploration and inferences of model uncertainty and robustness. With a case
study of gridded climate datasets (wind velocity and solar irradiance), we
demonstrate the performances of the framework in downscaling tasks with very
high scaling factors (up to $64\times$) and highlight the advantages of the
framework with a comprehensive comparison with commonly used downscaling
methods, including area-to-point (ATP) kriging, deep image prior (DIP),
enhanced deep super-resolution network (EDSR), enhanced super-resolution
generative adversarial networks (ESRGAN), and physics-informed
resolution-enhancing GAN (PhIRE GAN).
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14052" title="Abstract">arXiv:2402.14052</a> [<a href="/pdf/2402.14052" title="Download PDF">pdf</a>, <a href="/format/2402.14052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Leveraging Encoder-only Pre-trained Language Models for Effective  Keyphrase Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+W+U">Wasi Uddin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-COLING 2024 camera ready. arXiv admin note: text overlap with <a href="/abs/2212.10233">arXiv:2212.10233</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study addresses the application of encoder-only Pre-trained Language
Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of
domain-tailored encoder-only models compared to encoder-decoder models. We
investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG,
(2) optimal architectural decisions for employing encoder-only PLMs in KPG, and
(3) a performance comparison between in-domain encoder-only and encoder-decoder
PLMs across varied resource settings. Our findings, derived from extensive
experimentation in two domains reveal that with encoder-only PLMs, although KPE
with Conditional Random Fields slightly excels in identifying present
keyphrases, the KPG formulation renders a broader spectrum of keyphrase
predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges
as a strong and data-efficient strategy for KPG, outperforming general-domain
seq2seq PLMs. We also identify a favorable parameter allocation towards model
depth rather than width when employing encoder-decoder architectures
initialized with encoder-only PLMs. The study sheds light on the potential of
utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork
for future KPG methods. Our code and pre-trained checkpoints are released at
https://github.com/uclanlp/DeepKPG.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14073" title="Abstract">arXiv:2402.14073</a> [<a href="/pdf/2402.14073" title="Download PDF">pdf</a>, <a href="/format/2402.14073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Language Understanding from Screenshots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tianyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+A">Adithya Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our model and code are available at <a href="https://github.com/princeton-nlp/PTP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An emerging family of language models (LMs), capable of processing both text
and images within a single visual view, has the promise to unlock complex tasks
such as chart understanding and UI navigation. We refer to these models as
screenshot language models. Despite their appeal, existing screenshot LMs
substantially lag behind text-only models on language understanding tasks. To
close this gap, we adopt a simplified setting where the model inputs are
plain-text-rendered screenshots, and we focus on improving the text ability of
screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective,
which masks and recovers both image patches of screenshots and text within
screenshots. We also conduct extensive ablation studies on masking rates and
patch sizes, as well as designs for improving training stability. Our
pre-trained model, while solely taking visual inputs, achieves comparable
performance with BERT on 6 out of 8 GLUE tasks (within 2%) and improves up to
8% over prior work. Additionally, we extend PTP to train autoregressive
screenshot LMs and demonstrate its effectiveness--our models can significantly
reduce perplexity by utilizing the screenshot context. Together, we hope our
findings can inspire future research on developing powerful screenshot LMs and
extending their reach to broader applications.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14080" title="Abstract">arXiv:2402.14080</a> [<a href="/pdf/2402.14080" title="Download PDF">pdf</a>, <a href="/format/2402.14080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Normalized Conformal Prediction and Uncertainty Quantification  for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nolte%2C+D">Daniel Nolte</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Souparno Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+R">Ranadip Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep learning models are being adopted and applied on various critical
decision-making tasks, yet they are trained to provide point predictions
without providing degrees of confidence. The trustworthiness of deep learning
models can be increased if paired with uncertainty estimations. Conformal
Prediction has emerged as a promising method to pair machine learning models
with prediction intervals, allowing for a view of the model's uncertainty.
However, popular uncertainty estimation methods for conformal prediction fail
to provide heteroskedastic intervals that are equally accurate for all samples.
In this paper, we propose a method to estimate the uncertainty of each sample
by calculating the variance obtained from a Deep Regression Forest. We show
that the deep regression forest variance improves the efficiency and coverage
of normalized inductive conformal prediction on a drug response prediction
task.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14081" title="Abstract">arXiv:2402.14081</a> [<a href="/pdf/2402.14081" title="Download PDF">pdf</a>, <a href="/format/2402.14081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning of Noisy Time Series Collections Using Stochastic  Process Models with Motion Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bajaj%2C+C">Chandrajit Bajaj</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">While time series classification and forecasting problems have been
extensively studied, the cases of noisy time series data with arbitrary time
sequence lengths have remained challenging. Each time series instance can be
thought of as a sample realization of a noisy dynamical model, which is
characterized by a continuous stochastic process. For many applications, the
data are mixed and consist of several types of noisy time series sequences
modeled by multiple stochastic processes, making the forecasting and
classification tasks even more challenging. Instead of regressing data naively
and individually to each time series type, we take a latent variable model
approach using a mixtured Gaussian processes with learned spectral kernels.
More specifically, we auto-assign each type of noisy time series data a
signature vector called its motion code. Then, conditioned on each assigned
motion code, we infer a sparse approximation of the corresponding time series
using the concept of the most informative timestamps. Our unmixing
classification approach involves maximizing the likelihood across all the mixed
noisy time series sequences of varying lengths. This stochastic approach allows
us to learn not only within a single type of noisy time series data but also
across many underlying stochastic processes, giving us a way to learn multiple
dynamical models in an integrated and robust manner. The different learned
latent stochastic models allow us to generate specific sub-type forecasting. We
provide several quantitative comparisons demonstrating the performance of our
approach.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14083" title="Abstract">arXiv:2402.14083</a> [<a href="/pdf/2402.14083" title="Download PDF">pdf</a>, <a href="/format/2402.14083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond A*: Better Planning with Transformers via Search Dynamics  Bootstrapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lehnert%2C+L">Lucas Lehnert</a>, 
<a href="/search/cs?searchtype=author&query=Sukhbaatar%2C+S">Sainbayar Sukhbaatar</a>, 
<a href="/search/cs?searchtype=author&query=Mcvay%2C+P">Paul Mcvay</a>, 
<a href="/search/cs?searchtype=author&query=Rabbat%2C+M">Michael Rabbat</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">While Transformers have enabled tremendous progress in various application
settings, such architectures still lag behind traditional symbolic planners for
solving complex decision making tasks. In this work, we demonstrate how to
train Transformers to solve complex planning tasks and present Searchformer, a
Transformer model that optimally solves previously unseen Sokoban puzzles 93.7%
of the time, while using up to 26.8% fewer search steps than standard $A^*$
search. Searchformer is an encoder-decoder Transformer model trained to predict
the search dynamics of $A^*$. This model is then fine-tuned via expert
iterations to perform fewer search steps than $A^*$ search while still
generating an optimal plan. In our training method, $A^*$'s search dynamics are
expressed as a token sequence outlining when task states are added and removed
into the search tree during symbolic planning. In our ablation studies on maze
navigation, we find that Searchformer significantly outperforms baselines that
predict the optimal plan directly with a 5-10$\times$ smaller model size and a
10$\times$ smaller training dataset. We also demonstrate how Searchformer
scales to larger and more complex decision making tasks like Sokoban with
improved percentage of solved tasks and shortened search dynamics.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14086" title="Abstract">arXiv:2402.14086</a> [<a href="/pdf/2402.14086" title="Download PDF">pdf</a>, <a href="/format/2402.14086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LexC-Gen: Generating Data for Extremely Low-Resource Languages with  Large Language Models and Bilingual Lexicons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yong%2C+Z">Zheng-Xin Yong</a>, 
<a href="/search/cs?searchtype=author&query=Menghini%2C+C">Cristina Menghini</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+S+H">Stephen H. Bach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data scarcity in low-resource languages can be addressed with word-to-word
translations from labeled task data in high-resource languages using bilingual
lexicons. However, bilingual lexicons often have limited lexical overlap with
task data, which results in poor translation coverage and lexicon utilization.
We propose lexicon-conditioned data generation (LexC-Gen), a method that
generates low-resource-language classification task data at scale.
Specifically, LexC-Gen first uses high-resource-language words from bilingual
lexicons to generate lexicon-compatible task data, and then it translates them
into low-resource languages with bilingual lexicons via word translation.
Across 17 extremely low-resource languages, LexC-Gen generated data is
competitive with expert-translated gold data, and yields on average 5.6 and 8.9
points improvement over existing lexicon-based word translation methods on
sentiment analysis and topic classification tasks respectively. We show that
conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen
is also practical -- it only needs a single GPU to generate data at scale. It
works well with open-access LLMs, and its cost is one-fifth of the cost of
GPT4-based multilingual data generation.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14090" title="Abstract">arXiv:2402.14090</a> [<a href="/pdf/2402.14090" title="Download PDF">pdf</a>, <a href="/format/2402.14090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Environment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Edwin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sadie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tonghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Gasztowtt%2C+H">Henry Gasztowtt</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Stephan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code at <a href="https://github.com/ezhang7423/social-environment-design">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; General Economics (econ.GN); Machine Learning (stat.ML)

</div>
<p class="mathjax">Artificial Intelligence (AI) holds promise as a technology that can be used
to improve government and economic policy-making. This paper proposes a new
research agenda towards this end by introducing Social Environment Design, a
general framework for the use of AI for automated policy-making that connects
with the Reinforcement Learning, EconCS, and Computational Social Choice
communities. The framework seeks to capture general economic environments,
includes voting on policy objectives, and gives a direction for the systematic
analysis of government and economic policy through AI simulation. We highlight
key open problems for future research in AI-based policy-making. By solving
these challenges, we hope to achieve various social welfare objectives, thereby
promoting more ethical and responsible decision making.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14095" title="Abstract">arXiv:2402.14095</a> [<a href="/pdf/2402.14095" title="Download PDF">pdf</a>, <a href="/format/2402.14095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot generalization across architectures for visual classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gerrtiz%2C+E">Evan Gerrtiz</a>, 
<a href="/search/cs?searchtype=author&query=Dyballa%2C+L">Luciano Dyballa</a>, 
<a href="/search/cs?searchtype=author&query=Zucker%2C+S+W">Steven W. Zucker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Tiny Paper at ICLR 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generalization to unseen data is a key desideratum for deep networks, but its
relation to classification accuracy is unclear. Using a minimalist vision
dataset and a measure of generalizability, we show that popular networks, from
deep convolutional networks (CNNs) to transformers, vary in their power to
extrapolate to unseen classes both across layers and across architectures.
Accuracy is not a good predictor of generalizability, and generalization varies
non-monotonically with layer depth. Code is available at
https://github.com/dyballa/zero-shot-generalization.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14096" title="Abstract">arXiv:2402.14096</a> [<a href="/pdf/2402.14096" title="Download PDF">pdf</a>, <a href="/format/2402.14096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeTrans: Merging Human and Machine Attention for Neural Code  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Karas%2C+Z">Zachary Karas</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+A">Aakash Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Collin McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Leach%2C+K">Kevin Leach</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Neural code summarization leverages deep learning models to automatically
generate brief natural language summaries of code snippets. The development of
Transformer models has led to extensive use of attention during model design.
While existing work has primarily and almost exclusively focused on static
properties of source code and related structural representations like the
Abstract Syntax Tree (AST), few studies have considered human attention, that
is, where programmers focus while examining and comprehending code. In this
paper, we develop a method for incorporating human attention into machine
attention to enhance neural code summarization. To facilitate this
incorporation and vindicate this hypothesis, we introduce EyeTrans, which
consists of three steps: (1) we conduct an extensive eye-tracking human study
to collect and pre-analyze data for model training, (2) we devise a
data-centric approach to integrate human attention with machine attention in
the Transformer architecture, and (3) we conduct comprehensive experiments on
two code summarization tasks to demonstrate the effectiveness of incorporating
human attention into Transformers. Integrating human attention leads to an
improvement of up to 29.91% in Functional Summarization and up to 6.39% in
General Code Summarization performance, demonstrating the substantial benefits
of this combination. We further explore performance in terms of robustness and
efficiency by creating challenging summarization scenarios in which EyeTrans
exhibits interesting properties. We also visualize the attention map to depict
the simplifying effect of machine attention in the Transformer by incorporating
human attention. This work has the potential to propel AI research in software
engineering by introducing more human-centered approaches and data.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14098" title="Abstract">arXiv:2402.14098</a> [<a href="/pdf/2402.14098" title="Download PDF">pdf</a>, <a href="/format/2402.14098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intriguing Properties of Modern GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friedman%2C+R">Roy Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+Y">Yair Weiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Modern GANs achieve remarkable performance in terms of generating realistic
and diverse samples. This has led many to believe that ``GANs capture the
training data manifold''. In this work we show that this interpretation is
wrong. We empirically show that the manifold learned by modern GANs does not
fit the training distribution: specifically the manifold does not pass through
the training examples and passes closer to out-of-distribution images than to
in-distribution images. We also investigate the distribution over images
implied by the prior over the latent codes and study whether modern GANs learn
a density that approximates the training distribution. Surprisingly, we find
that the learned density is very far from the data distribution and that GANs
tend to assign higher density to out-of-distribution images. Finally, we
demonstrate that the set of images used to train modern GANs are often not part
of the typical set described by the GANs' distribution.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14101" title="Abstract">arXiv:2402.14101</a> [<a href="/pdf/2402.14101" title="Download PDF">pdf</a>, <a href="/format/2402.14101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot  Annotator Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golazizian%2C+P">Preni Golazizian</a>, 
<a href="/search/cs?searchtype=author&query=Omrani%2C+A">Ali Omrani</a>, 
<a href="/search/cs?searchtype=author&query=Ziabari%2C+A+S">Alireza S. Ziabari</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Morteza Dehghani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In subjective NLP tasks, where a single ground truth does not exist, the
inclusion of diverse annotators becomes crucial as their unique perspectives
significantly influence the annotations. In realistic scenarios, the annotation
budget often becomes the main determinant of the number of perspectives (i.e.,
annotators) included in the data and subsequent modeling. We introduce a novel
framework for annotation collection and modeling in subjective tasks that aims
to minimize the annotation budget while maximizing the predictive performance
for each annotator. Our framework has a two-stage design: first, we rely on a
small set of annotators to build a multitask model, and second, we augment the
model for a new perspective by strategically annotating a few samples per
annotator. To test our framework at scale, we introduce and release a unique
dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by
24 annotators for moral sentiment. We demonstrate that our framework surpasses
the previous SOTA in capturing the annotators' individual perspectives with as
little as 25% of the original annotation budget on two datasets. Furthermore,
our framework results in more equitable models, reducing the performance
disparity among annotators.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14103" title="Abstract">arXiv:2402.14103</a> [<a href="/pdf/2402.14103" title="Download PDF">pdf</a>, <a href="/ps/2402.14103" title="Download PostScript">ps</a>, <a href="/format/2402.14103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational-Statistical Gaps for Improper Learning in Sparse Linear  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buhai%2C+R">Rares-Darius Buhai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingqiu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tiegel%2C+S">Stefan Tiegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study computational-statistical gaps for improper learning in sparse
linear regression. More specifically, given $n$ samples from a $k$-sparse
linear model in dimension $d$, we ask what is the minimum sample complexity to
efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense
estimate for the regression vector that achieves non-trivial prediction error
on the $n$ samples. Information-theoretically this can be achieved using
$\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature,
there is no polynomial-time algorithm known to achieve the same guarantees
using less than $\Theta(d)$ samples without additional restrictions on the
model. Similarly, existing hardness results are either restricted to the proper
setting, in which the estimate must be sparse as well, or only apply to
specific algorithms.
<br />We give evidence that efficient algorithms for this task require at least
(roughly) $\Omega(k^2)$ samples. In particular, we show that an improper
learning algorithm for sparse linear regression can be used to solve sparse PCA
problems (with a negative spike) in their Wishart form, in regimes in which
efficient algorithms are widely believed to require at least $\Omega(k^2)$
samples. We complement our reduction with low-degree and statistical query
lower bounds for the sparse PCA problems from which we reduce.
<br />Our hardness results apply to the (correlated) random design setting in which
the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with
unknown covariance.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14105" title="Abstract">arXiv:2402.14105</a> [<a href="/pdf/2402.14105" title="Download PDF">pdf</a>, <a href="/format/2402.14105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Definitions and Performance Comparison of Consistency Models for  Parallel File Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mohror%2C+K">Kathryn Mohror</a>, 
<a href="/search/cs?searchtype=author&query=Snir%2C+M">Marc Snir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. Submitted to IEEE TPDS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">The semantics of HPC storage systems are defined by the consistency models to
which they abide. Storage consistency models have been less studied than their
counterparts in memory systems, with the exception of the POSIX standard and
its strict consistency model. The use of POSIX consistency imposes a
performance penalty that becomes more significant as the scale of parallel file
systems increases and the access time to storage devices, such as node-local
solid storage devices, decreases. While some efforts have been made to adopt
relaxed storage consistency models, these models are often defined informally
and ambiguously as by-products of a particular implementation. In this work, we
establish a connection between memory consistency models and storage
consistency models and revisit the key design choices of storage consistency
models from a high-level perspective. Further, we propose a formal and unified
framework for defining storage consistency models and a layered implementation
that can be used to easily evaluate their relative performance for different
I/O workloads. Finally, we conduct a comprehensive performance comparison of
two relaxed consistency models on a range of commonly-seen parallel I/O
workloads, such as checkpoint/restart of scientific applications and random
reads of deep learning applications. We demonstrate that for certain I/O
scenarios, a weaker consistency model can significantly improve the I/O
performance. For instance, in small random reads that typically found in deep
learning applications, session consistency achieved an 5x improvement in I/O
bandwidth compared to commit consistency, even at small scales.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14110" title="Abstract">arXiv:2402.14110</a> [<a href="/pdf/2402.14110" title="Download PDF">pdf</a>, <a href="/format/2402.14110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving Towards Stability and Efficiency: A Variable Time Gap Strategy  for Adaptive Cruise Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=El-Baklish%2C+S+K">Shaimaa K. El-Baklish</a>, 
<a href="/search/eess?searchtype=author&query=Kouvelas%2C+A">Anastasios Kouvelas</a>, 
<a href="/search/eess?searchtype=author&query=Makridis%2C+M+A">Michail A. Makridis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Automated vehicle technologies offer a promising avenue for enhancing traffic
efficiency, safety, and energy consumption. Among these, Adaptive Cruise
Control (ACC) systems stand out as a prevalent form of automation on today's
roads, with their time gap settings holding paramount importance. While
decreasing the average time headway tends to enhance traffic capacity, it
simultaneously raises concerns regarding safety and string stability. This
study introduces a novel variable time gap feedback control policy aimed at
striking a balance between maintaining a minimum time gap setting under
equilibrium car-following conditions, thereby improving traffic capacity, while
ensuring string stability to mitigate disturbances away from the equilibrium
flow. Leveraging nonlinear $H_\infty$ control technique, the strategy employs a
variable time gap component as the manipulated control signal, complemented by
a constant time gap component that predominates during car-following
equilibrium. The effectiveness of the proposed scheme is evaluated against its
constant time-gap counterpart calibrated using field platoon data from the
OpenACC dataset. Through numerical and traffic simulations, our findings
illustrate that the proposed algorithm effectively dampens perturbations within
vehicle platoons, leading to a more efficient and safer mixed traffic flow.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14111" title="Abstract">arXiv:2402.14111</a> [<a href="/pdf/2402.14111" title="Download PDF">pdf</a>, <a href="/ps/2402.14111" title="Download PostScript">ps</a>, <a href="/format/2402.14111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Crowdfunding Futures: Analyzing Campaign Outcomes through  Distributed Models and Big Data Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pipit%C3%B2%2C+G">Giuseppe Pipit&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Macca%2C+E">Emanuele Macca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Crowdfunding has emerged as a widespread strategy for startups seeking
financing, particularly through reward-based methods. However, understanding
its economic impact at both micro and macro levels requires thorough analysis,
often involving advanced studies on past campaigns to extract insights that
aiding companies in optimizing their crowdfunding project types and launch
methodologies. Such analyses are often beyond the scope of basic data analysis
techniques and frequently demand advanced machine learning tools, such as
distributed computing, due to the large volume of data involved. This study
aims to investigate and analyse the targets of reward-based crowdfunding
campaigns through machine learning techniques, employing distributed models and
structures. By harnessing the power of distributed computing, it unravels
intricate patterns and trends within crowdfunding data, thereby empowering
companies to refine their strategies and enhance the efficacy of their funding
endeavors. Through this multifaceted approach, a deeper understanding of the
economic dynamics underlying crowdfunding ecosystems can be attained, fostering
informed decision-making and sustainable growth within the startup landscape.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14114" title="Abstract">arXiv:2402.14114</a> [<a href="/pdf/2402.14114" title="Download PDF">pdf</a>, <a href="/format/2402.14114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-organ Self-supervised Contrastive Learning for Breast Lesion  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Figueiras%2C+H">Hugo Figueiras</a>, 
<a href="/search/cs?searchtype=author&query=Aidos%2C+H">Helena Aidos</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+N+C">Nuno Cruz Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning has proven to be an effective way to learn
representations in domains where annotated labels are scarce, such as medical
imaging. A widely adopted framework for this purpose is contrastive learning
and it has been applied to different scenarios. This paper seeks to advance our
understanding of the contrastive learning framework by exploring a novel
perspective: employing multi-organ datasets for pre-training models tailored to
specific organ-related target tasks. More specifically, our target task is
breast tumour segmentation in ultrasound images. The pre-training datasets
include ultrasound images from other organs, such as the lungs and heart, and
large datasets of natural images. Our results show that conventional
contrastive learning pre-training improves performance compared to supervised
baseline approaches. Furthermore, our pre-trained models achieve comparable
performance when fine-tuned with only half of the available labelled data. Our
findings also show the advantages of pre-training on diverse organ data for
improving performance in the downstream task.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14116" title="Abstract">arXiv:2402.14116</a> [<a href="/pdf/2402.14116" title="Download PDF">pdf</a>, <a href="/format/2402.14116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FanOutQA: Multi-Hop, Multi-Document Question Answering for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Andrew Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+A">Alyssa Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Dugan%2C+L">Liam Dugan</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures. In review at ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">One type of question that is commonly found in day-to-day scenarios is
``fan-out'' questions, complex multi-hop, multi-document reasoning questions
that require finding information about a large number of entities. However,
there exist few resources to evaluate this type of question-answering
capability among large language models. To evaluate complex reasoning in LLMs
more fully, we present FanOutQA, a high-quality dataset of fan-out
question-answer pairs and human-annotated decompositions with English Wikipedia
as the knowledge base. We formulate three benchmark settings across our dataset
and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,
finding that contemporary models still have room to improve reasoning over
inter-document dependencies in a long context. We provide our dataset and
open-source tools to run models to encourage evaluation at https://fanoutqa.com
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14118" title="Abstract">arXiv:2402.14118</a> [<a href="/pdf/2402.14118" title="Download PDF">pdf</a>, <a href="/format/2402.14118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Matrix Multiplication for Emergent Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wheatman%2C+B">Brian Wheatman</a>, 
<a href="/search/cs?searchtype=author&query=Madhyastha%2C+M">Meghana Madhyastha</a>, 
<a href="/search/cs?searchtype=author&query=Burns%2C+R">Randal Burns</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial intelligence workloads, especially transformer models, exhibit
emergent sparsity in which computations perform selective sparse access to
dense data. The workloads are inefficient on hardware designed for dense
computations and do not map well onto sparse data representations. We build a
vectorized and parallel matrix-multiplication system A X B = C that eliminates
unnecessary computations and avoids branches based on a runtime evaluation of
sparsity. We use a combination of dynamic code lookup to adapt to the specific
sparsity encoded in the B matrix and preprocessing of sparsity maps of the A
and B matrices to compute conditional branches once for the whole computation.
For a wide range of sparsity, from 60% to 95% zeros, our implementation
performs fewer instructions and increases performance when compared with Intel
MKL's dense or sparse matrix multiply routines. Benefits can be as large as 2
times speedup and 4 times fewer instructions.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14120" title="Abstract">arXiv:2402.14120</a> [<a href="/pdf/2402.14120" title="Download PDF">pdf</a>, <a href="/ps/2402.14120" title="Download PostScript">ps</a>, <a href="/format/2402.14120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Wait-Free Linearizable Implementations of Approximate Bounded  Counters Using Read-Write Registers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnen%2C+C">Colette Johnen</a>, 
<a href="/search/cs?searchtype=author&query=Khattabi%2C+A">Adnane Khattabi</a>, 
<a href="/search/cs?searchtype=author&query=Milani%2C+A">Alessia Milani</a>, 
<a href="/search/cs?searchtype=author&query=Welch%2C+J+L">Jennifer L. Welch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, to be published in SIROCCO 2024 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Relaxing the sequential specification of a shared object is a way to obtain
an implementation with better performance compared to implementing the original
specification. We apply this approach to the Counter object, under the
assumption that the number of times the Counter is incremented in any execution
is at most a known bound $m$. We consider the $k$-multiplicative-accurate
Counter object, where each read operation returns an approximate value that is
within a multiplicative factor $k$ of the accurate value. More specifically, a
read is allowed to return an approximate value $x$ of the number $v$ of
increments previously applied to the counter such that $v/k \le x \le vk$. We
present three algorithms to implement this object in a wait-free linearizable
manner in the shared memory model using read-write registers. All the
algorithms have read operations whose worst-case step complexity improves
exponentially on that for an exact $m$-bounded counter (which in turn improves
exponentially on that for an exact unbounded counter). Two of the algorithms
have read step complexity that is asymptotically optimal. The algorithms differ
in their requirements on $k$, step complexity of the increment operation, and
space complexity.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14123" title="Abstract">arXiv:2402.14123</a> [<a href="/pdf/2402.14123" title="Download PDF">pdf</a>, <a href="/format/2402.14123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeiSAM: Segment Anything with Deictic Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shindo%2C+H">Hikaru Shindo</a>, 
<a href="/search/cs?searchtype=author&query=Brack%2C+M">Manuel Brack</a>, 
<a href="/search/cs?searchtype=author&query=Sudhakaran%2C+G">Gopika Sudhakaran</a>, 
<a href="/search/cs?searchtype=author&query=Dhami%2C+D+S">Devendra Singh Dhami</a>, 
<a href="/search/cs?searchtype=author&query=Schramowski%2C+P">Patrick Schramowski</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large-scale, pre-trained neural networks have demonstrated strong
capabilities in various tasks, including zero-shot image segmentation. To
identify concrete objects in complex scenes, humans instinctively rely on
deictic descriptions in natural language, i.e., referring to something
depending on the context such as "The object that is on the desk and behind the
cup.". However, deep learning approaches cannot reliably interpret such deictic
representations due to their lack of reasoning capabilities in complex
scenarios. To remedy this issue, we propose DeiSAM -- a combination of large
pre-trained neural networks with differentiable logic reasoners -- for deictic
promptable segmentation. Given a complex, textual segmentation description,
DeiSAM leverages Large Language Models (LLMs) to generate first-order logic
rules and performs differentiable forward reasoning on generated scene graphs.
Subsequently, DeiSAM segments objects by matching them to the logically
inferred image regions. As part of our evaluation, we propose the Deictic
Visual Genome (DeiVG) dataset, containing paired visual input and complex,
deictic textual prompts. Our empirical results demonstrate that DeiSAM is a
substantial improvement over purely data-driven baselines for deictic
promptable segmentation.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14124" title="Abstract">arXiv:2402.14124</a> [<a href="/pdf/2402.14124" title="Download PDF">pdf</a>, <a href="/format/2402.14124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fake Resume Attacks: Data Poisoning on Online Job Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+M">Michiharu Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Thanh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at The Web Conference 2024 (WWW'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">While recent studies have exposed various vulnerabilities incurred from data
poisoning attacks in many web services, little is known about the vulnerability
on online professional job platforms (e.g., LinkedIn and Indeed). In this work,
first time, we demonstrate the critical vulnerabilities found in the common
Human Resources (HR) task of matching job seekers and companies on online job
platforms. Capitalizing on the unrestricted format and contents of job seekers'
resumes and easy creation of accounts on job platforms, we demonstrate three
attack scenarios: (1) company promotion attack to increase the likelihood of
target companies being recommended, (2) company demotion attack to decrease the
likelihood of target companies being recommended, and (3) user promotion attack
to increase the likelihood of certain users being matched to certain companies.
To this end, we develop an end-to-end "fake resume" generation framework,
titled FRANCIS, that induces systematic prediction errors via data poisoning.
Our empirical evaluation on real-world datasets reveals that data poisoning
attacks can markedly skew the results of matchmaking between job seekers and
companies, regardless of underlying models, with vulnerability amplified in
proportion to poisoning intensity. These findings suggest that the outputs of
various services from job platforms can be potentially hacked by malicious
users.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14128" title="Abstract">arXiv:2402.14128</a> [<a href="/pdf/2402.14128" title="Download PDF">pdf</a>, <a href="/ps/2402.14128" title="Download PostScript">ps</a>, <a href="/format/2402.14128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An expert system for diagnosing and treating heart disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandino%2C+B">Blake Fernandino</a>, 
<a href="/search/cs?searchtype=author&query=Bisheh%2C+M+S">Moein Samak Bisheh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Timely detection of illnesses is vital to prevent severe infections and
ensure effective treatment, as it's always better to prevent diseases than to
cure them. Sadly, many patients remain undiagnosed until their conditions
worsen, resulting in high death rates. Expert systems offer a solution by
automating early-stage diagnoses using a fuzzy rule-based approach. Our study
gathered data from various sources, including hospitals, to develop an expert
system aimed at identifying early signs of diseases, particularly heart
conditions. The diagnostic process involves collecting and processing test
results using the expert system, which categorizes disease risks and aids
physicians in treatment decisions. By incorporating expert systems into
clinical practice, we can improve the accuracy of disease detection and address
challenges in patient management, particularly in areas with limited medical
resources.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14129" title="Abstract">arXiv:2402.14129</a> [<a href="/pdf/2402.14129" title="Download PDF">pdf</a>, <a href="/format/2402.14129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Language and Graph Models for Semi-structured Information  Extraction on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chard%2C+K">Kyle Chard</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Relation extraction is an efficient way of mining the extraordinary wealth of
human knowledge on the Web. Existing methods rely on domain-specific training
data or produce noisy outputs. We focus here on extracting targeted relations
from semi-structured web pages given only a short description of the relation.
We present GraphScholarBERT, an open-domain information extraction method based
on a joint graph and language model structure. GraphScholarBERT can generalize
to previously unseen domains without additional data or training and produces
only clean extraction results matched to the search keyword. Experiments show
that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\%
compared to previous work in a zero-shot domain and zero-shot website setting.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14136" title="Abstract">arXiv:2402.14136</a> [<a href="/pdf/2402.14136" title="Download PDF">pdf</a>, <a href="/format/2402.14136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal  Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H+L">Ho Lyun Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Samplawski%2C+C">Colin Samplawski</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jason Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shiwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+L+M">Lance M. Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+D">Deepak Ganesan</a>, 
<a href="/search/cs?searchtype=author&query=Marlin%2C+B">Benjamin Marlin</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+M">Mani Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Constantly locating moving objects, i.e., geospatial tracking, is essential
for autonomous building infrastructure. Accurate and robust geospatial tracking
often leverages multimodal sensor fusion algorithms, which require large
datasets with time-aligned, synchronized data from various sensor types.
However, such datasets are not readily available. Hence, we propose GDTM, a
nine-hour dataset for multimodal object tracking with distributed multimodal
sensors and reconfigurable sensor node placements. Our dataset enables the
exploration of several research problems, such as optimizing architectures for
processing multimodal data, and investigating models' robustness to adverse
sensing conditions and sensor placement variances. A GitHub repository
containing the code, sample data, and checkpoints of this work is available at
https://github.com/nesl/GDTM.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14139" title="Abstract">arXiv:2402.14139</a> [<a href="/pdf/2402.14139" title="Download PDF">pdf</a>, <a href="/format/2402.14139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saikumar%2C+D">Dhananjay Saikumar</a>, 
<a href="/search/cs?searchtype=author&query=Varghese%2C+B">Blesson Varghese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EuroSys 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Efficient on-device convolutional neural network (CNN) training in
resource-constrained mobile and edge environments is an open challenge.
Backpropagation is the standard approach adopted, but it is GPU memory
intensive due to its strong inter-layer dependencies that demand intermediate
activations across the entire CNN model to be retained in GPU memory. This
necessitates smaller batch sizes to make training possible within the available
GPU memory budget, but in turn, results in a substantially high and impractical
training time. We introduce NeuroFlux, a novel CNN training system tailored for
memory-constrained scenarios. We develop two novel opportunities: firstly,
adaptive auxiliary networks that employ a variable number of filters to reduce
GPU memory usage, and secondly, block-specific adaptive batch sizes, which not
only cater to the GPU memory constraints but also accelerate the training
process. NeuroFlux segments the CNNs into blocks based on GPU memory usage and
further attaches an auxiliary network to each layer in these blocks. This
disrupts the typical layer dependencies under a new training paradigm -
'adaptive local learning'. Moreover, NeuroFlux adeptly caches intermediate
activations, eliminating redundant forward passes over previously trained
blocks, further accelerating the training process. The results are twofold when
compared to Backpropagation: on various hardware platforms, NeuroFlux
demonstrates training speed-ups of 2.3$\times$ to 6.1$\times$ under stringent
GPU memory budgets, and NeuroFlux generates streamlined models that have
10.9$\times$ to 29.4$\times$ fewer parameters without sacrificing accuracy.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14140" title="Abstract">arXiv:2402.14140</a> [<a href="/pdf/2402.14140" title="Download PDF">pdf</a>, <a href="/format/2402.14140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuantTM: Business-Centric Threat Quantification for Risk Management and  Cyber Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+der+Assen%2C+J">Jan von der Assen</a>, 
<a href="/search/cs?searchtype=author&query=Franco%2C+M+F">Muriel F. Franco</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Muyao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Threat modeling has emerged as a key process for understanding relevant
threats within businesses. However, understanding the importance of threat
events is rarely driven by the business incorporating the system. Furthermore,
prioritization of threat events often occurs based on abstract and qualitative
scoring. While such scores enable prioritization, they do not allow the results
to be easily interpreted by decision-makers. This can hinder downstream
activities, such as discussing security investments and a security control's
economic applicability. This article introduces QuantTM, an approach that
incorporates views from operational and strategic business representatives to
collect threat information during the threat modeling process to measure
potential financial loss incurred by a specific threat event. It empowers the
analysis of threats' impacts and the applicability of security controls, thus
supporting the threat analysis and prioritization from an economic perspective.
QuantTM comprises an overarching process for data collection and aggregation
and a method for business impact analysis. The performance and feasibility of
the QuantTM approach are demonstrated in a real-world case study conducted in a
Swiss SME to analyze the impacts of threats and economic benefits of security
controls. Secondly, it is shown that employing business impact analysis is
feasible and that the supporting prototype exhibits great usability.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14143" title="Abstract">arXiv:2402.14143</a> [<a href="/pdf/2402.14143" title="Download PDF">pdf</a>, <a href="/format/2402.14143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SecurePose: Automated Face Blurring and Human Movement Kinematics  Extraction from Videos Recorded in Clinical Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bajpai%2C+R">Rishabh Bajpai</a>, 
<a href="/search/cs?searchtype=author&query=Aravamuthan%2C+B">Bhooma Aravamuthan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Movement disorders are typically diagnosed by consensus-based expert
evaluation of clinically acquired patient videos. However, such broad sharing
of patient videos poses risks to patient privacy. Face blurring can be used to
de-identify videos, but this process is often manual and time-consuming.
Available automated face blurring techniques are subject to either excessive,
inconsistent, or insufficient facial blurring - all of which can be disastrous
for video assessment and patient privacy. Furthermore, assessing movement
disorders in these videos is often subjective. The extraction of quantifiable
kinematic features can help inform movement disorder assessment in these
videos, but existing methods to do this are prone to errors if using
pre-blurred videos. We have developed an open-source software called SecurePose
that can both achieve reliable face blurring and automated kinematic extraction
in patient videos recorded in a clinic setting using an iPad. SecurePose,
extracts kinematics using a pose estimation method (OpenPose), tracks and
uniquely identifies all individuals in the video, identifies the patient, and
performs face blurring. The software was validated on gait videos recorded in
outpatient clinic visits of 116 children with cerebral palsy. The validation
involved assessing intermediate steps of kinematics extraction and face
blurring with manual blurring (ground truth). Moreover, when SecurePose was
compared with six selected existing methods, it outperformed other methods in
automated face detection and achieved ceiling accuracy in 91.08% less time than
a robust manual face blurring method. Furthermore, ten experienced researchers
found SecurePose easy to learn and use, as evidenced by the System Usability
Scale. The results of this work validated the performance and usability of
SecurePose on clinically recorded gait videos for face blurring and kinematics
extraction.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14144" title="Abstract">arXiv:2402.14144</a> [<a href="/pdf/2402.14144" title="Download PDF">pdf</a>, <a href="/format/2402.14144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending identifiability results from isolated networks to embedded  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mapurunga%2C+E">Eduardo Mapurunga</a>, 
<a href="/search/eess?searchtype=author&query=Gevers%2C+M">Michel Gevers</a>, 
<a href="/search/eess?searchtype=author&query=Bazanella%2C+A+S">Alexandre S. Bazanella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 63rd IEEE Conference on Decision and Control (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper deals with the design of Excitation and Measurement Patterns
(EMPs) for the identification of dynamical networks, when the objective is to
identify only a subnetwork embedded in a larger network. Recent results have
shown how to construct EMPs that guarantee identifiability for a range of
networks with specific graph topologies, such as trees, loops, or Directed
Acyclic Graphs (DAGs). However, an EMP that is valid for the identification of
a subnetwork taken in isolation may no longer be valid when that subnetwork is
embedded in a larger network. Our main contribution is to exhibit conditions
under which it does remain valid, and to propose ways to enhance such EMP when
these conditions are not satisfied.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14146" title="Abstract">arXiv:2402.14146</a> [<a href="/pdf/2402.14146" title="Download PDF">pdf</a>, <a href="/format/2402.14146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Dynamic Multi-Reward Weighting for  Multi-Style Controllable Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Langis%2C+K">Karin de Langis</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+R">Ryan Koo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Style is an integral component of text that expresses a diverse set of
information, including interpersonal dynamics (e.g. formality) and the author's
emotions or attitudes (e.g. disgust). Humans often employ multiple styles
simultaneously. An open question is how large language models can be explicitly
controlled so that they weave together target styles when generating text: for
example, to produce text that is both negative and non-toxic. Previous work
investigates the controlled generation of a single style, or else controlled
generation of a style and other attributes. In this paper, we expand this into
controlling multiple styles simultaneously. Specifically, we investigate
various formulations of multiple style rewards for a reinforcement learning
(RL) approach to controlled multi-style generation. These reward formulations
include calibrated outputs from discriminators and dynamic weighting by
discriminator gradient magnitudes. We find that dynamic weighting generally
outperforms static weighting approaches, and we explore its effectiveness in 2-
and 3-style control, even compared to strong baselines like plug-and-play
model. All code and data for RL pipelines with multiple style attributes will
be publicly available.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14147" title="Abstract">arXiv:2402.14147</a> [<a href="/pdf/2402.14147" title="Download PDF">pdf</a>, <a href="/format/2402.14147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuo%2C+T">Tzu-Sheng Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Halfaker%2C+A">Aaron Halfaker</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Meng-Hsin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Holstein%2C+K">Kenneth Holstein</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haiyi Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2024 CHI Conference on Human Factors in
  Computing Systems (CHI '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">AI tools are increasingly deployed in community contexts. However, datasets
used to evaluate AI are typically created by developers and annotators outside
a given community, which can yield misleading conclusions about AI performance.
How might we empower communities to drive the intentional design and curation
of evaluation datasets for AI that impacts them? We investigate this question
on Wikipedia, an online community with multiple AI-based content moderation
tools deployed. We introduce Wikibench, a system that enables communities to
collaboratively curate AI evaluation datasets, while navigating ambiguities and
differences in perspective through discussion. A field study on Wikipedia shows
that datasets curated using Wikibench can effectively capture community
consensus, disagreement, and uncertainty. Furthermore, study participants used
Wikibench to shape the overall data curation process, including refining label
definitions, determining data inclusion criteria, and authoring data
statements. Based on our findings, we propose future directions for systems
that support community-driven data curation.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14151" title="Abstract">arXiv:2402.14151</a> [<a href="/pdf/2402.14151" title="Download PDF">pdf</a>, <a href="/format/2402.14151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIRCO: A Benchmark of Information Retrieval Tasks with Complex  Objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Weili Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Ramamohan Paturi</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+L">Leon Bergen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present the Benchmark of Information Retrieval (IR) tasks with Complex
Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve
documents given multi-faceted user objectives. The benchmark's complexity and
compact size make it suitable for evaluating large language model (LLM)-based
information retrieval systems. We present a modular framework for investigating
factors that may influence LLM performance on retrieval tasks, and identify a
simple baseline model which matches or outperforms existing approaches and more
complex alternatives. No approach achieves satisfactory performance on all
benchmark tasks, suggesting that stronger models and new retrieval protocols
are necessary to address complex user needs.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14152" title="Abstract">arXiv:2402.14152</a> [<a href="/pdf/2402.14152" title="Download PDF">pdf</a>, <a href="/format/2402.14152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ModSRAM: Algorithm-Hardware Co-Design for Large Number Modular  Multiplication in SRAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%2C+J">Jonathan Ku</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Haoxuan Shan</a>, 
<a href="/search/cs?searchtype=author&query=Samudrala%2C+S">Saichand Samudrala</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiawen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziru Li</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">JV Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DAC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Elliptic curve cryptography (ECC) is widely used in security applications
such as public key cryptography (PKC) and zero-knowledge proofs (ZKP). ECC is
composed of modular arithmetic, where modular multiplication takes most of the
processing time. Computational complexity and memory constraints of ECC limit
the performance. Therefore, hardware acceleration on ECC is an active field of
research. Processing-in-memory (PIM) is a promising approach to tackle this
problem. In this work, we design ModSRAM, the first 8T SRAM PIM architecture to
compute large-number modular multiplication efficiently. In addition, we
propose R4CSA-LUT, a new algorithm that reduces the cycles for an interleaved
algorithm and eliminates carry propagation for addition based on look-up tables
(LUT). ModSRAM is co-designed with R4CSA-LUT to support modular multiplication
and data reuse in memory with 52% cycle reduction compared to prior works with
only 32% area overhead.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14154" title="Abstract">arXiv:2402.14154</a> [<a href="/pdf/2402.14154" title="Download PDF">pdf</a>, <a href="/format/2402.14154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-Soc: Benchmarking Multimodal Large Language Models in Social Media  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minje Choi</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gaurav Verma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
<p class="mathjax">Social media platforms are hubs for multimodal information exchange,
encompassing text, images, and videos, making it challenging for machines to
comprehend the information or emotions associated with interactions in online
spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising
solution to address these challenges, yet struggle with accurately interpreting
human emotions and complex contents like misinformation. This paper introduces
MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of
multimodal social media content. MM-Soc compiles prominent multimodal datasets
and incorporates a novel large-scale YouTube tagging dataset, targeting a range
of tasks from misinformation detection, hate speech detection, and social
context generation. Through our exhaustive evaluation on ten size-variants of
four open-source MLLMs, we have identified significant performance disparities,
highlighting the need for advancements in models' social understanding
capabilities. Our analysis reveals that, in a zero-shot setting, various types
of MLLMs generally exhibit difficulties in handling social media tasks.
However, MLLMs demonstrate performance improvements post fine-tuning,
suggesting potential pathways for improvement.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14155" title="Abstract">arXiv:2402.14155</a> [<a href="/pdf/2402.14155" title="Download PDF">pdf</a>, <a href="/format/2402.14155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for  Intent Recognition?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mannekote%2C+A">Amogh Mannekote</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Boyer%2C+K+E">Kristy Elizabeth Boyer</a>, 
<a href="/search/cs?searchtype=author&query=Dorr%2C+B+J">Bonnie J. Dorr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Task-oriented dialogue systems are expected to handle a constantly expanding
set of intents and domains even after they have been deployed to support more
and more functionalities. To live up to this expectation, it becomes critical
to mitigate the catastrophic forgetting problem (CF) that occurs in continual
learning (CL) settings for a task such as intent recognition. While existing
dialogue systems research has explored replay-based and regularization-based
methods to this end, the effect of domain ordering on the CL performance of
intent recognition models remains unexplored. If understood well, domain
ordering has the potential to be an orthogonal technique that can be leveraged
alongside existing techniques such as experience replay. Our work fills this
gap by comparing the impact of three domain-ordering strategies (min-sum path,
max-sum path, random) on the CL performance of a generative intent recognition
model. Our findings reveal that the min-sum path strategy outperforms the
others in reducing catastrophic forgetting when training on the 220M T5-Base
model. However, this advantage diminishes with the larger 770M T5-Large model.
These results underscores the potential of domain ordering as a complementary
strategy for mitigating catastrophic forgetting in continually learning intent
recognition models, particularly in resource-constrained scenarios.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14158" title="Abstract">arXiv:2402.14158</a> [<a href="/pdf/2402.14158" title="Download PDF">pdf</a>, <a href="/format/2402.14158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOOLVERIFIER: Generalization to New Tools via Self-Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mekala%2C+D">Dheeraj Mekala</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Lanchantin%2C+J">Jack Lanchantin</a>, 
<a href="/search/cs?searchtype=author&query=Raileanu%2C+R">Roberta Raileanu</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi-Yu%2C+J">Jane Dwivedi-Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Teaching language models to use tools is an important milestone towards
building general assistants, but remains an open problem. While there has been
significant progress on learning to use specific tools via fine-tuning,
language models still struggle with learning how to robustly use new tools from
only a few demonstrations. In this work we introduce a self-verification method
which distinguishes between close candidates by self-asking contrastive
questions during (1) tool selection; and (2) parameter generation. We construct
synthetic, high-quality, self-generated data for this goal using Llama-2 70B,
which we intend to release publicly. Extensive experiments on 4 tasks from the
ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average
improvement of 22% over few-shot baselines, even in scenarios where the
distinctions between candidate tools are finely nuanced.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14159" title="Abstract">arXiv:2402.14159</a> [<a href="/pdf/2402.14159" title="Download PDF">pdf</a>, <a href="/format/2402.14159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping the Landscape of Independent Food Delivery Platforms in the  United States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liaqat%2C+A">Amna Liaqat</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+O+X">Owen Xingjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Espinosa%2C+M+C+F">Mariana Consuelo Fern&#xe1;ndez Espinosa</a>, 
<a href="/search/cs?searchtype=author&query=Manjunatha%2C+A">Ankhitha Manjunatha</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">Alexander Yang</a>, 
<a href="/search/cs?searchtype=author&query=Papakyriakopoulos%2C+O">Orestis Papakyriakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Monroy-Hern%C3%A1ndez%2C+A">Andr&#xe9;s Monroy-Hern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in CSCW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Beyond the well-known giants like Uber Eats and DoorDash, there are hundreds
of independent food delivery platforms in the United States. However, little is
known about the sociotechnical landscape of these ``indie'' platforms. In this
paper, we analyzed these platforms to understand why they were created, how
they operate, and what technologies they use. We collected data on 495 indie
platforms and detailed survey responses from 29 platforms. We found that
personalized, timely service is a central value of indie platforms, as is a
sense of responsibility to the local community they serve. Indie platforms are
motivated to provide fair rates for restaurants and couriers. These alternative
business practices differentiate them from mainstream platforms. Though indie
platforms have plans to expand, a lack of customizability in off-the-shelf
software prevents independent platforms from personalizing services for their
local communities. We show that these platforms are a widespread and
longstanding fixture of the food delivery market. We illustrate the diversity
of motivations and values to explain why a one-size-fits-all support is
insufficient, and we discuss the siloing of technology that inhibits platforms'
growth. Through these insights, we aim to promote future HCI research into the
potential development of public-interest technologies for local food delivery.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14160" title="Abstract">arXiv:2402.14160</a> [<a href="/pdf/2402.14160" title="Download PDF">pdf</a>, <a href="/format/2402.14160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursive Speculative Decoding: Accelerating LLM Inference via Sampling  Without Replacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+W">Wonseok Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Gagrani%2C+M">Mukul Gagrani</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+R">Raghavv Goel</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mingu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lott%2C+C">Christopher Lott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 82 pages, 9 figures, 54 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Speculative decoding is an inference-acceleration method for large language
models (LLMs) where a small language model generates a draft-token sequence
which is further verified by the target LLM in parallel. Recent works have
advanced this method by establishing a draft-token tree, achieving superior
performance over a single-sequence speculative decoding. However, those works
independently generate tokens at each level of the tree, not leveraging the
tree's entire diversifiability. Besides, their empirical superiority has been
shown for fixed length of sequences, implicitly granting more computational
resource to LLM for the tree-based methods. None of the existing works has
conducted empirical studies with fixed target computational budgets despite its
importance to resource-bounded devices. We present Recursive Speculative
Decoding (RSD), a novel tree-based method that samples draft tokens without
replacement and maximizes the diversity of the tree. During RSD's drafting, the
tree is built by either Gumbel-Top-$k$ trick that draws tokens without
replacement in parallel or Stochastic Beam Search that samples sequences
without replacement while early-truncating unlikely draft sequences and
reducing the computational cost of LLM. We empirically evaluate RSD with Llama
2 and OPT models, showing that RSD outperforms the baseline methods,
consistently for fixed draft sequence length and in most cases for fixed
computational budgets at LLM.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14162" title="Abstract">arXiv:2402.14162</a> [<a href="/pdf/2402.14162" title="Download PDF">pdf</a>, <a href="/format/2402.14162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Large Visual Language Models for Medical Imaging Analysis: An  Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van%2C+M">Minh-Hao Van</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Prateek Verma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, large language models (LLMs) have taken the spotlight in natural
language processing. Further, integrating LLMs with vision enables the users to
explore emergent abilities with multimodal data. Visual language models (VLMs),
such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on
various visio-linguistic tasks. Consequently, there are enormous applications
of large models that could be potentially used in the biomedical imaging field.
Along that direction, there is a lack of related work to show the ability of
large models to diagnose the diseases. In this work, we study the zero-shot and
few-shot robustness of VLMs on the medical imaging analysis tasks. Our
comprehensive experiments demonstrate the effectiveness of VLMs in analyzing
biomedical images such as brain MRIs, microscopic images of blood cells, and
chest X-rays.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14167" title="Abstract">arXiv:2402.14167</a> [<a href="/pdf/2402.14167" title="Download PDF">pdf</a>, <a href="/format/2402.14167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with  Trajectory Stitching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zizheng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">De-An Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+W">Weili Nie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiding Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jianfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sampling from diffusion probabilistic models (DPMs) is often expensive for
high-quality image generation and typically requires many steps with a large
model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a
simple yet efficient technique to improve the sampling efficiency with little
or no generation degradation. Instead of solely using a large DPM for the
entire sampling trajectory, T-Stitch first leverages a smaller DPM in the
initial steps as a cheap drop-in replacement of the larger DPM and switches to
the larger DPM at a later stage. Our key insight is that different diffusion
models learn similar encodings under the same training data distribution and
smaller models are capable of generating good global structures in the early
steps. Extensive experiments demonstrate that T-Stitch is training-free,
generally applicable for different architectures, and complements most existing
fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL,
for example, 40% of the early timesteps can be safely replaced with a 10x
faster DiT-S without performance drop on class-conditional ImageNet generation.
We further show that our method can also be used as a drop-in technique to not
only accelerate the popular pretrained stable diffusion (SD) models but also
improve the prompt alignment of stylized SD models from the public model zoo.
Code is released at https://github.com/NVlabs/T-Stitch
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14169" title="Abstract">arXiv:2402.14169</a> [<a href="/pdf/2402.14169" title="Download PDF">pdf</a>, <a href="/format/2402.14169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Temporal Bias Correction using a Machine Learning Attention model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nivron%2C+O">Omer Nivron</a>, 
<a href="/search/cs?searchtype=author&query=Wischik%2C+D+J">Damon J. Wischik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Climate models are biased with respect to real world observations and usually
need to be calibrated prior to impact studies. The suite of statistical methods
that enable such calibrations is called bias correction (BC). However, current
BC methods struggle to adjust for temporal biases, because they disregard the
dependence between consecutive time-points. As a result, climate statistics
with long-range temporal properties, such as heatwave duration and frequency,
cannot be corrected accurately, making it more difficult to produce reliable
impact studies on such climate statistics. In this paper, we offer a novel BC
methodology to correct for temporal biases. This is made possible by i)
re-thinking BC as a probability model rather than an algorithmic procedure, and
ii) adapting state-of-the-art machine-learning (ML) probabilistic attention
models to fit the BC task. With a case study of heatwave duration statistics in
Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current
climate model outputs and alternative BC methods.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14172" title="Abstract">arXiv:2402.14172</a> [<a href="/pdf/2402.14172" title="Download PDF">pdf</a>, <a href="/format/2402.14172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Source Software Field Research: Spanning Social and Practice  Networks for Re-Entering the Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goggins%2C+S+P">Sean P. Goggins</a>, 
<a href="/search/cs?searchtype=author&query=Lumbard%2C+K">Kevin Lumbard</a>, 
<a href="/search/cs?searchtype=author&query=Germonprez%2C+M">Matt Germonprez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Sociotechnical research increasingly includes the social sub-networks that
emerge from large-scale sociotechnical infrastructure, including the
infrastructure for building open source software. This paper addresses these
numerous sub-networks as advantageous for researchers. It provides a
methodological synthesis focusing on how researchers can best span adjacent
social sub-networks during engaged field research. Specifically, we describe
practices and artifacts that aid movement from one social subsystem within a
more extensive technical infrastructure to another. To surface the importance
of spanning sub-networks, we incorporate a discussion of social capital and the
role of technical infrastructure in its development for sociotechnical
researchers. We then characterize a five-step process for spanning social
sub-networks during engaged field research: commitment, context mapping, jargon
competence, returning value, and bridging. We then present our experience
studying corporate open source software projects and the role of that
experience in accelerating our work in open source scientific software research
as described through the lens of bridging social capital. Based on our
analysis, we offer recommendations for engaging in fieldwork in adjacent social
sub-networks that share a technical context and discussion of how the
relationship between social and technically acquired social capital is a
missing but critical methodological dimension for research on large-scale
sociotechnical research.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14173" title="Abstract">arXiv:2402.14173</a> [<a href="/pdf/2402.14173" title="Download PDF">pdf</a>, <a href="/ps/2402.14173" title="Download PostScript">ps</a>, <a href="/format/2402.14173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness and Incentive Compatibility via Percentage Fees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dobzinski%2C+S">Shahar Dobzinski</a>, 
<a href="/search/cs?searchtype=author&query=Oren%2C+S">Sigal Oren</a>, 
<a href="/search/cs?searchtype=author&query=Vondrak%2C+J">Jan Vondrak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study incentive-compatible mechanisms that maximize the Nash Social
Welfare. Since traditional incentive-compatible mechanisms cannot maximize the
Nash Social Welfare even approximately, we propose changing the traditional
model. Inspired by a widely used charging method (e.g., royalties, a lawyer
that charges some percentage of possible future compensation), we suggest
charging the players some percentage of their value of the outcome. We call
this model the \emph{percentage fee} model.
<br />We show that there is a mechanism that maximizes exactly the Nash Social
Welfare in every setting with non-negative valuations. Moreover, we prove an
analog of Roberts theorem that essentially says that if the valuations are
non-negative, then the only implementable social choice functions are those
that maximize weighted variants of the Nash Social Welfare. We develop
polynomial time incentive compatible approximation algorithms for the Nash
Social Welfare with subadditive valuations and prove some hardness results.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14174" title="Abstract">arXiv:2402.14174</a> [<a href="/pdf/2402.14174" title="Download PDF">pdf</a>, <a href="/format/2402.14174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blending Data-Driven Priors in Dynamic Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lidard%2C+J">Justin Lidard</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haimin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hancock%2C+A">Asher Hancock</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Contreras%2C+A+G">Albert Gim&#xf3; Contreras</a>, 
<a href="/search/cs?searchtype=author&query=Modi%2C+V">Vikash Modi</a>, 
<a href="/search/cs?searchtype=author&query=DeCastro%2C+J">Jonathan DeCastro</a>, 
<a href="/search/cs?searchtype=author&query=Gopinath%2C+D">Deepak Gopinath</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+G">Guy Rosman</a>, 
<a href="/search/cs?searchtype=author&query=Leonard%2C+N">Naomi Leonard</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M">Mar&#xed;a Santos</a>, 
<a href="/search/cs?searchtype=author&query=Fisac%2C+J+F">Jaime Fern&#xe1;ndez Fisac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">As intelligent robots like autonomous vehicles become increasingly deployed
in the presence of people, the extent to which these systems should leverage
model-based game-theoretic planners versus data-driven policies for safe,
interaction-aware motion planning remains an open question. Existing dynamic
game formulations assume all agents are task-driven and behave optimally.
However, in reality, humans tend to deviate from the decisions prescribed by
these models, and their behavior is better approximated under a noisy-rational
paradigm. In this work, we investigate a principled methodology to blend a
data-driven reference policy with an optimization-based game-theoretic policy.
We formulate KLGame, a type of non-cooperative dynamic game with
Kullback-Leibler (KL) regularization with respect to a general, stochastic, and
possibly multi-modal reference policy. Our method incorporates, for each
decision maker, a tunable parameter that permits modulation between task-driven
and data-driven behaviors. We propose an efficient algorithm for computing
multimodal approximate feedback Nash equilibrium strategies of KLGame in real
time. Through a series of simulated and real-world autonomous driving
scenarios, we demonstrate that KLGame policies can more effectively incorporate
guidance from the reference policy and account for noisily-rational human
behaviors versus non-regularized baselines.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14175" title="Abstract">arXiv:2402.14175</a> [<a href="/pdf/2402.14175" title="Download PDF">pdf</a>, <a href="/format/2402.14175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Contact-Aided Motion Planning for Tendon-Driven Continuum Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+P">Priyanka Rao</a>, 
<a href="/search/cs?searchtype=author&query=Salzman%2C+O">Oren Salzman</a>, 
<a href="/search/cs?searchtype=author&query=Burgner-Kahrs%2C+J">Jessica Burgner-Kahrs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Tendon-driven continuum robots (TDCRs), with their flexible backbones, offer
the advantage of being used for navigating complex, cluttered environments.
However, to do so, they typically require multiple segments, often leading to
complex actuation and control challenges. To this end, we propose a novel
approach to navigate cluttered spaces effectively for a single-segment long
TDCR which is the simplest topology from a mechanical point of view. Our key
insight is that by leveraging contact with the environment we can achieve
multiple curvatures without mechanical alterations to the robot. Specifically,
we propose a search-based motion planner for a single-segment TDCR. This
planner, guided by a specially designed heuristic, discretizes the
configuration space and employs a best-first search. The heuristic, crucial for
efficient navigation, provides an effective cost-to-go estimation while
respecting the kinematic constraints of the TDCR and environmental
interactions. We empirically demonstrate the efficiency of our planner-testing
over 525 queries in environments with both convex and non-convex obstacles, our
planner is demonstrated to have a success rate of about 80% while baselines
were not able to obtain a success rate higher than 30%. The difference is
attributed to our novel heuristic which is shown to significantly reduce the
required search space.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14177" title="Abstract">arXiv:2402.14177</a> [<a href="/pdf/2402.14177" title="Download PDF">pdf</a>, <a href="/format/2402.14177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Human Values in Online Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borenstein%2C+N">Nadav Borenstein</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Kaffee%2C+L">Lucie-Aim&#xe9;e Kaffee</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Human values play a vital role as an analytical tool in social sciences,
enabling the study of diverse dimensions within society as a whole and among
individual communities. This paper addresses the limitations of traditional
survey-based studies of human values by proposing a computational application
of Schwartz's values framework to Reddit, a platform organized into distinct
online communities. After ensuring the reliability of automated value
extraction tools for Reddit content, we automatically annotate six million
posts across 10,000 subreddits with Schwartz values. Our analysis unveils both
previously recorded and novel insights into the values prevalent within various
online communities. For instance, when examining subreddits with differing
opinions on controversial topics, we discover higher universalism values in the
Vegan subreddit compared to Carnivores. Additionally, our study of
geographically specific subreddits highlights the correlation between
traditional values and conservative U.S. states.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14179" title="Abstract">arXiv:2402.14179</a> [<a href="/pdf/2402.14179" title="Download PDF">pdf</a>, <a href="/format/2402.14179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bangla AI: A Framework for Machine Translation Utilizing Large Language  Models for Ethnic Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goni%2C+M+A">MD Ashraful Goni</a>, 
<a href="/search/cs?searchtype=author&query=Mostafa%2C+F">Fahad Mostafa</a>, 
<a href="/search/cs?searchtype=author&query=Kee%2C+K+F">Kerk F. Kee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ethnic media, which caters to diaspora communities in host nations, serves as
a vital platform for these communities to both produce content and access
information. Rather than utilizing the language of the host nation, ethnic
media delivers news in the language of the immigrant community. For instance,
in the USA, Bangla ethnic media presents news in Bangla rather than English.
This research delves into the prospective integration of large language models
(LLM) and multi-lingual machine translations (MMT) within the ethnic media
industry. It centers on the transformative potential of using LLM in MMT in
various facets of news translation, searching, and categorization. The paper
outlines a theoretical framework elucidating the integration of LLM and MMT
into the news searching and translation processes for ethnic media.
Additionally, it briefly addresses the potential ethical challenges associated
with the incorporation of LLM and MMT in news translation procedures.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14180" title="Abstract">arXiv:2402.14180</a> [<a href="/pdf/2402.14180" title="Download PDF">pdf</a>, <a href="/format/2402.14180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Transformers are Versatile In-Context Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vladymyrov%2C+M">Max Vladymyrov</a>, 
<a href="/search/cs?searchtype=author&query=von+Oswald%2C+J">Johannes von Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Sandler%2C+M">Mark Sandler</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent research has demonstrated that transformers, particularly linear
attention models, implicitly execute gradient-descent-like algorithms on data
provided in-context during their forward inference step. However, their
capability in handling more complex problems remains unexplored. In this paper,
we prove that any linear transformer maintains an implicit linear model and can
be interpreted as performing a variant of preconditioned gradient descent. We
also investigate the use of linear transformers in a challenging scenario where
the training data is corrupted with different levels of noise. Remarkably, we
demonstrate that for this problem linear transformers discover an intricate and
highly effective optimization algorithm, surpassing or matching in performance
many reasonable baselines. We reverse-engineer this algorithm and show that it
is a novel approach incorporating momentum and adaptive rescaling based on
noise levels. Our findings show that even linear transformers possess the
surprising ability to discover sophisticated optimization strategies.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14182" title="Abstract">arXiv:2402.14182</a> [<a href="/pdf/2402.14182" title="Download PDF">pdf</a>, <a href="/format/2402.14182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Machines and Humans Focus on Similar Code? Exploring Explainability  of Large Language Models in Code Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Karas%2C+Z">Zachary Karas</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Collin McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Leach%2C+K">Kevin Leach</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent language models have demonstrated proficiency in summarizing source
code. However, as in many other domains of machine learning, language models of
code lack sufficient explainability. Informally, we lack a formulaic or
intuitive understanding of what and how models learn from code. Explainability
of language models can be partially provided if, as the models learn to produce
higher-quality code summaries, they also align in deeming the same code parts
important as those identified by human programmers. In this paper, we report
negative results from our investigation of explainability of language models in
code summarization through the lens of human comprehension. We measure human
focus on code using eye-tracking metrics such as fixation counts and duration
in code summarization tasks. To approximate language model focus, we employ a
state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP
(SHapley Additive exPlanations), to identify which code tokens influence that
generation of summaries. Using these settings, we find no statistically
significant relationship between language models' focus and human programmers'
attention. Furthermore, alignment between model and human foci in this setting
does not seem to dictate the quality of the LLM-generated summaries. Our study
highlights an inability to align human focus with SHAP-based model focus
measures. This result calls for future investigation of multiple open questions
for explainable language models for code summarization and software engineering
tasks in general, including the training mechanisms of language models for
code, whether there is an alignment between human and model attention on code,
whether human attention can improve the development of language models, and
what other model focus measures are appropriate for improving explainability.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14183" title="Abstract">arXiv:2402.14183</a> [<a href="/pdf/2402.14183" title="Download PDF">pdf</a>, <a href="/format/2402.14183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parking of Connected Automated Vehicles: Vehicle Control, Parking  Assignment, and Multi-agent Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+X">Xu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+Y">Yongkeun Choi</a>, 
<a href="/search/eess?searchtype=author&query=Wong%2C+A">Alex Wong</a>, 
<a href="/search/eess?searchtype=author&query=Borrelli%2C+F">Francesco Borrelli</a>, 
<a href="/search/eess?searchtype=author&query=Moura%2C+S">Scott Moura</a>, 
<a href="/search/eess?searchtype=author&query=Woo%2C+S">Soomin Woo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a novel approach to optimize the parking efficiency for
fleets of Connected and Automated Vehicles (CAVs). We present a novel
multi-vehicle parking simulator, equipped with hierarchical path planning and
collision avoidance capabilities for individual CAVs. The simulator is designed
to capture the key decision-making processes in parking, from low-level vehicle
control to high-level parking assignment, and it enables the effective
assessment of parking strategies for large fleets of ground vehicles. We
formulate and compare different strategic parking spot assignments to minimize
a collective cost. While the proposed framework is designed to optimize various
objective functions, we choose the total parking time for the experiment, as it
is closely related to the reduction of vehicles' energy consumption and
greenhouse gas emissions. We validate the effectiveness of the proposed
strategies through empirical evaluation against a dataset of real-world parking
lot dynamics, realizing a substantial reduction in parking time by up to 43.8%.
This improvement is attributed to the synergistic benefits of driving
automation, the utilization of shared infrastructure state data, the exclusion
of pedestrian traffic, and the real-time computation of optimal parking spot
allocation.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14184" title="Abstract">arXiv:2402.14184</a> [<a href="/pdf/2402.14184" title="Download PDF">pdf</a>, <a href="/format/2402.14184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity-Aware Ensembling of Language Models Based on Topological Data  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Proskura%2C+P">Polina Proskura</a>, 
<a href="/search/cs?searchtype=author&query=Zaytsev%2C+A">Alexey Zaytsev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Ensembles are important tools for improving the performance of machine
learning models. In cases related to natural language processing, ensembles
boost the performance of a method due to multiple large models available in
open source. However, existing approaches mostly rely on simple averaging of
predictions by ensembles with equal weights for each model, ignoring
differences in the quality and conformity of models. We propose to estimate
weights for ensembles of NLP models using not only knowledge of their
individual performance but also their similarity to each other. By adopting
distance measures based on Topological Data Analysis (TDA), we improve our
ensemble. The quality improves for both text classification accuracy and
relevant uncertainty estimation.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14185" title="Abstract">arXiv:2402.14185</a> [<a href="/pdf/2402.14185" title="Download PDF">pdf</a>, <a href="/format/2402.14185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HINT: High-quality INPainting Transformer with Mask-Aware Encoding and  Enhanced Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Atapour-Abarghouei%2C+A">Amir Atapour-Abarghouei</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H+P+H">Hubert P. H. Shum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing image inpainting methods leverage convolution-based downsampling
approaches to reduce spatial dimensions. This may result in information loss
from corrupted images where the available information is inherently sparse,
especially for the scenario of large missing regions. Recent advances in
self-attention mechanisms within transformers have led to significant
improvements in many computer vision tasks including inpainting. However,
limited by the computational costs, existing methods cannot fully exploit the
efficacy of long-range modelling capabilities of such models. In this paper, we
propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT,
which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to
preserve the visible information extracted from the corrupted image while
maintaining the integrity of the information available for high-level
inferences made within the model. Moreover, we propose a Spatially-activated
Channel Attention Layer (SCAL), an efficient self-attention mechanism
interpreting spatial awareness to model the corrupted image at multiple scales.
To further enhance the effectiveness of SCAL, motivated by recent advanced in
speech recognition, we introduce a sandwich structure that places feed-forward
networks before and after the SCAL module. We demonstrate the superior
performance of HINT compared to contemporary state-of-the-art models on four
datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14187" title="Abstract">arXiv:2402.14187</a> [<a href="/pdf/2402.14187" title="Download PDF">pdf</a>, <a href="/format/2402.14187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Adoption to Adaption: Tracing the Diffusion of New Emojis on  Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+W">Wei Ai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the rapidly evolving landscape of social media, the introduction of new
emojis in Unicode release versions presents a structured opportunity to explore
digital language evolution. Analyzing a large dataset of sampled English
tweets, we examine how newly released emojis gain traction and evolve in
meaning. We find that community size of early adopters and emoji semantics are
crucial in determining their popularity. Certain emojis experienced notable
shifts in the meanings and sentiment associations during the diffusion process.
Additionally, we propose a novel framework utilizing language models to extract
words and pre-existing emojis with semantically similar contexts, which
enhances interpretation of new emojis. The framework demonstrates its
effectiveness in improving sentiment classification performance by substituting
unknown new emojis with familiar ones. This study offers a new perspective in
understanding how new language units are adopted, adapted, and integrated into
the fabric of online communication.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14193" title="Abstract">arXiv:2402.14193</a> [<a href="/pdf/2402.14193" title="Download PDF">pdf</a>, <a href="/ps/2402.14193" title="Download PostScript">ps</a>, <a href="/format/2402.14193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homomorphic Encryption Based on Post-Quantum Cryptography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+C+H">Abel C. H. Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the development of Shor's algorithm, some nondeterministic polynomial
(NP) time problems (e.g. prime factorization problems and discrete logarithm
problems) may be solved in polynomial time. In recent years, although some
homomorphic encryption algorithms have been proposed based on prime
factorization problems, the algorithms may be cracked by quantum computing
attacks. Therefore, this study proposes a post-quantum cryptography (PQC)-based
homomorphic encryption method which includes the homomorphic encryption
function based on a code-based cryptography method for avoiding quantum
computing attacks. Subsection 3.2 proposes mathematical models to prove the
feasibility of the proposed method, and Subsection 3.3 gives calculation
examples to present the detailed steps of the proposed method. In experimental
environments, the mainstream cryptography methods (i.e. RSA cryptography and
elliptic curve cryptography (ECC)) have been compared, and the results show
that the encryption time and decryption time of the proposed method are shorter
than other cryptography methods. Furthermore, the proposed method is designed
based on a non-negative matrix factorization problem (i.e. a NP problem) for
resisting quantum computing attacks.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14194" title="Abstract">arXiv:2402.14194</a> [<a href="/pdf/2402.14194" title="Download PDF">pdf</a>, <a href="/format/2402.14194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human  Racing Gameplay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weaver%2C+C">Catherine Weaver</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Ce Hao</a>, 
<a href="/search/cs?searchtype=author&query=Kawamoto%2C+K">Kenta Kawamoto</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Imitation learning learns a policy from demonstrations without requiring
hand-designed reward functions. In many robotic tasks, such as autonomous
racing, imitated policies must model complex environment dynamics and human
decision-making. Sequence modeling is highly effective in capturing intricate
patterns of motion sequences but struggles to adapt to new environments or
distribution shifts that are common in real-world robotics tasks. In contrast,
Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles
with sample inefficiency and handling complex motion patterns. Thus, we propose
BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a
Behavior Transformer (BeT) policy from human demonstrations with online AIL.
BeTAIL adds an AIL residual policy to the BeT policy to model the sequential
decision-making process of human experts and correct for out-of-distribution
states or shifts in environment dynamics. We test BeTAIL on three challenges
with expert-level demonstrations of real human gameplay in Gran Turismo Sport.
Our proposed residual BeTAIL reduces environment interactions and improves
racing performance and stability, even when the BeT is pretrained on different
tracks than downstream learning. Videos and code available at:
https://sites.google.com/berkeley.edu/BeTAIL/home.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14195" title="Abstract">arXiv:2402.14195</a> [<a href="/pdf/2402.14195" title="Download PDF">pdf</a>, <a href="/format/2402.14195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Reduce: Optimal Representations of Structured Data in  Prompting Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Younghun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungchul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have been widely used as general-purpose AI
agents showing comparable performance on many downstream tasks. However,
existing work shows that it is challenging for LLMs to integrate structured
data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand
long text data or select the most relevant evidence prior to inference, and
both approaches are not trivial.
<br />In this paper, we propose a framework, Learning to Reduce, that fine-tunes a
language model to generate a reduced version of an input context, given a task
description and context input. The model learns to reduce the input context
using On-Policy Reinforcement Learning and aims to improve the reasoning
performance of a fixed LLM. Experimental results illustrate that our model not
only achieves comparable accuracies in selecting the relevant evidence from an
input context, but also shows generalizability on different datasets. We
further show that our model helps improve the LLM's performance on downstream
tasks especially when the context is long.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14196" title="Abstract">arXiv:2402.14196</a> [<a href="/pdf/2402.14196" title="Download PDF">pdf</a>, <a href="/format/2402.14196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Seungtae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Rho%2C+D">Daniel Rho</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J+H">Jong Hwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunbyung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Despite the remarkable achievements of neural radiance fields (NeRF) in
representing 3D scenes and generating novel view images, the aliasing issue,
rendering "jaggies" or "blurry" images at varying camera distances, remains
unresolved in most existing approaches. The recently proposed mip-NeRF has
addressed this challenge by rendering conical frustums instead of rays.
However, it relies on MLP architecture to represent the radiance fields,
missing out on the fast training speed offered by the latest grid-based
methods. In this work, we present mip-Grid, a novel approach that integrates
anti-aliasing techniques into grid-based representations for radiance fields,
mitigating the aliasing artifacts while enjoying fast training time. The
proposed method generates multi-scale grids by applying simple convolution
operations over a shared grid representation and uses the scale-aware
coordinate to retrieve features at different scales from the generated
multi-scale grids. To test the effectiveness, we integrated the proposed method
into the two recent representative grid-based methods, TensoRF and K-Planes.
Experimental results demonstrate that mip-Grid greatly improves the rendering
performance of both methods and even outperforms mip-NeRF on multi-scale
datasets while achieving significantly faster training time. For code and demo
videos, please see https://stnamjef.github.io/mipgrid.github.io/.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14198" title="Abstract">arXiv:2402.14198</a> [<a href="/pdf/2402.14198" title="Download PDF">pdf</a>, <a href="/format/2402.14198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Inapproximability of Nash Equilibria in Public Goods Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinh%2C+J+D">J&#xe9;r&#xe9;mi Do Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Hollender%2C+A">Alexandros Hollender</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study public goods games, a type of game where every player has to decide
whether or not to produce a good which is public, i.e., neighboring players can
also benefit from it. Specifically, we consider a setting where the good is
indivisible and where the neighborhood structure is represented by a directed
graph, with the players being the nodes. Papadimitriou and Peng (2023) recently
showed that in this setting computing mixed Nash equilibria is PPAD-hard, and
that this remains the case even for $\varepsilon$-well-supported approximate
equilibria for some sufficiently small constant $\varepsilon$. In this work, we
strengthen this inapproximability result by showing that the problem remains
PPAD-hard for any non-trivial approximation parameter $\varepsilon$.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14200" title="Abstract">arXiv:2402.14200</a> [<a href="/pdf/2402.14200" title="Download PDF">pdf</a>, <a href="/format/2402.14200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Counseling Conversations: Domain Knowledge and  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Younghun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Goldwasser%2C+D">Dan Goldwasser</a>, 
<a href="/search/cs?searchtype=author&query=Reese%2C+L+S">Laura Schwab Reese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EACL 2024, 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding the dynamics of counseling conversations is an important task,
yet it is a challenging NLP problem regardless of the recent advance of
Transformer-based pre-trained language models. This paper proposes a systematic
approach to examine the efficacy of domain knowledge and large language models
(LLMs) in better representing conversations between a crisis counselor and a
help seeker. We empirically show that state-of-the-art language models such as
Transformer-based models and GPT models fail to predict the conversation
outcome. To provide richer context to conversations, we incorporate
human-annotated domain knowledge and LLM-generated features; simple integration
of domain knowledge and LLM features improves the model performance by
approximately 15%. We argue that both domain knowledge and LLM-generated
features can be exploited to better characterize counseling conversations when
they are used as an additional context to conversations.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14201" title="Abstract">arXiv:2402.14201</a> [<a href="/pdf/2402.14201" title="Download PDF">pdf</a>, <a href="/format/2402.14201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random-Order Online Interval Scheduling and Geometric Generalizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+M">Mohit Garg</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+D">Debajyoti Kar</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arindam Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">In the Maximum Independent Set of Hyperrectangles problem, we are given a set
of $n$ (possibly overlapping) $d$-dimensional axis-aligned hyperrectangles, and
the goal is to find a subset of non-overlapping hyperrectangles of maximum
cardinality. For $d=1$, this corresponds to the classical Interval Scheduling
problem, where a simple greedy algorithm returns an optimal solution. In the
offline setting, for $d$-dimensional hyperrectangles, polynomial time $(\log
n)^{O(d)}$-approximation algorithms are known. However, the problem becomes
notably challenging in the online setting, where the input objects
(hyperrectangles) appear one by one in an adversarial order, and on the arrival
of an object, the algorithm needs to make an immediate and irrevocable decision
whether or not to select the object while maintaining the feasibility. Even for
interval scheduling, an $\Omega(n)$ lower bound is known on the competitive
ratio.
<br />To circumvent these negative results, in this work, we study the online
maximum independent set of axis-aligned hyperrectangles in the random-order
arrival model, where the adversary specifies the set of input objects which
then arrive in a uniformly random order. Starting from the prototypical
secretary problem, the random-order model has received significant attention to
study algorithms beyond the worst-case competitive analysis. Surprisingly, we
show that the problem in the random-order model almost matches the best-known
offline approximation guarantees, up to polylogarithmic factors. In particular,
we give a simple $(\log n)^{O(d)}$-competitive algorithm for $d$-dimensional
hyperrectangles in this model, which runs in $\tilde{O_d}(n)$ time. Our
approach also yields $(\log n)^{O(d)}$-competitive algorithms in the
random-order model for more general objects such as $d$-dimensional fat objects
and ellipsoids.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14202" title="Abstract">arXiv:2402.14202</a> [<a href="/pdf/2402.14202" title="Download PDF">pdf</a>, <a href="/format/2402.14202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Graph Transformers via Positional Encodings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Black%2C+M">Mitchell Black</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhengchao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Mishne%2C+G">Gal Mishne</a>, 
<a href="/search/cs?searchtype=author&query=Nayyeri%2C+A">Amir Nayyeri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The distinguishing power of graph transformers is closely tied to the choice
of positional encoding: features used to augment the base transformer with
information about the graph. There are two primary types of positional
encoding: absolute positional encodings (APEs) and relative positional
encodings (RPEs). APEs assign features to each node and are given as input to
the transformer. RPEs instead assign a feature to each pair of nodes, e.g.,
graph distance, and are used to augment the attention block. A priori, it is
unclear which method is better for maximizing the power of the resulting graph
transformer. In this paper, we aim to understand the relationship between these
different types of positional encodings. Interestingly, we show that graph
transformers using APEs and RPEs are equivalent in terms of distinguishing
power. In particular, we demonstrate how to interchange APEs and RPEs while
maintaining their distinguishing power in terms of graph transformers. Based on
our theoretical results, we provide a study on several APEs and RPEs (including
the resistance distance and the recently introduced stable and expressive
positional encoding (SPE)) and compare their distinguishing power in terms of
transformers. We believe our work will help navigate the huge number of choices
of positional encoding and will provide guidance on the future design of
positional encodings for graph transformers.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14203" title="Abstract">arXiv:2402.14203</a> [<a href="/pdf/2402.14203" title="Download PDF">pdf</a>, <a href="/format/2402.14203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploratory Analysis of COVID Bot vs Human Disinformation  Dissemination stemming from the Disinformation Dozen on Telegram
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ng%2C+L+H+X">Lynnette Hui Xian Ng</a>, 
<a href="/search/cs?searchtype=author&query=Kloo%2C+I">Ian Kloo</a>, 
<a href="/search/cs?searchtype=author&query=Carley%2C+K+M">Kathleen M. Carley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Journal of Computational Social Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The COVID-19 pandemic of 2021 led to a worldwide health crisis that was
accompanied by an infodemic. A group of 12 social media personalities, dubbed
the ``Disinformation Dozen", were identified as key in spreading disinformation
regarding the COVID-19 virus, treatments, and vaccines. This study focuses on
the spread of disinformation propagated by this group on Telegram, a mobile
messaging and social media platform. After segregating users into three groups
-- the Disinformation Dozen, bots, and humans --, we perform an investigation
with a dataset of Telegram messages from January to June 2023, comparatively
analyzing temporal, topical, and network features. We observe that the
Disinformation Dozen are highly involved in the initial dissemination of
disinformation but are not the main drivers of the propagation of
disinformation. Bot users are extremely active in conversation threads, while
human users are active propagators of information, disseminating posts between
Telegram channels through the forwarding mechanism.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14205" title="Abstract">arXiv:2402.14205</a> [<a href="/pdf/2402.14205" title="Download PDF">pdf</a>, <a href="/format/2402.14205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression Robust Synthetic Speech Detection Using Patched Spectrogram  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+A+K+S">Amit Kumar Singh Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Ziyue Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Bhagtani%2C+K">Kratika Bhagtani</a>, 
<a href="/search/cs?searchtype=author&query=Bestagini%2C+P">Paolo Bestagini</a>, 
<a href="/search/cs?searchtype=author&query=Tubaro%2C+S">Stefano Tubaro</a>, 
<a href="/search/cs?searchtype=author&query=Delp%2C+E+J">Edward J. Delp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as long oral paper at ICMLA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Many deep learning synthetic speech generation tools are readily available.
The use of synthetic speech has caused financial fraud, impersonation of
people, and misinformation to spread. For this reason forensic methods that can
detect synthetic speech have been proposed. Existing methods often overfit on
one dataset and their performance reduces substantially in practical scenarios
such as detecting synthetic speech shared on social platforms. In this paper we
propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a
synthetic speech detector that converts a time domain speech signal to a
mel-spectrogram and processes it in patches using a transformer neural network.
We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our
experiments show that PS3DT performs well on ASVspoof2019 dataset compared to
other approaches using spectrogram for synthetic speech detection. We also
investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT
generalizes well than several existing methods on detecting synthetic speech
from an out-of-distribution dataset. We also evaluate robustness of PS3DT to
detect telephone quality synthetic speech and synthetic speech shared on social
platforms (compressed speech). PS3DT is robust to compression and can detect
telephone quality synthetic speech better than several existing methods.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14207" title="Abstract">arXiv:2402.14207</a> [<a href="/pdf/2402.14207" title="Download PDF">pdf</a>, <a href="/format/2402.14207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assisting in Writing Wikipedia-like Articles From Scratch with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yijia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yucheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kanell%2C+T+A">Theodore A. Kanell</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peter Xu</a>, 
<a href="/search/cs?searchtype=author&query=Khattab%2C+O">Omar Khattab</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+S">Monica S. Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review. 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study how to apply large language models to write grounded and organized
long-form articles from scratch, with comparable breadth and depth to Wikipedia
pages. This underexplored problem poses new challenges at the pre-writing
stage, including how to research the topic and prepare an outline prior to
writing. We propose STORM, a writing system for the Synthesis of Topic Outlines
through Retrieval and Multi-perspective Question Asking. STORM models the
pre-writing stage by (1) discovering diverse perspectives in researching the
given topic, (2) simulating conversations where writers carrying different
perspectives pose questions to a topic expert grounded on trusted Internet
sources, (3) curating the collected information to create an outline.
<br />For evaluation, we curate FreshWiki, a dataset of recent high-quality
Wikipedia articles, and formulate outline assessments to evaluate the
pre-writing stage. We further gather feedback from experienced Wikipedia
editors. Compared to articles generated by an outline-driven
retrieval-augmented baseline, more of STORM's articles are deemed to be
organized (by a 25% absolute increase) and broad in coverage (by 10%). The
expert feedback also helps identify new challenges for generating grounded long
articles, such as source bias transfer and over-association of unrelated facts.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14208" title="Abstract">arXiv:2402.14208</a> [<a href="/pdf/2402.14208" title="Download PDF">pdf</a>, <a href="/format/2402.14208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content Conditional Debiasing for Fair Text Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wenlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Blair Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Mitigating biases in machine learning models has gained increasing attention
in Natural Language Processing (NLP). Yet, only a few studies focus on fair
text embeddings, which are crucial yet challenging for real-world applications.
In this paper, we propose a novel method for learning fair text embeddings. We
achieve fairness while maintaining utility trade-off by ensuring conditional
independence between sensitive attributes and text embeddings conditioned on
the content. Specifically, we enforce that embeddings of texts with different
sensitive attributes but identical content maintain the same distance toward
the embedding of their corresponding neutral text. Furthermore, we address the
issue of lacking proper training data by using Large Language Models (LLMs) to
augment texts into different sensitive groups. Our extensive evaluations
demonstrate that our approach effectively improves fairness while preserving
the utility of embeddings, representing a pioneering effort in achieving
conditional independence for fair text embeddings.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14212" title="Abstract">arXiv:2402.14212</a> [<a href="/pdf/2402.14212" title="Download PDF">pdf</a>, <a href="/format/2402.14212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moonwalk: Inverse-Forward Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krylov%2C+D">Dmitrii Krylov</a>, 
<a href="/search/cs?searchtype=author&query=Karamzade%2C+A">Armin Karamzade</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+R">Roy Fox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Backpropagation, while effective for gradient computation, falls short in
addressing memory consumption, limiting scalability. This work explores
forward-mode gradient computation as an alternative in invertible networks,
showing its potential to reduce the memory footprint without substantial
drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian
product that accelerates the computation of forward gradients while retaining
the advantages of memory reduction and preserving the fidelity of true
gradients. Our method, Moonwalk, has a time complexity linear in the depth of
the network, unlike the quadratic time complexity of na\"ive forward, and
empirically reduces computation time by several orders of magnitude without
allocating more memory. We further accelerate Moonwalk by combining it with
reverse-mode differentiation to achieve time complexity comparable with
backpropagation while maintaining a much smaller memory footprint. Finally, we
showcase the robustness of our method across several architecture choices.
Moonwalk is the first forward-based method to compute true gradients in
invertible networks in computation time comparable to backpropagation and using
significantly less memory.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14215" title="Abstract">arXiv:2402.14215</a> [<a href="/pdf/2402.14215" title="Download PDF">pdf</a>, <a href="/format/2402.14215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu-Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yu-Xiao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data diversity and abundance are essential for improving the performance and
generalization of models in natural language processing and 2D vision. However,
3D vision domain suffers from the lack of 3D data, and simply combining
multiple 3D datasets for pretraining a 3D backbone does not yield significant
improvement, due to the domain discrepancies among different 3D datasets that
impede effective feature learning. In this work, we identify the main sources
of the domain discrepancies between 3D indoor scene datasets, and propose
Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on
multi-source 3D point clouds. Swin3D++ introduces domain-specific mechanisms to
Swin3D's modules to address domain discrepancies and enhance the network
capability on multi-source pretraining. Moreover, we devise a simple
source-augmentation strategy to increase the pretraining data scale and
facilitate supervised pretraining. We validate the effectiveness of our design,
and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining
methods on typical indoor scene understanding tasks. Our code and models will
be released at https://github.com/microsoft/Swin3D
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14220" title="Abstract">arXiv:2402.14220</a> [<a href="/pdf/2402.14220" title="Download PDF">pdf</a>, <a href="/format/2402.14220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Unknown Population Sizes Using the Hypergeometric  Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hodgson%2C+L">Liam Hodgson</a>, 
<a href="/search/cs?searchtype=author&query=Bzdok%2C+D">Danilo Bzdok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">The multivariate hypergeometric distribution describes sampling without
replacement from a discrete population of elements divided into multiple
categories. Addressing a gap in the literature, we tackle the challenge of
estimating discrete distributions when both the total population size and the
sizes of its constituent categories are unknown. Here, we propose a novel
solution using the hypergeometric likelihood to solve this estimation
challenge, even in the presence of severe under-sampling. We develop our
approach to account for a data generating process where the ground-truth is a
mixture of distributions conditional on a continuous latent variable, such as
with collaborative filtering, using the variational autoencoder framework.
Empirical data simulation demonstrates that our method outperforms other
likelihood functions used to model count data, both in terms of accuracy of
population size estimate and in its ability to learn an informative latent
space. We demonstrate our method's versatility through applications in NLP, by
inferring and estimating the complexity of latent vocabularies in text
excerpts, and in biology, by accurately recovering the true number of gene
transcripts from sparse single-cell genomics data.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14221" title="Abstract">arXiv:2402.14221</a> [<a href="/pdf/2402.14221" title="Download PDF">pdf</a>, <a href="/format/2402.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards singular optimality in the presence of local initial knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Hongyan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Pemmaraju%2C+S+V">Sriram V. Pemmaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The Knowledge Till rho CONGEST model is a variant of the classical CONGEST
model of distributed computing in which each vertex v has initial knowledge of
the radius-rho ball centered at v. The most commonly studied variants of the
CONGEST model are KT0 CONGEST in which nodes initially know nothing about their
neighbors and KT1 CONGEST in which nodes initially know the IDs of all their
neighbors. It has been shown that having access to neighbors' IDs (as in the
KT1 CONGEST model) can substantially reduce the message complexity of
algorithms for fundamental problems such as BROADCAST and MST. For example,
King, Kutten, and Thorup (PODC 2015) show how to construct an MST using just
Otilde(n) messages in the KT1 CONGEST model, whereas there is an Omega(m)
message lower bound for MST in the KT0 CONGEST model. Building on this result,
Gmyr and Pandurangen (DISC 2018) present a family of distributed randomized
algorithms for various global problems that exhibit a trade-off between message
and round complexity. These algorithms are based on constructing a sparse,
spanning subgraph called a danner. Specifically, given a graph G and any delta
in [0,1], their algorithm constructs (with high probability) a danner that has
diameter Otilde(D + n^{1-delta}) and Otilde(min{m,n^{1+delta}}) edges in
Otilde(n^{1-delta}) rounds while using Otilde(min{m,n^{1+\delta}}) messages,
where n, m, and D are the number of nodes, edges, and the diameter of G,
respectively. In the main result of this paper, we show that if we assume the
KT2 CONGEST model, it is possible to substantially improve the time-message
trade-off in constructing a danner. Specifically, we show in the KT2 CONGEST
model, how to construct a danner that has diameter Otilde(D + n^{1-2delta}) and
Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-2delta}) rounds while using
Otilde(min{m,n^{1+\delta}}) messages for any delta in [0,1/2].
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14224" title="Abstract">arXiv:2402.14224</a> [<a href="/pdf/2402.14224" title="Download PDF">pdf</a>, <a href="/format/2402.14224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framing in the Presence of Supporting Data: A Case Study in U.S.  Economic News
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leto%2C+A">Alexandria Leto</a>, 
<a href="/search/cs?searchtype=author&query=Pickens%2C+E">Elliot Pickens</a>, 
<a href="/search/cs?searchtype=author&query=Needell%2C+C+D">Coen D. Needell</a>, 
<a href="/search/cs?searchtype=author&query=Rothschild%2C+D">David Rothschild</a>, 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+M+L">Maria Leonor Pacheco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> total pages: 19; main body pages: 8; total figures: 19 submitted to Association for Computational Linguistics (ACL 2024), February 15th deadline
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The mainstream media has much leeway in what it chooses to cover and how it
covers it. These choices have real-world consequences on what people know and
their subsequent behaviors. However, the lack of objective measures to evaluate
editorial choices makes research in this area particularly difficult. In this
paper, we argue that there are newsworthy topics where objective measures exist
in the form of supporting data and propose a computational framework to analyze
editorial choices in this setup. We focus on the economy because the reporting
of economic indicators presents us with a relatively easy way to determine both
the selection and framing of various publications. Their values provide a
ground truth of how the economy is doing relative to how the publications
choose to cover it. To do this, we define frame prediction as a set of
interdependent tasks. At the article level, we learn to identify the reported
stance towards the general state of the economy. Then, for every numerical
quantity reported in the article, we learn to identify whether it corresponds
to an economic indicator and whether it is being reported in a positive or
negative way. To perform our analysis, we track six American publishers and
each article that appeared in the top 10 slots of their landing page between
2015 and 2023.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14227" title="Abstract">arXiv:2402.14227</a> [<a href="/pdf/2402.14227" title="Download PDF">pdf</a>, <a href="/ps/2402.14227" title="Download PostScript">ps</a>, <a href="/format/2402.14227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quaternion recurrent neural network with real-time recurrent learning  and maximum correntropy criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bourigault%2C+P">Pauline Bourigault</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongpo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mandic%2C+D+P">Danilo P. Mandic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We develop a robust quaternion recurrent neural network (QRNN) for real-time
processing of 3D and 4D data with outliers. This is achieved by combining the
real-time recurrent learning (RTRL) algorithm and the maximum correntropy
criterion (MCC) as a loss function. While both the mean square error and
maximum correntropy criterion are viable cost functions, it is shown that the
non-quadratic maximum correntropy loss function is less sensitive to outliers,
making it suitable for applications with multidimensional noisy or uncertain
data. Both algorithms are derived based on the novel generalised HR (GHR)
calculus, which allows for the differentiation of real functions of quaternion
variables and offers the product and chain rules, thus enabling elegant and
compact derivations. Simulation results in the context of motion prediction of
chest internal markers for lung cancer radiotherapy, which includes regular and
irregular breathing sequences, support the analysis.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14228" title="Abstract">arXiv:2402.14228</a> [<a href="/pdf/2402.14228" title="Download PDF">pdf</a>, <a href="/format/2402.14228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPR: Continual Human Preference Learning via Optimal Policy  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yehong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to
improve the alignment of Large Language Models (LLMs) with human preferences.
Given the evolving nature of human preferences, continual alignment becomes
more crucial and practical in comparison to traditional static alignment.
Nevertheless, making RLHF compatible with Continual Learning (CL) is
challenging due to its complex process. Meanwhile, directly learning new human
preferences may lead to Catastrophic Forgetting (CF) of historical preferences,
resulting in helpless or harmful outputs. To overcome these challenges, we
propose the Continual Optimal Policy Regularization (COPR) method, which draws
inspiration from the optimal policy theory. COPR utilizes a sampling
distribution as a demonstration and regularization constraints for CL. It
adopts the Lagrangian Duality (LD) method to dynamically regularize the current
policy based on the historically optimal policy, which prevents CF and avoids
over-emphasizing unbalanced objectives. We also provide formal proof for the
learnability of COPR. The experimental results show that COPR outperforms
strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4
evaluations and human assessment. Furthermore, we validate the robustness of
COPR under various CL settings, including different backbones, replay memory
sizes, and learning orders.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14230" title="Abstract">arXiv:2402.14230</a> [<a href="/pdf/2402.14230" title="Download PDF">pdf</a>, <a href="/format/2402.14230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MerRec: A Large-scale Multipurpose Mercari Dataset for  Consumer-to-Consumer Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lichi Li</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+Z+A">Zainul Abi Din</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=London%2C+S">Sam London</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Daptardar%2C+A">Ajay Daptardar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the evolving e-commerce field, recommendation systems crucially shape user
experience and engagement. The rise of Consumer-to-Consumer (C2C)
recommendation systems, noted for their flexibility and ease of access for
customer vendors, marks a significant trend. However, the academic focus
remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by
the limited C2C recommendation datasets that lack in item attributes, user
diversity, and scale. The intricacy of C2C recommendation systems is further
accentuated by the dual roles users assume as both sellers and buyers,
introducing a spectrum of less uniform and varied inputs. Addressing this, we
introduce MerRec, the first large-scale dataset specifically for C2C
recommendations, sourced from the Mercari e-commerce platform, covering
millions of users and products over 6 months in 2023. MerRec not only includes
standard features such as user_id, item_id, and session_id, but also unique
elements like timestamped action types, product taxonomy, and textual product
attributes, offering a comprehensive dataset for research. This dataset,
extensively evaluated across six recommendation tasks, establishes a new
benchmark for the development of advanced recommendation algorithms in
real-world scenarios, bridging the gap between academia and industry and
propelling the study of C2C recommendations.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14233" title="Abstract">arXiv:2402.14233</a> [<a href="/pdf/2402.14233" title="Download PDF">pdf</a>, <a href="/format/2402.14233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stand-Up Indulgent Gathering on Rings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bramas%2C+Q">Quentin Bramas</a>, 
<a href="/search/cs?searchtype=author&query=Kamei%2C+S">Sayaka Kamei</a>, 
<a href="/search/cs?searchtype=author&query=Lamani%2C+A">Anissa Lamani</a>, 
<a href="/search/cs?searchtype=author&query=Tixeuil%2C+S">S&#xe9;bastien Tixeuil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We consider a collection of $k \geq 2$ robots that evolve in a ring-shaped
network without common orientation, and address a variant of the crash-tolerant
gathering problem called the \emph{Stand-Up Indulgent Gathering} (SUIG): given
a collection of robots, if no robot crashes, robots have to meet at the same
arbitrary location, not known beforehand, in finite time; if one robot or more
robots crash on the same location, the remaining correct robots gather at the
location of the crashed robots. We aim at characterizing the solvability of the
SUIG problem without multiplicity detection capability.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14236" title="Abstract">arXiv:2402.14236</a> [<a href="/pdf/2402.14236" title="Download PDF">pdf</a>, <a href="/format/2402.14236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Design and Optimization of Distributed Filtering Circuits via  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ru-Yue Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Designing distributed filtering circuits (DFCs) is complex and
time-consuming, with the circuit performance relying heavily on the expertise
and experience of electronics engineers. However, manual design methods tend to
have exceedingly low-efficiency. This study proposes a novel end-to-end
automated method for fabricating circuits to improve the design of DFCs. The
proposed method harnesses reinforcement learning (RL) algorithms, eliminating
the dependence on the design experience of engineers. Thus, it significantly
reduces the subjectivity and constraints associated with circuit design. The
experimental findings demonstrate clear improvements in both design efficiency
and quality when comparing the proposed method with traditional engineer-driven
methods. In particular, the proposed method achieves superior performance when
designing complex or rapidly evolving DFCs. Furthermore, compared to existing
circuit automation design techniques, the proposed method demonstrates superior
design efficiency, highlighting the substantial potential of RL in circuit
design automation.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14241" title="Abstract">arXiv:2402.14241</a> [<a href="/pdf/2402.14241" title="Download PDF">pdf</a>, <a href="/ps/2402.14241" title="Download PostScript">ps</a>, <a href="/format/2402.14241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Self-supervised Pressure Map human keypoint Detection Approch:  Optimizing Generalization and Computational Efficiency Across Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengzhang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wenxia Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhiming Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5pages, 6figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In environments where RGB images are inadequate, pressure maps is a viable
alternative, garnering scholarly attention. This study introduces a novel
self-supervised pressure map keypoint detection (SPMKD) method, addressing the
current gap in specialized designs for human keypoint extraction from pressure
maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model,
which is a robust framework that integrates a lightweight encoder for precise
human keypoint detection, a fuser for efficient gradient propagation, and a
decoder that transforms human keypoints into reconstructed pressure maps. This
structure is further enhanced by the Classification-to-Regression Weight
Transfer (CRWT) method, which fine-tunes accuracy through initial
classification task training. This innovation not only enhances human keypoint
generalization without manual annotations but also showcases remarkable
efficiency and generalization, evidenced by a reduction to only $5.96\%$ in
FLOPs and $1.11\%$ in parameter count compared to the baseline methods.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14244" title="Abstract">arXiv:2402.14244</a> [<a href="/pdf/2402.14244" title="Download PDF">pdf</a>, <a href="/format/2402.14244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback  and Dynamic Distance Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinglin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shaofu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Hierarchical reinforcement learning (HRL) provides a promising solution for
complex tasks with sparse rewards of intelligent agents, which uses a
hierarchical framework that divides tasks into subgoals and completes them
sequentially. However, current methods struggle to find suitable subgoals for
ensuring a stable learning process. Without additional guidance, it is
impractical to rely solely on exploration or heuristics methods to determine
subgoals in a large goal space. To address the issue, We propose a general
hierarchical reinforcement learning framework incorporating human feedback and
dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating
human feedback into high-level policy learning, to find better subgoals. As for
low-level policy, MENTOR designs a dual policy for exploration-exploitation
decoupling respectively to stabilize the training. Furthermore, although humans
can simply break down tasks into subgoals to guide the right learning
direction, subgoals that are too difficult or too easy can still hinder
downstream learning efficiency. We propose the Dynamic Distance Constraint
(DDC) mechanism dynamically adjusting the space of optional subgoals. Thus
MENTOR can generate subgoals matching the low-level policy learning process
from easy to hard. Extensive experiments demonstrate that MENTOR uses a small
amount of human feedback to achieve significant improvement in complex tasks
with sparse rewards.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14245" title="Abstract">arXiv:2402.14245</a> [<a href="/pdf/2402.14245" title="Download PDF">pdf</a>, <a href="/format/2402.14245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Robotic Manipulation with AI Feedback from Multimodal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Lingzhi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yibin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at AAAI 2024 RL+LLMs Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, there has been considerable attention towards leveraging large
language models (LLMs) to enhance decision-making processes. However, aligning
the natural language text instructions generated by LLMs with the vectorized
operations required for execution presents a significant challenge, often
necessitating task-specific details. To circumvent the need for such
task-specific granularity, inspired by preference-based policy learning
approaches, we investigate the utilization of multimodal LLMs to provide
automated preference feedback solely from image inputs to guide
decision-making. In this study, we train a multimodal LLM, termed CriticGPT,
capable of understanding trajectory videos in robot manipulation tasks, serving
as a critic to offer analysis and preference feedback. Subsequently, we
validate the effectiveness of preference labels generated by CriticGPT from a
reward modeling perspective. Experimental evaluation of the algorithm's
preference accuracy demonstrates its effective generalization ability to new
tasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's
reward model efficiently guides policy learning, surpassing rewards based on
state-of-the-art pre-trained representation models.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14246" title="Abstract">arXiv:2402.14246</a> [<a href="/pdf/2402.14246" title="Download PDF">pdf</a>, <a href="/format/2402.14246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction-Based Anomaly Localization via Knowledge-Informed  Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+X">Xiaoxian Lao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunguang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Anomaly localization, which involves localizing anomalous regions within
images, is a significant industrial task. Reconstruction-based methods are
widely adopted for anomaly localization because of their low complexity and
high interpretability. Most existing reconstruction-based methods only use
normal samples to construct model. If anomalous samples are appropriately
utilized in the process of anomaly localization, the localization performance
can be improved. However, usually only weakly labeled anomalous samples are
available, which limits the improvement. In many cases, we can obtain some
knowledge of anomalies summarized by domain experts. Taking advantage of such
knowledge can help us better utilize the anomalous samples and thus further
improve the localization performance. In this paper, we propose a novel
reconstruction-based method named knowledge-informed self-training (KIST) which
integrates knowledge into reconstruction model through self-training.
Specifically, KIST utilizes weakly labeled anomalous samples in addition to the
normal ones and exploits knowledge to yield pixel-level pseudo-labels of the
anomalous samples. Based on the pseudo labels, a novel loss which promotes the
reconstruction of normal pixels while suppressing the reconstruction of
anomalous pixels is used. We conduct experiments on different datasets and
demonstrate the advantages of KIST over the existing reconstruction-based
methods.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14251" title="Abstract">arXiv:2402.14251</a> [<a href="/pdf/2402.14251" title="Download PDF">pdf</a>, <a href="/format/2402.14251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Interaction Situated: Designing User Acceptable Interaction for  Situated Visualization in Public Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wai%2C+T">Tong Wai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weiyue Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojuan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI 2024 full paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHI 2024 Proceedings of the CHI Conference on Human Factors in
  Computing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Situated visualization blends data into the real world to fulfill
individuals' contextual information needs. However, interacting with situated
visualization in public environments faces challenges posed by user acceptance
and contextual constraints. To explore appropriate interaction design, we first
conduct a formative study to identify user needs for data and interaction.
Informed by the findings, we summarize appropriate interaction modalities with
eye-based, hand-based and spatially-aware object interaction for situated
visualization in public environments. Then, through an iterative design process
with six users, we explore and implement interactive techniques for activating
and analyzing with situated visualization. To assess the effectiveness and
acceptance of these interactions, we integrate them into an AR prototype and
conduct a within-subjects study in public scenarios using conventional
hand-only interactions as the baseline. The results show that participants
preferred our prototype over the baseline, attributing their preference to the
interactions being more acceptable, flexible, and practical in public.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14252" title="Abstract">arXiv:2402.14252</a> [<a href="/pdf/2402.14252" title="Download PDF">pdf</a>, <a href="/format/2402.14252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Healthcare AI: Identifying and Designing Clinically Relevant  Vision-Language Applications for Radiology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+N">Nur Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+H">Hannah Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Wetscherek%2C+M+T">Maria T. Wetscherek</a>, 
<a href="/search/cs?searchtype=author&query=Bajwa%2C+J">Junaid Bajwa</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+J">Joseph Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Pinnock%2C+M+A">Mark A. Pinnock</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+S">Stephen Harris</a>, 
<a href="/search/cs?searchtype=author&query=de+Castro%2C+D+C">Daniel Coelho de Castro</a>, 
<a href="/search/cs?searchtype=author&query=Bannur%2C+S">Shruthi Bannur</a>, 
<a href="/search/cs?searchtype=author&query=Hyland%2C+S+L">Stephanie L. Hyland</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+P">Pratik Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Ranjit%2C+M">Mercy Ranjit</a>, 
<a href="/search/cs?searchtype=author&query=Bouzid%2C+K">Kenza Bouzid</a>, 
<a href="/search/cs?searchtype=author&query=Schwaighofer%2C+A">Anton Schwaighofer</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Garc%C3%ADa%2C+F">Fernando P&#xe9;rez-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+H">Harshita Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Oktay%2C+O">Ozan Oktay</a>, 
<a href="/search/cs?searchtype=author&query=Lungren%2C+M">Matthew Lungren</a>, 
<a href="/search/cs?searchtype=author&query=Alvarez-Valle%2C+J">Javier Alvarez-Valle</a>, 
<a href="/search/cs?searchtype=author&query=Nori%2C+A">Aditya Nori</a>, 
<a href="/search/cs?searchtype=author&query=Thieme%2C+A">Anja Thieme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recent advances in AI combine large language models (LLMs) with vision
encoders that bring forward unprecedented technical capabilities to leverage
for a wide range of healthcare applications. Focusing on the domain of
radiology, vision-language models (VLMs) achieve good performance results for
tasks such as generating radiology findings based on a patient's medical image,
or answering visual questions (e.g., 'Where are the nodules in this chest
X-ray?'). However, the clinical utility of potential applications of these
capabilities is currently underexplored. We engaged in an iterative,
multidisciplinary design process to envision clinically relevant VLM
interactions, and co-designed four VLM use concepts: Draft Report Generation,
Augmented Report Review, Visual Search and Querying, and Patient Imaging
History Highlights. We studied these concepts with 13 radiologists and
clinicians who assessed the VLM concepts as valuable, yet articulated many
design considerations. Reflecting on our findings, we discuss implications for
integrating VLM capabilities in radiology, and for healthcare AI more
generally.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14253" title="Abstract">arXiv:2402.14253</a> [<a href="/pdf/2402.14253" title="Download PDF">pdf</a>, <a href="/format/2402.14253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin-Yang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yu-Xiao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xin Tong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">As a promising 3D generation technique, multiview diffusion (MVD) has
received a lot of attention due to its advantages in terms of generalizability,
quality, and efficiency. By finetuning pretrained large image diffusion models
with 3D data, the MVD methods first generate multiple views of a 3D object
based on an image or text prompt and then reconstruct 3D shapes with multiview
3D reconstruction. However, the sparse views and inconsistent details in the
generated images make 3D reconstruction challenging. We present MVD$^2$, an
efficient 3D reconstruction method for multiview diffusion (MVD) images.
MVD$^2$ aggregates image features into a 3D feature volume by projection and
convolution and then decodes volumetric features into a 3D mesh. We train
MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of
3D shapes. To address the discrepancy between the generated multiview images
and ground-truth views of the 3D shapes, we design a simple-yet-efficient
view-dependent training scheme. MVD$^2$ improves the 3D generation quality of
MVD and is fast and robust to various MVD methods. After training, it can
efficiently decode 3D meshes from multiview images within one second. We train
MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its
superior performance in generating 3D models from multiview images generated by
different MVD methods, using both synthetic and real images as prompts.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14254" title="Abstract">arXiv:2402.14254</a> [<a href="/pdf/2402.14254" title="Download PDF">pdf</a>, <a href="/format/2402.14254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hierarchical decomposition for explaining ML performance discrepancies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jean Feng</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harvineet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Subbaswamy%2C+A">Adarsh Subbaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Gossmann%2C+A">Alexej Gossmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures in main body; 14 pages and 2 figures in appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Machine learning (ML) algorithms can often differ in performance across
domains. Understanding $\textit{why}$ their performance differs is crucial for
determining what types of interventions (e.g., algorithmic or operational) are
most effective at closing the performance gaps. Existing methods focus on
$\textit{aggregate decompositions}$ of the total performance gap into the
impact of a shift in the distribution of features $p(X)$ versus the impact of a
shift in the conditional distribution of the outcome $p(Y|X)$; however, such
coarse explanations offer only a few options for how one can close the
performance gap. $\textit{Detailed variable-level decompositions}$ that
quantify the importance of each variable to each term in the aggregate
decomposition can provide a much deeper understanding and suggest much more
targeted interventions. However, existing methods assume knowledge of the full
causal graph or make strong parametric assumptions. We introduce a
nonparametric hierarchical framework that provides both aggregate and detailed
decompositions for explaining why the performance of an ML algorithm differs
across domains, without requiring causal knowledge. We derive debiased,
computationally-efficient estimators, and statistical inference procedures for
asymptotically valid confidence intervals.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14258" title="Abstract">arXiv:2402.14258</a> [<a href="/pdf/2402.14258" title="Download PDF">pdf</a>, <a href="/format/2402.14258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eagle: Ethical Dataset Given from Real Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent studies have demonstrated that large language models (LLMs) have
ethical-related problems such as social biases, lack of moral reasoning, and
generation of offensive content. The existing evaluation metrics and methods to
address these ethical challenges use datasets intentionally created by
instructing humans to create instances including ethical problems. Therefore,
the data does not reflect prompts that users actually provide when utilizing
LLM services in everyday contexts. This may not lead to the development of safe
LLMs that can address ethical challenges arising in real-world applications. In
this paper, we create Eagle datasets extracted from real interactions between
ChatGPT and users that exhibit social biases, toxicity, and immoral problems.
Our experiments show that Eagle captures complementary aspects, not covered by
existing datasets proposed for evaluation and mitigation of such ethical
challenges. Our code is publicly available at
https://huggingface.co/datasets/MasahiroKaneko/eagle.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14259" title="Abstract">arXiv:2402.14259</a> [<a href="/pdf/2402.14259" title="Download PDF">pdf</a>, <a href="/format/2402.14259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form  Medical Question Answering Applications and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinhao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenxi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaidi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoshuang Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Uncertainty estimation plays a pivotal role in ensuring the reliability of
safety-critical human-AI interaction systems, particularly in the medical
domain. However, a general method for quantifying the uncertainty of free-form
answers has yet to be established in open-ended medical question-answering (QA)
tasks, where irrelevant words and sequences with limited semantic information
can be the primary source of uncertainty due to the presence of generative
inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which
calibrates the uncertainty proportion at both the word and sequence levels
according to the semantic relevance, with greater emphasis placed on keywords
and more relevant sequences when performing uncertainty quantification. We
compare WSE with 6 baseline methods on 5 free-form medical QA datasets,
utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE
exhibits superior performance on accurate uncertainty measurement under two
standard criteria for correctness evaluation (e.g., WSE outperforms existing
state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in
terms of the potential for real-world medical QA applications, we achieve a
significant enhancement in the performance of LLMs when employing sequences
with lower uncertainty, identified by WSE, as final answers (e.g., +6.36%
accuracy improvement on the COVID-QA dataset), without requiring any additional
task-specific fine-tuning or architectural modifications.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14261" title="Abstract">arXiv:2402.14261</a> [<a href="/pdf/2402.14261" title="Download PDF">pdf</a>, <a href="/format/2402.14261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Anisha Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Aaron Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chandel%2C+S">Shubham Chandel</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jinu Jang</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+S">Shaun Miller</a>, 
<a href="/search/cs?searchtype=author&query=Moghaddam%2C+R+Z">Roshanak Zilouchian Moghaddam</a>, 
<a href="/search/cs?searchtype=author&query=Mohylevskyy%2C+Y">Yevhen Mohylevskyy</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+N">Neel Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Tufano%2C+M">Michele Tufano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The integration of Large Language Models (LLMs) into Development Environments
(IDEs) has become a focal point in modern software development. LLMs such as
OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment
developer productivity by serving as intelligent, chat-driven programming
assistants. However, utilizing LLMs out of the box is unlikely to be optimal
for any given scenario. Rather, each system requires the LLM to be honed to its
set of heuristics to ensure the best performance. In this paper, we introduce
the Copilot evaluation harness: a set of data and tools for evaluating
LLM-guided IDE interactions, covering various programming scenarios and
languages. We propose our metrics as a more robust and information-dense
evaluation than previous state of the art evaluation systems. We design and
compute both static and execution based success metrics for scenarios
encompassing a wide range of developer tasks, including code generation from
natural language (generate), documentation generation from code (doc), test
case generation (test), bug-fixing (fix), and workspace understanding and query
resolution (workspace). These success metrics are designed to evaluate the
performance of LLMs within a given IDE and its respective parameter space. Our
learnings from evaluating three common LLMs using these metrics can inform the
development and validation of future scenarios in LLM guided IDEs.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14266" title="Abstract">arXiv:2402.14266</a> [<a href="/pdf/2402.14266" title="Download PDF">pdf</a>, <a href="/format/2402.14266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Solvers for Wyner Common Information with Application to  Multi-Modal Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Teng-Hui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+H+E">Hesham El Gamal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We propose two novel extensions of the Wyner common information optimization
problem. Each relaxes one fundamental constraints in Wyner's formulation. The
\textit{Variational Wyner Common Information} relaxes the matching constraint
to the known distribution while imposing conditional independence to the
feasible solution set. We derive a tight surrogate upper bound of the obtained
unconstrained Lagrangian via the theory of variational inference, which can be
minimized efficiently. Our solver caters to problems where conditional
independence holds with significantly reduced computation complexity; On the
other hand, the \textit{Bipartite Wyner Common Information} relaxes the
conditional independence constraint whereas the matching condition is enforced
on the feasible set. By leveraging the difference-of-convex structure of the
formulated optimization problem, we show that our solver is resilient to
conditional dependent sources. Both solvers are provably convergent (local
stationary points), and empirically, they obtain more accurate solutions to
Wyner's formulation with substantially less runtime. Moreover, them can be
extended to unknown distribution settings by parameterizing the common
randomness as a member of the exponential family of distributions. Our
approaches apply to multi-modal clustering problems, where multiple modalities
of observations come from the same cluster. Empirically, our solvers outperform
the state-of-the-art multi-modal clustering algorithms with significantly
improved performance.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14268" title="Abstract">arXiv:2402.14268</a> [<a href="/pdf/2402.14268" title="Download PDF">pdf</a>, <a href="/format/2402.14268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Detect Misinformation in Scientific News  Reporting?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yupeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+A+M">Aishwarya Muralidharan Nair</a>, 
<a href="/search/cs?searchtype=author&query=Eyimife%2C+E">Elyon Eyimife</a>, 
<a href="/search/cs?searchtype=author&query=Soofi%2C+N+J">Nastaran Jamalipour Soofi</a>, 
<a href="/search/cs?searchtype=author&query=Subbalakshmi%2C+K+P">K.P. Subbalakshmi</a>, 
<a href="/search/cs?searchtype=author&query=Wullert%2C+J+R">John R. Wullert II</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+C">Chumki Basu</a>, 
<a href="/search/cs?searchtype=author&query=Shallcross%2C+D">David Shallcross</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Scientific facts are often spun in the popular press with the intent to
influence public opinion and action, as was evidenced during the COVID-19
pandemic. Automatic detection of misinformation in the scientific domain is
challenging because of the distinct styles of writing in these two media types
and is still in its nascence. Most research on the validity of scientific
reporting treats this problem as a claim verification challenge. In doing so,
significant expert human effort is required to generate appropriate claims. Our
solution bypasses this step and addresses a more real-world scenario where such
explicit, labeled claims may not be available. The central research question of
this paper is whether it is possible to use large language models (LLMs) to
detect misinformation in scientific reporting. To this end, we first present a
new labeled dataset SciNews, containing 2.4k scientific news stories drawn from
trusted and untrustworthy sources, paired with related abstracts from the
CORD-19 database. Our dataset includes both human-written and LLM-generated
news articles, making it more comprehensive in terms of capturing the growing
trend of using LLMs to generate popular press articles. Then, we identify
dimensions of scientific validity in science news articles and explore how this
can be integrated into the automated detection of scientific misinformation. We
propose several baseline architectures using LLMs to automatically detect false
representations of scientific findings in the popular press. For each of these
architectures, we use several prompt engineering strategies including
zero-shot, few-shot, and chain-of-thought prompting. We also test these
architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,
Llama2-13B.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14269" title="Abstract">arXiv:2402.14269</a> [<a href="/pdf/2402.14269" title="Download PDF">pdf</a>, <a href="/format/2402.14269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Mechanism in a Dynamic Stochastic Knapsack Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jihyeok Jung</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chan-Oi Song</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Deok-Joo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kiho Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figures, presented in AAAI 38th conference on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">This study introduces an optimal mechanism in a dynamic stochastic knapsack
environment. The model features a single seller who has a fixed quantity of a
perfectly divisible item. Impatient buyers with a piece-wise linear utility
function arrive randomly and they report the two-dimensional private
information: marginal value and demanded quantity. We derive a
revenue-maximizing dynamic mechanism in a finite discrete time framework that
satisfies incentive compatibility, individual rationality, and feasibility
conditions. It is achieved by characterizing buyers' utility and deriving the
Bellman equation. Moreover, we propose the essential penalty scheme for
incentive compatibility, as well as the allocation and payment policies.
Lastly, we propose algorithms to approximate the optimal policy, based on the
Monte Carlo simulation-based regression method and reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14270" title="Abstract">arXiv:2402.14270</a> [<a href="/pdf/2402.14270" title="Download PDF">pdf</a>, <a href="/format/2402.14270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Take the Bull by the Horns: Hard Sample-Reweighted Continual Training  Improves LLM Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sow%2C+D">Daouda Sow</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the rapidly advancing arena of large language models (LLMs), a key
challenge is to enhance their capabilities amid a looming shortage of
high-quality training data. Our study starts from an empirical strategy for the
light continual training of LLMs using their original pre-training data sets,
with a specific focus on selective retention of samples that incur moderately
high losses. These samples are deemed informative and beneficial for model
refinement, contrasting with the highest-loss samples, which would be discarded
due to their correlation with data noise and complexity. We then formalize this
strategy into a principled framework of Instance-Reweighted Distributionally
Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the
training focus on informative samples through an instance reweighting
mechanism, streamlined by a closed-form solution for straightforward
integration into established training protocols. Through rigorous
experimentation with various models and datasets, our findings indicate that
our sample-targeted methods significantly improve LLM performance across
multiple benchmarks, in both continual pre-training and instruction tuning
scenarios. Our codes are available at
https://github.com/VITA-Group/HardFocusTraining.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14272" title="Abstract">arXiv:2402.14272</a> [<a href="/pdf/2402.14272" title="Download PDF">pdf</a>, <a href="/format/2402.14272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qsnail: A Questionnaire Dataset for Sequential Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The questionnaire is a professional research methodology used for both
qualitative and quantitative analysis of human opinions, preferences,
attitudes, and behaviors. However, designing and evaluating questionnaires
demands significant effort due to their intricate and complex structure.
Questionnaires entail a series of questions that must conform to intricate
constraints involving the questions, options, and overall structure.
Specifically, the questions should be relevant and specific to the given
research topic and intent. The options should be tailored to the questions,
ensuring they are mutually exclusive, completed, and ordered sensibly.
Moreover, the sequence of questions should follow a logical order, grouping
similar topics together. As a result, automatically generating questionnaires
presents a significant challenge and this area has received limited attention
primarily due to the scarcity of high-quality datasets. To address these
issues, we present Qsnail, the first dataset specifically constructed for the
questionnaire generation task, which comprises 13,168 human-written
questionnaires gathered from online platforms. We further conduct experiments
on Qsnail, and the results reveal that retrieval models and traditional
generative models do not fully align with the given research topic and intents.
Large language models, while more closely related to the research topic and
intents, exhibit significant limitations in terms of diversity and specificity.
Despite enhancements through the chain-of-thought prompt and finetuning,
questionnaires generated by language models still fall short of human-written
questionnaires. Therefore, questionnaire generation is challenging and needs to
be further explored. The dataset is available at:
https://github.com/LeiyanGithub/qsnail.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14273" title="Abstract">arXiv:2402.14273</a> [<a href="/pdf/2402.14273" title="Download PDF">pdf</a>, <a href="/format/2402.14273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Act as Knowledge Bases at Scale?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qiyuan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating responses to complex queries through large-scale
pre-training. However, the efficacy of these models in memorizing and reasoning
among large-scale structured knowledge, especially world knowledge that
explicitly covers abundant factual information remains questionable. Addressing
this gap, our research investigates whether LLMs can effectively store, recall,
and reason with knowledge on a large scale comparable to latest knowledge bases
(KBs) such as Wikidata. Specifically, we focus on three crucial aspects to
study the viability: (1) the efficiency of LLMs with different sizes in
memorizing the exact knowledge in the large-scale KB; (2) the flexibility of
recalling the memorized knowledge in response to natural language queries; (3)
the capability to infer new knowledge through reasoning. Our findings indicate
that while LLMs hold promise as large-scale KBs capable of retrieving and
responding with flexibility, enhancements in their reasoning capabilities are
necessary to fully realize their potential.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14277" title="Abstract">arXiv:2402.14277</a> [<a href="/pdf/2402.14277" title="Download PDF">pdf</a>, <a href="/format/2402.14277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GATE X-E : A Challenge Set for Gender-Fair Translations from  Weakly-Gendered Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rarrick%2C+S">Spencer Rarrick</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+R">Ranjita Naik</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+S">Sundar Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhary%2C+V">Vishal Chowdhary</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2311.08836">arXiv:2311.08836</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural Machine Translation (NMT) continues to improve in quality and
adoption, yet the inadvertent perpetuation of gender bias remains a significant
concern. Despite numerous studies on gender bias in translations into English
from weakly gendered-languages, there are no benchmarks for evaluating this
phenomenon or for assessing mitigation strategies. To address this gap, we
introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus,
that consists of human translations from Turkish, Hungarian, Finnish, and
Persian into English. Each translation is accompanied by feminine, masculine,
and neutral variants. The dataset, which contains between 1250 and 1850
instances for each of the four language pairs, features natural sentences with
a wide range of sentence lengths and domains, challenging translation rewriters
on various linguistic phenomena. Additionally, we present a translation gender
rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open
source our contributions to encourage further research on gender debiasing.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14278" title="Abstract">arXiv:2402.14278</a> [<a href="/pdf/2402.14278" title="Download PDF">pdf</a>, <a href="/format/2402.14278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality Bounds for Sampling Hamming Slices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kane%2C+D+M">Daniel M. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Ostuni%2C+A">Anthony Ostuni</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kewen Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in STOC24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Spurred by the influential work of Viola (Journal of Computing 2012), the
past decade has witnessed an active line of research into the complexity of
(approximately) sampling distributions, in contrast to the traditional focus on
the complexity of computing functions.
<br />We build upon and make explicit earlier implicit results of Viola to provide
superconstant lower bounds on the locality of Boolean functions approximately
sampling the uniform distribution over binary strings of particular Hamming
weights, both exactly and modulo an integer, answering questions of Viola
(Journal of Computing 2012) and Filmus, Leigh, Riazanov, and Sokolov (RANDOM
2023). Applications to data structure lower bounds and quantum-classical
separations are discussed.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14279" title="Abstract">arXiv:2402.14279</a> [<a href="/pdf/2402.14279" title="Download PDF">pdf</a>, <a href="/format/2402.14279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating the Linguistic Gap with Phonemic Representations for Robust  Multilingual Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Haeji Jung</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changdae Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jooeon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+J">Jimin Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kyungwoo Song</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D+R">David R. Mortensen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Approaches to improving multilingual language understanding often require
multiple languages during the training phase, rely on complicated training
techniques, and -- importantly -- struggle with significant performance gaps
between high-resource and low-resource languages. We hypothesize that the
performance gaps between languages are affected by linguistic gaps between
those languages and provide a novel solution for robust multilingual language
modeling by employing phonemic representations (specifically, using phonemes as
input tokens to LMs rather than subwords). We present quantitative evidence
from three cross-lingual tasks that demonstrate the effectiveness of phonemic
representation, which is further justified by a theoretical analysis of the
cross-lingual performance gap.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14280" title="Abstract">arXiv:2402.14280</a> [<a href="/pdf/2402.14280" title="Download PDF">pdf</a>, <a href="/format/2402.14280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Navigation using Landmark-based Localization in a GPS-denied  Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+G">Ganesh Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Madria%2C+S">Sanjay Madria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In modern battlefield scenarios, the reliance on GPS for navigation can be a
critical vulnerability. Adversaries often employ tactics to deny or deceive GPS
signals, necessitating alternative methods for the localization and navigation
of mobile troops. Range-free localization methods such as DV-HOP rely on
radio-based anchors and their average hop distance which suffers from accuracy
and stability in a dynamic and sparse network topology. Vision-based approaches
like SLAM and Visual Odometry use sensor fusion techniques for map generation
and pose estimation that are more sophisticated and computationally expensive.
This paper proposes a novel framework that integrates landmark-based
localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the
future state of moving entities along the battlefield. Our framework utilizes
safe trajectory information generated by the troop control center by
considering identifiable landmarks and pre-defined hazard maps. It performs
point inclusion tests on the convex hull of the trajectory segments to ensure
the safety and survivability of a moving entity and determines the next point
forward decisions. We present a simulated battlefield scenario for two
different approaches (with EKF and without EKF) that guide a moving entity
through an obstacle and hazard-free path. Using the proposed method, we
observed a percent error of 6.51% lengthwise in safe trajectory estimation with
an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error
(FDE) of 3.27m. The results demonstrate that our approach not only ensures the
safety of the mobile units by keeping them within the secure trajectory but
also enhances operational effectiveness by adapting to the evolving threat
landscape.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14281" title="Abstract">arXiv:2402.14281</a> [<a href="/pdf/2402.14281" title="Download PDF">pdf</a>, <a href="/format/2402.14281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Landmark-Aware Visual Navigation Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+F">Faith Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+B+B">Bryan Bo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dana%2C+K">Kristin Dana</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Ashwin Ashok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Map representation learned by expert demonstrations has shown promising
research value. However, recent advancements in the visual navigation field
face challenges due to the lack of human datasets in the real world for
efficient supervised representation learning of the environments. We present a
Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised
learning of human-centric exploration policies and map building. We collect RGB
observation and human point-click pairs as a human annotator explores virtual
and real-world environments with the goal of full coverage exploration of the
space. The human annotators also provide distinct landmark examples along each
trajectory, which we intuit will simplify the task of map or graph building and
localization. These human point-clicks serve as direct supervision for waypoint
prediction when learning to explore in environments. Our dataset covers a wide
spectrum of scenes, including rooms in indoor environments, as well as walkways
outdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14285" title="Abstract">arXiv:2402.14285</a> [<a href="/pdf/2402.14285" title="Download PDF">pdf</a>, <a href="/format/2402.14285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ghatare%2C+A">Adishree Ghatare</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+C+S">Chandramouli S Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Gururani%2C+S">Siddharth Gururani</a>, 
<a href="/search/cs?searchtype=author&query=Oore%2C+S">Sageev Oore</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yisong Yue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We study the problem of symbolic music generation (e.g., generating piano
rolls), with a technical focus on non-differentiable rule guidance. Musical
rules are often expressed in symbolic form on note characteristics, such as
note density or chord progression, many of which are non-differentiable which
pose a challenge when using them for guided diffusion. We propose Stochastic
Control Guidance (SCG), a novel guidance method that only requires forward
evaluation of rule functions that can work with pre-trained diffusion models in
a plug-and-play way, thus achieving training-free guidance for
non-differentiable rules for the first time. Additionally, we introduce a
latent diffusion architecture for symbolic music generation with high time
resolution, which can be composed with SCG in a plug-and-play fashion. Compared
to standard strong baselines in symbolic music generation, this framework
demonstrates marked advancements in music quality and rule-based
controllability, outperforming current state-of-the-art generators in a variety
of settings. For detailed demonstrations, please visit our project site:
https://scg-rule-guided-music.github.io/.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14289" title="Abstract">arXiv:2402.14289</a> [<a href="/pdf/2402.14289" title="Download PDF">pdf</a>, <a href="/format/2402.14289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TinyLLaVA: A Framework of Small-scale Large Multimodal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Baichuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Ying Hu</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+X">Xi Weng</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Junlong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xien Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Ji Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our model weights and codes will be made public at <a href="https://github.com/DLCV-BUAA/TinyLLaVABench">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We present the TinyLLaVA framework that provides a unified perspective in
designing and analyzing the small-scale Large Multimodal Models (LMMs). We
empirically study the effects of different vision encoders, connection modules,
language models, training data and training recipes. Our extensive experiments
showed that better quality of data combined with better training recipes,
smaller LMMs can consistently achieve on-par performances compared to bigger
LMMs. Under our framework, we train a family of small-scale LMMs. Our best
model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B
models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as
baselines for future research in terms of data scaling, training setups and
model selections. Our model weights and codes will be made public.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14290" title="Abstract">arXiv:2402.14290</a> [<a href="/pdf/2402.14290" title="Download PDF">pdf</a>, <a href="/format/2402.14290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEV-LM: Controlled Edit Vector Language Model for Shaping Natural  Language Generations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moorjani%2C+S">Samraj Moorjani</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+A">Adit Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+H">Hari Sundaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, accepted into EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As large-scale language models become the standard for text generation, there
is a greater need to tailor the generations to be more or less concise,
targeted, and informative, depending on the audience/application. Existing
control approaches primarily adjust the semantic (e.g., emotion, topics),
structural (e.g., syntax tree, parts-of-speech), and lexical (e.g.,
keyword/phrase inclusion) properties of text, but are insufficient to
accomplish complex objectives such as pacing which control the complexity and
readability of the text. In this paper, we introduce CEV-LM - a lightweight,
semi-autoregressive language model that utilizes constrained edit vectors to
control three complementary metrics (speed, volume, and circuitousness) that
quantify the shape of text (e.g., pacing of content). We study an extensive set
of state-of-the-art CTG models and find that CEV-LM provides significantly more
targeted and precise control of these three metrics while preserving semantic
content, using less training data, and containing fewer parameters.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14292" title="Abstract">arXiv:2402.14292</a> [<a href="/pdf/2402.14292" title="Download PDF">pdf</a>, <a href="/format/2402.14292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saharaline: A Collective Social Support Intervention for Teachers in  Low-Income Indian Schools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varanasi%2C+R+A">Rama Adithya Varanasi</a>, 
<a href="/search/cs?searchtype=author&query=Vashistha%2C+A">Aditya Vashistha</a>, 
<a href="/search/cs?searchtype=author&query=Dell%2C+N">Nicola Dell</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Conference on Human Factors in Computing Systems (CHI2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper presents Saharaline, an intervention designed to provide
collective social support for teachers in low-income schools. Implemented as a
WhatsApp-based helpline, Saharaline enables teachers to reach out for
personalized, long-term assistance with a wide range of problems and stressors,
including pedagogical, emotional, and technological challenges. Depending on
the support needed, teachers' requests are routed to appropriate domain experts
-- staff employed by educational non-profit organizations who understand
teachers' on-the-ground realities -- who offer localized and contextualized
assistance. Via a three-month exploratory deployment with 28 teachers in India,
we show how Saharaline's design enabled a collective of diverse education
experts to craft and deliver localized solutions that teachers could
incorporate into their practice. We conclude by reflecting on the efficacy of
our intervention in low-resource work contexts and provide recommendations to
enhance collective social support interventions similar to Saharaline.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14293" title="Abstract">arXiv:2402.14293</a> [<a href="/pdf/2402.14293" title="Download PDF">pdf</a>, <a href="/format/2402.14293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Concept Graph Recovery and Question  Answering in NLP Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Boming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Sixun Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+T">Tianwei She</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+A">Aosong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lecue%2C+F">Freddy Lecue</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the domain of Natural Language Processing (NLP), Large Language Models
(LLMs) have demonstrated promise in text-generation tasks. However, their
educational applications, particularly for domain-specific queries, remain
underexplored. This study investigates LLMs' capabilities in educational
scenarios, focusing on concept graph recovery and question-answering (QA). We
assess LLMs' zero-shot performance in creating domain-specific concept graphs
and introduce TutorQA, a new expert-verified NLP-focused benchmark for
scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA
pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating
concept graphs with LLMs for answering diverse questions. Our results indicate
that LLMs' zero-shot concept graph recovery is competitive with supervised
methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs
achieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis
show that CGLLM generates answers with more fine-grained concepts.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14294" title="Abstract">arXiv:2402.14294</a> [<a href="/pdf/2402.14294" title="Download PDF">pdf</a>, <a href="/format/2402.14294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-arity PAC learning via exchangeability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coregliano%2C+L+N">Leonardo N. Coregliano</a>, 
<a href="/search/cs?searchtype=author&query=Malliaris%2C+M">Maryanthe Malliaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 145 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic (math.LO); Statistics Theory (math.ST)

</div>
<p class="mathjax">We develop a theory of high-arity PAC learning, which is statistical learning
in the presence of "structured correlation". In this theory, hypotheses are
either graphs, hypergraphs or, more generally, structures in finite relational
languages, and i.i.d. sampling is replaced by sampling an induced substructure,
producing an exchangeable distribution. We prove a high-arity version of the
fundamental theorem of statistical learning by characterizing high-arity
(agnostic) PAC learnability in terms of finiteness of a purely combinatorial
dimension and in terms of an appropriate version of uniform convergence.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14296" title="Abstract">arXiv:2402.14296</a> [<a href="/pdf/2402.14296" title="Download PDF">pdf</a>, <a href="/format/2402.14296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Biases of Large Language Models in Stance Detection with  Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jingqian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved remarkable progress in many
natural language processing tasks. However, our experiment reveals that, in
stance detection tasks, LLMs may generate biased stances due to spurious
sentiment-stance correlation and preference towards certain individuals and
topics, thus harming their performance. Therefore, in this paper, we propose to
Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In
which, a novel gated calibration network is devised to mitigate the biases on
the stance reasoning results from LLMs. Further, to make the calibration more
accurate and generalizable, we construct counterfactual augmented data to
rectify stance biases. Experimental results on in-target and zero-shot stance
detection tasks show that the proposed MB-Cal can effectively mitigate biases
of LLMs, achieving state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14297" title="Abstract">arXiv:2402.14297</a> [<a href="/pdf/2402.14297" title="Download PDF">pdf</a>, <a href="/format/2402.14297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantics-Empowered Space-Air-Ground-Sea Integrated Network: New  Paradigm, Frameworks, and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+S">Siqi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shaohua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Junlan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haibo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rongxing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In the coming sixth generation (6G) communication era, to provide seamless
and ubiquitous connections, the space-air-ground-sea integrated network
(SAGSIN) is envisioned to address the challenges of communication coverage in
areas with difficult conditions, such as the forest, desert, and sea.
Considering the fundamental limitations of the SAGSIN including large-scale
scenarios, highly dynamic channels, and limited device capabilities,
traditional communications based on Shannon information theory cannot satisfy
the communication demands. Moreover, bit-level reconstruction is usually
redundant for many human-to-machine or machine-to-machine applications in the
SAGSIN. Therefore, it is imperative to consider high-level communications
towards semantics exchange, called semantic communications. In this survey,
according to the interpretations of the term "semantics", including
"significance", "meaning", and "effectiveness-related information", we review
state-of-the-art works on semantic communications from three perspectives,
which are 1) significance representation and protection, 2) meaning similarity
measurement and meaning enhancement, and 3) ultimate effectiveness and
effectiveness yielding. Sequentially, three types of semantic communication
systems can be correspondingly introduced, namely the significance-oriented,
meaning-oriented, and effectiveness/task-oriented semantic communication
systems. Implementation of the above three types of systems in the SAGSIN
necessitates a new perception-communication-computing-actuation-integrated
paradigm (PCCAIP), where all the available perception, computing, and actuation
techniques jointly facilitates significance-oriented sampling &amp; transmission,
semantic extraction &amp; reconstruction, and task decision. Finally, we point out
some future challenges on semantic communications in the SAGSIN. ...
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14298" title="Abstract">arXiv:2402.14298</a> [<a href="/pdf/2402.14298" title="Download PDF">pdf</a>, <a href="/format/2402.14298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Stance Detection: New Datasets and Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jingqian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Stance detection is a challenging task that aims to identify public opinion
from social media platforms with respect to specific targets. Previous work on
stance detection largely focused on pure texts. In this paper, we study
multi-modal stance detection for tweets consisting of texts and images, which
are prevalent in today's fast-growing social media platforms where people often
post multi-modal messages. To this end, we create five new multi-modal stance
detection datasets of different domains based on Twitter, in which each example
consists of a text and an image. In addition, we propose a simple yet effective
Targeted Multi-modal Prompt Tuning framework (TMPT), where target information
is leveraged to learn multi-modal stance features from textual and visual
modalities. Experimental results on our three benchmark datasets show that the
proposed TMPT achieves state-of-the-art performance in multi-modal stance
detection.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14299" title="Abstract">arXiv:2402.14299</a> [<a href="/pdf/2402.14299" title="Download PDF">pdf</a>, <a href="/format/2402.14299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We Choose to Go to Space: Agent-driven Human and Multi-Robot  Collaboration in Microgravity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+M">Miao Xin</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Z">Zhongrui You</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Taoran Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingjia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Haotian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+G">Guojing Ge</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yuchen Ji</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Shentong Mo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jian Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present SpaceAgents-1, a system for learning human and multi-robot
collaboration (HMRC) strategies under microgravity conditions. Future space
exploration requires humans to work together with robots. However, acquiring
proficient robot skills and adept collaboration under microgravity conditions
poses significant challenges within ground laboratories. To address this issue,
we develop a microgravity simulation environment and present three typical
configurations of intra-cabin robots. We propose a hierarchical heterogeneous
multi-agent collaboration architecture: guided by foundation models, a
Decision-Making Agent serves as a task planner for human-robot collaboration,
while individual Skill-Expert Agents manage the embodied control of robots.
This mechanism empowers the SpaceAgents-1 system to execute a range of
intricate long-horizon HMRC tasks.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14300" title="Abstract">arXiv:2402.14300</a> [<a href="/pdf/2402.14300" title="Download PDF">pdf</a>, <a href="/format/2402.14300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Framework Uniting Visual In-context Learning with Masked Image  Modeling to Improve Ultrasound Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Felfeliyan%2C+B">Banafshe Felfeliyan</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Shrimanti Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Knight%2C+J">Jessica Knight</a>, 
<a href="/search/cs?searchtype=author&query=Alves-Pereira%2C+F">Fatima Alves-Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Keen%2C+C">Christopher Keen</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BCpper%2C+J">Jessica K&#xfc;pper</a>, 
<a href="/search/cs?searchtype=author&query=Hareendranathan%2C+A+R">Abhilash Rakkunedeth Hareendranathan</a>, 
<a href="/search/cs?searchtype=author&query=Jaremko%2C+J+L">Jacob L. Jaremko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conventional deep learning models deal with images one-by-one, requiring
costly and time-consuming expert labeling in the field of medical imaging, and
domain-specific restriction limits model generalizability. Visual in-context
learning (ICL) is a new and exciting area of research in computer vision.
Unlike conventional deep learning, ICL emphasizes the model's ability to adapt
to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we
proposed a new simple visual ICL method called SimICL, combining visual ICL
pairing images with masked image modeling (MIM) designed for self-supervised
learning. We validated our method on bony structures segmentation in a wrist
ultrasound (US) dataset with limited annotations, where the clinical objective
was to segment bony structures to help with further fracture detection. We used
a test set containing 3822 images from 18 patients for bony region
segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96
and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and
visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU
increasing up to 0.10 and 0.16. This remarkably high agreement with limited
manual annotations indicates SimICL could be used for training AI models even
on small US datasets. This could dramatically decrease the human expert time
required for image labeling compared to conventional approaches, and enhance
the real-world use of AI assistance in US image analysis.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14301" title="Abstract">arXiv:2402.14301</a> [<a href="/pdf/2402.14301" title="Download PDF">pdf</a>, <a href="/format/2402.14301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenSERP: Large Language Models for Whole Page Presentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Suyu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+G">Guangwei Weng</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+M">Mridu Narang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xia Song</a>, 
<a href="/search/cs?searchtype=author&query=Tiwary%2C+S">Saurabh Tiwary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The advent of large language models (LLMs) brings an opportunity to minimize
the effort in search engine result page (SERP) organization. In this paper, we
propose GenSERP, a framework that leverages LLMs with vision in a few-shot
setting to dynamically organize intermediate search results, including
generated chat answers, website snippets, multimedia data, knowledge panels
into a coherent SERP layout based on a user's query. Our approach has three
main stages: (1) An information gathering phase where the LLM continuously
orchestrates API tools to retrieve different types of items, and proposes
candidate layouts based on the retrieved items, until it's confident enough to
generate the final result. (2) An answer generation phase where the LLM
populates the layouts with the retrieved content. In this phase, the LLM
adaptively optimize the ranking of items and UX configurations of the SERP.
Consequently, it assigns a location on the page to each item, along with the UX
display details. (3) A scoring phase where an LLM with vision scores all the
generated SERPs based on how likely it can satisfy the user. It then send the
one with highest score to rendering. GenSERP features two generation paradigms.
First, coarse-to-fine, which allow it to approach optimal layout in a more
manageable way, (2) beam search, which give it a better chance to hit the
optimal solution compared to greedy decoding. Offline experimental results on
real-world data demonstrate how LLMs can contextually organize heterogeneous
search results on-the-fly and provide a promising user experience.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14303" title="Abstract">arXiv:2402.14303</a> [<a href="/pdf/2402.14303" title="Download PDF">pdf</a>, <a href="/format/2402.14303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Meshed Anatomy: Towards a comprehensive finite element hexahedral  mesh derived from open atlases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huynh%2C+A+T">Andy Trung Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Zwick%2C+B">Benjamin Zwick</a>, 
<a href="/search/cs?searchtype=author&query=Halle%2C+M">Michael Halle</a>, 
<a href="/search/cs?searchtype=author&query=Wittek%2C+A">Adam Wittek</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+K">Karol Miller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Computational simulations using methods such as the finite element (FE)
method rely on high-quality meshes for achieving accurate results. This study
introduces a method for creating a high-quality hexahedral mesh using the Open
Anatomy Project's brain atlas. Our atlas-based FE hexahedral mesh of the brain
mitigates potential inaccuracies and uncertainties due to segmentation - a
process that often requires input of an inexperienced analyst. It accomplishes
this by leveraging existing segmentation from the atlas. We further extend the
mesh's usability by forming a two-way correspondence between the atlas and
mesh. This feature facilitates property assignment for computational
simulations and enhances result analysis within an anatomical context. We
demonstrate the application of the mesh by solving the electroencephalography
(EEG) forward problem. Our method simplifies the mesh creation process,
reducing time and effort, and provides a more comprehensive and contextually
enriched visualisation of simulation outcomes.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14304" title="Abstract">arXiv:2402.14304</a> [<a href="/pdf/2402.14304" title="Download PDF">pdf</a>, <a href="/format/2402.14304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Navigation with Embodied Intelligence: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruyue Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 182 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As a long-term vision in the field of artificial intelligence, the core goal
of embodied intelligence is to improve the perception, understanding, and
interaction capabilities of agents and the environment. Vision-language
navigation (VLN), as a critical research path to achieve embodied intelligence,
focuses on exploring how agents use natural language to communicate effectively
with humans, receive and understand instructions, and ultimately rely on visual
information to achieve accurate navigation. VLN integrates artificial
intelligence, natural language processing, computer vision, and robotics. This
field faces technical challenges but shows potential for application such as
human-computer interaction. However, due to the complex process involved from
language understanding to action execution, VLN faces the problem of aligning
visual information and language instructions, improving generalization ability,
and many other challenges. This survey systematically reviews the research
progress of VLN and details the research direction of VLN with embodied
intelligence. After a detailed summary of its system architecture and research
based on methods and commonly used benchmark datasets, we comprehensively
analyze the problems and challenges faced by current research and explore the
future development direction of this field, aiming to provide a practical
reference for researchers.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14305" title="Abstract">arXiv:2402.14305</a> [<a href="/pdf/2402.14305" title="Download PDF">pdf</a>, <a href="/format/2402.14305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient Pareto-optimal Utility-Fairness between Groups in  Repeated Rankings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mai%2C+P+D">Phuong Dinh Mai</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D">Duc-Trong Le</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T">Tuan-Anh Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+D">Dung D. Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we tackle the problem of computing a sequence of rankings with
the guarantee of the Pareto-optimal balance between (1) maximizing the utility
of the consumers and (2) minimizing unfairness between producers of the items.
Such a multi-objective optimization problem is typically solved using a
combination of a scalarization method and linear programming on bi-stochastic
matrices, representing the distribution of possible rankings of items. However,
the above-mentioned approach relies on Birkhoff-von Neumann (BvN)
decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with
$n$ being the number of items, making it impractical for large-scale systems.
To address this drawback, we introduce a novel approach to the above problem by
using the Expohedron - a permutahedron whose points represent all achievable
exposures of items. On the Expohedron, we profile the Pareto curve which
captures the trade-off between group fairness and user utility by identifying a
finite number of Pareto optimal solutions. We further propose an efficient
method by relaxing our optimization problem on the Expohedron's circumscribed
$n$-sphere, which significantly improve the running time. Moreover, the
approximate Pareto curve is asymptotically close to the real Pareto optimal
curve as the number of substantial solutions increases. Our methods are
applicable with different ranking merits that are non-decreasing functions of
item relevance. The effectiveness of our methods are validated through
experiments on both synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14307" title="Abstract">arXiv:2402.14307</a> [<a href="/pdf/2402.14307" title="Download PDF">pdf</a>, <a href="/format/2402.14307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An FPGA-Based Accelerator Enabling Efficient Support for CNNs with  Arbitrary Kernel Sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Miaoxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages,4 figures. This work has been accepted by 2024 lEEE International Symposium on Circuits and Systems (lSCAS 2024) as a regular paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Convolutional neural networks (CNNs) with large kernels, drawing inspiration
from the key operations of vision transformers (ViTs), have demonstrated
impressive performance in various vision-based applications. To address the
issue of computational efficiency degradation in existing designs for
supporting large-kernel convolutions, an FPGA-based inference accelerator is
proposed for the efficient deployment of CNNs with arbitrary kernel sizes.
Firstly, a Z-flow method is presented to optimize the computing data flow by
maximizing data reuse opportunity. Besides, the proposed design, incorporating
the kernel-segmentation (Kseg) scheme, enables extended support for
large-kernel convolutions, significantly reducing the storage requirements for
overlapped data. Moreover, based on the analysis of typical block structures in
emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are
developed to optimize CNN deployments from both computation and transmission
perspectives. The proposed hardware accelerator, evaluated on Intel Arria 10
FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the
same network. Particularly, it demonstrates efficient support for large-kernel
CNNs, achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and
PyConvResNet-50, respectively, both of which are implemented on hardware for
the first time.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14308" title="Abstract">arXiv:2402.14308</a> [<a href="/pdf/2402.14308" title="Download PDF">pdf</a>, <a href="/format/2402.14308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jie Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+W">Wei Xi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenxian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Danping Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We introduce Ground-Fusion, a low-cost sensor fusion simultaneous
localization and mapping (SLAM) system for ground vehicles. Our system features
efficient initialization, effective sensor anomaly detection and handling,
real-time dense color mapping, and robust localization in diverse environments.
We tightly integrate RGB-D images, inertial measurements, wheel odometer and
GNSS signals within a factor graph to achieve accurate and reliable
localization both indoors and outdoors. To ensure successful initialization, we
propose an efficient strategy that comprises three different methods:
stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore,
we develop mechanisms to detect sensor anomalies and degradation, handling them
adeptly to maintain system accuracy. Our experimental results on both public
and self-collected datasets demonstrate that Ground-Fusion outperforms existing
low-cost SLAM systems in corner cases. We release the code and datasets at
https://github.com/SJTU-ViSYS/Ground-Fusion.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14309" title="Abstract">arXiv:2402.14309</a> [<a href="/pdf/2402.14309" title="Download PDF">pdf</a>, <a href="/format/2402.14309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-TLA: An Efficient and Lightweight Small Object Detection Model  based on YOLOv5
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+C">Chun-Lin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ru-Yue Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object detection, a crucial aspect of computer vision, has seen significant
advancements in accuracy and robustness. Despite these advancements, practical
applications still face notable challenges, primarily the inaccurate detection
or missed detection of small objects. In this paper, we propose YOLO-TLA, an
advanced object detection model building on YOLOv5. We first introduce an
additional detection layer for small objects in the neck network pyramid
architecture, thereby producing a feature map of a larger scale to discern
finer features of small objects. Further, we integrate the C3CrossCovn module
into the backbone network. This module uses sliding window feature extraction,
which effectively minimizes both computational demand and the number of
parameters, rendering the model more compact. Additionally, we have
incorporated a global attention mechanism into the backbone network. This
mechanism combines the channel information with global information to create a
weighted feature map. This feature map is tailored to highlight the attributes
of the object of interest, while effectively ignoring irrelevant details. In
comparison to the baseline YOLOv5s model, our newly developed YOLO-TLA model
has shown considerable improvements on the MS COCO validation dataset, with
increases of 4.6% in mAP@0.5 and 4% in mAP@0.5:0.95, all while keeping the
model size compact at 9.49M parameters. Further extending these improvements to
the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in
mAP@0.5 and mAP@0.5:0.95, respectively, with a total of 27.53M parameters.
These results validate the YOLO-TLA model's efficient and effective performance
in small object detection, achieving high accuracy with fewer parameters and
computational demands.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14310" title="Abstract">arXiv:2402.14310</a> [<a href="/pdf/2402.14310" title="Download PDF">pdf</a>, <a href="/format/2402.14310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize  Encoded Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jinlan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Huangfu%2C+S">Shenzhen Huangfu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have recently showcased remarkable
generalizability in various domains. Despite their extensive knowledge, LLMs
still face challenges in efficiently utilizing encoded knowledge to develop
accurate and logical reasoning processes. To mitigate this problem, we
introduced Hint-before-Solving Prompting (HSP), which guides the model to
generate hints (e.g., specific knowledge or key ideas) for solving the problem
and then generate solutions containing intermediate reasoning steps. Since HSP
is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied
HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results
of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs
demonstrate that HSP can effectively improve the accuracy of reasoning tasks:
(1) By applying high-quality hint-enhanced HSP to CoT prompting,
Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free
LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned
Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We
make our code and dataset publicly available at
\url{https://github.com/jinlanfu/HSP}.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14311" title="Abstract">arXiv:2402.14311</a> [<a href="/pdf/2402.14311" title="Download PDF">pdf</a>, <a href="/format/2402.14311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Font Style Interpolation with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondo%2C+T">Tetta Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Takezaki%2C+S">Shumpei Takezaki</a>, 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fonts have huge variations in their styles and give readers different
impressions. Therefore, generating new fonts is worthy of giving new
impressions to readers. In this paper, we employ diffusion models to generate
new font styles by interpolating a pair of reference fonts with different
styles. More specifically, we propose three different interpolation approaches,
image-blending, condition-blending, and noise-blending, with the diffusion
models. We perform qualitative and quantitative experimental analyses to
understand the style generation ability of the three approaches. According to
experimental results, three proposed approaches can generate not only expected
font styles but also somewhat serendipitous font styles. We also compare the
approaches with a state-of-the-art style-conditional Latin-font generative
network model to confirm the validity of using the diffusion models for the
style interpolation task.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14313" title="Abstract">arXiv:2402.14313</a> [<a href="/pdf/2402.14313" title="Download PDF">pdf</a>, <a href="/format/2402.14313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Kern -- Set-wise Estimation of Optimal Letter Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakatsuru%2C+K">Kei Nakatsuru</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Kerning is the task of setting appropriate horizontal spaces for all possible
letter pairs of a certain font. One of the difficulties of kerning is that the
appropriate space differs for each letter pair. Therefore, for a total of 52
capital and small letters, we need to adjust $52 \times 52 = 2704$ different
spaces. Another difficulty is that there is neither a general procedure nor
criterion for automatic kerning; therefore, kerning is still done manually or
with heuristics. In this paper, we tackle kerning by proposing two
machine-learning models, called pairwise and set-wise models. The former is a
simple deep neural network that estimates the letter space for two given letter
images. In contrast, the latter is a Transformer-based model and estimates the
letter spaces for three or more given letter images. For example, the set-wise
model simultaneously estimates 2704 spaces for 52 letter images for a certain
font. Among the two models, the set-wise model is not only more efficient but
also more accurate because its internal self-attention mechanism allows for
more consistent kerning for all letters. Experimental results on about 2500
Google fonts and their quantitative and qualitative analyses show that the
set-wise model has an average estimation error of only about 5.3 pixels when
the average letter space of all fonts and letter pairs is about 115 pixels.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14314" title="Abstract">arXiv:2402.14314</a> [<a href="/pdf/2402.14314" title="Download PDF">pdf</a>, <a href="/format/2402.14314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Typographic Text Generation with Off-the-Shelf Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peong%2C+K">KhayTze Peong</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>, 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent diffusion-based generative models show promise in their ability to
generate text images, but limitations in specifying the styles of the generated
texts render them insufficient in the realm of typographic design. This paper
proposes a typographic text generation system to add and modify text on
typographic designs while specifying font styles, colors, and text effects. The
proposed system is a novel combination of two off-the-shelf methods for
diffusion models, ControlNet and Blended Latent Diffusion. The former functions
to generate text images under the guidance of edge conditions specifying stroke
contours. The latter blends latent noise in Latent Diffusion Models (LDM) to
add typographic text naturally onto an existing background. We first show that
given appropriate text edges, ControlNet can generate texts in specified fonts
while incorporating effects described by prompts. We further introduce text
edge manipulation as an intuitive and customizable way to produce texts with
complex effects such as ``shadows'' and ``reflections''. Finally, with the
proposed system, we successfully add and modify texts on a predefined
background while preserving its overall coherence.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14316" title="Abstract">arXiv:2402.14316</a> [<a href="/pdf/2402.14316" title="Download PDF">pdf</a>, <a href="/format/2402.14316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Place Anything into Any Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Controllable video editing has demonstrated remarkable potential across
diverse applications, particularly in scenarios where capturing or re-capturing
real-world videos is either impractical or costly. This paper introduces a
novel and efficient system named Place-Anything, which facilitates the
insertion of any object into any video solely based on a picture or text
description of the target object or element. The system comprises three
modules: 3D generation, video reconstruction, and 3D target insertion. This
integrated approach offers an efficient and effective solution for producing
and editing high-quality videos by seamlessly inserting realistic objects.
Through a user study, we demonstrate that our system can effortlessly place any
object into any video using just a photograph of the object. Our demo video can
be found at https://youtu.be/afXqgLLRnTE. Please also visit our project page
https://place-anything.github.io to get access.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14317" title="Abstract">arXiv:2402.14317</a> [<a href="/pdf/2402.14317" title="Download PDF">pdf</a>, <a href="/format/2402.14317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oscillations between Grid-Forming Converters in Weakly Connected  Offshore WPPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghimire%2C+S">Sulav Ghimire</a>, 
<a href="/search/eess?searchtype=author&query=Kkuni%2C+K+V">Kanakesh V. Kkuni</a>, 
<a href="/search/eess?searchtype=author&query=Guerreiro%2C+G+M+G">Gabriel M. G. Guerreiro</a>, 
<a href="/search/eess?searchtype=author&query=Guest%2C+E+D">Emerson D. Guest</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+K+H">Kim H. Jensen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guangya Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies control interactions between grid-forming (GFM) converters
exhibited by power and frequency oscillations in a weakly connected offshore
wind power plant (WPP). Two GFM controls are considered, namely virtual
synchronous machine (VSM) and virtual admittance (VAdm) based GFM. The GFM
control methods are implemented in wind turbine generators (WTGs) of a verified
aggregated model of a WPP and the control interaction between these GFM WTGs is
studied for several cases: cases with the same GFM control methods, and cases
with different GFM control methods. A sensitivity analysis is performed for the
observed oscillations to understand which system parameter affects the
oscillations the most. Several solution methods are proposed and the
inapplicability of some of the conventional solution methods are elaborated in
this paper.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14318" title="Abstract">arXiv:2402.14318</a> [<a href="/pdf/2402.14318" title="Download PDF">pdf</a>, <a href="/format/2402.14318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing generalization capability of text ranking models in Polish
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadas%2C+S">S&#x142;awomir Dadas</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C4%99bowiec%2C+M">Ma&#x142;gorzata Gr&#x119;bowiec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Retrieval-augmented generation (RAG) is becoming an increasingly popular
technique for integrating internal knowledge bases with large language models.
In a typical RAG pipeline, three models are used, responsible for the
retrieval, reranking, and generation stages. In this article, we focus on the
reranking problem for the Polish language, examining the performance of
rerankers and comparing their results with available retrieval models. We
conduct a comprehensive evaluation of existing models and those trained by us,
utilizing a benchmark of 41 diverse information retrieval tasks for the Polish
language. The results of our experiments show that most models struggle with
out-of-domain generalization. However, a combination of effective optimization
method and a large training dataset allows for building rerankers that are both
compact in size and capable of generalization. The best of our models
establishes a new state-of-the-art for reranking in the Polish language,
outperforming existing models with up to 30 times more parameters.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14320" title="Abstract">arXiv:2402.14320</a> [<a href="/pdf/2402.14320" title="Download PDF">pdf</a>, <a href="/format/2402.14320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve  Knowledge Base Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+C">Chang Zong</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weiming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+E">Eliot Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jian Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent progress with LLM-based agents has shown promising results across
various tasks. However, their use in answering questions from knowledge bases
remains largely unexplored. Implementing a KBQA system using traditional
methods is challenging due to the shortage of task-specific training data and
the complexity of creating task-focused model structures. In this paper, we
present Triad, a unified framework that utilizes an LLM-based agent with three
roles for KBQA tasks. The agent is assigned three roles to tackle different
KBQA subtasks: agent as a generalist for mastering various subtasks, as a
decision maker for the selection of candidates, and as an advisor for answering
questions with knowledge. Our KBQA framework is executed in four phases,
involving the collaboration of the agent's multiple roles. We evaluated the
performance of our framework using three benchmark datasets, and the results
show that our framework outperforms state-of-the-art systems on the LC-QuAD and
YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14323" title="Abstract">arXiv:2402.14323</a> [<a href="/pdf/2402.14323" title="Download PDF">pdf</a>, <a href="/format/2402.14323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REPOFUSE: Repository-Level Code Completion with Fused Dual Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Ming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gehao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xunjin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+P">Peng Di</a>, 
<a href="/search/cs?searchtype=author&query=jiang%2C+w">wei jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+G">Gang Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of language models in code assistance has spurred the proposal of
repository-level code completion as a means to enhance prediction accuracy,
utilizing the context from the entire codebase. However, this amplified context
can inadvertently increase inference latency, potentially undermining the
developer experience and deterring tool adoption-a challenge we termed the
Context-Latency Conundrum. This paper introduces RepoGenix, a pioneering
solution designed to enhance repository-level code completion without the
latency trade-off. RepoGenix uniquely fuses two types of contexts: the analogy
context, rooted in code analogies, and the rationale context, which encompasses
in-depth semantic relationships. We propose a novel rank truncated generation
(RTG) technique that efficiently condenses these contexts into prompts with
restricted size. This enables RepoGenix to deliver precise code completions
while maintaining inference efficiency. Through testing with the CrossCodeEval
suite, RepoGenix has demonstrated a significant leap over existing models,
achieving a 40.90% to 59.75% increase in exact match (EM) accuracy for code
completions and a 26.8% enhancement in inference speed. Beyond experimental
validation, RepoGenix has been integrated into the workflow of a large
enterprise, where it actively supports various coding tasks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14326" title="Abstract">arXiv:2402.14326</a> [<a href="/pdf/2402.14326" title="Download PDF">pdf</a>, <a href="/format/2402.14326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video  Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Mingxuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xuedou Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhiqing Luo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jianhua He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Offloading computing to edge servers is a promising solution to support
growing video understanding applications at resource-constrained IoT devices.
Recent efforts have been made to enhance the scalability of such systems by
reducing inference costs on edge servers. However, existing research is not
directly applicable to pixel-level vision tasks such as video semantic
segmentation (VSS), partly due to the fluctuating VSS accuracy and segment
bitrate caused by the dynamic video content. In response, we present Penance, a
new edge inference cost reduction framework. By exploiting softmax outputs of
VSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes
model selection and compression settings to minimize the inference cost while
meeting the required accuracy within the available bandwidth constraints. We
implement Penance in a commercial IoT device with only CPUs. Experimental
results show that Penance consumes a negligible 6.8% more computation resources
than the optimal strategy while satisfying accuracy and bandwidth constraints
with a low failure rate.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14327" title="Abstract">arXiv:2402.14327</a> [<a href="/pdf/2402.14327" title="Download PDF">pdf</a>, <a href="/format/2402.14327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subobject-level Image Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Delong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cahyawijaya%2C+S">Samuel Cahyawijaya</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+P">Pascale Fung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Transformer-based vision models typically tokenize images into fixed-size
square patches as input units, which lacks the adaptability to image content
and overlooks the inherent pixel grouping structure. Inspired by the subword
tokenization widely adopted in language models, we propose an image tokenizer
at a subobject level, where the subobjects are represented by semantically
meaningful image segments obtained by segmentation models (e.g., segment
anything models). To implement a learning system based on subobject
tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to
compress subobject segments of varying sizes and shapes into compact embedding
vectors, then fed the subobject embeddings into a large language model for
vision language learning. Empirical results demonstrated that our
subobject-level tokenization significantly facilitates efficient learning of
translating images into object and attribute descriptions compared to the
traditional patch-level tokenization. Codes and models will be open-sourced at
https://github.com/ChenDelong1999/subobjects.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14328" title="Abstract">arXiv:2402.14328</a> [<a href="/pdf/2402.14328" title="Download PDF">pdf</a>, <a href="/format/2402.14328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Patching Compositional Reasoning in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Gangwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Ying Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work In Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">LLMs have marked a revolutonary shift, yet they falter when faced with
compositional reasoning tasks. Our research embarks on a quest to uncover the
root causes of compositional reasoning failures of LLMs, uncovering that most
of them stem from the improperly generated or leveraged implicit reasoning
results. Inspired by our empirical findings, we resort to Logit Lens and an
intervention experiment to dissect the inner hidden states of LLMs. This deep
dive reveals that implicit reasoning results indeed surface within middle
layers and play a causative role in shaping the final explicit reasoning
results. Our exploration further locates multi-head self-attention (MHSA)
modules within these layers, which emerge as the linchpins in accurate
generation and leveraing of implicit reasoning results. Grounded on the above
findings, we develop CREME, a lightweight method to patch errors in
compositional reasoning via editing the located MHSA modules. Our empirical
evidence stands testament to CREME's effectiveness, paving the way for
autonomously and continuously enhancing compositional reasoning capabilities in
language models.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14332" title="Abstract">arXiv:2402.14332</a> [<a href="/pdf/2402.14332" title="Download PDF">pdf</a>, <a href="/format/2402.14332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Large to Small Datasets: Size Generalization for Clustering  Algorithm Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatziafratis%2C+V">Vaggos Chatziafratis</a>, 
<a href="/search/cs?searchtype=author&query=Karmarkar%2C+I">Ishani Karmarkar</a>, 
<a href="/search/cs?searchtype=author&query=Vitercik%2C+E">Ellen Vitercik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In clustering algorithm selection, we are given a massive dataset and must
efficiently select which clustering algorithm to use. We study this problem in
a semi-supervised setting, with an unknown ground-truth clustering that we can
only access through expensive oracle queries. Ideally, the clustering
algorithm's output will be structurally close to the ground truth. We approach
this problem by introducing a notion of size generalization for clustering
algorithm accuracy. We identify conditions under which we can (1) subsample the
massive clustering instance, (2) evaluate a set of candidate algorithms on the
smaller instance, and (3) guarantee that the algorithm with the best accuracy
on the small instance will have the best accuracy on the original big instance.
We provide theoretical size generalization guarantees for three classic
clustering algorithms: single-linkage, k-means++, and (a smoothed variant of)
Gonzalez's k-centers heuristic. We validate our theoretical analysis with
empirical results, observing that on real-world clustering instances, we can
use a subsample of as little as 5% of the data to identify which algorithm is
best on the full dataset.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14334" title="Abstract">arXiv:2402.14334</a> [<a href="/pdf/2402.14334" title="Download PDF">pdf</a>, <a href="/format/2402.14334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSTRUCTIR: A Benchmark for Instruction Following of Information  Retrieval Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hanseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Haebin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hansol Jang</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+C">Changwook Jun</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the critical need to align search targets with users' intention,
retrievers often only prioritize query information without delving into the
users' intended search context. Enhancing the capability of retrievers to
understand intentions and preferences of users, akin to language model
instructions, has the potential to yield more aligned search targets. Prior
studies restrict the application of instructions in information retrieval to a
task description format, neglecting the broader context of diverse and evolving
search scenarios. Furthermore, the prevailing benchmarks utilized for
evaluation lack explicit tailoring to assess instruction-following ability,
thereby hindering progress in this field. In response to these limitations, we
propose a novel benchmark,INSTRUCTIR, specifically designed to evaluate
instruction-following ability in information retrieval tasks. Our approach
focuses on user-aligned instructions tailored to each query instance,
reflecting the diverse characteristics inherent in real-world search scenarios.
Through experimental analysis, we observe that retrievers fine-tuned to follow
task-style instructions, such as INSTRUCTOR, can underperform compared to their
non-instruction-tuned counterparts. This underscores potential overfitting
issues inherent in constructing retrievers trained on existing
instruction-aware retrieval datasets.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14335" title="Abstract">arXiv:2402.14335</a> [<a href="/pdf/2402.14335" title="Download PDF">pdf</a>, <a href="/format/2402.14335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperFast: Instant Classification for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonet%2C+D">David Bonet</a>, 
<a href="/search/cs?searchtype=author&query=Montserrat%2C+D+M">Daniel Mas Montserrat</a>, 
<a href="/search/cs?searchtype=author&query=Gir%C3%B3-i-Nieto%2C+X">Xavier Gir&#xf3;-i-Nieto</a>, 
<a href="/search/cs?searchtype=author&query=Ioannidis%2C+A+G">Alexander G. Ioannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures, AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Training deep learning models and performing hyperparameter tuning can be
computationally demanding and time-consuming. Meanwhile, traditional machine
learning methods like gradient-boosting algorithms remain the preferred choice
for most tabular data applications, while neural network alternatives require
extensive hyperparameter tuning or work only in toy datasets under limited
settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork
designed for instant classification of tabular data in a single forward pass.
HyperFast generates a task-specific neural network tailored to an unseen
dataset that can be directly used for classification inference, removing the
need for training a model. We report extensive experiments with OpenML and
genomic data, comparing HyperFast to competing tabular data neural networks,
traditional ML methods, AutoML systems, and boosting machines. HyperFast shows
highly competitive results, while being significantly faster. Additionally, our
approach demonstrates robust adaptability across a variety of classification
tasks with little to no fine-tuning, positioning HyperFast as a strong solution
for numerous applications and rapid model deployment. HyperFast introduces a
promising paradigm for fast classification, with the potential to substantially
decrease the computational burden of deep learning. Our code, which offers a
scikit-learn-like interface, along with the trained HyperFast model, can be
found at https://github.com/AI-sandbox/HyperFast.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14337" title="Abstract">arXiv:2402.14337</a> [<a href="/pdf/2402.14337" title="Download PDF">pdf</a>, <a href="/format/2402.14337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hazel Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Rationales behind answers not only explain model decisions but boost language
models to reason well on complex reasoning tasks. However, obtaining impeccable
rationales is often impossible. Besides, it is non-trivial to estimate the
degree to which the rationales are faithful enough to encourage model
performance. Thus, such reasoning tasks often compel models to output correct
answers under undesirable rationales and are sub-optimal compared to what the
models are fully capable of. In this work, we propose how to deal with
imperfect rationales causing aleatoric uncertainty. We first define the
ambiguous rationales with entropy scores of given rationales, using model prior
beliefs as informativeness. We then guide models to select one of two different
reasoning models according to the ambiguity of rationales. We empirically argue
that our proposed method produces robust performance superiority against the
adversarial quality of rationales and low-resource settings.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14340" title="Abstract">arXiv:2402.14340</a> [<a href="/pdf/2402.14340" title="Download PDF">pdf</a>, <a href="/format/2402.14340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for  Monocular Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sangwon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+D">Daejune Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Duksu Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, under review for a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Monocular depth estimation (MDE) is essential for numerous applications yet
is impeded by the substantial computational demands of accurate deep learning
models. To mitigate this, we introduce a novel Teacher-Independent Explainable
Knowledge Distillation (TIE-KD) framework that streamlines the knowledge
transfer from complex teacher models to compact student networks, eliminating
the need for architectural similarity. The cornerstone of TIE-KD is the Depth
Probability Map (DPM), an explainable feature map that interprets the teacher's
output, enabling feature-based knowledge distillation solely from the teacher's
response. This approach allows for efficient student learning, leveraging the
strengths of feature-based distillation. Extensive evaluation of the KITTI
dataset indicates that TIE-KD not only outperforms conventional response-based
KD methods but also demonstrates consistent efficacy across diverse teacher and
student architectures. The robustness and adaptability of TIE-KD underscore its
potential for applications requiring efficient and interpretable models,
affirming its practicality for real-world deployment.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14344" title="Abstract">arXiv:2402.14344</a> [<a href="/pdf/2402.14344" title="Download PDF">pdf</a>, <a href="/format/2402.14344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster-then-Match: Efficient Management of Human-Centric, Cell-Less 6G  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiaramello%2C+E">Emma Chiaramello</a>, 
<a href="/search/cs?searchtype=author&query=Chiasserini%2C+C+F">Carla Fabiana Chiasserini</a>, 
<a href="/search/cs?searchtype=author&query=Malandrino%2C+F">Francesco Malandrino</a>, 
<a href="/search/cs?searchtype=author&query=Nordio%2C+A">Alessandro Nordio</a>, 
<a href="/search/cs?searchtype=author&query=Parazzini%2C+M">Marta Parazzini</a>, 
<a href="/search/cs?searchtype=author&query=Valcarce%2C+A">Alvaro Valcarce</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE WoWMoM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In 5G and beyond (5GB) networks, the notion of cell tends to blur, as a set
of points-of-access (PoAs) using different technologies often cover overlapping
areas. In this context, high-quality decisions are needed about (i) which PoA
to use when serving an end user and (ii) how to manage PoAs, e.g., how to set
their power levels. To address this challenge, we present Cluster-then-Match
(CtM), an efficient algorithm making joint decisions about user assignment and
PoA management. Following the human-centric networking paradigm, such decisions
account not only for the performance of the network, but also for the level of
electromagnetic field exposure to which human bodies incur and energy
consumption. Our performance evaluation shows how CtM can match the performance
of state-of-the-art network management schemes, while reducing electromagnetic
emissions and energy consumption by over 80%.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14345" title="Abstract">arXiv:2402.14345</a> [<a href="/pdf/2402.14345" title="Download PDF">pdf</a>, <a href="/format/2402.14345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Error-Matching Exclusion Method for Accelerating Visual SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaxing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In Visual SLAM, achieving accurate feature matching consumes a significant
amount of time, severely impacting the real-time performance of the system.
This paper proposes an accelerated method for Visual SLAM by integrating GMS
(Grid-based Motion Statistics) with RANSAC (Random Sample Consensus) for the
removal of mismatched features. The approach first utilizes the GMS algorithm
to estimate the quantity of matched pairs within the neighborhood and ranks the
matches based on their confidence. Subsequently, the Random Sample Consensus
(RANSAC) algorithm is employed to further eliminate mismatched features. To
address the time-consuming issue of randomly selecting all matched pairs, this
method transforms it into the problem of prioritizing sample selection from
high-confidence matches. This enables the iterative solution of the optimal
model. Experimental results demonstrate that the proposed method achieves a
comparable accuracy to the original GMS-RANSAC while reducing the average
runtime by 24.13% on the KITTI, TUM desk, and TUM doll datasets.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14346" title="Abstract">arXiv:2402.14346</a> [<a href="/pdf/2402.14346" title="Download PDF">pdf</a>, <a href="/format/2402.14346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependable Distributed Training of Compressed Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malandrino%2C+F">Francesco Malandrino</a>, 
<a href="/search/cs?searchtype=author&query=Di+Giacomo%2C+G">Giuseppe Di Giacomo</a>, 
<a href="/search/cs?searchtype=author&query=Levorato%2C+M">Marco Levorato</a>, 
<a href="/search/cs?searchtype=author&query=Chiasserini%2C+C+F">Carla Fabiana Chiasserini</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE WoWMoM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The existing work on the distributed training of machine learning (ML) models
has consistently overlooked the distribution of the achieved learning quality,
focusing instead on its average value. This leads to a poor dependability}of
the resulting ML models, whose performance may be much worse than expected. We
fill this gap by proposing DepL, a framework for dependable learning
orchestration, able to make high-quality, efficient decisions on (i) the data
to leverage for learning, (ii) the models to use and when to switch among them,
and (iii) the clusters of nodes, and the resources thereof, to exploit. For
concreteness, we consider as possible available models a full DNN and its
compressed versions. Unlike previous studies, DepL guarantees that a target
learning quality is reached with a target probability, while keeping the
training cost at a minimum. We prove that DepL has constant competitive ratio
and polynomial complexity, and show that it outperforms the state-of-the-art by
over 27% and closely matches the optimum.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14353" title="Abstract">arXiv:2402.14353</a> [<a href="/pdf/2402.14353" title="Download PDF">pdf</a>, <a href="/format/2402.14353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Emerging Trends in 5G Malicious Traffic Analysis and  Incremental Learning Intrusion Detection Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fok%2C+K+W">Kar Wai Fok</a>, 
<a href="/search/cs?searchtype=author&query=Thing%2C+V+L+L">Vrizlynn L. L. Thing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The popularity of 5G networks poses a huge challenge for malicious traffic
detection technology. The reason for this is that as the use of 5G technology
increases, so does the risk of malicious traffic activity on 5G networks.
Malicious traffic activity in 5G networks not only has the potential to disrupt
communication services, but also to compromise sensitive data. This can have
serious consequences for individuals and organizations. In this paper, we first
provide an in-depth study of 5G technology and 5G security. Next we analyze and
discuss the latest malicious traffic detection under AI and their applicability
to 5G networks, and compare the various traffic detection aspects addressed by
SOTA. The SOTA in 5G traffic detection is also analyzed. Next, we propose seven
criteria for traffic monitoring datasets to confirm their suitability for
future traffic detection studies. Finally, we present three major issues that
need to be addressed for traffic detection in 5G environment. The concept of
incremental learning techniques is proposed and applied in the experiments, and
the experimental results prove to be able to solve the three problems to some
extent.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14354" title="Abstract">arXiv:2402.14354</a> [<a href="/pdf/2402.14354" title="Download PDF">pdf</a>, <a href="/format/2402.14354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a  Gradient-Aware Mask and Semantic Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+A">Anqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haiyue Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kezhi Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised depth estimation has evolved into an image reconstruction
task that minimizes a photometric loss. While recent methods have made strides
in indoor depth estimation, they often produce inconsistent depth estimation in
textureless areas and unsatisfactory depth discrepancies at object boundaries.
To address these issues, in this work, we propose GAM-Depth, developed upon two
novel components: gradient-aware mask and semantic constraints. The
gradient-aware mask enables adaptive and robust supervision for both key areas
and textureless regions by allocating weights based on gradient magnitudes.The
incorporation of semantic constraints for indoor self-supervised depth
estimation improves depth discrepancies at object boundaries, leveraging a
co-optimization network and proxy semantic labels derived from a pretrained
segmentation model. Experimental studies on three indoor datasets, including
NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing
methods and achieves state-of-the-art performance, signifying a meaningful step
forward in indoor depth estimation. Our code will be available at
https://github.com/AnqiCheng1234/GAM-Depth.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14355" title="Abstract">arXiv:2402.14355</a> [<a href="/pdf/2402.14355" title="Download PDF">pdf</a>, <a href="/format/2402.14355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule or Story, Which is a Better Commonsense Expression for Talking with  Large Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+N">Ning Bian</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xianpei Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaojie Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Ben He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Le Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Building machines with commonsense has been a longstanding challenge in NLP
due to the reporting bias of commonsense rules and the exposure bias of
rule-based commonsense reasoning. In contrast, humans convey and pass down
commonsense implicitly through stories. This paper investigates the inherent
commonsense ability of large language models (LLMs) expressed through
storytelling. We systematically investigate and compare stories and rules for
retrieving and leveraging commonsense in LLMs. Experimental results on 28
commonsense QA datasets show that stories outperform rules as the expression
for retrieving commonsense from LLMs, exhibiting higher generation confidence
and commonsense accuracy. Moreover, stories are the more effective commonsense
expression for answering questions regarding daily events, while rules are more
effective for scientific questions. This aligns with the reporting bias of
commonsense in text corpora. We further show that the correctness and relevance
of commonsense stories can be further improved via iterative self-supervised
fine-tuning. These findings emphasize the importance of using appropriate
language to express, retrieve, and leverage commonsense for LLMs, highlighting
a promising direction for better exploiting their commonsense abilities.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14359" title="Abstract">arXiv:2402.14359</a> [<a href="/pdf/2402.14359" title="Download PDF">pdf</a>, <a href="/format/2402.14359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Scientific Summarization Evaluation: Grounding Explainable  Metrics on Facet-aware Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tairan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Taicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The summarization capabilities of pretrained and large language models (LLMs)
have been widely validated in general areas, but their use in scientific
corpus, which involves complex sentences and specialized knowledge, has been
less assessed. This paper presents conceptual and experimental analyses of
scientific summarization, highlighting the inadequacies of traditional
evaluation methods, such as $n$-gram, embedding comparison, and QA,
particularly in providing explanations, grasping scientific concepts, or
identifying key content. Subsequently, we introduce the Facet-aware Metric
(FM), employing LLMs for advanced semantic matching to evaluate summaries based
on different aspects. This facet-aware approach offers a thorough evaluation of
abstracts by decomposing the evaluation task into simpler subtasks.Recognizing
the absence of an evaluation benchmark in this domain, we curate a Facet-based
scientific summarization Dataset (FD) with facet-level annotations. Our
findings confirm that FM offers a more logical approach to evaluating
scientific summaries. In addition, fine-tuned smaller models can compete with
LLMs in scientific contexts, while LLMs have limitations in learning from
in-context information in scientific domains. This suggests an area for future
enhancement of LLMs.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14361" title="Abstract">arXiv:2402.14361</a> [<a href="/pdf/2402.14361" title="Download PDF">pdf</a>, <a href="/format/2402.14361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenTab: Advancing Large Language Models as Open-domain Table Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+K">Kezhi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiani Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhengyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+C">Chuan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) trained on large volumes of data excel at
various natural language tasks, but they cannot handle tasks requiring
knowledge that has not been trained on previously. One solution is to use a
retriever that fetches relevant information to expand LLM's knowledge scope.
However, existing textual-oriented retrieval-based LLMs are not ideal on
structured table data due to diversified data modalities and large table sizes.
In this work, we propose OpenTab, an open-domain table reasoning framework
powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant
tables and then generates SQL programs to parse the retrieved tables
efficiently. Utilizing the intermediate data derived from the SQL executions,
it conducts grounded inference to produce accurate response. Extensive
experimental evaluation shows that OpenTab significantly outperforms baselines
in both open- and closed-domain settings, achieving up to 21.5% higher
accuracy. We further run ablation studies to validate the efficacy of our
proposed designs of the system.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14366" title="Abstract">arXiv:2402.14366</a> [<a href="/pdf/2402.14366" title="Download PDF">pdf</a>, <a href="/format/2402.14366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Detecting Annotation-Induced Faults of Static  Analyzers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huaien Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yu Pei</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shuyun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S+H">Shin Hwei Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Static analyzers can reason about the properties and behaviors of programs
and detect various issues without executing them. Hence, they should extract
the necessary information to understand the analyzed program well. Annotation
has been a widely used feature for different purposes in Java since the
introduction of Java 5. Annotations can change program structures and convey
semantics information without awareness of static analyzers, consequently
leading to imprecise analysis results. This paper presents the first
comprehensive study of annotation-induced faults (AIF) by analyzing 246 issues
in six open-source and popular static analyzers (i.e., PMD, SpotBugs,
CheckStyle, Infer, SonarQube, and Soot). We analyzed the issues' root causes,
symptoms, and fix strategies and derived ten findings and some practical
guidelines for detecting and repairing annotation-induced faults. Moreover, we
developed an automated testing framework called AnnaTester based on three
metamorphic relations originating from the findings. AnnaTester generated new
tests based on the official test suites of static analyzers and unveiled 43 new
faults, 20 of which have been fixed. The results confirm the value of our study
and its findings.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14367" title="Abstract">arXiv:2402.14367</a> [<a href="/pdf/2402.14367" title="Download PDF">pdf</a>, <a href="/format/2402.14367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning for Frequent Subgraph Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianyu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Andrew Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiaxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Oral Presentation in The Graph Representation Learning and Beyond (GRL+) Workshop from The 37th International Conference on Ma- chine Learning, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Identifying frequent subgraphs, also called network motifs, is crucial in
analyzing and predicting properties of real-world networks. However, finding
large commonly-occurring motifs remains a challenging problem not only due to
its NP-hard subroutine of subgraph counting, but also the exponential growth of
the number of possible subgraphs patterns. Here we present Subgraph Pattern
Miner (SPMiner), a novel neural approach for approximately finding frequent
subgraphs in a large target graph. SPMiner combines graph neural networks,
order embedding space, and an efficient search strategy to identify network
subgraph patterns that appear most frequently in the target graph. SPMiner
first decomposes the target graph into many overlapping subgraphs and then
encodes each subgraph into an order embedding space. SPMiner then uses a
monotonic walk in the order embedding space to identify frequent motifs.
Compared to existing approaches and possible neural alternatives, SPMiner is
more accurate, faster, and more scalable. For 5- and 6-node motifs, we show
that SPMiner can almost perfectly identify the most frequent motifs while being
100x faster than exact enumeration methods. In addition, SPMiner can also
reliably identify frequent 10-node motifs, which is well beyond the size limit
of exact enumeration approaches. And last, we show that SPMiner can find large
up to 20 node motifs with 10-100x higher frequency than those found by current
approximate methods.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14369" title="Abstract">arXiv:2402.14369</a> [<a href="/pdf/2402.14369" title="Download PDF">pdf</a>, <a href="/format/2402.14369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable and Provably Fair Exposure Control for Large-Scale Recommender  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Togashi%2C+R">Riku Togashi</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+Y">Yuta Saito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Typical recommendation and ranking methods aim to optimize the satisfaction
of users, but they are often oblivious to their impact on the items (e.g.,
products, jobs, news, video) and their providers. However, there has been a
growing understanding that the latter is crucial to consider for a wide range
of applications, since it determines the utility of those being recommended.
Prior approaches to fairness-aware recommendation optimize a regularized
objective to balance user satisfaction and item fairness based on some notion
such as exposure fairness. These existing methods have been shown to be
effective in controlling fairness, however, most of them are computationally
inefficient, limiting their applications to only unrealistically small-scale
situations. This indeed implies that the literature does not yet provide a
solution to enable a flexible control of exposure in the industry-scale
recommender systems where millions of users and items exist. To enable a
computationally efficient exposure control even for such large-scale systems,
this work develops a scalable, fast, and fair method called
\emph{\textbf{ex}posure-aware \textbf{ADMM} (\textbf{exADMM})}. exADMM is based
on implicit alternating least squares (iALS), a conventional scalable algorithm
for collaborative filtering, but optimizes a regularized objective to achieve a
flexible control of accuracy-fairness tradeoff. A particular technical
challenge in developing exADMM is the fact that the fairness regularizer
destroys the separability of optimization subproblems for users and items,
which is an essential property to ensure the scalability of iALS. Therefore, we
develop a set of optimization tools to enable yet scalable fairness control
with provable convergence guarantees as a basis of our algorithm.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14371" title="Abstract">arXiv:2402.14371</a> [<a href="/pdf/2402.14371" title="Download PDF">pdf</a>, <a href="/format/2402.14371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HR-APR: APR-agnostic Framework with Uncertainty Estimation and  Hierarchical Refinement for Camera Relocalisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huajian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Prisacariu%2C+V">Victor Prisacariu</a>, 
<a href="/search/cs?searchtype=author&query=Braud%2C+T">Tristan Braud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in in 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Absolute Pose Regressors (APRs) directly estimate camera poses from monocular
images, but their accuracy is unstable for different queries. Uncertainty-aware
APRs provide uncertainty information on the estimated pose, alleviating the
impact of these unreliable predictions. However, existing uncertainty modelling
techniques are often coupled with a specific APR architecture, resulting in
suboptimal performance compared to state-of-the-art (SOTA) APR methods. This
work introduces a novel APR-agnostic framework, HR-APR, that formulates
uncertainty estimation as cosine similarity estimation between the query and
database features. It does not rely on or affect APR network architecture,
which is flexible and computationally efficient. In addition, we take advantage
of the uncertainty for pose refinement to enhance the performance of APR. The
extensive experiments demonstrate the effectiveness of our framework, reducing
27.4\% and 15.2\% of computational overhead on the 7Scenes and Cambridge
Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14373" title="Abstract">arXiv:2402.14373</a> [<a href="/pdf/2402.14373" title="Download PDF">pdf</a>, <a href="/format/2402.14373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Language Model Is a Good Guide for Large Language Model in Chinese  Entity Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuemei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qi Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, large language models (LLMs) have been successful in relational
extraction (RE) tasks, especially in the few-shot learning. An important
problem in the field of RE is long-tailed data, while not much attention is
currently paid to this problem using LLM approaches. Therefore, in this paper,
we propose SLCoLM, a model collaboration framework, to mitigate the data
long-tail problem. In our framework, We use the
``\textit{Training-Guide-Predict}'' strategy to combine the strengths of
pre-trained language models (PLMs) and LLMs, where a task-specific PLM
framework acts as a tutor, transfers task knowledge to the LLM, and guides the
LLM in performing RE tasks. Our experiments on a RE dataset rich in relation
types show that the approach in this paper facilitates RE of long-tail relation
types.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14374" title="Abstract">arXiv:2402.14374</a> [<a href="/pdf/2402.14374" title="Download PDF">pdf</a>, <a href="/format/2402.14374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-loop Data-Enabled Predictive Control and its equivalence with  Closed-loop Subspace Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dinkla%2C+R">Rogier Dinkla</a>, 
<a href="/search/eess?searchtype=author&query=Mulders%2C+S">Sebastiaan Mulders</a>, 
<a href="/search/eess?searchtype=author&query=Oomen%2C+T">Tom Oomen</a>, 
<a href="/search/eess?searchtype=author&query=van+Wingerden%2C+J">Jan-Willem van Wingerden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DOIs for code and data are available in paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Factors like improved data availability and increasing system complexity have
sparked interest in data-driven predictive control (DDPC) methods like
Data-enabled Predictive Control (DeePC). However, closed-loop identification
bias arises in the presence of noise, which reduces the effectiveness of
obtained control policies. In this paper we propose Closed-loop Data-enabled
Predictive Control (CL-DeePC), a framework that unifies different approaches to
address this challenge. To this end, CL-DeePC incorporates instrumental
variables (IVs) to synthesize and sequentially apply consistent single or
multi-step-ahead predictors. Furthermore, a computationally efficient CL-DeePC
implementation is developed that reveals an equivalence with Closed-loop
Subspace Predictive Control (CL-SPC). Compared to DeePC, CL-DeePC simulations
demonstrate superior reference tracking, with a sensitivity study finding a 48%
lower susceptibility to noise-induced reference tracking performance
degradation.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14376" title="Abstract">arXiv:2402.14376</a> [<a href="/pdf/2402.14376" title="Download PDF">pdf</a>, <a href="/format/2402.14376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Complexity of Finding Dissimilar Shortest Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Funayama%2C+R">Ryo Funayama</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+Y">Yasuaki Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Uno%2C+T">Takeaki Uno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> many figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider the problem of finding ``dissimilar'' $k$ shortest paths from $s$
to $t$ in an edge-weighted directed graph $D$, where the dissimilarity is
measured by the minimum pairwise Hamming distances between these paths. More
formally, given an edge-weighted directed graph $D = (V, A)$, two specified
vertices $s, t \in V$, and integers $d, k$, the goal of Dissimilar Shortest
Paths is to decide whether $D$ has $k$ shortest paths $P_1, \dots, P_k$ from
$s$ to $t$ such that $|A(P_i) \mathbin{\triangle} A(P_j)| \ge d$ for distinct
$P_i$ and $P_j$. We design a deterministic algorithm to solve Dissimilar
Shortest Paths with running time $2^{O(3^kdk^2)}n^{O(1)}$, that is, Dissimilar
Shortest Paths is fixed-parameter tractable parameterized by $k + d$. To
complement this positive result, we show that Dissimilar Shortest Paths is
W[1]-hard when parameterized by only $k$ and paraNP-hard parameterized by $d$.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14379" title="Abstract">arXiv:2402.14379</a> [<a href="/pdf/2402.14379" title="Download PDF">pdf</a>, <a href="/format/2402.14379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novi jezi&#x10d;ki modeli za srpski jezik
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%A0kori%C4%87%2C+M">Mihailo &#x160;kori&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Serbian language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The paper will briefly present the development history of transformer-based
language models for the Serbian language. Several new models for text
generation and vectorization, trained on the resources of the Society for
Language Resources and Technologies, will also be presented. Ten selected
vectorization models for Serbian, including two new ones, will be compared on
four natural language processing tasks. Paper will analyze which models are the
best for each selected task, how does their size and the size of their training
sets affect the performance on those tasks, and what is the optimal setting to
train the best language models for the Serbian language.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14380" title="Abstract">arXiv:2402.14380</a> [<a href="/pdf/2402.14380" title="Download PDF">pdf</a>, <a href="/format/2402.14380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only  Moving Object Segmentation and Ego-Velocity Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Changsong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yimin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huimin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuwei Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Moving object segmentation (MOS) and Ego velocity estimation (EVE) are vital
capabilities for mobile systems to achieve full autonomy. Several approaches
have attempted to achieve MOSEVE using a LiDAR sensor. However, LiDAR sensors
are typically expensive and susceptible to adverse weather conditions. Instead,
millimeter-wave radar (MWR) has gained popularity in robotics and autonomous
driving for real applications due to its cost-effectiveness and resilience to
bad weather. Nonetheless, publicly available MOSEVE datasets and approaches
using radar data are limited. Some existing methods adopt point convolutional
networks from LiDAR-based approaches, ignoring the specific artifacts and the
valuable radial velocity information of radar measurements, leading to
suboptimal performance. In this paper, we propose a novel transformer network
that effectively addresses the sparsity and noise issues and leverages the
radial velocity measurements of radar points using our devised radar self- and
cross-attention mechanisms. Based on that, our method achieves accurate EVE of
the robot and performs MOS using only radar data simultaneously. To thoroughly
evaluate the MOSEVE performance of our method, we annotated the radar points in
the public View-of-Delft (VoD) dataset and additionally constructed a new radar
dataset in various environments. The experimental results demonstrate the
superiority of our approach over existing state-of-the-art methods. The code is
available at https://github.com/ORCA-Uboat/RadarMOSEVE.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14382" title="Abstract">arXiv:2402.14382</a> [<a href="/pdf/2402.14382" title="Download PDF">pdf</a>, <a href="/format/2402.14382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Temporal Knowledge Graph Forecasting with Large Language  Models via Chain-of-History Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yuwei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Ding Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based
on given histories. Most recent graph-based models excel at capturing
structural information within TKGs but lack semantic comprehension abilities.
Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has
emerged. However, the existing LLM-based model exhibits three shortcomings: (1)
It only focuses on the first-order history for prediction while ignoring
high-order historical information, resulting in the provided information for
LLMs being extremely limited. (2) LLMs struggle with optimal reasoning
performance under heavy historical information loads. (3) For TKG prediction,
the temporal reasoning capability of LLM alone is limited. To address the first
two challenges, we propose Chain-of-History (CoH) reasoning which explores
high-order histories step-by-step, achieving effective utilization of
high-order historical information for LLMs on TKG prediction. To address the
third issue, we design CoH as a paly-and-plug module to enhance the performance
of graph-based models for TKG prediction. Extensive experiments on three
datasets and backbones demonstrate the effectiveness of CoH.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14384" title="Abstract">arXiv:2402.14384</a> [<a href="/pdf/2402.14384" title="Download PDF">pdf</a>, <a href="/format/2402.14384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Network with Soft-Dynamic Time Warping and  Parallel Reconstruction for Energy Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+H">Hardik Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Valadi%2C+J">Jayaraman Valadi</a>, 
<a href="/search/cs?searchtype=author&query=Arjunan%2C+P">Pandarasamy Arjunan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AI4TS Workshop AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we employ a 1D deep convolutional generative adversarial
network (DCGAN) for sequential anomaly detection in energy time series data.
Anomaly detection involves gradient descent to reconstruct energy
sub-sequences, identifying the noise vector that closely generates them through
the generator network. Soft-DTW is used as a differentiable alternative for the
reconstruction loss and is found to be superior to Euclidean distance.
Combining reconstruction loss and the latent space's prior probability
distribution serves as the anomaly score. Our novel method accelerates
detection by parallel computation of reconstruction of multiple points and
shows promise in identifying anomalous energy consumption in buildings, as
evidenced by performing experiments on hourly energy time series from 15
buildings.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14385" title="Abstract">arXiv:2402.14385</a> [<a href="/pdf/2402.14385" title="Download PDF">pdf</a>, <a href="/format/2402.14385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WindDragon: Enhancing wind power forecasting with Automated Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keisler%2C+J">Julie Keisler</a> (EDF R\&amp;D OSIRIS, EDF R\&amp;D), 
<a href="/search/cs?searchtype=author&query=Naour%2C+E+L">Etienne Le Naour</a> (ISIR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph); Machine Learning (stat.ML)

</div>
<p class="mathjax">Achieving net zero carbon emissions by 2050 requires the integration of
increasing amounts of wind power into power grids. This energy source poses a
challenge to system operators due to its variability and uncertainty.
Therefore, accurate forecasting of wind power is critical for grid operation
and system balancing. This paper presents an innovative approach to short-term
(1 to 6 hour horizon) windpower forecasting at a national level. The method
leverages Automated Deep Learning combined with Numerical Weather Predictions
wind speed maps to accurately forecast wind power.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14386" title="Abstract">arXiv:2402.14386</a> [<a href="/pdf/2402.14386" title="Download PDF">pdf</a>, <a href="/format/2402.14386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Workspace Analysis for Laparoscopic Rectal Surgery : A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thomieres%2C+A">Alexandra Thomieres</a> (CHU Nantes), 
<a href="/search/cs?searchtype=author&query=Khanzode%2C+D">Dhruva Khanzode</a> (CSIR, AcSIR, LS2N - &#xe9;quipe RoMas), 
<a href="/search/cs?searchtype=author&query=Duchalais%2C+E">Emilie Duchalais</a> (CHU Nantes), 
<a href="/search/cs?searchtype=author&query=Jha%2C+R">Ranjan Jha</a> (CSIR, AcSIR), 
<a href="/search/cs?searchtype=author&query=Chablat%2C+D">Damien Chablat</a> (LS2N, LS2N - &#xe9;quipe RoMas)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 33rd International Conference on Robotics in Alpe-Adria-Danube
  Region, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The integration of medical imaging, computational analysis, and robotic
technology has brought about a significant transformation in minimally invasive
surgical procedures, particularly in the realm of laparoscopic rectal surgery
(LRS). This specialized surgical technique, aimed at addressing rectal cancer,
requires an in-depth comprehension of the spatial dynamics within the narrow
space of the pelvis. Leveraging Magnetic Resonance Imaging (MRI) scans as a
foundational dataset, this study incorporates them into Computer-Aided Design
(CAD) software to generate precise three-dimensional (3D) reconstructions of
the patient's anatomy. At the core of this research is the analysis of the
surgical workspace, a critical aspect in the optimization of robotic
interventions. Sophisticated computational algorithms process MRI data within
the CAD environment, meticulously calculating the dimensions and contours of
the pelvic internal regions. The outcome is a nuanced understanding of both
viable and restricted zones during LRS, taking into account factors such as
curvature, diameter variations, and potential obstacles. This paper delves
deeply into the complexities of workspace analysis for robotic LRS,
illustrating the seamless collaboration between medical imaging, CAD software,
and surgical robotics. Through this interdisciplinary approach, the study aims
to surpass traditional surgical methodologies, offering novel insights for a
paradigm shift in optimizing robotic interventions within the complex
environment of the pelvis.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14388" title="Abstract">arXiv:2402.14388</a> [<a href="/pdf/2402.14388" title="Download PDF">pdf</a>, <a href="/format/2402.14388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Welding Robotization via Operator Skill Identification,  Modeling, and Human-Machine Collaboration: Experimental Protocol  Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%A9nat%2C+A">Antoine L&#xe9;nat</a> (CETIM, LS2N, LS2N - &#xe9;quipe RoMas, LS2N - &#xe9;quipe PACCE), 
<a href="/search/cs?searchtype=author&query=Cheminat%2C+O">Olivier Cheminat</a> (CETIM), 
<a href="/search/cs?searchtype=author&query=Chablat%2C+D">Damien Chablat</a> (LS2N, LS2N - &#xe9;quipe RoMas), 
<a href="/search/cs?searchtype=author&query=Charron%2C+C">Camilo Charron</a> (LS2N, UR2)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Human-Computer Interaction, Springer,
  2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The industry of the future, also known as Industry 5.0, aims to modernize
production tools, digitize workshops, and cultivate the invaluable human
capital within the company. Industry 5.0 can't be done without fostering a
workforce that is not only technologically adept but also has enhanced skills
and knowledge. Specifically, collaborative robotics plays a key role in
automating strenuous or repetitive tasks, enabling human cognitive functions to
contribute to quality and innovation. In manual manufacturing, however, some of
these tasks remain challenging to automate without sacrificing quality. In
certain situations, these tasks require operators to dynamically organize their
mental, perceptual, and gestural activities. In other words, skills that are
not yet adequately explained and digitally modeled to allow a machine in an
industrial context to reproduce them, even in an approximate manner. Some tasks
in welding serve as a perfect example. Drawing from the knowledge of cognitive
and developmental psychology, professional didactics, and collaborative
robotics research, our work aims to find a way to digitally model manual
manufacturing skills to enhance the automation of tasks that are still
challenging to robotize. Using welding as an example, we seek to develop, test,
and deploy a methodology transferable to other domains. The purpose of this
article is to present the experimental setup used to achieve these objectives.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14389" title="Abstract">arXiv:2402.14389</a> [<a href="/pdf/2402.14389" title="Download PDF">pdf</a>, <a href="/format/2402.14389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Securing Transactions: A Hybrid Dependable Ensemble Machine Learning  Model using IHT-LR and Grid Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Talukder%2C+M+A">Md. Alamin Talukder</a>, 
<a href="/search/cs?searchtype=author&query=Hossen%2C+R">Rakib Hossen</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+A">Md Ashraf Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+N">Mohammed Nasir Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Acharjee%2C+U+K">Uzzal Kumar Acharjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Q1, Scopus, ISI, ESCI, IF: 4.8 (Accepted on Jan 19, 2024 - Cybersecurity, Springer Open Journal)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; General Finance (q-fin.GN)

</div>
<p class="mathjax">Financial institutions and businesses face an ongoing challenge from
fraudulent transactions, prompting the need for effective detection methods.
Detecting credit card fraud is crucial for identifying and preventing
unauthorized transactions.Timely detection of fraud enables investigators to
take swift actions to mitigate further losses. However, the investigation
process is often time-consuming, limiting the number of alerts that can be
thoroughly examined each day. Therefore, the primary objective of a fraud
detection model is to provide accurate alerts while minimizing false alarms and
missed fraud cases. In this paper, we introduce a state-of-the-art hybrid
ensemble (ENS) dependable Machine learning (ML) model that intelligently
combines multiple algorithms with proper weighted optimization using Grid
search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor
(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To
address the data imbalance issue, we employ the Instant Hardness Threshold
(IHT) technique in conjunction with Logistic Regression (LR), surpassing
conventional approaches. Our experiments are conducted on a publicly available
credit card dataset comprising 284,807 transactions. The proposed model
achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a
perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid
ensemble model outperforms existing works, establishing a new benchmark for
detecting fraudulent transactions in high-frequency scenarios. The results
highlight the effectiveness and reliability of our approach, demonstrating
superior performance metrics and showcasing its exceptional potential for
real-world fraud detection applications.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14391" title="Abstract">arXiv:2402.14391</a> [<a href="/pdf/2402.14391" title="Download PDF">pdf</a>, <a href="/format/2402.14391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction  Prediction via Microenvironment-Aware Protein Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Protein-Protein Interactions (PPIs) are fundamental in various biological
processes and play a key role in life activities. The growing demand and cost
of experimental PPI assays require computational methods for efficient PPI
prediction. While existing methods rely heavily on protein sequence for PPI
prediction, it is the protein structure that is the key to determine the
interactions. To take both protein modalities into account, we define the
microenvironment of an amino acid residue by its sequence and structural
contexts, which describe the surrounding chemical properties and geometric
features. In addition, microenvironments defined in previous work are largely
based on experimentally assayed physicochemical properties, for which the
"vocabulary" is usually extremely small. This makes it difficult to cover the
diversity and complexity of microenvironments. In this paper, we propose
Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which
encodes microenvironments into chemically meaningful discrete codes via a
sufficiently large microenvironment "vocabulary" (i.e., codebook). Moreover, we
propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM),
to capture the dependencies between different microenvironments by randomly
masking the codebook and reconstructing the input. With the learned
microenvironment codebook, we can reuse it as an off-the-shelf tool to
efficiently and effectively encode proteins of different sizes and functions
for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can
scale to PPI prediction with millions of PPIs with superior trade-offs between
effectiveness and computational efficiency than the state-of-the-art
competitors.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14392" title="Abstract">arXiv:2402.14392</a> [<a href="/pdf/2402.14392" title="Download PDF">pdf</a>, <a href="/format/2402.14392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Relevant Feature from Global Representation Memory for Visual  Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pinxue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinglun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+W">Weifeng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9pages,5 figures, accepted py the Thirty-seventh Conference on Neural Information Processing Systems(Neurips 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reference features from a template or historical frames are crucial for
visual object tracking. Prior works utilize all features from a fixed template
or memory for visual object tracking. However, due to the dynamic nature of
videos, the required reference historical information for different search
regions at different time steps is also inconsistent. Therefore, using all
features in the template and memory can lead to redundancy and impair tracking
performance. To alleviate this issue, we propose a novel tracking paradigm,
consisting of a relevance attention mechanism and a global representation
memory, which can adaptively assist the search region in selecting the most
relevant historical information from reference features. Specifically, the
proposed relevance attention mechanism in this work differs from previous
approaches in that it can dynamically choose and build the optimal global
representation memory for the current frame by accessing cross-frame
information globally. Moreover, it can flexibly read the relevant historical
information from the constructed memory to reduce redundancy and counteract the
negative effects of harmful information. Extensive experiments validate the
effectiveness of the proposed method, achieving competitive performance on five
challenging datasets with 71 FPS.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14393" title="Abstract">arXiv:2402.14393</a> [<a href="/pdf/2402.14393" title="Download PDF">pdf</a>, <a href="/format/2402.14393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Parsing Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunchong Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenghu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph pooling compresses graph information into a compact representation.
State-of-the-art graph pooling methods follow a hierarchical approach, which
reduces the graph size step-by-step. These methods must balance memory
efficiency with preserving node information, depending on whether they use node
dropping or node clustering. Additionally, fixed pooling ratios or numbers of
pooling layers are predefined for all graphs, which prevents personalized
pooling structures from being captured for each individual graph. In this work,
inspired by bottom-up grammar induction, we propose an efficient graph parsing
algorithm to infer the pooling structure, which then drives graph pooling. The
resulting Graph Parsing Network (GPN) adaptively learns personalized pooling
structure for each individual graph. GPN benefits from the discrete assignments
generated by the graph parsing algorithm, allowing good memory efficiency while
preserving node information intact. Experimental results on standard benchmarks
demonstrate that GPN outperforms state-of-the-art graph pooling methods in
graph classification tasks while being able to achieve competitive performance
in node classification tasks. We also conduct a graph reconstruction task to
show GPN's ability to preserve node information and measure both memory and
time efficiency through relevant tests.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14395" title="Abstract">arXiv:2402.14395</a> [<a href="/pdf/2402.14395" title="Download PDF">pdf</a>, <a href="/format/2402.14395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Image Synthesis with Unconditional Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chae%2C+J">Jungwoo Chae</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Go%2C+S">Sooyeon Go</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+K">Kyungmook Choi</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, Project Page: <a href="https://hhyunn2.github.io/SIS_UncondG/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic image synthesis (SIS) aims to generate realistic images that match
given semantic masks. Despite recent advances allowing high-quality results and
precise spatial control, they require a massive semantic segmentation dataset
for training the models. Instead, we propose to employ a pre-trained
unconditional generator and rearrange its feature maps according to proxy
masks. The proxy masks are prepared from the feature maps of random samples in
the generator by simple clustering. The feature rearranger learns to rearrange
original feature maps to match the shape of the proxy masks that are either
from the original sample itself or from random samples. Then we introduce a
semantic mapper that produces the proxy masks from various input conditions
including semantic masks. Our method is versatile across various applications
such as free-form spatial editing of real images, sketch-to-photo, and even
scribble-to-photo. Experiments validate advantages of our method on a range of
datasets: human faces, animal faces, and buildings.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14397" title="Abstract">arXiv:2402.14397</a> [<a href="/pdf/2402.14397" title="Download PDF">pdf</a>, <a href="/format/2402.14397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Form Bounds for DP-SGD against Record-level Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cherubin%2C+G">Giovanni Cherubin</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6pf%2C+B">Boris K&#xf6;pf</a>, 
<a href="/search/cs?searchtype=author&query=Paverd%2C+A">Andrew Paverd</a>, 
<a href="/search/cs?searchtype=author&query=Tople%2C+S">Shruti Tople</a>, 
<a href="/search/cs?searchtype=author&query=Wutschitz%2C+L">Lukas Wutschitz</a>, 
<a href="/search/cs?searchtype=author&query=Zanella-B%C3%A9guelin%2C+S">Santiago Zanella-B&#xe9;guelin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning models trained with differentially-private (DP) algorithms
such as DP-SGD enjoy resilience against a wide range of privacy attacks.
Although it is possible to derive bounds for some attacks based solely on an
$(\varepsilon,\delta)$-DP guarantee, meaningful bounds require a small enough
privacy budget (i.e., injecting a large amount of noise), which results in a
large loss in utility. This paper presents a new approach to evaluate the
privacy of machine learning models against specific record-level threats, such
as membership and attribute inference, without the indirection through DP. We
focus on the popular DP-SGD algorithm, and derive simple closed-form bounds.
Our proofs model DP-SGD as an information theoretic channel whose inputs are
the secrets that an attacker wants to infer (e.g., membership of a data record)
and whose outputs are the intermediate model parameters produced by iterative
optimization. We obtain bounds for membership inference that match
state-of-the-art techniques, whilst being orders of magnitude faster to
compute. Additionally, we present a novel data-dependent bound against
attribute inference. Our results provide a direct, interpretable, and practical
way to evaluate the privacy of trained models against specific inference
threats without sacrificing utility.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14398" title="Abstract">arXiv:2402.14398</a> [<a href="/pdf/2402.14398" title="Download PDF">pdf</a>, <a href="/format/2402.14398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion  and Image Attribute Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mengqi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhendong Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 18 figures, published to AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">GAN-based image attribute editing firstly leverages GAN Inversion to project
real images into the latent space of GAN and then manipulates corresponding
latent codes. Recent inversion methods mainly utilize additional high-bit
features to improve image details preservation, as low-bit codes cannot
faithfully reconstruct source images, leading to the loss of details. However,
during editing, existing works fail to accurately complement the lost details
and suffer from poor editability. The main reason is they inject all the lost
details indiscriminately at one time, which inherently induces the position and
quantity of details to overfit source images, resulting in inconsistent content
and artifacts in edited images. This work argues that details should be
gradually injected into both the reconstruction and editing process in a
multi-stage coarse-to-fine manner for better detail preservation and high
editability. Therefore, a novel dual-stream framework is proposed to accurately
complement details at each stage. The Reconstruction Stream is employed to
embed coarse-to-fine lost details into residual features and then adaptively
add them to the GAN generator. In the Editing Stream, residual features are
accurately aligned by our Selective Attention mechanism and then injected into
the editing process in a multi-stage manner. Extensive experiments have shown
the superiority of our framework in both reconstruction accuracy and editing
quality compared with existing methods.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14399" title="Abstract">arXiv:2402.14399</a> [<a href="/pdf/2402.14399" title="Download PDF">pdf</a>, <a href="/format/2402.14399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream  Paradigm for Live Streaming Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+F">Fengqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Baigong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liqin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guorui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yanan Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Live streaming recommender system is specifically designed to recommend
real-time live streaming of interest to users. Due to the dynamic changes of
live content, improving the timeliness of the live streaming recommender system
is a critical problem. Intuitively, the timeliness of the data determines the
upper bound of the timeliness that models can learn. However, none of the
previous works addresses the timeliness problem of the live streaming
recommender system from the perspective of data stream design. Employing the
conventional fixed window data stream paradigm introduces a trade-off dilemma
between labeling accuracy and timeliness. In this paper, we propose a new data
stream design paradigm, dubbed Sliver, that addresses the timeliness and
accuracy problem of labels by reducing the window size and implementing a
sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco
strategy reducing the latency between request and impression to improve the
timeliness of the recommendation service and features by periodically
requesting the recommendation service. To demonstrate the effectiveness of our
approach, we conduct offline experiments on a multi-task live streaming dataset
with labeling timestamps collected from the Kuaishou live streaming platform.
Experimental results demonstrate that Sliver outperforms two fixed-window data
streams with varying window sizes across all targets in four typical multi-task
recommendation models. Furthermore, we deployed Sliver on the Kuaishou live
streaming platform. Results of the online A/B test show a significant
improvement in click-through rate (CTR), and new follow number (NFN), further
validating the effectiveness of Sliver.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14400" title="Abstract">arXiv:2402.14400</a> [<a href="/pdf/2402.14400" title="Download PDF">pdf</a>, <a href="/format/2402.14400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holmberg%2C+D">Daniel Holmberg</a>, 
<a href="/search/cs?searchtype=author&query=Airaksinen%2C+M">Manu Airaksinen</a>, 
<a href="/search/cs?searchtype=author&query=Marchi%2C+V">Viviana Marchi</a>, 
<a href="/search/cs?searchtype=author&query=Guzzetta%2C+A">Andrea Guzzetta</a>, 
<a href="/search/cs?searchtype=author&query=Kivi%2C+A">Anna Kivi</a>, 
<a href="/search/cs?searchtype=author&query=Haataja%2C+L">Leena Haataja</a>, 
<a href="/search/cs?searchtype=author&query=Vanhatalo%2C+S">Sampsa Vanhatalo</a>, 
<a href="/search/cs?searchtype=author&query=Roos%2C+T">Teemu Roos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures. Code repository available via <a href="https://github.com/deinal/infant-aagcn">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Reliable methods for the neurodevelopmental assessment of infants are
essential for early detection of medical issues that may need prompt
interventions. Spontaneous motor activity, or `kinetics', is shown to provide a
powerful surrogate measure of upcoming neurodevelopment. However, its
assessment is by and large qualitative and subjective, focusing on visually
identified, age-specific gestures. Here, we follow an alternative approach,
predicting infants' neurodevelopmental maturation based on data-driven
evaluation of individual motor patterns. We utilize 3D video recordings of
infants processed with pose-estimation to extract spatio-temporal series of
anatomical landmarks, and apply adaptive graph convolutional networks to
predict the actual age. We show that our data-driven approach achieves
improvement over traditional machine learning baselines based on manually
engineered features.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14401" title="Abstract">arXiv:2402.14401</a> [<a href="/pdf/2402.14401" title="Download PDF">pdf</a>, <a href="/format/2402.14401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model Based Visual Compensation Guidance and Visual Difference  Analysis for No-Reference Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Leida Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Maoguo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA)
methods still suffer from finding a balance between learning feature
information at the pixel level of the image and capturing high-level feature
information and the efficient utilization of the obtained high-level feature
information remains a challenge. As a novel class of state-of-the-art (SOTA)
generative model, the diffusion model exhibits the capability to model
intricate relationships, enabling a comprehensive understanding of images and
possessing a better learning of both high-level and low-level visual features.
In view of these, we pioneer the exploration of the diffusion model into the
domain of NR-IQA. Firstly, we devise a new diffusion restoration network that
leverages the produced enhanced image and noise-containing images,
incorporating nonlinear features obtained during the denoising process of the
diffusion model, as high-level visual information. Secondly, two visual
evaluation branches are designed to comprehensively analyze the obtained
high-level feature information. These include the visual compensation guidance
branch, grounded in the transformer architecture and noise embedding strategy,
and the visual difference analysis branch, built on the ResNet architecture and
the residual transposed attention block. Extensive experiments are conducted on
seven public NR-IQA datasets, and the results demonstrate that the proposed
model outperforms SOTA methods for NR-IQA.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14402" title="Abstract">arXiv:2402.14402</a> [<a href="/pdf/2402.14402" title="Download PDF">pdf</a>, <a href="/format/2402.14402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Safe Sequential Learning via Efficient Knowledge Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cen-You Li</a>, 
<a href="/search/cs?searchtype=author&query=Duennbier%2C+O">Olaf Duennbier</a>, 
<a href="/search/cs?searchtype=author&query=Toussaint%2C+M">Marc Toussaint</a>, 
<a href="/search/cs?searchtype=author&query=Rakitsch%2C+B">Barbara Rakitsch</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+C">Christoph Zimmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Sequential learning methods such as active learning and Bayesian optimization
select the most informative data to learn about a task. In many medical or
engineering applications, the data selection is constrained by a priori unknown
safety conditions. A promissing line of safe learning methods utilize Gaussian
processes (GPs) to model the safety probability and perform data selection in
areas with high safety confidence. However, accurate safety modeling requires
prior knowledge or consumes data. In addition, the safety confidence centers
around the given observations which leads to local exploration. As transferable
source knowledge is often available in safety critical experiments, we propose
to consider transfer safe sequential learning to accelerate the learning of
safety. We further consider a pre-computation of source components to reduce
the additional computational load that is introduced by incorporating source
data. In this paper, we theoretically analyze the maximum explorable safe
regions of conventional safe learning methods. Furthermore, we empirically
demonstrate that our approach 1) learns a task with lower data consumption, 2)
globally explores multiple disjoint safe regions under guidance of the source
knowledge, and 3) operates with computation comparable to conventional safe
learning methods.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14404" title="Abstract">arXiv:2402.14404</a> [<a href="/pdf/2402.14404" title="Download PDF">pdf</a>, <a href="/format/2402.14404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Menghan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+P">Peng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Probing and enhancing large language models' reasoning capacity remains a
crucial open question. Here we re-purpose the reverse dictionary task as a case
study to probe LLMs' capacity for conceptual inference. We use in-context
learning to guide the models to generate the term for an object concept implied
in a linguistic description. Models robustly achieve high accuracy in this
task, and their representation space encodes information about object
categories and fine-grained features. Further experiments suggest that the
conceptual inference ability as probed by the reverse-dictionary task predicts
model's general reasoning performance across multiple benchmarks, despite
similar syntactic generalization behaviors across models. Explorative analyses
suggest that prompting LLMs with description$\Rightarrow$word examples may
induce generalization beyond surface-level differences in task construals and
facilitate models on broader commonsense reasoning problems.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14407" title="Abstract">arXiv:2402.14407</a> [<a href="/pdf/2402.14407" title="Download PDF">pdf</a>, <a href="/format/2402.14407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Actionless Video Pre-Training via Discrete Diffusion for  Efficient Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoran He</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+C">Chenjia Bai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. In this paper, we introduce a novel framework that leverages a
unified discrete diffusion to combine generative pre-training on human videos
and policy fine-tuning on a small number of action-labeled robot videos. We
start by compressing both human and robot videos into unified video tokens. In
the pre-training stage, we employ a discrete diffusion model with a
mask-and-replace diffusion strategy to predict future video tokens in the
latent space. In the fine-tuning stage, we harness the imagined future videos
to guide low-level action learning trained on a limited set of robot data.
Experiments demonstrate that our method generates high-fidelity future videos
for planning and enhances the fine-tuned policies compared to previous
state-of-the-art approaches with superior generalization ability. Our project
website is available at https://video-diff.github.io/.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14408" title="Abstract">arXiv:2402.14408</a> [<a href="/pdf/2402.14408" title="Download PDF">pdf</a>, <a href="/format/2402.14408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferring BERT Capabilities from High-Resource to Low-Resource  Languages Using Vocabulary Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rybak%2C+P">Piotr Rybak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models have revolutionized the natural language
understanding landscape, most notably BERT (Bidirectional Encoder
Representations from Transformers). However, a significant challenge remains
for low-resource languages, where limited data hinders the effective training
of such models. This work presents a novel approach to bridge this gap by
transferring BERT capabilities from high-resource to low-resource languages
using vocabulary matching. We conduct experiments on the Silesian and Kashubian
languages and demonstrate the effectiveness of our approach to improve the
performance of BERT models even when the target language has minimal training
data. Our results highlight the potential of the proposed technique to
effectively train BERT models for low-resource languages, thus democratizing
access to advanced language understanding models.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14409" title="Abstract">arXiv:2402.14409</a> [<a href="/pdf/2402.14409" title="Download PDF">pdf</a>, <a href="/format/2402.14409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tug-of-War Between Knowledge: Exploring and Resolving Knowledge  Conflicts in Retrieval-Augmented Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhuoran Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Pengfei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaojian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiexin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiuxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Retrieval-augmented language models (RALMs) have demonstrated significant
potential in refining and expanding their internal memory by retrieving
evidence from external sources. However, RALMs will inevitably encounter
knowledge conflicts when integrating their internal memory with external
sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between
knowledge, limiting their practical applicability. In this paper, we focus on
exploring and resolving knowledge conflicts in RALMs. First, we present an
evaluation framework for assessing knowledge conflicts across various
dimensions. Then, we investigate the behavior and preference of RALMs from the
following two perspectives: (1) Conflicts between internal memory and external
sources: We find that stronger RALMs emerge with the Dunning-Kruger effect,
persistently favoring their faulty internal memory even when correct evidence
is provided. Besides, RALMs exhibit an availability bias towards common
knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:
We reveal that RALMs follow the principle of majority rule, leaning towards
placing trust in evidence that appears more frequently. Moreover, we find that
RALMs exhibit confirmation bias, and are more willing to choose evidence that
is consistent with their internal memory. To solve the challenge of knowledge
conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding
(CD2) to better calibrate the model's confidence. Experimental results
demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14410" title="Abstract">arXiv:2402.14410</a> [<a href="/pdf/2402.14410" title="Download PDF">pdf</a>, <a href="/format/2402.14410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-machine social systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsvetkova%2C+M">Milena Tsvetkova</a>, 
<a href="/search/cs?searchtype=author&query=Yasseri%2C+T">Taha Yasseri</a>, 
<a href="/search/cs?searchtype=author&query=Pescetelli%2C+N">Niccolo Pescetelli</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+T">Tobias Werner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">From fake accounts on social media and generative-AI bots such as ChatGPT to
high-frequency trading algorithms on financial markets and self-driving
vehicles on the streets, robots, bots, and algorithms are proliferating and
permeating our communication channels, social interactions, economic
transactions, and transportation arteries. Networks of multiple interdependent
and interacting humans and autonomous machines constitute complex adaptive
social systems where the collective outcomes cannot be simply deduced from
either human or machine behavior alone. Under this paradigm, we review recent
experimental, theoretical, and observational research from across a range of
disciplines - robotics, human-computer interaction, web science, complexity
science, computational social science, finance, economics, political science,
social psychology, and sociology. We identify general dynamics and patterns in
situations of competition, coordination, cooperation, contagion, and collective
decision-making, and contextualize them in four prominent existing
human-machine communities: high-frequency trading markets, the social media
platform formerly known as Twitter, the open-collaboration encyclopedia
Wikipedia, and the news aggregation and discussion community Reddit. We
conclude with suggestions for the research, design, and governance of
human-machine social systems, which are necessary to reduce misinformation,
prevent financial crashes, improve road safety, overcome labor market
disruptions, and enable a better human future.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14411" title="Abstract">arXiv:2402.14411</a> [<a href="/pdf/2402.14411" title="Download PDF">pdf</a>, <a href="/format/2402.14411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> J-UniMorph: Japanese Morphological Annotation through the Universal  Feature Schema
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsuzaki%2C+K">Kosuke Matsuzaki</a>, 
<a href="/search/cs?searchtype=author&query=Taniguchi%2C+M">Masaya Taniguchi</a>, 
<a href="/search/cs?searchtype=author&query=Inui%2C+K">Kentaro Inui</a>, 
<a href="/search/cs?searchtype=author&query=Sakaguchi%2C+K">Keisuke Sakaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce a Japanese Morphology dataset, J-UniMorph, developed based on
the UniMorph feature schema. This dataset addresses the unique and rich verb
forms characteristic of the language's agglutinative nature. J-UniMorph
distinguishes itself from the existing Japanese subset of UniMorph, which is
automatically extracted from Wiktionary. On average, the Wiktionary Edition
features around 12 inflected forms for each word and is primarily dominated by
denominal verbs (i.e., [noun] +suru (do-PRS)). Morphologically, this form is
equivalent to the verb suru (do). In contrast, J-UniMorph explores a much
broader and more frequently used range of verb forms, offering 118 inflected
forms for each word on average. It includes honorifics, a range of politeness
levels, and other linguistic nuances, emphasizing the distinctive
characteristics of the Japanese language. This paper presents detailed
statistics and characteristics of J-UniMorph, comparing it with the Wiktionary
Edition. We release J-UniMorph and its interactive visualizer publicly
available, aiming to support cross-linguistic research and various
applications.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14415" title="Abstract">arXiv:2402.14415</a> [<a href="/pdf/2402.14415" title="Download PDF">pdf</a>, <a href="/format/2402.14415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via  Direct Taylor-based Grid Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Renyi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+P">Peng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tieru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Rui Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Coordinate-based neural implicit representation or implicit fields have been
widely studied for 3D geometry representation or novel view synthesis.
Recently, a series of efforts have been devoted to accelerating the speed and
improving the quality of the coordinate-based implicit field learning. Instead
of learning heavy MLPs to predict the neural implicit values for the query
coordinates, neural voxels or grids combined with shallow MLPs have been
proposed to achieve high-quality implicit field learning with reduced
optimization time. On the other hand, lightweight field representations such as
linear grid have been proposed to further improve the learning speed. In this
paper, we aim for both fast and high-quality implicit field learning, and
propose TaylorGrid, a novel implicit field representation which can be
efficiently computed via direct Taylor expansion optimization on 2D or 3D
grids. As a general representation, TaylorGrid can be adapted to different
implicit fields learning tasks such as SDF learning or NeRF. From extensive
quantitative and qualitative comparisons, TaylorGrid achieves a balance between
the linear grid and neural voxels, showing its superiority in fast and
high-quality implicit field learning.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14418" title="Abstract">arXiv:2402.14418</a> [<a href="/pdf/2402.14418" title="Download PDF">pdf</a>, <a href="/format/2402.14418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Evaluation for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kostumov%2C+V">Vasily Kostumov</a>, 
<a href="/search/cs?searchtype=author&query=Nutfullin%2C+B">Bulat Nutfullin</a>, 
<a href="/search/cs?searchtype=author&query=Pilipenko%2C+O">Oleg Pilipenko</a>, 
<a href="/search/cs?searchtype=author&query=Ilyushin%2C+E">Eugene Ilyushin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in
popularity recently due to their impressive performance in several
vision-language tasks. Current evaluation methods, however, overlook an
essential component: uncertainty, which is crucial for a comprehensive
assessment of VLMs. Addressing this oversight, we present a benchmark
incorporating uncertainty quantification into evaluating VLMs.
<br />Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question
Answering (VQA) task. We examine models on 5 datasets that evaluate various
vision-language capabilities.
<br />Using conformal prediction as an uncertainty estimation approach, we
demonstrate that the models' uncertainty is not aligned with their accuracy.
Specifically, we show that models with the highest accuracy may also have the
highest uncertainty, which confirms the importance of measuring it for VLMs.
Our empirical findings also reveal a correlation between model uncertainty and
its language model part.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14424" title="Abstract">arXiv:2402.14424</a> [<a href="/pdf/2402.14424" title="Download PDF">pdf</a>, <a href="/format/2402.14424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating Psychological Hypothesis Generation with AI: Large Language  Models Meet Causal Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Song Tong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kai Mao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kaiping Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Leveraging the synergy between causal knowledge graphs and a large language
model (LLM), our study introduces a groundbreaking approach for computational
hypothesis generation in psychology. We analyzed 43,312 psychology articles
using a LLM to extract causal relation pairs. This analysis produced a
specialized causal graph for psychology. Applying link prediction algorithms,
we generated 130 potential psychological hypotheses focusing on `well-being',
then compared them against research ideas conceived by doctoral scholars and
those produced solely by the LLM. Interestingly, our combined approach of a LLM
and causal graphs mirrored the expert-level insights in terms of novelty,
clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =
4.32, p&lt;0.001, respectively). This alignment was further corroborated using
deep semantic analysis. Our results show that combining LLM with machine
learning techniques such as causal knowledge graphs can revolutionize automated
discovery in psychology, extracting novel insights from the extensive
literature. This work stands at the crossroads of psychology and artificial
intelligence, championing a new enriched paradigm for data-driven hypothesis
generation in psychological research.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14427" title="Abstract">arXiv:2402.14427</a> [<a href="/pdf/2402.14427" title="Download PDF">pdf</a>, <a href="/format/2402.14427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text me the data: Generating Ground Pressure Sequence from Textual  Descriptions for HAR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ray%2C+L+S+S">Lala Shakti Swarup Ray</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+S">Sungho Suh</a>, 
<a href="/search/cs?searchtype=author&query=Krupp%2C+L">Lars Krupp</a>, 
<a href="/search/cs?searchtype=author&query=Rey%2C+V+F">Vitor Fortes Rey</a>, 
<a href="/search/cs?searchtype=author&query=Lukowicz%2C+P">Paul Lukowicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PerCom2024WiP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Signal Processing (eess.SP)

</div>
<p class="mathjax">In human activity recognition (HAR), the availability of substantial ground
truth is necessary for training efficient models. However, acquiring ground
pressure data through physical sensors itself can be cost-prohibitive,
time-consuming. To address this critical need, we introduce Text-to-Pressure
(T2P), a framework designed to generate extensive ground pressure sequences
from textual descriptions of human activities using deep learning techniques.
We show that the combination of vector quantization of sensor data along with
simple text conditioned auto regressive strategy allows us to obtain
high-quality generated pressure sequences from textual descriptions with the
help of discrete latent correlation between text and pressure maps. We achieved
comparable performance on the consistency between text and generated motion
with an R squared value of 0.722, Masked R squared value of 0.892, and FID
score of 1.83. Additionally, we trained a HAR model with the the synthesized
data and evaluated it on pressure dynamics collected by a real pressure sensor
which is on par with a model trained on only real data. Combining both real and
synthesized training data increases the overall macro F1 score by 5.9 percent.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14428" title="Abstract">arXiv:2402.14428</a> [<a href="/pdf/2402.14428" title="Download PDF">pdf</a>, <a href="/format/2402.14428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KoCoSa: Korean Context-aware Sarcasm Detection Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yumin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+H">Heejae Suh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mingi Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+D">Dongyeon Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwanhee Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sarcasm is a way of verbal irony where someone says the opposite of what they
mean, often to ridicule a person, situation, or idea. It is often difficult to
detect sarcasm in the dialogue since detecting sarcasm should reflect the
context (i.e., dialogue history). In this paper, we introduce a new dataset for
the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware
Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and
the labels for this task on the last response. To build the dataset, we propose
an efficient sarcasm detection dataset generation pipeline: 1) generating new
sarcastic dialogues from source dialogues with large language models, 2)
automatic and manual filtering of abnormal and toxic dialogues, and 3) human
annotation for the sarcasm detection task. We also provide a simple but
effective baseline for the Korean sarcasm detection task trained on our
dataset. Experimental results on the dataset show that our baseline system
outperforms strong baselines like large language models, such as GPT-3.5, in
the Korean sarcasm detection task. We show that the sarcasm detection task
relies deeply on the existence of sufficient context. We will release the
dataset at https://anonymous.4open.science/r/KoCoSa-2372.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14430" title="Abstract">arXiv:2402.14430</a> [<a href="/pdf/2402.14430" title="Download PDF">pdf</a>, <a href="/format/2402.14430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Training of Federated Models with Extremely Label Deficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xinmei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024, 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm
for collaboratively training machine learning models using distributed data
with label deficiency. Advanced FSSL methods predominantly focus on training a
single model on each client. However, this approach could lead to a discrepancy
between the objective functions of labeled and unlabeled data, resulting in
gradient conflicts. To alleviate gradient conflict, we propose a novel
twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by
providing insights from different perspectives of labeled and unlabeled data.
In particular, Twin-sight concurrently trains a supervised model with a
supervised objective function while training an unsupervised model using an
unsupervised objective function. To enhance the synergy between these two
models, Twin-sight introduces a neighbourhood-preserving constraint, which
encourages the preservation of the neighbourhood relationship among data
features extracted by both models. Our comprehensive experiments on four
benchmark datasets provide substantial evidence that Twin-sight can
significantly outperform state-of-the-art methods across various experimental
settings, demonstrating the efficacy of the proposed Twin-sight.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14432" title="Abstract">arXiv:2402.14432</a> [<a href="/pdf/2402.14432" title="Download PDF">pdf</a>, <a href="/format/2402.14432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Influence of Driving Context on Lateral Driving Style  Preferences: A Simulator-Based Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haselberger%2C+J">Johann Haselberger</a>, 
<a href="/search/eess?searchtype=author&query=B%C3%B6hle%2C+M">Maximilian B&#xf6;hle</a>, 
<a href="/search/eess?searchtype=author&query=Schick%2C+B">Bernhard Schick</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+S">Steffen M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 11 figures, The dataset, comprising anonymized sociodemographics, questionnaire responses, and simulator measurements along with labels, is openly accessible at <a href="https://www.kaggle.com/datasets/jhaselberger/idcld-subject-study-on-driving-style-preferences">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Technological advancements focus on developing comfortable and acceptable
driving characteristics in autonomous vehicles. Present driving functions
predominantly possess predefined parameters, and there is no universally
accepted driving style for autonomous vehicles. Although driving may be
technically safe, the passenger might still feel insecure due to a mismatch in
driving styles between the human and the autonomous system. Incorporating
driving style preferences into automated vehicles enhances acceptance, reduces
uncertainty, and poses the opportunity to expedite their adoption. Despite the
increased research focus on driving styles, there remains a need for
comprehensive studies investigating how variations in the driving context
impact the assessment of automated driving functions. Therefore, this work
evaluates lateral driving style preferences for autonomous vehicles on rural
roads, considering different weather and traffic situations. A controlled study
(N = 32) was conducted with a variety of German participants utilizing a
high-fidelity driving simulator. The subjects experienced four different
driving styles, including mimicking of their own driving behavior under two
weather conditions. A notable preference for the more passive driving style
became evident based on statistical analyses of participants' responses during
and after the drives. A low curve-cutting gradient, moderate lateral and
longitudinal acceleration constraints, and a pronounced reaction to oncoming
traffic characterize this style. This study could not confirm the hypothesis
that subjects prefer to be driven by mimicking their own driving behavior.
Furthermore, the study illustrated that weather conditions and oncoming traffic
substantially influence the perceived comfort during autonomous rides.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14433" title="Abstract">arXiv:2402.14433</a> [<a href="/pdf/2402.14433" title="Download PDF">pdf</a>, <a href="/format/2402.14433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Language Model&#x27;s Guide Through Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+R%C3%BCtte%2C+D">Dimitri von R&#xfc;tte</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Concept guidance has emerged as a cheap and simple way to control the
behavior of language models by probing their hidden representations for concept
vectors and using them to perturb activations at inference time. While the
focus of previous work has largely been on truthfulness, in this paper we
extend this framework to a richer set of concepts such as appropriateness,
humor, creativity and quality, and explore to what degree current detection and
guidance strategies work in these challenging settings. To facilitate
evaluation, we develop a novel metric for concept guidance that takes into
account both the success of concept elicitation as well as the potential
degradation in fluency of the guided model. Our extensive experiments reveal
that while some concepts such as truthfulness more easily allow for guidance
with current techniques, novel concepts such as appropriateness or humor either
remain difficult to elicit, need extensive tuning to work, or even experience
confusion. Moreover, we find that probes with optimal detection accuracies do
not necessarily make for the optimal guides, contradicting previous
observations for truthfulness. Our work warrants a deeper investigation into
the interplay between detectability, guidability, and the nature of the
concept, and we hope that our rich experimental test-bed for guidance research
inspires stronger follow-up approaches.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14440" title="Abstract">arXiv:2402.14440</a> [<a href="/pdf/2402.14440" title="Download PDF">pdf</a>, <a href="/format/2402.14440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommender for Its Purpose: Repeat and Exploration in Food Delivery  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peijie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender systems have been widely used for various scenarios, such as
e-commerce, news, and music, providing online contents to help and enrich
users' daily life. Different scenarios hold distinct and unique
characteristics, calling for domain-specific investigations and corresponding
designed recommender systems. Therefore, in this paper, we focus on food
delivery recommendations to unveil unique features in this domain, where users
order food online and enjoy their meals shortly after delivery. We first
conduct an in-depth analysis on food delivery datasets. The analysis shows that
repeat orders are prevalent for both users and stores, and situations'
differently influence repeat and exploration consumption in the food delivery
recommender systems. Moreover, we revisit the ability of existing
situation-aware methods for repeat and exploration recommendations
respectively, and find them unable to effectively solve both tasks
simultaneously. Based on the analysis and experiments, we have designed two
separate recommendation models -- ReRec for repeat orders and ExpRec for
exploration orders; both are simple in their design and computation. We conduct
experiments on three real-world food delivery datasets, and our proposed models
outperform various types of baselines on repeat, exploration, and combined
recommendation tasks. This paper emphasizes the importance of dedicated
analyses and methods for domain-specific characteristics for the recommender
system studies.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14449" title="Abstract">arXiv:2402.14449</a> [<a href="/pdf/2402.14449" title="Download PDF">pdf</a>, <a href="/format/2402.14449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On decentralized computation of the leader&#x27;s strategy in bi-level games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Maljkovic%2C+M">Marko Maljkovic</a>, 
<a href="/search/eess?searchtype=author&query=Nilsson%2C+G">Gustav Nilsson</a>, 
<a href="/search/eess?searchtype=author&query=Geroliminis%2C+N">Nikolas Geroliminis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Motivated by the omnipresence of hierarchical structures in many real-world
applications, this study delves into the intricate realm of bi-level games,
with a specific focus on exploring local Stackelberg equilibria as a solution
concept. While existing literature offers various methods tailored to specific
game structures featuring one leader and multiple followers, a comprehensive
framework providing formal convergence guarantees to a local Stackelberg
equilibrium appears to be lacking. Drawing inspiration from sensitivity results
for nonlinear programs and guided by the imperative to maintain scalability and
preserve agent privacy, we propose a decentralized approach based on the
projected gradient descent with the Armijo stepsize rule. The main challenge
here lies in assuring the existence and well-posedness of Jacobians that
describe the leader's decision's influence on the achieved equilibrium of the
followers. By meticulous tracking of the Implicit Function Theorem requirements
at each iteration, we establish formal convergence guarantees to a local
Stackelberg equilibrium for a broad class of bi-level games. Building on our
prior work on quadratic aggregative Stackelberg games, we also introduce a
decentralized warm-start procedure based on the consensus alternating direction
method of multipliers addressing the previously reported initialization issues.
Finally, we provide empirical validation through two case studies in smart
mobility, showcasing the effectiveness of our general method in handling
general convex constraints, and the effectiveness of its extension in tackling
initialization issues.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14453" title="Abstract">arXiv:2402.14453</a> [<a href="/pdf/2402.14453" title="Download PDF">pdf</a>, <a href="/format/2402.14453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gobara%2C+S">Seiji Gobara</a>, 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Education that suits the individual learning level is necessary to improve
students' understanding. The first step in achieving this purpose by using
large language models (LLMs) is to adjust the textual difficulty of the
response to students. This work analyzes how LLMs can implicitly adjust text
difficulty between user input and its generated text. To conduct the
experiments, we created a new dataset from Stack-Overflow to explore the
performance of question-answering-based conversation. Experimental results on
the Stack-Overflow dataset and the TSCC dataset, including multi-turn
conversation show that LLMs can implicitly handle text difficulty between user
input and its generated response. We also observed that some LLMs can surpass
humans in handling text difficulty and the importance of instruction-tuning.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14454" title="Abstract">arXiv:2402.14454</a> [<a href="/pdf/2402.14454" title="Download PDF">pdf</a>, <a href="/format/2402.14454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCPA: Long-term Person Re-Identification via Contrastive Clothing and  Pose Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+D">Vuong D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+K">Shishir K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Long-term Person Re-Identification (LRe-ID) aims at matching an individual
across cameras after a long period of time, presenting variations in clothing,
pose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and
Pose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body
shape information which is cloth-invariant using a Relation Graph Attention
Network. Training a robust LRe-ID model requires a wide range of clothing
variations and expensive cloth labeling, which is lacked in current LRe-ID
datasets. To address this, we perform clothing and pose transfer across
identities to generate images of more clothing variations and of different
persons wearing similar clothing. The augmented batch of images serve as inputs
to our proposed Fine-grained Contrastive Losses, which not only supervise the
Re-ID model to learn discriminative person embeddings under long-term scenarios
but also ensure in-distribution data generation. Results on LRe-ID datasets
demonstrate the effectiveness of our CCPA framework.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14455" title="Abstract">arXiv:2402.14455</a> [<a href="/pdf/2402.14455" title="Download PDF">pdf</a>, <a href="/format/2402.14455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;It Must Be Gesturing Towards Me&quot;: Gesture-Based Interaction between  Autonomous Vehicles and Pedestrians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiang Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuxin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tingmin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Haolin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zherui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jiangtao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages,22 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Interacting with pedestrians understandably and efficiently is one of the
toughest challenges faced by autonomous vehicles (AVs) due to the limitations
of current algorithms and external human-machine interfaces (eHMIs). In this
paper, we design eHMIs based on gestures inspired by the most popular method of
interaction between pedestrians and human drivers. Eight common gestures were
selected to convey AVs' yielding or non-yielding intentions at uncontrolled
crosswalks from previous literature. Through a VR experiment (N1 = 31) and a
following online survey (N2 = 394), we discovered significant differences in
the usability of gesture-based eHMIs compared to current eHMIs. Good
gesture-based eHMIs increase the efficiency of pedestrian-AV interaction while
ensuring safety. Poor gestures, however, cause misinterpretation. The
underlying reasons were explored: ambiguity regarding the recipient of the
signal and whether the gestures are precise, polite, and familiar to
pedestrians. Based on this empirical evidence, we discuss potential
opportunities and provide valuable insights into developing comprehensible
gesture-based eHMIs in the future to support better interaction between AVs and
other road users.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14456" title="Abstract">arXiv:2402.14456</a> [<a href="/pdf/2402.14456" title="Download PDF">pdf</a>, <a href="/format/2402.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+X">Xuan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jiaya Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Thanks to advances in deep learning techniques, Human Pose Estimation (HPE)
has achieved significant progress in natural scenarios. However, these models
perform poorly in artificial scenarios such as painting and sculpture due to
the domain gap, constraining the development of virtual reality and augmented
reality. With the growth of model size, retraining the whole model on both
natural and artificial data is computationally expensive and inefficient. Our
research aims to bridge the domain gap between natural and artificial scenarios
with efficient tuning strategies. Leveraging the potential of language models,
we enhance the adaptability of traditional pose estimation models across
diverse scenarios with a novel framework called VLPose. VLPose leverages the
synergy between language and vision to extend the generalization and robustness
of pose estimation models beyond the traditional domains. Our approach has
demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO,
respectively, compared to state-of-the-art tuning strategies.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14457" title="Abstract">arXiv:2402.14457</a> [<a href="/pdf/2402.14457" title="Download PDF">pdf</a>, <a href="/ps/2402.14457" title="Download PostScript">ps</a>, <a href="/format/2402.14457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Annotation and Classification of Relevant Clauses in  Terms-and-Conditions Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bizzaro%2C+P+G">Pietro Giovanni Bizzaro</a>, 
<a href="/search/cs?searchtype=author&query=Della+Valentina%2C+E">Elena Della Valentina</a>, 
<a href="/search/cs?searchtype=author&query=Napolitano%2C+M">Maurizio Napolitano</a>, 
<a href="/search/cs?searchtype=author&query=Mana%2C+N">Nadia Mana</a>, 
<a href="/search/cs?searchtype=author&query=Zancanaro%2C+M">Massimo Zancanaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-review version of the paper accepted to the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we propose a new annotation scheme to classify different types
of clauses in Terms-and-Conditions contracts with the ultimate goal of
supporting legal experts to quickly identify and assess problematic issues in
this type of legal documents. To this end, we built a small corpus of
Terms-and-Conditions contracts and finalized an annotation scheme of 14
categories, eventually reaching an inter-annotator agreement of 0.92. Then, for
11 of them, we experimented with binary classification tasks using few-shot
prompting with a multilingual T5 and two fine-tuned versions of two BERT-based
LLMs for Italian. Our experiments showed the feasibility of automatic
classification of our categories by reaching accuracies ranging from .79 to .95
on validation tasks.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14458" title="Abstract">arXiv:2402.14458</a> [<a href="/pdf/2402.14458" title="Download PDF">pdf</a>, <a href="/format/2402.14458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLAS-multi: A Multilingual Corpus of Automatically Generated Natural  Language Argumentation Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruiz-Dolz%2C+R">Ramon Ruiz-Dolz</a>, 
<a href="/search/cs?searchtype=author&query=Taverner%2C+J">Joaquin Taverner</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+J">John Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Reed%2C+C">Chris Reed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Some of the major limitations identified in the areas of argument mining,
argument generation, and natural language argument analysis are related to the
complexity of annotating argumentatively rich data, the limited size of these
corpora, and the constraints that represent the different languages and domains
in which these data is annotated. To address these limitations, in this paper
we present the following contributions: (i) an effective methodology for the
automatic generation of natural language arguments in different topics and
languages, (ii) the largest publicly available corpus of natural language
argumentation schemes, and (iii) a set of solid baselines and fine-tuned models
for the automatic identification of argumentation schemes.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14460" title="Abstract">arXiv:2402.14460</a> [<a href="/pdf/2402.14460" title="Download PDF">pdf</a>, <a href="/ps/2402.14460" title="Download PostScript">ps</a>, <a href="/format/2402.14460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reframing the Expected Free Energy: Four Formulations and a Unification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Champion%2C+T">Th&#xe9;ophile Champion</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+H">Howard Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Markovi%C4%87%2C+D">Dimitrije Markovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Grze%C5%9B%2C+M">Marek Grze&#x15b;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Active inference is a leading theory of perception, learning and decision
making, which can be applied to neuroscience, robotics, psychology, and machine
learning. Active inference is based on the expected free energy, which is
mostly justified by the intuitive plausibility of its formulations, e.g., the
risk plus ambiguity and information gain / pragmatic value formulations. This
paper seek to formalize the problem of deriving these formulations from a
single root expected free energy definition, i.e., the unification problem.
Then, we study two settings, each one having its own root expected free energy
definition. In the first setting, no justification for the expected free energy
has been proposed to date, but all the formulations can be recovered from it.
However, in this setting, the agent cannot have arbitrary prior preferences
over observations. Indeed, only a limited class of prior preferences over
observations is compatible with the likelihood mapping of the generative model.
In the second setting, a justification of the root expected free energy
definition is known, but this setting only accounts for two formulations, i.e.,
the risk over states plus ambiguity and entropy plus expected energy
formulations.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14461" title="Abstract">arXiv:2402.14461</a> [<a href="/pdf/2402.14461" title="Download PDF">pdf</a>, <a href="/format/2402.14461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph  Generation in OR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jialun Pei</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Diandian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Manxi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yueming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P">Pheng-Ann Heng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene graph generation (SGG) of surgical procedures is crucial in enhancing
holistically cognitive intelligence in the operating room (OR). However,
previous works have primarily relied on the multi-stage learning that generates
semantic scene graphs dependent on intermediate processes with pose estimation
and object detection, which may compromise model efficiency and efficacy, also
impose extra annotation burden. In this study, we introduce a novel
single-stage bimodal transformer framework for SGG in the OR, termed
S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D
point clouds for SGG in an end-to-end manner. Concretely, our model embraces a
View-Sync Transfusion scheme to encourage multi-view visual information
interaction. Concurrently, a Geometry-Visual Cohesion operation is designed to
integrate the synergic 2D semantic features into 3D point cloud features.
Moreover, based on the augmented feature, we propose a novel relation-sensitive
transformer decoder that embeds dynamic entity-pair queries and relational
trait priors, which enables the direct prediction of entity-pair relations for
graph generation without intermediate steps. Extensive experiments have
validated the superior SGG performance and lower computational cost of
S^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3%
Precision increase and 24.2M reduction in model parameters. We further compared
our method with generic single-stage SGG methods with broader metrics for a
comprehensive evaluation, with consistently better performance achieved. The
code will be made available.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14464" title="Abstract">arXiv:2402.14464</a> [<a href="/pdf/2402.14464" title="Download PDF">pdf</a>, <a href="/format/2402.14464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth  Supervision for Indoor Multi-View 3D Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chenxi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yuenan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Weicai Ye</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binbin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">NeRF-Det has achieved impressive performance in indoor multi-view 3D
detection by innovatively utilizing NeRF to enhance representation learning.
Despite its notable performance, we uncover three decisive shortcomings in its
current design, including semantic ambiguity, inappropriate sampling, and
insufficient utilization of depth supervision. To combat the aforementioned
problems, we present three corresponding solutions: 1) Semantic Enhancement. We
project the freely available 3D segmentation annotations onto the 2D plane and
leverage the corresponding 2D semantic maps as the supervision signal,
significantly enhancing the semantic awareness of multi-view detectors. 2)
Perspective-aware Sampling. Instead of employing the uniform sampling strategy,
we put forward the perspective-aware sampling policy that samples densely near
the camera while sparsely in the distance, more effectively collecting the
valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to
directly regressing the depth values that are difficult to optimize, we divide
the depth range of each scene into a fixed number of ordinal bins and
reformulate the depth prediction as the combination of the classification of
depth bins as well as the regression of the residual depth values, thereby
benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has
exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets.
Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9%
in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at
https://github.com/mrsempress/NeRF-Detplusplus.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14469" title="Abstract">arXiv:2402.14469</a> [<a href="/pdf/2402.14469" title="Download PDF">pdf</a>, <a href="/format/2402.14469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reimagining Anomalies: What If Anomalies Were Normal?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liznerski%2C+P">Philipp Liznerski</a>, 
<a href="/search/cs?searchtype=author&query=Varshneya%2C+S">Saurabh Varshneya</a>, 
<a href="/search/cs?searchtype=author&query=Calikus%2C+E">Ece Calikus</a>, 
<a href="/search/cs?searchtype=author&query=Fellenz%2C+S">Sophie Fellenz</a>, 
<a href="/search/cs?searchtype=author&query=Kloft%2C+M">Marius Kloft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages; preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep learning-based methods have achieved a breakthrough in image anomaly
detection, but their complexity introduces a considerable challenge to
understanding why an instance is predicted to be anomalous. We introduce a
novel explanation method that generates multiple counterfactual examples for
each anomaly, capturing diverse concepts of anomalousness. A counterfactual
example is a modification of the anomaly that is perceived as normal by the
anomaly detector. The method provides a high-level semantic explanation of the
mechanism that triggered the anomaly detector, allowing users to explore
"what-if scenarios." Qualitative and quantitative analyses across various image
datasets show that the method applied to state-of-the-art anomaly detectors can
achieve high-quality semantic explanations of detectors.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14471" title="Abstract">arXiv:2402.14471</a> [<a href="/pdf/2402.14471" title="Download PDF">pdf</a>, <a href="/ps/2402.14471" title="Download PostScript">ps</a>, <a href="/format/2402.14471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BUGFIX: towards a common language and framework for the AutomaticProgram  Repair community
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meyer%2C+B">Bertrand Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Kananchuk%2C+V">Viktoryia Kananchuk</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Li Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Techniques of Automatic Program Repair (APR) have the potential of thoroughly
facilitating the task of producing quality software. After a promising start,
however, progress in making APR practical has been hindered by the lack of a
common framework to support the multiplicity of APR ideas and tools, and of
target programming languages and environments.
<br />In this position paper we outline a general framework to enable the APR
community to benefit from each other\'s advances, in particular through a
standard language for describing bugs and their fixes. Such a common framework
(which is also applicable to work on fault seeding) could be a tremendous
benefit to researchers and developers of Interactive Development Environments
(IDEs) who are working to make APR an effective part of the practical
experience of software developers.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14473" title="Abstract">arXiv:2402.14473</a> [<a href="/pdf/2402.14473" title="Download PDF">pdf</a>, <a href="/format/2402.14473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Behavior-Aware Transformer for Multi-Behavior Sequential  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jiajie Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaochao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zibin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaolin Zheng</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 31st ACM International Conference on
  Multimedia. 2023: 6321-6331
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sequential Recommendation (SR) captures users' dynamic preferences by
modeling how users transit among items. However, SR models that utilize only
single type of behavior interaction data encounter performance degradation when
the sequences are short. To tackle this problem, we focus on Multi-Behavior
Sequential Recommendation (MBSR) in this paper, which aims to leverage
time-evolving heterogeneous behavioral dependencies for better exploring users'
potential intents on the target behavior. Solving MBSR is challenging. On the
one hand, users exhibit diverse multi-behavior patterns due to personal
characteristics. On the other hand, there exists comprehensive co-influence
between behavior correlations and item collaborations, the intensity of which
is deeply affected by temporal factors. To tackle these challenges, we propose
a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem,
which models personalized patterns and multifaceted sequential collaborations
in a novel way to boost recommendation performance. First, PBAT develops a
personalized behavior pattern generator in the representation layer, which
extracts dynamic and discriminative behavior patterns for sequential learning.
Second, PBAT reforms the self-attention layer with a behavior-aware
collaboration extractor, which introduces a fused behavior-aware attention
mechanism for incorporating both behavioral and temporal impacts into
collaborative transitions. We conduct experiments on three benchmark datasets
and the results demonstrate the effectiveness and interpretability of our
framework. Our implementation code is released at
https://github.com/TiliaceaeSU/PBAT.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14474" title="Abstract">arXiv:2402.14474</a> [<a href="/pdf/2402.14474" title="Download PDF">pdf</a>, <a href="/format/2402.14474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Science with LLMs and Interpretable Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bordt%2C+S">Sebastian Bordt</a>, 
<a href="/search/cs?searchtype=author&query=Lengerich%2C+B">Ben Lengerich</a>, 
<a href="/search/cs?searchtype=author&query=Nori%2C+H">Harsha Nori</a>, 
<a href="/search/cs?searchtype=author&query=Caruana%2C+R">Rich Caruana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> XAI4Sci Workshop at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent years have seen important advances in the building of interpretable
models, machine learning models that are designed to be easily understood by
humans. In this work, we show that large language models (LLMs) are remarkably
good at working with interpretable models, too. In particular, we show that
LLMs can describe, interpret, and debug Generalized Additive Models (GAMs).
Combining the flexibility of LLMs with the breadth of statistical patterns
accurately described by GAMs enables dataset summarization, question answering,
and model critique. LLMs can also improve the interaction between domain
experts and interpretable models, and generate hypotheses about the underlying
phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an
open-source LLM-GAM interface.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14475" title="Abstract">arXiv:2402.14475</a> [<a href="/pdf/2402.14475" title="Download PDF">pdf</a>, <a href="/format/2402.14475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynGMA: a robust approach for learning stochastic differential equations  from data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Aiqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qianxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Learning unknown stochastic differential equations (SDEs) from observed data
is a significant and challenging task with applications in various fields.
Current approaches often use neural networks to represent drift and diffusion
functions, and construct likelihood-based loss by approximating the transition
density to train these networks. However, these methods often rely on one-step
stochastic numerical schemes, necessitating data with sufficiently high time
resolution. In this paper, we introduce novel approximations to the transition
density of the parameterized SDE: a Gaussian density approximation inspired by
the random perturbation theory of dynamical systems, and its extension, the
dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust
density approximation, our method exhibits superior accuracy compared to
baseline methods in learning the fully unknown drift and diffusion functions
and computing the invariant distribution from trajectory data. And it is
capable of handling trajectory data with low time resolution and variable, even
uncontrollable, time step sizes, such as data generated from Gillespie's
stochastic simulations. We then conduct several experiments across various
scenarios to verify the advantages and robustness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14480" title="Abstract">arXiv:2402.14480</a> [<a href="/pdf/2402.14480" title="Download PDF">pdf</a>, <a href="/format/2402.14480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems  in LLM Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuekang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gelei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guosheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kailong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Augmented generation techniques such as Retrieval-Augmented Generation (RAG)
and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing
large language model (LLM) outputs with external knowledge and cached
information. However, the integration of vector databases, which serve as a
backbone for these augmentations, introduces critical challenges, particularly
in ensuring accurate vector matching. False vector matching in these databases
can significantly compromise the integrity and reliability of LLM outputs,
leading to misinformation or erroneous responses. Despite the crucial impact of
these issues, there is a notable research gap in methods to effectively detect
and address false vector matches in LLM-augmented generation. This paper
presents MeTMaP, a metamorphic testing framework developed to identify false
vector matching in LLM-augmented generation systems. We derive eight
metamorphic relations (MRs) from six NLP datasets, which form our method's
core, based on the idea that semantically similar texts should match and
dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets
for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over
203 vector matching configurations, involving 29 embedding models and 7
distance metrics, uncovers significant inaccuracies. The results, showing a
maximum accuracy of only 41.51\% on our tests compared to the original
datasets, emphasize the widespread issue of false matches in vector matching
methods and the critical need for effective detection and mitigation in
LLM-augmented applications.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14481" title="Abstract">arXiv:2402.14481</a> [<a href="/pdf/2402.14481" title="Download PDF">pdf</a>, <a href="/format/2402.14481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Causal Discovery: a case study on 5G telecommunication  data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biza%2C+K">Konstantina Biza</a>, 
<a href="/search/cs?searchtype=author&query=Ntroumpogiannis%2C+A">Antonios Ntroumpogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Triantafillou%2C+S">Sofia Triantafillou</a>, 
<a href="/search/cs?searchtype=author&query=Tsamardinos%2C+I">Ioannis Tsamardinos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">We introduce the concept of Automated Causal Discovery (AutoCD), defined as
any system that aims to fully automate the application of causal discovery and
causal reasoning methods. AutoCD's goal is to deliver all causal information
that an expert human analyst would and answer a user's causal queries. We
describe the architecture of such a platform, and illustrate its performance on
synthetic data sets. As a case study, we apply it on temporal telecommunication
data. The system is general and can be applied to a plethora of causal
discovery problems.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14482" title="Abstract">arXiv:2402.14482</a> [<a href="/pdf/2402.14482" title="Download PDF">pdf</a>, <a href="/format/2402.14482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpanSeq: Similarity-based sequence data splitting method for improved  development and assessment of deep learning projects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Florensa%2C+A+F">Alfred Ferrer Florensa</a>, 
<a href="/search/cs?searchtype=author&query=Armenteros%2C+J+J+A">Jose Juan Almagro Armenteros</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+H">Henrik Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Aarestrup%2C+F+M">Frank M&#xf8;ller Aarestrup</a>, 
<a href="/search/cs?searchtype=author&query=Clausen%2C+P+T+L+C">Philip Thomas Lanken Conradsen Clausen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The use of deep learning models in computational biology has increased
massively in recent years, and is expected to do so further with the current
advances in fields like Natural Language Processing. These models, although
able to draw complex relations between input and target, are also largely
inclined to learn noisy deviations from the pool of data used during their
development. In order to assess their performance on unseen data (their
capacity to generalize), it is common to randomly split the available data in
development (train/validation) and test sets. This procedure, although
standard, has lately been shown to produce dubious assessments of
generalization due to the existing similarity between samples in the databases
used. In this work, we present SpanSeq, a database partition method for machine
learning that can scale to most biological sequences (genes, proteins and
genomes) in order to avoid data leakage between sets. We also explore the
effect of not restraining similarity between sets by reproducing the
development of the state-of-the-art model DeepLoc, not only confirming the
consequences of randomly splitting databases on the model assessment, but
expanding those repercussions to the model development. SpanSeq is available
for downloading and installing at
https://github.com/genomicepidemiology/SpanSeq.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14483" title="Abstract">arXiv:2402.14483</a> [<a href="/pdf/2402.14483" title="Download PDF">pdf</a>, <a href="/format/2402.14483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly  Stable On-Policy Data-Driven LQR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Borghesi%2C+M">Marco Borghesi</a>, 
<a href="/search/eess?searchtype=author&query=Bosso%2C+A">Alessandro Bosso</a>, 
<a href="/search/eess?searchtype=author&query=Notarstefano%2C+G">Giuseppe Notarstefano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This article introduces a novel framework for data-driven linear quadratic
regulator (LQR) design. First, we introduce a reinforcement learning paradigm
for on-policy data-driven LQR, where exploration and exploitation are
simultaneously performed while guaranteeing robust stability of the whole
closed-loop system encompassing the plant and the control/learning dynamics.
Then, we propose Model Reference Adaptive Reinforcement Learning (MR-ARL), a
control architecture integrating tools from reinforcement learning and model
reference adaptive control. The approach stands on a variable reference model
containing the currently identified value function. Then, an adaptive
stabilizer is used to ensure convergence of the applied policy to the optimal
one, convergence of the plant to the optimal reference model, and overall
robust closed-loop stability. The proposed framework provides theoretical
robustness certificates against real-world perturbations such as measurement
noise, plant nonlinearities, or slowly varying parameters. The effectiveness of
the proposed architecture is validated via realistic numerical simulations.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14484" title="Abstract">arXiv:2402.14484</a> [<a href="/pdf/2402.14484" title="Download PDF">pdf</a>, <a href="/format/2402.14484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takayanagi%2C+T">Takehiro Takayanagi</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+M">Masahiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+R">Ryotaro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Sakaji%2C+H">Hiroki Sakaji</a>, 
<a href="/search/cs?searchtype=author&query=Izumi%2C+K">Kiyoshi Izumi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Causality is fundamental in human cognition and has drawn attention in
diverse research fields. With growing volumes of textual data, discerning
causalities within text data is crucial, and causal text mining plays a pivotal
role in extracting meaningful patterns. This study conducts comprehensive
evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce
a benchmark that extends beyond general English datasets, including
domain-specific and non-English datasets. We also provide an evaluation
framework to ensure fair comparisons between ChatGPT and previous approaches.
Finally, our analysis outlines the limitations and future challenges in
employing ChatGPT for causal text mining. Specifically, our analysis reveals
that ChatGPT serves as a good starting point for various datasets. However,
when equipped with a sufficient amount of training data, previous models still
surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency
to falsely recognize non-causal sequences as causal sequences. These issues
become even more pronounced with advanced versions of the model, such as GPT-4.
In addition, we highlight the constraints of ChatGPT in handling complex
causality types, including both intra/inter-sentential and implicit causality.
The model also faces challenges with effectively leveraging in-context learning
and domain adaptation. Our code is available on
\url{https://github.com/retarfi/gemcausal}
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14485" title="Abstract">arXiv:2402.14485</a> [<a href="/pdf/2402.14485" title="Download PDF">pdf</a>, <a href="/format/2402.14485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine-Checked Categorical Diagrammatic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guillemet%2C+B">Beno&#xee;t Guillemet</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">Assia Mahboubi</a>, 
<a href="/search/cs?searchtype=author&query=Piquerez%2C+M">Matthieu Piquerez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This paper describes a formal proof library, developed using the Coq proof
assistant, designed to assist users in writing correct diagrammatic proofs, for
1-categories. This library proposes a deep-embedded, domain-specific formal
language, which features dedicated proof commands to automate the synthesis,
and the verification, of the technical parts often eluded in the literature.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14486" title="Abstract">arXiv:2402.14486</a> [<a href="/pdf/2402.14486" title="Download PDF">pdf</a>, <a href="/ps/2402.14486" title="Download PostScript">ps</a>, <a href="/format/2402.14486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Bounded Contracts Learnable and Approximately Optimal?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yurong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaohua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiaotie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyi Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
<p class="mathjax">This paper considers the hidden-action model of the principal-agent problem,
in which a principal incentivizes an agent to work on a project using a
contract. We investigate whether contracts with bounded payments are learnable
and approximately optimal. Our main results are two learning algorithms that
can find a nearly optimal bounded contract using a polynomial number of
queries, under two standard assumptions in the literature: a costlier action
for the agent leads to a better outcome distribution for the principal, and the
agent's cost/effort has diminishing returns. Our polynomial query complexity
upper bound shows that standard assumptions are sufficient for achieving an
exponential improvement upon the known lower bound for general instances.
Unlike the existing algorithms, which relied on discretizing the contract
space, our algorithms directly learn the underlying outcome distributions. As
for the approximate optimality of bounded contracts, we find that they could be
far from optimal in terms of multiplicative or additive approximation, but
satisfy a notion of mixed approximation.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14488" title="Abstract">arXiv:2402.14488</a> [<a href="/pdf/2402.14488" title="Download PDF">pdf</a>, <a href="/format/2402.14488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinshuo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-Coling 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The present study introduces the knowledge-augmented generator, which is
specifically designed to produce information that remains grounded in
contextual knowledge, regardless of alterations in the context. Previous
research has predominantly focused on examining hallucinations stemming from
static input, such as in the domains of summarization or machine translation.
However, our investigation delves into the faithfulness of generative question
answering in the presence of dynamic knowledge. Our objective is to explore the
existence of hallucinations arising from parametric memory when contextual
knowledge undergoes changes, while also analyzing the underlying causes for
their occurrence. In order to efficiently address this issue, we propose a
straightforward yet effective measure for detecting such hallucinations.
Intriguingly, our investigation uncovers that all models exhibit a tendency to
generate previous answers as hallucinations. To gain deeper insights into the
underlying causes of this phenomenon, we conduct a series of experiments that
verify the critical role played by context in hallucination, both during
training and testing, from various perspectives.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14489" title="Abstract">arXiv:2402.14489</a> [<a href="/pdf/2402.14489" title="Download PDF">pdf</a>, <a href="/format/2402.14489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Class of Topological Pseudodistances for Fast Comparison of  Persistence Diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nu%C3%B1ez%2C+R+K">Rolando Kindelan Nu&#xf1;ez</a>, 
<a href="/search/cs?searchtype=author&query=Petrache%2C+M">Mircea Petrache</a>, 
<a href="/search/cs?searchtype=author&query=Cerda%2C+M">Mauricio Cerda</a>, 
<a href="/search/cs?searchtype=author&query=Hitschfeld%2C+N">Nancy Hitschfeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation and poster on the 38th Annual AAAI Conference on Artificial Intelligence (AAAI24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Algebraic Topology (math.AT)

</div>
<p class="mathjax">Persistence diagrams (PD)s play a central role in topological data analysis,
and are used in an ever increasing variety of applications. The comparison of
PD data requires computing comparison metrics among large sets of PDs, with
metrics which are accurate, theoretically sound, and fast to compute.
Especially for denser multi-dimensional PDs, such comparison metrics are
lacking. While on the one hand, Wasserstein-type distances have high accuracy
and theoretical guarantees, they incur high computational cost. On the other
hand, distances between vectorizations such as Persistence Statistics (PS)s
have lower computational cost, but lack the accuracy guarantees and in general
they are not guaranteed to distinguish PDs (i.e. the two PS vectors of
different PDs may be equal). In this work we introduce a class of
pseudodistances called Extended Topological Pseudodistances (ETD)s, which have
tunable complexity, and can approximate Sliced and classical Wasserstein
distances at the high-complexity extreme, while being computationally lighter
and close to Persistence Statistics at the lower complexity extreme, and thus
allow users to interpolate between the two metrics. We build theoretical
comparisons to show how to fit our new distances at an intermediate level
between persistence vectorizations and Wasserstein distances. We also
experimentally verify that ETDs outperform PSs in terms of accuracy and
outperform Wasserstein and Sliced Wasserstein distances in terms of
computational complexity.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14490" title="Abstract">arXiv:2402.14490</a> [<a href="/pdf/2402.14490" title="Download PDF">pdf</a>, <a href="/format/2402.14490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imbalanced Data Clustering using Equilibrium K-Means
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yudong He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Imbalanced data, characterized by an unequal distribution of data points
across different clusters, poses a challenge for traditional hard and fuzzy
clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and
fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium
K-means (EKM), a novel and simple K-means-type algorithm that alternates
between just two steps, yielding significantly improved clustering results for
imbalanced data by reducing the tendency of centroids to crowd together in the
center of large clusters. We also present a unifying perspective for HKM, FKM,
and EKM, showing they are essentially gradient descent algorithms with an
explicit relationship to Newton's method. EKM has the same time and space
complexity as FKM but offers a clearer physical meaning for its membership
definition. We illustrate the performance of EKM on two synthetic and ten real
datasets, comparing it to various clustering algorithms, including HKM, FKM,
maximum-entropy fuzzy clustering, two FKM variations designed for imbalanced
data, and the Gaussian mixture model. The results demonstrate that EKM performs
competitively on balanced data while significantly outperforming other
techniques on imbalanced data. For high-dimensional data clustering, we
demonstrate that a more discriminative representation can be obtained by
mapping high-dimensional data via deep neural networks into a low-dimensional,
EKM-friendly space. Deep clustering with EKM improves clustering accuracy by
35% on an imbalanced dataset derived from MNIST compared to deep clustering
based on HKM.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14492" title="Abstract">arXiv:2402.14492</a> [<a href="/pdf/2402.14492" title="Download PDF">pdf</a>, <a href="/format/2402.14492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wei Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fine-tuning large language models (LLMs) on multi-task instruction-following
data has been proven to be a powerful learning paradigm for improving their
zero-shot capabilities on new tasks. Recent works about high-quality
instruction-following data generation and selection require amounts of human
labor to conceive model-understandable instructions for the given tasks and
carefully filter the LLM-generated data. In this work, we introduce an
automatic instruction augmentation method named INSTRAUG in multimodal tasks.
It starts from a handful of basic and straightforward meta instructions but can
expand an instruction-following dataset by 30 times. Results on two popular
multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show
that INSTRAUG can significantly improve the alignment of multimodal large
language models (MLLMs) across 12 multimodal tasks, which is even equivalent to
the benefits of scaling up training data multiple times.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14493" title="Abstract">arXiv:2402.14493</a> [<a href="/pdf/2402.14493" title="Download PDF">pdf</a>, <a href="/ps/2402.14493" title="Download PostScript">ps</a>, <a href="/format/2402.14493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Pseudopolynomial Time Algorithm for Subset Sum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jiayi Lian</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuchen Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guochuan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We investigate pseudo-polynomial time algorithms for Subset Sum. Given a
multi-set $X$ of $n$ positive integers and a target $t$, Subset Sum asks
whether some subset of $X$ sums to $t$. Bringmann proposes an $\tilde{O}(n +
t)$-time algorithm [Bringmann SODA'17], and an open question has naturally
arisen: can Subset Sum be solved in $O(n + w)$ time? Here $w$ is the maximum
integer in $X$. We make a progress towards resolving the open question by
proposing an $\tilde{O}(n + \sqrt{wt})$-time algorithm.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14494" title="Abstract">arXiv:2402.14494</a> [<a href="/pdf/2402.14494" title="Download PDF">pdf</a>, <a href="/format/2402.14494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yueyan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tingfeng Hui</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daichi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In a realistic dialogue system, the input information from users is often
subject to various types of input perturbations, which affects the slot-filling
task. Although rule-based data augmentation methods have achieved satisfactory
results, they fail to exhibit the desired generalization when faced with
unknown noise disturbances. In this study, we address the challenges posed by
input perturbations in slot filling by proposing Noise-BERT, a unified
Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework
incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and
Sentence Noisiness Discrimination, aiming to guide the pre-trained language
model in capturing accurate slot information and noise distribution. During
fine-tuning, we employ a contrastive learning loss to enhance the semantic
representation of entities and labels. Additionally, we introduce an
adversarial attack training strategy to improve the model's robustness.
Experimental results demonstrate the superiority of our proposed approach over
state-of-the-art models, and further analysis confirms its effectiveness and
generalization ability.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14498" title="Abstract">arXiv:2402.14498</a> [<a href="/pdf/2402.14498" title="Download PDF">pdf</a>, <a href="/format/2402.14498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Collision-Aware Cable Grasping Method in Cluttered Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+K">Kaixin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaopeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianwei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a Cable Grasping-Convolutional Neural Network designed to
facilitate robust cable grasping in cluttered environments. Utilizing physics
simulations, we generate an extensive dataset that mimics the intricacies of
cable grasping, factoring in potential collisions between cables and robotic
grippers. We employ the Approximate Convex Decomposition technique to dissect
the non-convex cable model, with grasp quality autonomously labeled based on
simulated grasping attempts. The CG-CNN is refined using this simulated dataset
and enhanced through domain randomization techniques. Subsequently, the trained
model predicts grasp quality, guiding the optimal grasp pose to the robot
controller for execution. Grasping efficacy is assessed across both synthetic
and real-world settings. Given our model implicit collision sensitivity, we
achieved commendable success rates of 92.3% for known cables and 88.4% for
unknown cables, surpassing contemporary state-of-the-art approaches.
Supplementary materials can be found at
https://leizhang-public.github.io/cg-cnn/ .
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14499" title="Abstract">arXiv:2402.14499</a> [<a href="/pdf/2402.14499" title="Download PDF">pdf</a>, <a href="/format/2402.14499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;My Answer is C&quot;: First-Token Probabilities Do Not Match Text Answers in  Instruction-Tuned Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Bolei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chengzhi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Weber-Genzel%2C+L">Leon Weber-Genzel</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>, 
<a href="/search/cs?searchtype=author&query=Kreuter%2C+F">Frauke Kreuter</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+D">Dirk Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The open-ended nature of language generation makes the evaluation of
autoregressive large language models (LLMs) challenging. One common evaluation
approach uses multiple-choice questions (MCQ) to limit the response space. The
model is then evaluated by ranking the candidate answers by the log probability
of the first token prediction. However, first-tokens may not consistently
reflect the final response output, due to model's diverse response styles such
as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is
not indicative of model behaviour when interacting with users. But by how much?
We evaluate how aligned first-token evaluation is with the text output along
several dimensions, namely final option choice, refusal rate, choice
distribution and robustness under prompt perturbation. Our results show that
the two approaches are severely misaligned on all dimensions, reaching mismatch
rates over 60%. Models heavily fine-tuned on conversational or safety data are
especially impacted. Crucially, models remain misaligned even when we
increasingly constrain prompts, i.e., force them to start with an option letter
or example template. Our findings i) underscore the importance of inspecting
the text output, too and ii) caution against relying solely on first-token
evaluation.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14503" title="Abstract">arXiv:2402.14503</a> [<a href="/pdf/2402.14503" title="Download PDF">pdf</a>, <a href="/format/2402.14503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Human-AI Collaboration in Music Therapy Through Co-Design  with Therapists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingjing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gueyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yucheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jiangtao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The rapid development of musical AI technologies has expanded the creative
potential of various musical activities, ranging from music style
transformation to music generation. However, little research has investigated
how musical AIs can support music therapists, who urgently need new technology
support. This study used a mixed method, including semi-structured interviews
and a participatory design approach. By collaborating with music therapists, we
explored design opportunities for musical AIs in music therapy. We presented
the co-design outcomes involving the integration of musical AIs into a music
therapy process, which was developed from a theoretical framework rooted in
emotion-focused therapy. After that, we concluded the benefits and concerns
surrounding music AIs from the perspective of music therapists. Based on our
findings, we discussed the opportunities and design implications for applying
musical AIs to music therapy. Our work offers valuable insights for developing
human-AI collaborative music systems in therapy involving complex procedures
and specific requirements.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14505" title="Abstract">arXiv:2402.14505</a> [<a href="/pdf/2402.14505" title="Download PDF">pdf</a>, <a href="/format/2402.14505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Seamless Adaptation of Pre-trained Models for Visual Place  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Feng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiangyuan Lan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies show that vision models pre-trained in generic visual learning
tasks with large-scale data can provide useful feature representations for a
wide range of visual perception problems. However, few attempts have been made
to exploit pre-trained foundation models in visual place recognition (VPR). Due
to the inherent difference in training objectives and data between the tasks of
model pre-training and VPR, how to bridge the gap and fully unleash the
capability of pre-trained models for VPR is still a key issue to address. To
this end, we propose a novel method to realize seamless adaptation of
pre-trained models for VPR. Specifically, to obtain both global and local
features that focus on salient landmarks for discriminating places, we design a
hybrid adaptation method to achieve both global and local adaptation
efficiently, in which only lightweight adapters are tuned without adjusting the
pre-trained model. Besides, to guide effective adaptation, we propose a mutual
nearest neighbor local feature loss, which ensures proper dense local features
are produced for local matching and avoids time-consuming spatial verification
in re-ranking. Experimental results show that our method outperforms the
state-of-the-art methods with less training data and training time, and uses
about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based
spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the
time of submission). The code is released at
https://github.com/Lu-Feng/SelaVPR.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14519" title="Abstract">arXiv:2402.14519</a> [<a href="/pdf/2402.14519" title="Download PDF">pdf</a>, <a href="/ps/2402.14519" title="Download PostScript">ps</a>, <a href="/format/2402.14519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD  Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bandla%2C+K">Kasi Bandla</a>, 
<a href="/search/eess?searchtype=author&query=Pal%2C+D">Dipankar Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 12 figures, to be submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Strong-ARM Dynamic Latch Comparators are widely used in high-speed
analog-to-digital converters (ADCs), sense amplifiers in memory, RFID
applications, and data receivers. This paper presents different methods to
improve the performance of Strong-Arm latch-based comparators. The comparator's
significant features such as power dissipation, propagation delay, offset
voltage, clock feedthrough, area, and kickback noises are discussed and
compared with state-of-the-art candidate topologies. Simulation results show
that the new comparator topologies of Strong-ARM Dynamic Latch proposed by
these authors gave the best results. The proposed designs are tested. The
simulations are carried out using UMC 180nm double metal, double poly standard
CMOS process technology, for a 100 MHz clock, at 1.8V supply-rail on the
Cadence Virtuoso EDA platform.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14521" title="Abstract">arXiv:2402.14521</a> [<a href="/pdf/2402.14521" title="Download PDF">pdf</a>, <a href="/format/2402.14521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Malaysian English News Decoded: A Linguistic Resource for Named Entity  and Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chanthran%2C+M+R">Mohan Raj Chanthran</a>, 
<a href="/search/cs?searchtype=author&query=Soon%2C+L">Lay-Ki Soon</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+H+F">Huey Fang Ong</a>, 
<a href="/search/cs?searchtype=author&query=Selvaretnam%2C+B">Bhawani Selvaretnam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Standard English and Malaysian English exhibit notable differences, posing
challenges for natural language processing (NLP) tasks on Malaysian English.
Unfortunately, most of the existing datasets are mainly based on standard
English and therefore inadequate for improving NLP tasks in Malaysian English.
An experiment using state-of-the-art Named Entity Recognition (NER) solutions
on Malaysian English news articles highlights that they cannot handle
morphosyntactic variations in Malaysian English. To the best of our knowledge,
there is no annotated dataset available to improvise the model. To address
these issues, we constructed a Malaysian English News (MEN) dataset, which
contains 200 news articles that are manually annotated with entities and
relations. We then fine-tuned the spaCy NER tool and validated that having a
dataset tailor-made for Malaysian English could improve the performance of NER
in Malaysian English significantly. This paper presents our effort in the data
acquisition, annotation methodology, and thorough analysis of the annotated
dataset. To validate the quality of the annotation, inter-annotator agreement
was used, followed by adjudication of disagreements by a subject matter expert.
Upon completion of these tasks, we managed to develop a dataset with 6,061
entities and 3,268 relation instances. Finally, we discuss on spaCy fine-tuning
setup and analysis on the NER performance. This unique dataset will contribute
significantly to the advancement of NLP research in Malaysian English, allowing
researchers to accelerate their progress, particularly in NER and relation
extraction. The dataset and annotation guideline has been published on Github.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14522" title="Abstract">arXiv:2402.14522</a> [<a href="/pdf/2402.14522" title="Download PDF">pdf</a>, <a href="/format/2402.14522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap  for Prompt-Based Large Language Models and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hainiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Task embedding, a meta-learning technique that captures task-specific
information, has become prevalent, especially in areas such as multi-task
learning, model editing, and interpretability. However, it faces challenges
with the emergence of prompt-guided Large Language Models (LLMs) operating in a
gradientfree manner. Existing task embedding methods rely on fine-tuned,
task-specific language models, which hinders the adaptability of task
embeddings across diverse models, especially prompt-based LLMs. To unleash the
power of task embedding in the era of LLMs, we propose a framework for unified
task embeddings (FUTE), harmonizing task embeddings from various models,
including smaller language models and LLMs with varied prompts, within a single
vector space. Such uniformity enables the comparison and analysis of
similarities amongst different models, extending the scope and utility of
existing task embedding methods in addressing multi-model scenarios, whilst
maintaining their performance to be comparable to architecture-specific
methods.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14523" title="Abstract">arXiv:2402.14523</a> [<a href="/pdf/2402.14523" title="Download PDF">pdf</a>, <a href="/format/2402.14523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chevi%2C+R">Rendi Chevi</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://rendchevi.github.io/daisy-tts">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We often verbally express emotions in a multifaceted manner, they may vary in
their intensities and may be expressed not just as a single but as a mixture of
emotions. This wide spectrum of emotions is well-studied in the structural
model of emotions, which represents variety of emotions as derivative products
of primary emotions with varying degrees of intensity. In this paper, we
propose an emotional text-to-speech design to simulate a wider spectrum of
emotions grounded on the structural model. Our proposed design, Daisy-TTS,
incorporates a prosody encoder to learn emotionally-separable prosody embedding
as a proxy for emotion. This emotion representation allows the model to
simulate: (1) Primary emotions, as learned from the training samples, (2)
Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by
scaling the emotion embedding, and (4) Emotions polarity, by negating the
emotion embedding. Through a series of perceptual evaluations, Daisy-TTS
demonstrated overall higher emotional speech naturalness and emotion
perceiveability compared to the baseline.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14525" title="Abstract">arXiv:2402.14525</a> [<a href="/pdf/2402.14525" title="Download PDF">pdf</a>, <a href="/format/2402.14525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6ksu%2C+Y">Yasemin G&#xf6;ksu</a>, 
<a href="/search/cs?searchtype=author&query=De+Almeida+Correia%2C+A">Antonio De Almeida Correia</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+V">Vignesh Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Kshirsagar%2C+A">Alap Kshirsagar</a>, 
<a href="/search/cs?searchtype=author&query=Koert%2C+D">Dorothea Koert</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>, 
<a href="/search/cs?searchtype=author&query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Late Breaking Report in The ACM/IEEE International Conference on Human Robot Interaction (HRI) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bimanual handovers are crucial for transferring large, deformable or delicate
objects. This paper proposes a framework for generating kinematically
constrained human-like bimanual robot motions to ensure seamless and natural
robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to
reactively generate suitable response trajectories for a robot based on the
observed human partner's motion. The trajectories are adapted with task space
constraints to ensure accurate handovers. Results from a pilot study show that
our approach is perceived as more human--like compared to a baseline Inverse
Kinematics approach.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14526" title="Abstract">arXiv:2402.14526</a> [<a href="/pdf/2402.14526" title="Download PDF">pdf</a>, <a href="/format/2402.14526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Data Sampling for Language Model Training with Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yunfan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhaoye Fei</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data plays a fundamental role in the training of Large Language Models
(LLMs). While attention has been paid to the collection and composition of
datasets, determining the data sampling strategy in training remains an open
question. Most LLMs are trained with a simple strategy, random sampling.
However, this sampling strategy ignores the unbalanced nature of training data
distribution, which can be sub-optimal. In this paper, we propose ClusterClip
Sampling to balance the text distribution of training data for better model
training. Specifically, ClusterClip Sampling utilizes data clustering to
reflect the data distribution of the training set and balances the common
samples and rare samples during training based on the cluster results. A
repetition clip operation is introduced to mitigate the overfitting issue led
by samples from certain clusters. Extensive experiments validate the
effectiveness of ClusterClip Sampling, which outperforms random sampling and
other cluster-based sampling variants under various training datasets and large
language models.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14527" title="Abstract">arXiv:2402.14527</a> [<a href="/pdf/2402.14527" title="Download PDF">pdf</a>, <a href="/format/2402.14527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning on Transcriptomic Data: Model Quality and Performance  Trade-Offs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hannemann%2C+A">Anika Hannemann</a>, 
<a href="/search/cs?searchtype=author&query=Ewald%2C+J">Jan Ewald</a>, 
<a href="/search/cs?searchtype=author&query=Seeger%2C+L">Leo Seeger</a>, 
<a href="/search/cs?searchtype=author&query=Buchmann%2C+E">Erik Buchmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Genomics (q-bio.GN)

</div>
<p class="mathjax">Machine learning on large-scale genomic or transcriptomic data is important
for many novel health applications. For example, precision medicine tailors
medical treatments to patients on the basis of individual biomarkers, cellular
and molecular states, etc. However, the data required is sensitive, voluminous,
heterogeneous, and typically distributed across locations where dedicated
machine learning hardware is not available. Due to privacy and regulatory
reasons, it is also problematic to aggregate all data at a trusted third
party.Federated learning is a promising solution to this dilemma, because it
enables decentralized, collaborative machine learning without exchanging raw
data. In this paper, we perform comparative experiments with the federated
learning frameworks TensorFlow Federated and Flower. Our test case is the
training of disease prognosis and cell type classification models. We train the
models with distributed transcriptomic data, considering both data
heterogeneity and architectural heterogeneity. We measure model quality,
robustness against privacy-enhancing noise, computational performance and
resource overhead. Each of the federated learning frameworks has different
strengths. However, our experiments confirm that both frameworks can readily
build models on transcriptomic data, without transferring personal raw data to
a third party with abundant computational resources.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14528" title="Abstract">arXiv:2402.14528</a> [<a href="/pdf/2402.14528" title="Download PDF">pdf</a>, <a href="/format/2402.14528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACE : Off-Policy Actor-Critic with Causality-Aware Entropy  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tianying Ji</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guowei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The varying significance of distinct primitive behaviors during the policy
learning process has been overlooked by prior model-free RL algorithms.
Leveraging this insight, we explore the causal relationship between different
action dimensions and rewards to evaluate the significance of various primitive
behaviors during training. We introduce a causality-aware entropy term that
effectively identifies and prioritizes actions with high potential impacts for
efficient exploration. Furthermore, to prevent excessive focus on specific
primitive behaviors, we analyze the gradient dormancy phenomenon and introduce
a dormancy-guided reset mechanism to further enhance the efficacy of our
method. Our proposed algorithm, ACE: Off-policy Actor-critic with
Causality-aware Entropy regularization, demonstrates a substantial performance
advantage across 29 diverse continuous control tasks spanning 7 domains
compared to model-free RL baselines, which underscores the effectiveness,
versatility, and efficient sample efficiency of our approach. Benchmark results
and videos are available at https://ace-rl.github.io/.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14531" title="Abstract">arXiv:2402.14531</a> [<a href="/pdf/2402.14531" title="Download PDF">pdf</a>, <a href="/ps/2402.14531" title="Download PostScript">ps</a>, <a href="/format/2402.14531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt  Politeness on LLM Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziqi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Horio%2C+K">Kaito Horio</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+D">Daisuke Kawahara</a>, 
<a href="/search/cs?searchtype=author&query=Sekine%2C+S">Satoshi Sekine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We investigate the impact of politeness levels in prompts on the performance
of large language models (LLMs). Polite language in human communications often
garners more compliance and effectiveness, while rudeness can cause aversion,
impacting response quality. We consider that LLMs mirror human communication
traits, suggesting they align with human cultural norms. We assess the impact
of politeness in prompts on LLMs across English, Chinese, and Japanese tasks.
We observed that impolite prompts often result in poor performance, but overly
polite language does not guarantee better outcomes. The best politeness level
is different according to the language. This phenomenon suggests that LLMs not
only reflect human behavior but are also influenced by language, particularly
in different cultural contexts. Our findings highlight the need to factor in
politeness for cross-cultural natural language processing and LLM usage.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14532" title="Abstract">arXiv:2402.14532</a> [<a href="/pdf/2402.14532" title="Download PDF">pdf</a>, <a href="/format/2402.14532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Variational Inference of Lightweight Bayesian Neural  Networks with Heteroscedastic Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schodt%2C+D+J">David J. Schodt</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+R">Ryan Brown</a>, 
<a href="/search/cs?searchtype=author&query=Merritt%2C+M">Michael Merritt</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Samuel Park</a>, 
<a href="/search/cs?searchtype=author&query=Menolascino%2C+D">Delsin Menolascino</a>, 
<a href="/search/cs?searchtype=author&query=Peot%2C+M+A">Mark A. Peot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural
Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric
uncertainties are learned as outputs of the BNN in addition to the predictive
means, however doing so may necessitate adding more learnable parameters to the
network. In this work, we demonstrate that both the heteroscedastic aleatoric
and epistemic variance can be embedded into the variances of learned BNN
parameters, improving predictive performance for lightweight networks. By
complementing this approach with a moment propagation approach to inference, we
introduce a relatively simple framework for sampling-free variational inference
suitable for lightweight BNNs.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14533" title="Abstract">arXiv:2402.14533</a> [<a href="/pdf/2402.14533" title="Download PDF">pdf</a>, <a href="/format/2402.14533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for  GPT-3.5, GPT-4 and Bard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+A">Ariel Rosenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Lazebnik%2C+T">Teddy Lazebnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are capable of generating text that is similar
to or surpasses human quality. However, it is unclear whether LLMs tend to
exhibit distinctive linguistic styles akin to how human authors do. Through a
comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech
(POS) distribution, dependency distribution, and sentiment of texts generated
by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse
inputs. The results point to significant linguistic variations which, in turn,
enable us to attribute a given text to its LLM origin with a favorable 88\%
accuracy using a simple off-the-shelf classification model. Theoretical and
practical implications of this intriguing finding are discussed.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14536" title="Abstract">arXiv:2402.14536</a> [<a href="/pdf/2402.14536" title="Download PDF">pdf</a>, <a href="/format/2402.14536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization via Causal Adjustment for Cross-Domain Sentiment  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Domain adaption has been widely adapted for cross-domain sentiment analysis
to transfer knowledge from the source domain to the target domain. Whereas,
most methods are proposed under the assumption that the target (test) domain is
known, making them fail to generalize well on unknown test data that is not
always available in practice. In this paper, we focus on the problem of domain
generalization for cross-domain sentiment analysis. Specifically, we propose a
backdoor adjustment-based causal model to disentangle the domain-specific and
domain-invariant representations that play essential roles in tackling domain
shift. First, we rethink the cross-domain sentiment analysis task in a causal
view to model the causal-and-effect relationships among different variables.
Then, to learn an invariant feature representation, we remove the effect of
domain confounders (e.g., domain knowledge) using the backdoor adjustment. A
series of experiments over many homologous and diverse datasets show the great
performance and robustness of our model by comparing it with the
state-of-the-art domain generalization baselines.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14539" title="Abstract">arXiv:2402.14539</a> [<a href="/pdf/2402.14539" title="Download PDF">pdf</a>, <a href="/format/2402.14539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transforming Norm-based To Graph-based Spatial Representation for  Spatio-Temporal Epidemiological Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazebnik%2C+T">Teddy Lazebnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Pandemics, with their profound societal and economic impacts, pose
significant threats to global health, mortality rates, economic stability, and
political landscapes. In response to the persistent challenges posed by
emerging and reemerging pandemics, numerous studies have employed
spatio-temporal models to enhance our understanding and management of these
complex phenomena. These spatio-temporal models can be roughly divided into two
main spatial categories: norm-based and graph-based trade-offering between
accuracy, computational burden, and representational feasibility. In this
study, we explore the ability to transform from norm-based to graph-based
spatial representation for these models. We introduce a novel framework for
this task together with twelve possible implementations using a wide range of
heuristic optimization approaches. Our findings show that by leveraging
agent-based simulations and heuristic algorithms for the graph node's location
and population's spatial walk dynamics approximation one can use graph-based
spatial representation without losing much of the model's accuracy and
expressiveness. For three real-world cases, the best-performing algorithmic
configuration archives 94\% accuracy presence, on average. Moreover, an
analysis of synthetic cases shows the proposed framework is relatively robust,
as fluctuation in both spatial and temporal dynamics is not badly reflected by
the framework's performance.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14540" title="Abstract">arXiv:2402.14540</a> [<a href="/pdf/2402.14540" title="Download PDF">pdf</a>, <a href="/format/2402.14540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Truthful Item-Acquiring Mechanisms for Reward Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+L">Liang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In this research, we study the problem that a collector acquires items from
the owner based on the item qualities the owner declares and an independent
appraiser's assessments. The owner is interested in maximizing the probability
that the collector acquires the items and is the only one who knows the items'
factual quality. The appraiser performs her duties with impartiality, but her
assessment may be subject to random noises, so it may not accurately reflect
the factual quality of the items. The main challenge lies in devising
mechanisms that prompt the owner to reveal accurate information, thereby
optimizing the collector's expected reward. We consider the menu size of
mechanisms as a measure of their practicability and study its impact on the
attainable expected reward. For the single-item setting, we design optimal
mechanisms with a monotone increasing menu size. Although the reward gap
between the simplest and optimal mechanisms is bounded, we show that simple
mechanisms with a small menu size cannot ensure any positive fraction of the
optimal reward of mechanisms with a larger menu size. For the multi-item
setting, we show that an ordinal mechanism that only takes the owner's ordering
of the items as input is not incentive-compatible. We then propose a set of
Union mechanisms that combine single-item mechanisms. Moreover, we run
experiments to examine these mechanisms' robustness against the independent
appraiser's assessment accuracy and the items' acquiring rate.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14542" title="Abstract">arXiv:2402.14542</a> [<a href="/pdf/2402.14542" title="Download PDF">pdf</a>, <a href="/ps/2402.14542" title="Download PostScript">ps</a>, <a href="/format/2402.14542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the definition of set tolerances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=J%C3%A4ger%2C+G">Gerold J&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Turkensteen%2C+M">Marcel Turkensteen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Optimal solutions of combinatorial optimization problems can be sensitive to
changes in the cost of one or more elements. Single and set tolerances measure
the largest / smallest possible change such that the current solution remains
optimal and other solutions become non-optimal for cost changes in one or more
elements, respectively. The current definition only applies to subsets of
elements. In this paper, we broaden the definition to all elements, for single
tolerances, and to all subsets of elements for set tolerances, while proving
that key computational and theoretical properties still apply to the new
definitions.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14543" title="Abstract">arXiv:2402.14543</a> [<a href="/pdf/2402.14543" title="Download PDF">pdf</a>, <a href="/ps/2402.14543" title="Download PostScript">ps</a>, <a href="/format/2402.14543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-frequency Resonances in Grid-Forming Converters: Causes and Damping  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+F">Fangzhou Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+T">Tianhua Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zejie Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiongfei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Grid-forming voltage-source converter (GFM-VSC) may experience low-frequency
resonances, such as synchronous resonance (SR) and sub-synchronous resonance
(SSR), in the output power. This paper offers a comprehensive study on the root
causes of low-frequency resonances with GFM-VSC systems and the damping control
methods. The typical GFM control structures are introduced first, along with a
mapping between the resonances and control loops. Then, the causes of SR and
SSR are discussed, highlighting the impacts of control interactions on the
resonances. Further, the recent advancements in stabilizing control methods for
SR and SSR are critically reviewed with experimental tests of a GFM-VSC under
different grid conditions.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14544" title="Abstract">arXiv:2402.14544</a> [<a href="/pdf/2402.14544" title="Download PDF">pdf</a>, <a href="/format/2402.14544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> {A New Hope}: Contextual Privacy Policies for Mobile Applications and An  Approach Toward Automated Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shidong Pan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhen Tao</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T">Thong Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dawen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhenchang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sherry Xu</a>, 
<a href="/search/cs?searchtype=author&query=Staples%2C+M">Mark Staples</a>, 
<a href="/search/cs?searchtype=author&query=Rakotoarivelo%2C+T">Thierry Rakotoarivelo</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> USENIX Security 2024. arXiv admin note: text overlap with <a href="/abs/2307.01691">arXiv:2307.01691</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Privacy policies have emerged as the predominant approach to conveying
privacy notices to mobile application users. In an effort to enhance both
readability and user engagement, the concept of contextual privacy policies
(CPPs) has been proposed by researchers. The aim of CPPs is to fragment privacy
policies into concise snippets, displaying them only within the corresponding
contexts within the application's graphical user interfaces (GUIs). In this
paper, we first formulate CPP in mobile application scenario, and then present
a novel multimodal framework, named SeePrivacy, specifically designed to
automatically generate CPPs for mobile applications. This method uniquely
integrates vision-based GUI understanding with privacy policy analysis,
achieving 0.88 precision and 0.90 recall to detect contexts, as well as 0.98
precision and 0.96 recall in extracting corresponding policy segments. A human
evaluation shows that 77% of the extracted privacy policy segments were
perceived as well-aligned with the detected contexts. These findings suggest
that SeePrivacy could serve as a significant tool for bolstering user
interaction with, and understanding of, privacy policies. Furthermore, our
solution has the potential to make privacy notices more accessible and
inclusive, thus appealing to a broader demographic. A demonstration of our work
can be accessed at https://cpp4app.github.io/SeePrivacy/
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14545" title="Abstract">arXiv:2402.14545</a> [<a href="/pdf/2402.14545" title="Download PDF">pdf</a>, <a href="/format/2402.14545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More: Mitigating Multimodal Hallucination from an EOS Decision  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zihao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qin Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large Multimodal Models (LMMs) often suffer from multimodal hallucinations,
wherein they may create content that is not present in the visual inputs. In
this paper, we explore a new angle of this issue: overly detailed training data
hinders the model's ability to timely terminate generation, leading to
continued outputs beyond visual perception limits. By investigating how the
model decides to terminate generation with EOS, the special end-of-sentence
token, we find that the model assesses the completeness of the entire sequence
by comparing the generated text with the image. This observation suggests that
the model possesses an inherent potential of making proper EOS decisions based
on its visual perception to avoid overly lengthy outputs. To take advantage of
such potential, we explore two methods to mitigate multimodal hallucinations: a
training objective that enables the model to reduce hallucinations by learning
from regular instruction data, and a data filtering strategy to prevent harmful
training data from exacerbating model hallucinations. Both methods
significantly improve the hallucination performance of LMMs, without requiring
any additional data or knowledge.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14547" title="Abstract">arXiv:2402.14547</a> [<a href="/pdf/2402.14547" title="Download PDF">pdf</a>, <a href="/format/2402.14547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniPred: Language Models as Universal Regressors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xingyou Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+O">Oscar Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chansoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bangding">Bangding</a> (Jeffrey)
<a href="/search/cs?searchtype=author&query=Yang">Yang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Daiyi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Perel%2C+S">Sagi Perel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yutian Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)

</div>
<p class="mathjax">Over the broad landscape of experimental design, regression has been a
powerful tool to accurately predict the outcome metrics of a system or model
given a set of parameters, but has been traditionally restricted to methods
which are only applicable to a specific task. In this paper, we propose
OmniPred, a framework for training language models as universal end-to-end
regressors over $(x,y)$ evaluation data from diverse real world experiments.
Using data sourced from Google Vizier, one of the largest blackbox optimization
databases in the world, our extensive experiments demonstrate that through only
textual representations of mathematical parameters and values, language models
are capable of very precise numerical regression, and if given the opportunity
to train over multiple tasks, can significantly outperform traditional
regression models.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14548" title="Abstract">arXiv:2402.14548</a> [<a href="/pdf/2402.14548" title="Download PDF">pdf</a>, <a href="/format/2402.14548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transition State Clustering for Interaction Segmentation and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahne%2C+F">Fabian Hahne</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+V">Vignesh Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Kshirsagar%2C+A">Alap Kshirsagar</a>, 
<a href="/search/cs?searchtype=author&query=Koert%2C+D">Dorothea Koert</a>, 
<a href="/search/cs?searchtype=author&query=Stock-Homburg%2C+R+M">Ruth Maria Stock-Homburg</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>, 
<a href="/search/cs?searchtype=author&query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Late Breaking Report in The ACM/IEEE International Conference on Human Robot Interaction (HRI) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Hidden Markov Models with an underlying Mixture of Gaussian structure have
proven effective in learning Human-Robot Interactions from demonstrations for
various interactive tasks via Gaussian Mixture Regression. However, a mismatch
occurs when segmenting the interaction using only the observed state of the
human compared to the joint state of the human and the robot. To enhance this
underlying segmentation and subsequently the predictive abilities of such
Gaussian Mixture-based approaches, we take a hierarchical approach by learning
an additional mixture distribution on the states at the transition boundary.
This helps prevent misclassifications that usually occur in such states. We
find that our framework improves the performance of the underlying Gaussian
Mixture-based approach, which we evaluate on various interactive tasks such as
handshaking and fistbumps.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14550" title="Abstract">arXiv:2402.14550</a> [<a href="/pdf/2402.14550" title="Download PDF">pdf</a>, <a href="/format/2402.14550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Circular Pattern Matching under Edit Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charalampopoulos%2C+P">Panagiotis Charalampopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Pissis%2C+S+P">Solon P. Pissis</a>, 
<a href="/search/cs?searchtype=author&query=Radoszewski%2C+J">Jakub Radoszewski</a>, 
<a href="/search/cs?searchtype=author&query=Rytter%2C+W">Wojciech Rytter</a>, 
<a href="/search/cs?searchtype=author&query=Wale%C5%84%2C+T">Tomasz Wale&#x144;</a>, 
<a href="/search/cs?searchtype=author&query=Zuba%2C+W">Wiktor Zuba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of a paper accepted to STACS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the $k$-Edit Circular Pattern Matching ($k$-Edit CPM) problem, we are
given a length-$n$ text $T$, a length-$m$ pattern $P$, and a positive integer
threshold $k$, and we are to report all starting positions of the substrings of
$T$ that are at edit distance at most $k$ from some cyclic rotation of $P$. In
the decision version of the problem, we are to check if any such substring
exists. Very recently, Charalampopoulos et al. [ESA 2022] presented
$O(nk^2)$-time and $O(nk \log^3 k)$-time solutions for the reporting and
decision versions of $k$-Edit CPM, respectively. Here, we show that the
reporting and decision versions of $k$-Edit CPM can be solved in $O(n+(n/m)
k^6)$ time and $O(n+(n/m) k^5 \log^3 k)$ time, respectively, thus obtaining the
first algorithms with a complexity of the type $O(n+(n/m) \mathrm{poly}(k))$
for this problem. Notably, our algorithms run in $O(n)$ time when
$m=\Omega(k^6)$ and are superior to the previous respective solutions when
$m=\omega(k^4)$. We provide a meta-algorithm that yields efficient algorithms
in several other interesting settings, such as when the strings are given in a
compressed form (as straight-line programs), when the strings are dynamic, or
when we have a quantum computer.
<br />We obtain our solutions by exploiting the structure of approximate circular
occurrences of $P$ in $T$, when $T$ is relatively short w.r.t. $P$. Roughly
speaking, either the starting positions of approximate occurrences of rotations
of $P$ form $O(k^4)$ intervals that can be computed efficiently, or some
rotation of $P$ is almost periodic (is at a small edit distance from a string
with small period). Dealing with the almost periodic case is the most
technically demanding part of this work; we tackle it using properties of
locked fragments (originating from [Cole and Hariharan, SICOMP 2002]).
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14551" title="Abstract">arXiv:2402.14551</a> [<a href="/pdf/2402.14551" title="Download PDF">pdf</a>, <a href="/format/2402.14551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for  Optimized Learning Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Killick%2C+G">George Killick</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+L">Lipeng Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Aragon-Camarasa%2C+G">Gerardo Aragon-Camarasa</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Mccreadie%2C+R">Richard Mccreadie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.14893">arXiv:2308.14893</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">State-of-the-art pre-trained image models predominantly adopt a two-stage
approach: initial unsupervised pre-training on large-scale datasets followed by
task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been
demonstrated that CE can compromise model generalization and stability. While
recent works employing contrastive learning address some of these limitations
by enhancing the quality of embeddings and producing better decision
boundaries, they often overlook the importance of hard negative mining and rely
on resource intensive and slow training using large sample batches. To counter
these issues, we introduce a novel approach named CLCE, which integrates
Label-Aware Contrastive Learning with CE. Our approach not only maintains the
strengths of both loss functions but also leverages hard negative mining in a
synergistic way to enhance performance. Experimental results demonstrate that
CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks,
achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in
transfer learning settings with the BEiT-3 model. Importantly, our proposed
CLCE approach effectively mitigates the dependency of contrastive learning on
large batch sizes such as 4096 samples per batch, a limitation that has
previously constrained the application of contrastive learning in
budget-limited hardware environments.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14552" title="Abstract">arXiv:2402.14552</a> [<a href="/pdf/2402.14552" title="Download PDF">pdf</a>, <a href="/format/2402.14552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $k$-Plane Insertion into Plane Drawings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katheder%2C+J">Julia Katheder</a>, 
<a href="/search/cs?searchtype=author&query=Kindermann%2C+P">Philipp Kindermann</a>, 
<a href="/search/cs?searchtype=author&query=Klute%2C+F">Fabian Klute</a>, 
<a href="/search/cs?searchtype=author&query=Parada%2C+I">Irene Parada</a>, 
<a href="/search/cs?searchtype=author&query=Rutter%2C+I">Ignaz Rutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We introduce the $k$-Plane Insertion into Plane drawing ($k$-PIP) problem:
given a plane drawing of a planar graph $G$ and a set of edges $F$, insert the
edges in $F$ into the drawing such that the resulting drawing is $k$-plane. In
this paper, we focus on the $1$-PIP scenario. We present a linear-time
algorithm for the case that $G$ is a triangulation, while proving
NP-completeness for the case that $G$ is biconnected and $F$ forms a path or a
matching.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14556" title="Abstract">arXiv:2402.14556</a> [<a href="/pdf/2402.14556" title="Download PDF">pdf</a>, <a href="/format/2402.14556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum computing in civil engineering: Limitations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ploennigs%2C+J">Joern Ploennigs</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+M">Markus Berger</a>, 
<a href="/search/cs?searchtype=author&query=Mevissen%2C+M">Martin Mevissen</a>, 
<a href="/search/cs?searchtype=author&query=Smarsly%2C+K">Kay Smarsly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to icccbe
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">Quantum computing is a new computational paradigm with the potential to solve
certain computationally challenging problems much faster than traditional
approaches. Civil engineering encompasses many computationally challenging
problems, which leads to the question of how well quantum computing is suitable
for solving civil engineering problems and how much impact and implications to
the field of civil engineering can be expected when deploying quantum computing
for solving these problems. To address these questions, we will, in this paper,
first introduce the fundamentals of quantum computing. Thereupon, we will
analyze the problem classes to elucidate where quantum computing holds the
potential to outperform traditional computers and, focusing on the limitations,
where quantum computing is not considered the most suitable solution. Finally,
we will review common complex computation use cases in civil engineering and
evaluate the potential and the limitations of being improved by quantum
computing.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14558" title="Abstract">arXiv:2402.14558</a> [<a href="/pdf/2402.14558" title="Download PDF">pdf</a>, <a href="/format/2402.14558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urlana%2C+A">Ashok Urlana</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+C+V">Charaka Vinayak Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Ajeet Kumar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Garlapati%2C+B+M">Bala Mallikarjunarao Garlapati</a>, 
<a href="/search/cs?searchtype=author&query=Chalamala%2C+S+R">Srinivasa Rao Chalamala</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Rahul Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have become the secret ingredient driving
numerous industrial applications, showcasing their remarkable versatility
across a diverse spectrum of tasks. From natural language processing and
sentiment analysis to content generation and personalized recommendations,
their unparalleled adaptability has facilitated widespread adoption across
industries. This transformative shift driven by LLMs underscores the need to
explore the underlying associated challenges and avenues for enhancement in
their utilization. In this paper, our objective is to unravel and evaluate the
obstacles and opportunities inherent in leveraging LLMs within an industrial
context. To this end, we conduct a survey involving a group of industry
practitioners, develop four research questions derived from the insights
gathered, and examine 68 industry papers to address these questions and derive
meaningful conclusions.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14563" title="Abstract">arXiv:2402.14563</a> [<a href="/pdf/2402.14563" title="Download PDF">pdf</a>, <a href="/format/2402.14563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wizard of Oz Experimentation for Language Technology Applications:  Challenges and Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6gl%2C+S">Stephan Schl&#xf6;gl</a>, 
<a href="/search/cs?searchtype=author&query=Doherty%2C+G">Gavin Doherty</a>, 
<a href="/search/cs?searchtype=author&query=Luz%2C+S">Saturnino Luz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Schlogl, S., Doherty, G., &amp; Luz, S. (2015). Wizard of Oz
  Experimentation for Language Technology Applications: Challenges and Tools.
  Interacting with Computers 27(6), pp. 592-615
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Wizard of OZ (WOZ) is a well-established method for simulating the
functionality and user experience of future systems. Using a human wizard to
mimic certain operations of a potential system is particularly useful in
situations where extensive engineering effort would otherwise be needed to
explore the design possibilities offered by such operations. The WOZ method has
been widely used in connection with speech and language technologies, but
advances in sensor technology and pattern recognition as well as new
application areas such as human-robot interaction have made it increasingly
relevant to the design of a wider range of interactive systems. In such cases
achieving acceptable performance at the user interface level often hinges on
resource intensive improvements such as domain tuning, which are better done
once the overall design is relatively stable. While WOZ is recognised as a
valuable prototyping technique, surprisingly little effort has been put into
exploring it from a methodological point of view. Starting from a survey of the
literature, this paper presents a systematic investigation and analysis of the
design space for WOZ for language technology applications, and proposes a
generic architecture for tool support that supports the integration of
components for speech recognition and synthesis as well as for machine
translation. This architecture is instantiated in WebWOZ - a new web-based
open-source WOZ prototyping platform. The viability of generic support is
explored empirically through a series of evaluations. Researchers from a
variety of backgrounds were able to create experiments, independent of their
previous experience with WOZ. The approach was further validated through a
number of real experiments, which also helped to identify a number of
possibilities for additional support, and flagged potential issues relating to
consistency in Wizard performance.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14566" title="Abstract">arXiv:2402.14566</a> [<a href="/pdf/2402.14566" title="Download PDF">pdf</a>, <a href="/format/2402.14566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Visualisation of Medical Image Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nwabufo%2C+I+V">Ifeoma Veronica Nwabufo</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hm%2C+J+N">Jan Niklas B&#xf6;hm</a>, 
<a href="/search/cs?searchtype=author&query=Berens%2C+P">Philipp Berens</a>, 
<a href="/search/cs?searchtype=author&query=Kobak%2C+D">Dmitry Kobak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning methods based on data augmentations, such as SimCLR,
BYOL, or DINO, allow obtaining semantically meaningful representations of image
datasets and are widely used prior to supervised fine-tuning. A recent
self-supervised learning method, $t$-SimCNE, uses contrastive learning to
directly train a 2D representation suitable for visualisation. When applied to
natural image datasets, $t$-SimCNE yields 2D visualisations with semantically
meaningful clusters. In this work, we used $t$-SimCNE to visualise medical
image datasets, including examples from dermatology, histology, and blood
microscopy. We found that increasing the set of data augmentations to include
arbitrary rotations improved the results in terms of class separability,
compared to data augmentations used for natural images. Our 2D representations
show medically relevant structures and can be used to aid data exploration and
annotation, improving on common approaches for data visualisation.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14567" title="Abstract">arXiv:2402.14567</a> [<a href="/pdf/2402.14567" title="Download PDF">pdf</a>, <a href="/format/2402.14567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CesASMe and Staticdeps: static detection of memory-carried dependencies  for code analyzers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bastian%2C+T">Th&#xe9;ophile Bastian</a>, 
<a href="/search/cs?searchtype=author&query=Pompougnac%2C+H">Hugo Pompougnac</a>, 
<a href="/search/cs?searchtype=author&query=Dutilleul%2C+A">Alban Dutilleul</a>, 
<a href="/search/cs?searchtype=author&query=Rastello%2C+F">Fabrice Rastello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">A variety of code analyzers, such as IACA, uiCA, llvm-mca or Ithemal, strive
to statically predict the throughput of a computation kernel. Each analyzer is
based on its own simplified CPU model reasoning at the scale of a basic block.
Facing this diversity, evaluating their strengths and weaknesses is important
to guide both their usage and their enhancement.
<br />We present CesASMe, a fully-tooled solution to evaluate code analyzers on
C-level benchmarks composed of a benchmark derivation procedure that feeds an
evaluation harness. We conclude that memory-carried data dependencies are a
major source of imprecision for these tools. We tackle this issue with
staticdeps, a static analyzer extracting memory-carried data dependencies,
including across loop iterations, from an assembly basic block. We integrate
its output to uiCA, a state-of-the-art code analyzer, to evaluate staticdeps'
impact on a code analyzer's precision through CesASMe.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14568" title="Abstract">arXiv:2402.14568</a> [<a href="/pdf/2402.14568" title="Download PDF">pdf</a>, <a href="/format/2402.14568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named  Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junjie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the impressive capabilities of large language models (LLMs), their
performance on information extraction tasks is still not entirely satisfactory.
However, their remarkable rewriting capabilities and extensive world knowledge
offer valuable insights to improve these tasks. In this paper, we propose
$LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot
NER task. To overcome the limitations of existing data augmentation methods
that compromise semantic integrity and address the uncertainty inherent in
LLM-generated text, we leverage the distinctive characteristics of the NER task
by augmenting the original data at both the contextual and entity levels. Our
approach involves employing 14 contextual rewriting strategies, designing
entity replacements of the same type, and incorporating noise injection to
enhance robustness. Extensive experiments demonstrate the effectiveness of our
approach in enhancing NER model performance with limited data. Furthermore,
additional analyses provide further evidence supporting the assertion that the
quality of the data we generate surpasses that of other existing methods.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14569" title="Abstract">arXiv:2402.14569</a> [<a href="/pdf/2402.14569" title="Download PDF">pdf</a>, <a href="/format/2402.14569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformable Gaussian Reward Function for Socially-Aware Navigation  with Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinyeob Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Sumin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sungwoo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Beomjoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yura%2C+J">Jargalbaatar Yura</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robot navigation has transitioned from prioritizing obstacle avoidance to
adopting socially aware navigation strategies that accommodate human presence.
As a result, the recognition of socially aware navigation within dynamic
human-centric environments has gained prominence in the field of robotics.
Although reinforcement learning technique has fostered the advancement of
socially aware navigation, defining appropriate reward functions, especially in
congested environments, has posed a significant challenge. These rewards,
crucial in guiding robot actions, demand intricate human-crafted design due to
their complex nature and inability to be automatically set. The multitude of
manually designed rewards poses issues with hyperparameter redundancy,
imbalance, and inadequate representation of unique object characteristics. To
address these challenges, we introduce a transformable gaussian reward function
(TGRF). The TGRF significantly reduces the burden of hyperparameter tuning,
displays adaptability across various reward functions, and demonstrates
accelerated learning rates, particularly excelling in crowded environments
utilizing deep reinforcement learning (DRL). We introduce and validate TGRF
through sections highlighting its conceptual background, characteristics,
experiments, and real-world application, paving the way for a more effective
and adaptable approach in robotics.The complete source code is available on
https://github.com/JinnnK/TGRF
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14576" title="Abstract">arXiv:2402.14576</a> [<a href="/pdf/2402.14576" title="Download PDF">pdf</a>, <a href="/format/2402.14576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niknia%2C+F">Farnaz Niknia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Ping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aakash Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+A+S">Adib S. Rezaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper addresses the escalating challenge of redundant data transmission
in networks. The surge in traffic has strained backhaul links and backbone
networks, prompting the exploration of caching solutions at the edge router.
Existing work primarily relies on Markov Decision Processes (MDP) for caching
issues, assuming fixed-time interval decisions; however, real-world scenarios
involve random request arrivals, and despite the critical role of various file
characteristics in determining an optimal caching policy, none of the related
existing work considers all these file characteristics in forming a caching
policy. In this paper, first, we formulate the caching problem using a
semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature
of real-world scenarios allowing for caching decisions at random times upon
file requests. Then, we propose a double deep Q-learning-based caching approach
that comprehensively accounts for file features such as lifetime, size, and
importance. Simulation results demonstrate the superior performance of our
approach compared to a recent Deep Reinforcement Learning-based method.
Furthermore, we extend our work to include a Transfer Learning (TL) approach to
account for changes in file request rates in the SMDP framework. The proposed
TL approach exhibits fast convergence, even in scenarios with increased
differences in request rates between source and target domains, presenting a
promising solution to the dynamic challenges of caching in real-world
environments.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14577" title="Abstract">arXiv:2402.14577</a> [<a href="/pdf/2402.14577" title="Download PDF">pdf</a>, <a href="/format/2402.14577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruifei He</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chuhui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haoru Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yingchen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+S">Song Bai</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Learning-based Text-to-Image (TTI) models like Stable Diffusion have
revolutionized the way visual content is generated in various domains. However,
recent research has shown that nonnegligible social bias exists in current
state-of-the-art TTI systems, which raises important concerns. In this work, we
target resolving the social bias in TTI diffusion models. We begin by
formalizing the problem setting and use the text descriptions of bias groups to
establish an unsafe direction for guiding the diffusion process. Next, we
simplify the problem into a weight optimization problem and attempt a
Reinforcement solver, Policy Gradient, which shows sub-optimal performance with
slow convergence. Further, to overcome limitations, we propose an iterative
distribution alignment (IDA) method. Despite its simplicity, we show that IDA
shows efficiency and fast convergence in resolving the social bias in TTI
diffusion models. Our code will be released.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14579" title="Abstract">arXiv:2402.14579</a> [<a href="/pdf/2402.14579" title="Download PDF">pdf</a>, <a href="/format/2402.14579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Role Classification in Scientific Charts Using Multimodal  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hye Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lell%2C+N">Nicolas Lell</a>, 
<a href="/search/cs?searchtype=author&query=Scherp%2C+A">Ansgar Scherp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text role classification involves classifying the semantic role of textual
elements within scientific charts. For this task, we propose to finetune two
pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on
chart datasets. The transformers utilize the three modalities of text, image,
and layout as input. We further investigate whether data augmentation and
balancing methods help the performance of the models. The models are evaluated
on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in
all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the
ICPR22 test dataset, beating the best-performing model from the ICPR22
CHART-Infographics challenge. Moreover, the robustness of the models is tested
on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the
models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz,
for which we added labels for the text roles. Findings indicate that even in
cases where there is limited training data, transformers can be used with the
help of data augmentation and balancing methods. The source code and datasets
are available on GitHub under
https://github.com/hjkimk/text-role-classification
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14580" title="Abstract">arXiv:2402.14580</a> [<a href="/pdf/2402.14580" title="Download PDF">pdf</a>, <a href="/format/2402.14580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Savvy: Trustworthy Autonomous Vehicles Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoker%2C+A">Ali Shoker</a>, 
<a href="/search/cs?searchtype=author&query=Yasmin%2C+R">Rehana Yasmin</a>, 
<a href="/search/cs?searchtype=author&query=Esteves-Verissimo%2C+P">Paulo Esteves-Verissimo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The increasing interest in Autonomous Vehicles (AV) is notable due to
business, safety, and performance reasons. While there is salient success in
recent AV architectures, hinging on the advancements in AI models, there is a
growing number of fatal incidents that impedes full AVs from going mainstream.
This calls for the need to revisit the fundamentals of building safety-critical
AV architectures. However, this direction should not deter leveraging the power
of AI. To this end, we propose Savvy, a new trustworthy intelligent AV
architecture that achieves the best of both worlds. Savvy makes a clear
separation between the control plane and the data plane to guarantee the
safety-first principles. The former assume control to ensure safety using
design-time defined rules, while launching the latter for optimizing decisions
as much as possible within safety time-bounds. This is achieved through guided
Time-aware predictive quality degradation (TPQD): using dynamic ML models that
can be tuned to provide either richer or faster outputs based on the available
safety time bounds. For instance, Savvy allows to safely identify an elephant
as an obstacle (a mere object) the earliest possible, rather than optimally
recognizing it as an elephant when it is too late. This position paper presents
the Savvy's motivations and concept, whereas empirical evaluation is a work in
progress.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14581" title="Abstract">arXiv:2402.14581</a> [<a href="/pdf/2402.14581" title="Download PDF">pdf</a>, <a href="/format/2402.14581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Communication-assisted Physical Layer Security over Fading  Wiretap Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%2C+X">Xidong Mu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, this paper is accepted by ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A novel semantic communication (SC)-assisted secrecy transmission framework
is proposed. In particular, the legitimate transmitter (Tx) sends the
superimposed semantic and bit stream to the legitimate receiver (Rx), where the
information may be eavesdropped by the malicious node (EVE). As the EVE merely
has the conventional bit-oriented communication structure, the semantic signal
acts as the type of beneficial information-bearing artificial noise (AN), which
not only keeps strictly confidential to the EVE but also interferes with the
EVE. The ergodic (equivalent) secrecy rate over fading wiretap channels is
maximized by jointly optimizing the transmit power, semantic-bit power
splitting ratio, and the successive interference cancellation decoding order at
the Tx, subject to both the instantaneous peak and long-term average power
constraints. To address this non-convex problem, both the optimal and
suboptimal algorithms are developed by employing the Lagrangian dual method and
the successive convex approximation method, respectively. Numerical results
show that the proposed SC-assisted secrecy transmission scheme can
significantly enhance the physical layer security compared to the baselines
using the conventional bit-oriented communication and no-information-bearing
AN. It also shows that the proposed suboptimal algorithm can achieve a
near-optimal performance.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14582" title="Abstract">arXiv:2402.14582</a> [<a href="/pdf/2402.14582" title="Download PDF">pdf</a>, <a href="/format/2402.14582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancement of High-definition Map Update Service Through Coverage-aware  and Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Redondo%2C+J">Jeffrey Redondo</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhenhui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Aslam%2C+N">Nauman Aslam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">High-definition (HD) Map systems will play a pivotal role in advancing
autonomous driving to a higher level, thanks to the significant improvement
over traditional two-dimensional (2D) maps. Creating an HD Map requires a huge
amount of on-road and off-road data. Typically, these raw datasets are
collected and uploaded to cloud-based HD map service providers through
vehicular networks. Nevertheless, there are challenges in transmitting the raw
data over vehicular wireless channels due to the dynamic topology. As the
number of vehicles increases, there is a detrimental impact on service quality,
which acts as a barrier to a real-time HD Map system for collaborative driving
in Autonomous Vehicles (AV). In this paper, to overcome network congestion, a
Q-learning coverage-time-awareness algorithm is presented to optimize the
quality of service for vehicular networks and HD map updates. The algorithm is
evaluated in an environment that imitates a dynamic scenario where vehicles
enter and leave. Results showed an improvement in latency for HD map data of
$75\%$, $73\%$, and $10\%$ compared with IEEE802.11p without Quality of Service
(QoS), IEEE802.11 with QoS, and IEEE802.11p with new access category (AC) for
HD map, respectively.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14583" title="Abstract">arXiv:2402.14583</a> [<a href="/pdf/2402.14583" title="Download PDF">pdf</a>, <a href="/format/2402.14583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness  in Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holst%2C+V">Vincent Holst</a>, 
<a href="/search/cs?searchtype=author&query=Algaba%2C+A">Andres Algaba</a>, 
<a href="/search/cs?searchtype=author&query=Tori%2C+F">Floriano Tori</a>, 
<a href="/search/cs?searchtype=author&query=Wenmackers%2C+S">Sylvia Wenmackers</a>, 
<a href="/search/cs?searchtype=author&query=Ginis%2C+V">Vincent Ginis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures, 3 extended data figures, and Supplementary Information. In submission to Nature
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Park et al. [1] reported a decline in the disruptiveness of scientific and
technological knowledge over time. Their main finding is based on the
computation of CD indices, a measure of disruption in citation networks [2],
across almost 45 million papers and 3.9 million patents. Due to a factual
plotting mistake, database entries with zero references were omitted in the CD
index distributions, hiding a large number of outliers with a maximum CD index
of one, while keeping them in the analysis [1]. Our reanalysis shows that the
reported decline in disruptiveness can be attributed to a relative decline of
these database entries with zero references. Notably, this was not caught by
the robustness checks included in the manuscript. The regression adjustment
fails to control for the hidden outliers as they correspond to a discontinuity
in the CD index. Proper evaluation of the Monte-Carlo simulations reveals that,
because of the preservation of the hidden outliers, even random citation
behaviour replicates the observed decline in disruptiveness. Finally, while
these papers and patents with supposedly zero references are the hidden drivers
of the reported decline, their source documents predominantly do make
references, exposing them as pure dataset artefacts.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14585" title="Abstract">arXiv:2402.14585</a> [<a href="/pdf/2402.14585" title="Download PDF">pdf</a>, <a href="/format/2402.14585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandits with Abstention under Expert Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasteris%2C+S">Stephen Pasteris</a>, 
<a href="/search/cs?searchtype=author&query=Rumi%2C+A">Alberto Rumi</a>, 
<a href="/search/cs?searchtype=author&query=Thiessen%2C+M">Maximilian Thiessen</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shota Saito</a>, 
<a href="/search/cs?searchtype=author&query=Miyauchi%2C+A">Atsushi Miyauchi</a>, 
<a href="/search/cs?searchtype=author&query=Vitale%2C+F">Fabio Vitale</a>, 
<a href="/search/cs?searchtype=author&query=Herbster%2C+M">Mark Herbster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the classic problem of prediction with expert advice under bandit
feedback. Our model assumes that one action, corresponding to the learner's
abstention from play, has no reward or loss on every trial. We propose the CBA
algorithm, which exploits this assumption to obtain reward bounds that can
significantly improve those of the classical Exp4 algorithm. We can view our
problem as the aggregation of confidence-rated predictors when the learner has
the option of abstention from play. Importantly, we are the first to achieve
bounds on the expected cumulative reward for general confidence-rated
predictors. In the special case of specialists we achieve a novel reward bound,
significantly improving previous bounds of SpecialistExp (treating abstention
as another action). As an example application, we discuss learning unions of
balls in a finite metric space. In this contextual setting, we devise an
efficient implementation of CBA, reducing the runtime from quadratic to almost
linear in the number of contexts. Preliminary experiments show that CBA
improves over existing bandit algorithms.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14586" title="Abstract">arXiv:2402.14586</a> [<a href="/pdf/2402.14586" title="Download PDF">pdf</a>, <a href="/format/2402.14586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yan Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ligang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daolun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present a novel framework, called FrameNeRF, designed to apply
off-the-shelf fast high-fidelity NeRF models with fast training speed and high
rendering quality for few-shot novel view synthesis tasks. The training
stability of fast high-fidelity models is typically constrained to dense views,
making them unsuitable for few-shot novel view synthesis tasks. To address this
limitation, we utilize a regularization model as a data generator to produce
dense views from sparse inputs, facilitating subsequent training of fast
high-fidelity models. Since these dense views are pseudo ground truth generated
by the regularization model, original sparse images are then used to fine-tune
the fast high-fidelity model. This process helps the model learn realistic
details and correct artifacts introduced in earlier stages. By leveraging an
off-the-shelf regularization model and a fast high-fidelity model, our approach
achieves state-of-the-art performance across various benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14588" title="Abstract">arXiv:2402.14588</a> [<a href="/pdf/2402.14588" title="Download PDF">pdf</a>, <a href="/ps/2402.14588" title="Download PostScript">ps</a>, <a href="/format/2402.14588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do digital threats change requirements for the software industry?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halttunen%2C+V">Veikko Halttunen</a> (University of Jyv&#xe4;skyl&#xe4;)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Digital systems are, by definition, the core of digital transformation. This
has led many to think that the system being considered in digital
transformation is solely software. I argue that this approach is a fatal
mistake, and it has induced a great number of already realized problems and
even a greater number of concerns about the future. These problems and concerns
have become evident along with rising requirements for sustainability and
responsibility. In this paper, I call for a better understanding of the digital
society in its entirety. By digital society I mean the societal system that is
affected by the digital systems and the ongoing societal trans-formations. When
shifting the focus to the effects of digital systems on societies, we are
forced to consider all the anticipated outcomes, both desirable and undesirable
ones. Unfortunately, the mainstream research has ignored, to a large extent,
the potential threats, and unwanted outcomes, of digitalization, which makes
the efforts to change software businesses to be more sustainable, difficult to
succeed. In my paper, I will provide an overall picture of current and future
challenges of digital societies and discuss what these challenges mean to the
software industry in future. The easiest way to start with, is to learn from
earlier experiences, especially from the unsuccessful stories.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14589" title="Abstract">arXiv:2402.14589</a> [<a href="/pdf/2402.14589" title="Download PDF">pdf</a>, <a href="/ps/2402.14589" title="Download PostScript">ps</a>, <a href="/format/2402.14589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding an AI-imposed Taylor&#x27;s Version of all music history
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collins%2C+N">Nick Collins</a>, 
<a href="/search/cs?searchtype=author&query=Grierson%2C+M">Mick Grierson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">As future musical AIs adhere closely to human music, they may form their own
attachments to particular human artists in their databases, and these biases
may in the worst case lead to potential existential threats to all musical
history. AI super fans may act to corrupt the historical record and extant
recordings in favour of their own preferences, and preservation of the
diversity of world music culture may become even more of a pressing issue than
the imposition of 12 tone equal temperament or other Western homogenisations.
We discuss the technical capability of AI cover software and produce Taylor's
Versions of famous tracks from Western pop history as provocative examples; the
quality of these productions does not affect the overall argument (which might
even see a future AI try to impose the sound of paperclips onto all existing
audio files, let alone Taylor Swift). We discuss some potential defenses
against the danger of future musical monopolies, whilst analysing the
feasibility of a maximal 'Taylor Swiftication' of the complete musical record.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14590" title="Abstract">arXiv:2402.14590</a> [<a href="/pdf/2402.14590" title="Download PDF">pdf</a>, <a href="/format/2402.14590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up LLM Reviews for Google Ads Content Moderation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+W">Wei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dogra%2C+T">Tushar Dogra</a>, 
<a href="/search/cs?searchtype=author&query=Stretcu%2C+O">Otilia Stretcu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Y">Yu-Han Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tiantian Fang</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+D">Dongjin Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chun-Ta Lu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+E">Enming Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+C">Chih-Chun Chia</a>, 
<a href="/search/cs?searchtype=author&query=Fuxman%2C+A">Ariel Fuxman</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangzhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Tek%2C+M">Mehmet Tek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) are powerful tools for content moderation, but
their inference costs and latency make them prohibitive for casual use on large
datasets, such as the Google Ads repository. This study proposes a method for
scaling up LLM reviews for content moderation in Google Ads. First, we use
heuristics to select candidates via filtering and duplicate removal, and create
clusters of ads for which we select one representative ad per cluster. We then
use LLMs to review only the representative ads. Finally, we propagate the LLM
decisions for the representative ads back to their clusters. This method
reduces the number of reviews by more than 3 orders of magnitude while
achieving a 2x recall compared to a baseline non-LLM model. The success of this
approach is a strong function of the representations used in clustering and
label propagation; we found that cross-modal similarity representations yield
better results than uni-modal representations.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14591" title="Abstract">arXiv:2402.14591</a> [<a href="/pdf/2402.14591" title="Download PDF">pdf</a>, <a href="/format/2402.14591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Speed Detector For Low-Powered Devices In Aerial Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ashish Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Behera%2C+L">Laxmidhar Behera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 9 Figures, 8 Tables, IEEE Robotics and Automation Letters (IEEE RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous aerial harvesting is a highly complex problem because it requires
numerous interdisciplinary algorithms to be executed on mini low-powered
computing devices. Object detection is one such algorithm that is
compute-hungry. In this context, we make the following contributions: (i) Fast
Fruit Detector (FFD), a resource-efficient, single-stage, and
postprocessing-free object detector based on our novel latent object
representation (LOR) module, query assignment, and prediction strategy. FFD
achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded
device while co-existing with other time-critical sub-systems such as control,
grasping, SLAM, a major achievement of this work. (ii) a method to generate
vast amounts of training data without exhaustive manual labelling of fruit
images since they consist of a large number of instances, which increases the
labelling cost and time. (iii) an open-source fruit detection dataset having
plenty of very small-sized instances that are difficult to detect. Our
exhaustive evaluations on our and MinneApple dataset show that FFD, being only
a single-scale detector, is more accurate than many representative detectors,
e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale
Faster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and
multi-scale YOLO-v8 by 0.3 while being considerably faster.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14594" title="Abstract">arXiv:2402.14594</a> [<a href="/pdf/2402.14594" title="Download PDF">pdf</a>, <a href="/format/2402.14594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Assessment of Tutoring Practices using Retrieval-Augmented  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zifei">Zifei</a> (FeiFei)Han, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jionghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gurung%2C+A">Ashish Gurung</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+D+R">Danielle R. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Borchers%2C+C">Conrad Borchers</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivang Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Koedinger%2C+K+R">Kenneth R. Koedinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 page Workshop paper, AAAI2024 Workshop on AI for Education - Bridging Innovation and Responsibility, Large Language Model, Personalized Tutor Training, Automatic Assessment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14595" title="Abstract">arXiv:2402.14595</a> [<a href="/pdf/2402.14595" title="Download PDF">pdf</a>, <a href="/format/2402.14595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agile Requirement Change Management Model for Global Software  Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koulecar%2C+N">Neha Koulecar</a>, 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+B">Bachan Ghimire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">We propose a noble, comprehensive and robust agile requirements change
management (ARCM) model that addresses the limitations of existing models and
is tailored for agile software development in the global software development
paradigm. To achieve this goal, we conducted an exhaustive literature review
and an empirical study with RCM industry experts. Our study evaluated the
effectiveness of the proposed RCM model in a real-world setting and identifies
any limitations or areas for improvement. The results of our study provide
valuable insights into how the proposed RCM model can be applied in agile
global software development environments to improve software development
practices and optimize project success rates.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14596" title="Abstract">arXiv:2402.14596</a> [<a href="/pdf/2402.14596" title="Download PDF">pdf</a>, <a href="/ps/2402.14596" title="Download PostScript">ps</a>, <a href="/format/2402.14596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of LLMs in Sustainable Smart Cities: Applications, Challenges,  and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ullah%2C+A">Amin Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+S">Saddam Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+I">Irfan Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+Z">Zafar Ali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Smart cities stand as pivotal components in the ongoing pursuit of elevating
urban living standards, facilitating the rapid expansion of urban areas while
efficiently managing resources through sustainable and scalable innovations. In
this regard, as emerging technologies like Artificial Intelligence (AI), the
Internet of Things (IoT), big data analytics, and fog and edge computing have
become increasingly prevalent, smart city applications grapple with various
challenges, including the potential for unauthorized disclosure of confidential
and sensitive data. The seamless integration of emerging technologies has
played a vital role in sustaining the dynamic pace of their development. This
paper explores the substantial potential and applications of Deep Learning
(DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing
(NLP), and large language models (LLMs) in optimizing ICT processes within
smart cities. We aim to spotlight the vast potential of these technologies as
foundational elements that technically strengthen the realization and
advancement of smart cities, underscoring their significance in driving
innovation within this transformative urban milieu. Our discourse culminates
with an exploration of the formidable challenges that DL, FL, IoT, Blockchain,
NLP, and LLMs face within these contexts, and we offer insights into potential
future directions.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14597" title="Abstract">arXiv:2402.14597</a> [<a href="/pdf/2402.14597" title="Download PDF">pdf</a>, <a href="/ps/2402.14597" title="Download PostScript">ps</a>, <a href="/format/2402.14597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Style Identification Using Semi-Supervised Self-Taught Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayyoub%2C+H+Y">Hani Y. Ayyoub</a>, 
<a href="/search/cs?searchtype=author&query=Al-Kadi%2C+O+S">Omar S. Al-Kadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures, journal paper in IEEE Transactions on Learning Technologies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Education is a dynamic field that must be adaptable to sudden changes and
disruptions caused by events like pandemics, war, and natural disasters related
to climate change. When these events occur, traditional classrooms with
traditional or blended delivery can shift to fully online learning, which
requires an efficient learning environment that meets students' needs. While
learning management systems support teachers' productivity and creativity, they
typically provide the same content to all learners in a course, ignoring their
unique learning styles. To address this issue, we propose a semi-supervised
machine learning approach that detects students' learning styles using a data
mining technique. We use the commonly used Felder Silverman learning style
model and demonstrate that our semi-supervised method can produce reliable
classification models with few labeled data. We evaluate our approach on two
different courses and achieve an accuracy of 88.83% and 77.35%, respectively.
Our work shows that educational data mining and semi-supervised machine
learning techniques can identify different learning styles and create a
personalized learning environment.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14598" title="Abstract">arXiv:2402.14598</a> [<a href="/pdf/2402.14598" title="Download PDF">pdf</a>, <a href="/format/2402.14598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-inspired Distributed Memorization Learning for Efficient  Feature-free Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jianming Lv</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Depin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zequan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaobin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Sijun Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Compared with gradient based artificial neural networks, biological neural
networks usually show a more powerful generalization ability to quickly adapt
to unknown environments without using any gradient back-propagation procedure.
Inspired by the distributed memory mechanism of human brains, we propose a
novel gradient-free Distributed Memorization Learning mechanism, namely DML, to
support quick domain adaptation of transferred models. In particular, DML
adopts randomly connected neurons to memorize the association of input signals,
which are propagated as impulses, and makes the final decision by associating
the distributed memories based on their confidence. More importantly, DML is
able to perform reinforced memorization based on unlabeled data to quickly
adapt to a new domain without heavy fine-tuning of deep features, which makes
it very suitable for deploying on edge devices. Experiments based on four
cross-domain real-world datasets show that DML can achieve superior performance
of real-time domain adaptation compared with traditional gradient based MLP
with more than 10% improvement of accuracy while reducing 87% of the timing
cost of optimization.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14599" title="Abstract">arXiv:2402.14599</a> [<a href="/pdf/2402.14599" title="Download PDF">pdf</a>, <a href="/format/2402.14599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing SCADA Security: Developing a Host-Based Intrusion Detection  System to Safeguard Against Cyberattacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+O">Omer Sen</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+T">Tarek Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Ulbig%2C+A">Andreas Ulbig</a>, 
<a href="/search/cs?searchtype=author&query=Henze%2C+M">Martin Henze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the increasing reliance of smart grids on correctly functioning SCADA
systems and their vulnerability to cyberattacks, there is a pressing need for
effective security measures. SCADA systems are prone to cyberattacks, posing
risks to critical infrastructure. As there is a lack of host-based intrusion
detection systems specifically designed for the stable nature of SCADA systems,
the objective of this work is to propose a host-based intrusion detection
system tailored for SCADA systems in smart grids. The proposed system utilizes
USB device identification, flagging, and process memory scanning to monitor and
detect anomalies in SCADA systems, providing enhanced security measures.
Evaluation in three different scenarios demonstrates the tool's effectiveness
in detecting and disabling malware. The proposed approach effectively
identifies potential threats and enhances the security of SCADA systems in
smart grids, providing a promising solution to protect against cyberattacks.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14600" title="Abstract">arXiv:2402.14600</a> [<a href="/pdf/2402.14600" title="Download PDF">pdf</a>, <a href="/format/2402.14600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model-Based Multiobjective Optimization for Gasoline Blending  Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wenxuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Renchu He</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yen%2C+G+G">Gary G. Yen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Gasoline blending scheduling uses resource allocation and operation
sequencing to meet a refinery's production requirements. The presence of
nonlinearity, integer constraints, and a large number of decision variables
adds complexity to this problem, posing challenges for traditional and
evolutionary algorithms. This paper introduces a novel multiobjective
optimization approach driven by a diffusion model (named DMO), which is
designed specifically for gasoline blending scheduling. To address integer
constraints and generate feasible schedules, the diffusion model creates
multiple intermediate distributions between Gaussian noise and the feasible
domain. Through iterative processes, the solutions transition from Gaussian
noise to feasible schedules while optimizing the objectives using the gradient
descent method. DMO achieves simultaneous objective optimization and constraint
adherence. Comparative tests are conducted to evaluate DMO's performance across
various scales. The experimental results demonstrate that DMO surpasses
state-of-the-art multiobjective evolutionary algorithms in terms of efficiency
when solving gasoline blending scheduling problems.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14601" title="Abstract">arXiv:2402.14601</a> [<a href="/pdf/2402.14601" title="Download PDF">pdf</a>, <a href="/format/2402.14601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing Generative AI to Adaptive Learning in Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recent surge in generative AI technologies, such as large language models
and diffusion models, have boosted the development of AI applications in
various domains, including science, finance, and education. Concurrently,
adaptive learning, a concept that has gained substantial interest in the
educational sphere, has proven its efficacy in enhancing students' learning
efficiency. In this position paper, we aim to shed light on the intersectional
studies of these two methods, which combine generative AI with adaptive
learning concepts. By presenting discussions about the benefits, challenges,
and potentials in this field, we argue that this union will contribute
significantly to the development of the next stage learning format in
education.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14602" title="Abstract">arXiv:2402.14602</a> [<a href="/pdf/2402.14602" title="Download PDF">pdf</a>, <a href="/format/2402.14602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t mention it: An approach to assess challenges to using software  mentions for citation and discoverability research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Druskat%2C+S">Stephan Druskat</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+N+P+C">Neil P. Chue Hong</a>, 
<a href="/search/cs?searchtype=author&query=Buzzard%2C+S">Sammie Buzzard</a>, 
<a href="/search/cs?searchtype=author&query=Konovalov%2C+O">Olexandr Konovalov</a>, 
<a href="/search/cs?searchtype=author&query=Kornek%2C+P">Patrick Kornek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, 8 tables. Revision of a submission to PeerJ Computer Science withdrawn due to impracticalities of examining a sufficient sample size
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Datasets collecting software mentions from scholarly publications can
potentially be used for research into the software that has been used in the
published research, as well as into the practice of software citation.
Recently, new software mention datasets with different characteristics have
been published. We present an approach to assess the usability of such datasets
for research on research software. Our approach includes sampling and data
preparation, manual annotation for quality and mention characteristics, and
annotation analysis. We applied it to two software mention datasets for
evaluation based on qualitative observation. Doing this, we were able to find
challenges to working with the selected datasets to do research. Main issues
refer to the structure of the dataset, the quality of the extracted mentions
(54% and 23% of mentions respectively are not to software), and software
accessibility. While one dataset does not provide links to mentioned software
at all, the other does so in a way that can impede quantitative research
endeavors: (1) Links may come from different sources and each point to
different software for the same mention. (2) The quality of the automatically
retrieved links is generally poor (in our sample, 65.4% link the wrong
software). (3) Links exist only for a small subset (in our sample, 20.5%) of
mentions, which may lead to skewed or disproportionate samples. However, the
greatest challenge and underlying issue in working with software mention
datasets is the still suboptimal practice of software citation: Software should
not be mentioned, it should be cited following the software citation
principles.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14603" title="Abstract">arXiv:2402.14603</a> [<a href="/pdf/2402.14603" title="Download PDF">pdf</a>, <a href="/format/2402.14603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Resonate-and-Fire Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Higuchi%2C+S">Saya Higuchi</a>, 
<a href="/search/cs?searchtype=author&query=Kairat%2C+S">Sebastian Kairat</a>, 
<a href="/search/cs?searchtype=author&query=Otte%2C+S+M+B+S">Sander M. Bohte. Sebastian Otte</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The resonate-and-fire (RF) neuron, introduced over two decades ago, is a
simple, efficient, yet biologically plausible spiking neuron model, which can
extract frequency patterns within the time domain due to its resonating
membrane dynamics. However, previous RF formulations suffer from intrinsic
shortcomings that limit effective learning and prevent exploiting the
principled advantage of RF neurons. Here, we introduce the balanced RF (BRF)
neuron, which alleviates some of the intrinsic limitations of vanilla RF
neurons and demonstrates its effectiveness within recurrent spiking neural
networks (RSNNs) on various sequence learning tasks. We show that networks of
BRF neurons achieve overall higher task performance, produce only a fraction of
the spikes, and require significantly fewer parameters as compared to modern
RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable
training convergence, even when bridging many hundreds of time steps during
backpropagation through time (BPTT). These results underscore that our BRF-RSNN
is a strong candidate for future large-scale RSNN architectures, further lines
of research in SNN methodology, and more efficient hardware implementations.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14604" title="Abstract">arXiv:2402.14604</a> [<a href="/pdf/2402.14604" title="Download PDF">pdf</a>, <a href="/format/2402.14604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embeddings and near-neighbor searching with constant additive error for  hyperbolic spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunku Park</a>, 
<a href="/search/cs?searchtype=author&query=Vigneron%2C+A">Antoine Vigneron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We give an embedding of the Poincar\'e halfspace $H^D$ into a discrete metric
space based on a binary tiling of $H^D$, with additive distortion $O(\log D)$.
It yields the following results. We show that any subset $P$ of $n$ points in
$H^D$ can be embedded into a graph-metric with $2^{O(D)}n$ vertices and edges,
and with additive distortion $O(\log D)$. We also show how to construct, for
any $k$, an $O(k\log D)$-purely additive spanner of $P$ with $2^{O(D)}n$
Steiner vertices and $2^{O(D)}n \cdot \lambda_k(n)$ edges, where $\lambda_k(n)$
is the $k$th-row inverse Ackermann function. Finally, we present a data
structure for approximate near-neighbor searching in $H^D$, with construction
time $2^{O(D)}n\log n$, query time $2^{O(D)}\log n$ and additive error $O(\log
D)$. These constructions can be done in $2^{O(D)}n \log n$ time.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14606" title="Abstract">arXiv:2402.14606</a> [<a href="/pdf/2402.14606" title="Download PDF">pdf</a>, <a href="/format/2402.14606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaogang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Blessing%2C+D">Denis Blessing</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinkai Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Reuss%2C+M">Moritz Reuss</a>, 
<a href="/search/cs?searchtype=author&query=Donat%2C+A">Atalay Donat</a>, 
<a href="/search/cs?searchtype=author&query=Lioutikov%2C+R">Rudolf Lioutikov</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Imitation learning with human data has demonstrated remarkable success in
teaching robots in a wide range of skills. However, the inherent diversity in
human behavior leads to the emergence of multi-modal data distributions,
thereby presenting a formidable challenge for existing imitation learning
algorithms. Quantifying a model's capacity to capture and replicate this
diversity effectively is still an open problem. In this work, we introduce
simulation benchmark environments and the corresponding Datasets with Diverse
human Demonstrations for Imitation Learning (D3IL), designed explicitly to
evaluate a model's ability to learn multi-modal behavior. Our environments are
designed to involve multiple sub-tasks that need to be solved, consider
manipulation of multiple objects which increases the diversity of the behavior
and can only be solved by policies that rely on closed loop sensory feedback.
Other available datasets are missing at least one of these challenging
properties. To address the challenge of diversity quantification, we introduce
tractable metrics that provide valuable insights into a model's ability to
acquire and reproduce diverse behaviors. These metrics offer a practical means
to assess the robustness and versatility of imitation learning algorithms.
Furthermore, we conduct a thorough evaluation of state-of-the-art methods on
the proposed task suite. This evaluation serves as a benchmark for assessing
their capability to learn diverse behaviors. Our findings shed light on the
effectiveness of these methods in tackling the intricate problem of capturing
and generalizing multi-modal human behaviors, offering a valuable reference for
the design of future imitation learning algorithms.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14609" title="Abstract">arXiv:2402.14609</a> [<a href="/pdf/2402.14609" title="Download PDF">pdf</a>, <a href="/format/2402.14609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Complex Qeury Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weifeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaxin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lixin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Databases (cs.DB)

</div>
<p class="mathjax">Complex logical query answering is a challenging task in knowledge graphs
(KGs) that has been widely studied. The ability to perform complex logical
reasoning is essential and supports various graph reasoning-based downstream
tasks, such as search engines. Recent approaches are proposed to represent KG
entities and logical queries into embedding vectors and find answers to logical
queries from the KGs. However, existing proposed methods mainly focus on
querying a single KG and cannot be applied to multiple graphs. In addition,
directly sharing KGs with sensitive information may incur privacy risks, making
it impractical to share and construct an aggregated KG for reasoning to
retrieve query answers. Thus, it remains unknown how to answer queries on
multi-source KGs. An entity can be involved in various knowledge graphs and
reasoning on multiple KGs and answering complex queries on multi-source KGs is
important in discovering knowledge cross graphs. Fortunately, federated
learning is utilized in knowledge graphs to collaboratively learn
representations with privacy preserved. Federated knowledge graph embeddings
enrich the relations in knowledge graphs to improve the representation quality.
However, these methods only focus on one-hop relations and cannot perform
complex reasoning tasks. In this paper, we apply federated learning to complex
query-answering tasks to reason over multi-source knowledge graphs while
preserving privacy. We propose a Federated Complex Query Answering framework
(FedCQA), to reason over multi-source KGs avoiding sensitive raw data
transmission to protect privacy. We conduct extensive experiments on three
real-world datasets and evaluate retrieval performance on various types of
complex queries.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14610" title="Abstract">arXiv:2402.14610</a> [<a href="/pdf/2402.14610" title="Download PDF">pdf</a>, <a href="/format/2402.14610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Scalable Docker-Based Emulations of Blockchain Networks for  Research and Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pennino%2C+D">Diego Pennino</a>, 
<a href="/search/cs?searchtype=author&query=Pizzonia%2C+M">Maurizio Pizzonia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Blockchain, like any other complex technology, needs a strong testing
methodology to support its evolution in both research and development contexts.
Setting up meaningful tests for permissionless blockchain technology is a
notoriously complex task for several reasons: software is complex, large number
of nodes are involved, network is non ideal, etc. Developers usually adopt
small virtual laboratories or costly real devnets, based on real software.
Researchers usually prefer simulations of a large number of nodes, based on
simplified models. In this paper, we aim to obtain the advantages of both
approaches, i.e., performing large, realistic, inexpensive, and flexible
experiments, using real blockchain software within a virtual environment. To do
that, we tackle the challenge of running large blockchain networks in a single
physical machine, leveraging Linux and Docker. We analyze a number of problems
that arise when large blockchain networks are emulated and we provide technical
solutions for all of them. Finally, we describe two experiences of emulating
fairly large blockchain networks on a single machine, adopting both research
oriented and production oriented software, and involving up to more than 3000
containers.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14611" title="Abstract">arXiv:2402.14611</a> [<a href="/pdf/2402.14611" title="Download PDF">pdf</a>, <a href="/format/2402.14611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Dimensional Collapse in Self-supervised Contrastive Learning  for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassanpour%2C+J">Jamshid Hassanpour</a>, 
<a href="/search/cs?searchtype=author&query=Srivastav%2C+V">Vinkle Srivastav</a>, 
<a href="/search/cs?searchtype=author&query=Mutter%2C+D">Didier Mutter</a>, 
<a href="/search/cs?searchtype=author&query=Padoy%2C+N">Nicolas Padoy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning (SSL) approaches have achieved great success when
the amount of labeled data is limited. Within SSL, models learn robust feature
representations by solving pretext tasks. One such pretext task is contrastive
learning, which involves forming pairs of similar and dissimilar input samples,
guiding the model to distinguish between them. In this work, we investigate the
application of contrastive learning to the domain of medical image analysis.
Our findings reveal that MoCo v2, a state-of-the-art contrastive learning
method, encounters dimensional collapse when applied to medical images. This is
attributed to the high degree of inter-image similarity shared between the
medical images. To address this, we propose two key contributions: local
feature learning and feature decorrelation. Local feature learning improves the
ability of the model to focus on the local regions of the image, while feature
decorrelation removes the linear dependence among the features. Our
experimental findings demonstrate that our contributions significantly enhance
the model's performance in the downstream task of medical segmentation, both in
the linear evaluation and full fine-tuning settings. This work illustrates the
importance of effectively adapting SSL techniques to the characteristics of
medical imaging tasks.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14614" title="Abstract">arXiv:2402.14614</a> [<a href="/pdf/2402.14614" title="Download PDF">pdf</a>, <a href="/format/2402.14614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Counterexamples to \textit{Tokenization and the Noiseless Channel}
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cognetta%2C+M">Marco Cognetta</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Sangwhan Moon</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, to appear in LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In \textit{Tokenization and the Noiseless Channel}
\cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an
intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer
which leads to the highest R\'enyi efficiency of the unigram distribution
should be chosen. The R\'enyi efficiency is thus treated as a predictor of
downstream performance (e.g., predicting BLEU for a machine translation task),
without the expensive step of training multiple models with different
tokenizers. Although useful, the predictive power of this metric is not
perfect, and the authors note there are additional qualities of a good
tokenization scheme that R\'enyi efficiency alone cannot capture.
<br />We describe two variants of BPE tokenization which can arbitrarily increase
R\'enyi efficiency while decreasing the downstream model performance. These
counterexamples expose cases where R\'enyi efficiency fails as an intrinsic
tokenization metric and thus give insight for building more accurate
predictors.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14615" title="Abstract">arXiv:2402.14615</a> [<a href="/pdf/2402.14615" title="Download PDF">pdf</a>, <a href="/format/2402.14615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Entropy-Stable Discontinuous Galerkin Discretization of the Ideal  Multi-Ion Magnetohydrodynamics System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rueda-Ram%C3%ADrez%2C+A+M">Andr&#xe9;s M Rueda-Ram&#xed;rez</a>, 
<a href="/search/math?searchtype=author&query=Sikstel%2C+A">Aleksey Sikstel</a>, 
<a href="/search/math?searchtype=author&query=Gassner%2C+G+J">Gregor J Gassner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">In this paper, we present an entropy-stable (ES) discretization using a nodal
discontinuous Galerkin (DG) method for the ideal multi-ion
magneto-hydrodynamics (MHD) equations.
<br />We start by performing a continuous entropy analysis of the ideal multi-ion
MHD system, described by, e.g., [Toth (2010) Multi-Ion Magnetohydrodynamics]
\cite{Toth2010}, which describes the motion of multi-ion plasmas with
independent momentum and energy equations for each ion species. Following the
continuous entropy analysis, we propose an algebraic manipulation to the
multi-ion MHD system, such that entropy consistency can be transferred from the
continuous analysis to its discrete approximation. Moreover, we augment the
system of equations with a generalized Lagrange multiplier (GLM) technique to
have an additional cleaning mechanism of the magnetic field divergence error.
<br />We first derive robust entropy-conservative (EC) fluxes for the alternative
formulation of the multi-ion GLM-MHD system that satisfy a Tadmor-type
condition and are consistent with existing EC fluxes for single-fluid GLM-MHD
equations. Using these numerical two-point fluxes, we construct high-order EC
and ES DG discretizations of the ideal multi-ion MHD system using collocated
Legendre--Gauss--Lobatto summation-by-parts (SBP) operators. The resulting
nodal DG schemes satisfy the second-law of thermodynamics at the semi-discrete
level, while maintaining high-order convergence and local node-wise
conservation properties.
<br />We demonstrate the high-order convergence, and the EC and ES properties of
our scheme with numerical validation experiments. Moreover, we demonstrate the
importance of the GLM divergence technique and the ES discretization to improve
the robustness properties of a DG discretization of the multi-ion MHD system by
solving a challenging magnetized Kelvin-Helmholtz instability problem that
exhibits MHD turbulence.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14616" title="Abstract">arXiv:2402.14616</a> [<a href="/pdf/2402.14616" title="Download PDF">pdf</a>, <a href="/format/2402.14616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Word Splitting on the Semantic Content of Contextualized  Word Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soler%2C+A+G">Aina Gar&#xed; Soler</a>, 
<a href="/search/cs?searchtype=author&query=Labeau%2C+M">Matthieu Labeau</a>, 
<a href="/search/cs?searchtype=author&query=Clavel%2C+C">Chlo&#xe9; Clavel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">When deriving contextualized word representations from language models, a
decision needs to be made on how to obtain one for out-of-vocabulary (OOV)
words that are segmented into subwords. What is the best way to represent these
words with a single vector, and are these representations of worse quality than
those of in-vocabulary words? We carry out an intrinsic evaluation of
embeddings from different models on semantic similarity tasks involving OOV
words. Our analysis reveals, among other interesting findings, that the quality
of representations of words that are split is often, but not always, worse than
that of the embeddings of known words. Their similarity values, however, must
be interpreted with caution.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14619" title="Abstract">arXiv:2402.14619</a> [<a href="/pdf/2402.14619" title="Download PDF">pdf</a>, <a href="/format/2402.14619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seer: Proactive Revenue-Aware Scheduling for Live Streaming Services in  Crowdsourced Cloud-Edge Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongtian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaofei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">As live streaming services skyrocket, Crowdsourced Cloud-edge service
Platforms (CCPs) have surfaced as pivotal intermediaries catering to the
mounting demand. Despite the role of stream scheduling to CCPs' Quality of
Service (QoS) and throughput, conventional optimization strategies struggle to
enhancing CCPs' revenue, primarily due to the intricate relationship between
resource utilization and revenue. Additionally, the substantial scale of CCPs
magnifies the difficulties of time-intensive scheduling. To tackle these
challenges, we propose Seer, a proactive revenue-aware scheduling system for
live streaming services in CCPs. The design of Seer is motivated by meticulous
measurements of real-world CCPs environments, which allows us to achieve
accurate revenue modeling and overcome three key obstacles that hinder the
integration of prediction and optimal scheduling. Utilizing an innovative
Pre-schedule-Execute-Re-schedule paradigm and flexible scheduling modes, Seer
achieves efficient revenue-optimized scheduling in CCPs. Extensive evaluations
demonstrate Seer's superiority over competitors in terms of revenue,
utilization, and anomaly penalty mitigation, boosting CCPs revenue by 147% and
expediting scheduling $3.4 \times$ faster.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14621" title="Abstract">arXiv:2402.14621</a> [<a href="/pdf/2402.14621" title="Download PDF">pdf</a>, <a href="/format/2402.14621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> latrend: A Framework for Clustering Longitudinal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teuling%2C+N+D">Niek Den Teuling</a>, 
<a href="/search/cs?searchtype=author&query=Pauws%2C+S">Steffen Pauws</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Heuvel%2C+E">Edwin van den Heuvel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Clustering of longitudinal data is used to explore common trends among
subjects over time for a numeric measurement of interest. Various R packages
have been introduced throughout the years for identifying clusters of
longitudinal patterns, summarizing the variability in trajectories between
subject in terms of one or more trends. We introduce the R package "latrend" as
a framework for the unified application of methods for longitudinal clustering,
enabling comparisons between methods with minimal coding. The package also
serves as an interface to commonly used packages for clustering longitudinal
data, including "dtwclust", "flexmix", "kml", "lcmm", "mclust", "mixAK", and
"mixtools". This enables researchers to easily compare different approaches,
implementations, and method specifications. Furthermore, researchers can build
upon the standard tools provided by the framework to quickly implement new
cluster methods, enabling rapid prototyping. We demonstrate the functionality
and application of the latrend package on a synthetic dataset based on the
therapy adherence patterns of patients with sleep apnea.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14622" title="Abstract">arXiv:2402.14622</a> [<a href="/pdf/2402.14622" title="Download PDF">pdf</a>, <a href="/format/2402.14622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Keywords to Structured Summaries: Streamlining Scholarly Knowledge  Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsabadi%2C+M">Mahsa Shamsabadi</a>, 
<a href="/search/cs?searchtype=author&query=D%27Souza%2C+J">Jennifer D&#x27;Souza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Digital Libraries (cs.DL)

</div>
<p class="mathjax">This short paper highlights the growing importance of information retrieval
(IR) engines in the scientific community, addressing the inefficiency of
traditional keyword-based search engines due to the rising volume of
publications. The proposed solution involves structured records, underpinning
advanced information technology (IT) tools, including visualization dashboards,
to revolutionize how researchers access and filter articles, replacing the
traditional text-heavy approach. This vision is exemplified through a proof of
concept centered on the ``reproductive number estimate of infectious diseases''
research theme, using a fine-tuned large language model (LLM) to automate the
creation of structured records to populate a backend database that now goes
beyond keywords. The result is a next-generation IR method accessible at
https://orkg.org/usecases/r0-estimates.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14623" title="Abstract">arXiv:2402.14623</a> [<a href="/pdf/2402.14623" title="Download PDF">pdf</a>, <a href="/format/2402.14623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboScript: Code Generation for Free-Form Manipulation Tasks across Real  and Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiaojun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianming Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Silang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhecheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhixuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages of main paper, 4 pages of appendix; 10 figures in main paper, 3 figures in appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Rapid progress in high-level task planning and code generation for open-world
robot manipulation has been witnessed in Embodied AI. However, previous studies
put much effort into general common sense reasoning and task planning
capabilities of large-scale language or multi-modal models, relatively little
effort on ensuring the deployability of generated code on real robots, and
other fundamental components of autonomous robot systems including robot
perception, motion planning, and control. To bridge this ``ideal-to-real'' gap,
this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot
manipulation pipeline powered by code generation; and 2) a code generation
benchmark for robot manipulation tasks in free-form natural language. The
RobotScript platform addresses this gap by emphasizing the unified interface
with both simulation and real robots, based on abstraction from the Robot
Operating System (ROS), ensuring syntax compliance and simulation validation
with Gazebo. We demonstrate the adaptability of our code generation framework
across multiple robot embodiments, including the Franka and UR5 robot arms, and
multiple grippers. Additionally, our benchmark assesses reasoning abilities for
physical space and constraints, highlighting the differences between GPT-3.5,
GPT-4, and Gemini in handling complex physical interactions. Finally, we
present a thorough evaluation on the whole system, exploring how each module in
the pipeline: code generation, perception, motion planning, and even object
geometric properties, impact the overall performance of the system.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14625" title="Abstract">arXiv:2402.14625</a> [<a href="/pdf/2402.14625" title="Download PDF">pdf</a>, <a href="/format/2402.14625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Putting the Count Back Into Accountability: An Audit of Social Media  Transparency Disclosures, Focusing on Sexual Exploitation of Minors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grimm%2C+R">Robert Grimm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This paper explores a lightweight, quantitative audit methodology for
transparency disclosures called scrappy audits. It amounts to little more than
treating redundant and repeated disclosures as opportunities for validating
quantities. The paper applies two concrete audits to social media disclosures
about content moderation. The first compares legally mandated reports about the
sexual exploitation of minors as disclosed by social media and the national
clearinghouse receiving them. The second compares historical quantities
included in platforms' CSV files across two subsequent disclosures of the data.
Despite their simplicity, these scrappy audits are nonetheless effective. Out
of 16 surveyed social media platforms, 11 make transparency disclosures about
content moderation and 8 meet the prerequisites of one audit. Yet only
4~platforms pass their audits. The paper continues probing the limits of
transparency data by presenting a data-driven overview of the online sexual
exploitation of minors. Accordingly, the analysis is particularly careful to
identify threats to validity as well as potentially helpful, but unavailable
statistics. Likewise, it identifies major shortcomings of widely used
technologies for the automated detection of images and videos depicting sexual
abuse of minors. Overall, the data shows an alarming growth in such material
over the last decade. However, there also are strong indicators that current
statistics, which treat all such material the same, are large and unhelpful
overcounts. Notably, many technical violations of the law, e.g., teenagers
sexting, are not necessarily grounded in actual harm to minors but still
reported as such.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14627" title="Abstract">arXiv:2402.14627</a> [<a href="/pdf/2402.14627" title="Download PDF">pdf</a>, <a href="/format/2402.14627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermal-Aware Floorplanner for 3D IC, including TSVs, Liquid  Microchannels and Thermal Domains Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cuesta%2C+D">David Cuesta</a>, 
<a href="/search/cs?searchtype=author&query=Risco-Mart%C3%ADn%2C+J+L">Jos&#xe9; L. Risco-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Ayala%2C+J+L">Jos&#xe9; L. Ayala</a>, 
<a href="/search/cs?searchtype=author&query=Hidalgo%2C+J+I">J. Ignacio Hidalgo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Soft Computing, 34, 2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">3D stacked technology has emerged as an effective mechanism to overcome
physical limits and communication delays found in 2D integration. However, 3D
technology also presents several drawbacks that prevent its smooth application.
Two of the major concerns are heat reduction and power density distribution. In
our work, we propose a novel 3D thermal-aware floorplanner that includes: (1)
an effective thermal-aware process with 3 different evolutionary algorithms
that aim to solve the soft computing problem of optimizing the placement of
functional units and through silicon vias, as well as the smooth inclusion of
active cooling systems and new design strategies,(2) an approximated thermal
model inside the optimization loop, (3) an optimizer for active cooling (liquid
channels), and (4) a novel technique based on air channel placement designed to
isolate thermal domains have been also proposed. The experimental work is
conducted for a realistic many-core single-chip architecture based on the
Niagara design. Results show promising improvements of the thermal and
reliability metrics, and also show optimal scaling capabilities to target
future-trend many-core systems.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14632" title="Abstract">arXiv:2402.14632</a> [<a href="/pdf/2402.14632" title="Download PDF">pdf</a>, <a href="/format/2402.14632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring NAT64 Usage in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boswell%2C+E">Elizabeth Boswell</a>, 
<a href="/search/cs?searchtype=author&query=McQuistin%2C+S">Stephen McQuistin</a>, 
<a href="/search/cs?searchtype=author&query=Perkins%2C+C">Colin Perkins</a>, 
<a href="/search/cs?searchtype=author&query=Strowes%2C+S">Stephen Strowes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages (incl. appendix), 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">NAT64 is an IPv6 transition mechanism that enables IPv6-only hosts to access
the IPv4 Internet. Understanding the deployment of NAT64, and its performance
impact, is crucial to the success of the IPv6 transition, by encouraging
IPv6-only deployments. We develop a set of tests for detecting NAT64 and apply
them to the RIPE Atlas testbed, finding 224 probes, in 43 networks, that can
use NAT64 to access the IPv4 Internet. Using 34 dual stack probes, that have
both NAT64 and native IPv4 access, to compare performance, we find that NAT64
paths are, on average, 23.13% longer, with 17.47% higher round-trip times.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14633" title="Abstract">arXiv:2402.14633</a> [<a href="/pdf/2402.14633" title="Download PDF">pdf</a>, <a href="/format/2402.14633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Efficient Implementation for Online $k$-server Problem on Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khadiev%2C+K">Kamil Khadiev</a>, 
<a href="/search/cs?searchtype=author&query=Yagafarov%2C+M">Maxim Yagafarov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TAMC 2024. arXiv admin note: text overlap with <a href="/abs/2008.00270">arXiv:2008.00270</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider online algorithms for the $k$-server problem on trees of size
$n$. Chrobak and Larmore proposed a $k$-competitive algorithm for this problem
that has the optimal competitive ratio. However, the existing implementations
have $O\left(k^2 + k\cdot \log n\right)$ or $O\left(k(\log n)^2\right)$ time
complexity for processing a query, where $n$ is the number of nodes. We propose
a new time-efficient implementation of this algorithm that has $O(n)$ time
complexity for preprocessing and $O\left(k\log k\right)$ time for processing a
query. The new algorithm is faster than both existing algorithms and the time
complexity for query processing does not depend on the tree size.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14634" title="Abstract">arXiv:2402.14634</a> [<a href="/pdf/2402.14634" title="Download PDF">pdf</a>, <a href="/format/2402.14634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Sicheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+S">Saif Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qikang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Guimbreti%C3%A8re%2C+F">Fran&#xe7;ois Guimbreti&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 7 tables, The 30th Annual International Conference on Mobile Computing and Networking (ACM MobiCom 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In this paper, we present GazeTrak, the first acoustic-based eye tracking
system on glasses. Our system only needs one speaker and four microphones
attached to each side of the glasses. These acoustic sensors capture the
formations of the eyeballs and the surrounding areas by emitting encoded
inaudible sound towards eyeballs and receiving the reflected signals. These
reflected signals are further processed to calculate the echo profiles, which
are fed to a customized deep learning pipeline to continuously infer the gaze
position. In a user study with 20 participants, GazeTrak achieves an accuracy
of 3.6{\deg} within the same remounting session and 4.9{\deg} across different
sessions with a refreshing rate of 83.3 Hz and a power signature of 287.9 mW.
Furthermore, we report the performance of our gaze tracking system fully
implemented on an MCU with a low-power CNN accelerator (MAX78002). In this
configuration, the system runs at up to 83.3 Hz and has a total power signature
of 95.4 mW with a 30 Hz FPS.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14642" title="Abstract">arXiv:2402.14642</a> [<a href="/pdf/2402.14642" title="Download PDF">pdf</a>, <a href="/format/2402.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Radiance Fields for Edge Video Compression and Metaverse  Integration in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%A0lapak%2C+E">Eugen &#x160;lapak</a>, 
<a href="/search/cs?searchtype=author&query=Dopiriak%2C+M">Mat&#xfa;&#x161; Dopiriak</a>, 
<a href="/search/cs?searchtype=author&query=Faruque%2C+M+A+A">Mohammad Abdullah Al Faruque</a>, 
<a href="/search/cs?searchtype=author&query=Gazda%2C+J">Juraj Gazda</a>, 
<a href="/search/cs?searchtype=author&query=Levorato%2C+M">Marco Levorato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The metaverse is a virtual space that combines physical and digital elements,
creating immersive and connected digital worlds. For autonomous mobility, it
enables new possibilities with edge computing and digital twins (DTs) that
offer virtual prototyping, prediction, and more. DTs can be created with 3D
scene reconstruction methods that capture the real world's geometry,
appearance, and dynamics. However, sending data for real-time DT updates in the
metaverse, such as camera images and videos from connected autonomous vehicles
(CAVs) to edge servers, can increase network congestion, costs, and latency,
affecting metaverse services. Herein, a new method is proposed based on
distributed radiance fields (RFs), multi-access edge computing (MEC) network
for video compression and metaverse DT updates. RF-based encoder and decoder
are used to create and restore representations of camera images. The method is
evaluated on a dataset of camera images from the CARLA simulator. Data savings
of up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFs
instead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR)
and structural similarity index measure (SSIM) qualitative metrics for the
reconstructed images. Possible uses and challenges for the metaverse and
autonomous mobility are also discussed.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14645" title="Abstract">arXiv:2402.14645</a> [<a href="/pdf/2402.14645" title="Download PDF">pdf</a>, <a href="/ps/2402.14645" title="Download PostScript">ps</a>, <a href="/format/2402.14645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Linear Regression and Lattice Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupte%2C+A">Aparna Gupte</a>, 
<a href="/search/cs?searchtype=author&query=Vafa%2C+N">Neekon Vafa</a>, 
<a href="/search/cs?searchtype=author&query=Vaikuntanathan%2C+V">Vinod Vaikuntanathan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Sparse linear regression (SLR) is a well-studied problem in statistics where
one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector
$y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is,
$\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find
a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean
squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$.
While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig
selector solve SLR when the design matrix is well-conditioned, no general
algorithm is known, nor is there any formal evidence of hardness in an
average-case setting with respect to all efficient algorithms.
<br />We give evidence of average-case hardness of SLR w.r.t. all efficient
algorithms assuming the worst-case hardness of lattice problems. Specifically,
we give an instance-by-instance reduction from a variant of the bounded
distance decoding (BDD) problem on lattices to SLR, where the condition number
of the lattice basis that defines the BDD instance is directly related to the
restricted eigenvalue condition of the design matrix, which characterizes some
of the classical statistical-computational gaps for sparse linear regression.
Also, by appealing to worst-case to average-case reductions from the world of
lattices, this shows hardness for a distribution of SLR instances; while the
design matrices are ill-conditioned, the resulting SLR instances are in the
identifiable regime.
<br />Furthermore, for well-conditioned (essentially) isotropic Gaussian design
matrices, where Lasso is known to behave well in the identifiable regime, we
show hardness of outputting any good solution in the unidentifiable regime
where there are many solutions, assuming the worst-case hardness of standard
and well-studied lattice problems.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14646" title="Abstract">arXiv:2402.14646</a> [<a href="/pdf/2402.14646" title="Download PDF">pdf</a>, <a href="/format/2402.14646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLoRA: Continuous low-rank adaptation for reduced implicit neural  modeling of parameterized partial differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berman%2C+J">Jules Berman</a>, 
<a href="/search/cs?searchtype=author&query=Peherstorfer%2C+B">Benjamin Peherstorfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">This work introduces reduced models based on Continuous Low Rank Adaptation
(CoLoRA) that pre-train neural networks for a given partial differential
equation and then continuously adapt low-rank weights in time to rapidly
predict the evolution of solution fields at new physics parameters and new
initial conditions. The adaptation can be either purely data-driven or via an
equation-driven variational approach that provides Galerkin-optimal
approximations. Because CoLoRA approximates solution fields locally in time,
the rank of the weights can be kept small, which means that only few training
trajectories are required offline so that CoLoRA is well suited for data-scarce
regimes. Predictions with CoLoRA are orders of magnitude faster than with
classical methods and their accuracy and parameter efficiency is higher
compared to other neural network approaches.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14648" title="Abstract">arXiv:2402.14648</a> [<a href="/pdf/2402.14648" title="Download PDF">pdf</a>, <a href="/format/2402.14648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Invariance Regularization in Adversarial Training to Improve  Robustness-Accuracy Trade-off
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waseda%2C+F">Futa Waseda</a>, 
<a href="/search/cs?searchtype=author&query=Echizen%2C+I">Isao Echizen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although adversarial training has been the state-of-the-art approach to
defend against adversarial examples (AEs), they suffer from a
robustness-accuracy trade-off. In this work, we revisit representation-based
invariance regularization to learn discriminative yet adversarially invariant
representations, aiming to mitigate this trade-off. We empirically identify two
key issues hindering invariance regularization: (1) a "gradient conflict"
between invariance loss and classification objectives, indicating the existence
of "collapsing solutions," and (2) the mixture distribution problem arising
from diverged distributions of clean and adversarial inputs. To address these
issues, we propose Asymmetrically Representation-regularized Adversarial
Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor
in the invariance loss to avoid "collapsing solutions," inspired by a recent
non-contrastive self-supervised learning approach, and a split-BatchNorm (BN)
structure to resolve the mixture distribution problem. Our method significantly
improves the robustness-accuracy trade-off by learning adversarially invariant
representations without sacrificing discriminative power. Furthermore, we
discuss the relevance of our findings to knowledge-distillation-based defense
methods, contributing to a deeper understanding of their relative successes.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14650" title="Abstract">arXiv:2402.14650</a> [<a href="/pdf/2402.14650" title="Download PDF">pdf</a>, <a href="/format/2402.14650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaussianPro: 3D Gaussian Splatting with Progressive Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kai Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xiaoxiao Long</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaizhi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuejin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See the project page for code, data: <a href="https://kcheng1021.github.io/gaussianpro.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advent of 3D Gaussian Splatting (3DGS) has recently brought about a
revolution in the field of neural rendering, facilitating high-quality
renderings at real-time speed. However, 3DGS heavily depends on the initialized
point cloud produced by Structure-from-Motion (SfM) techniques. When tackling
with large-scale scenes that unavoidably contain texture-less surfaces, the SfM
techniques always fail to produce enough points in these surfaces and cannot
provide good initialization for 3DGS. As a result, 3DGS suffers from difficult
optimization and low-quality renderings. In this paper, inspired by classical
multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that
applies a progressive propagation strategy to guide the densification of the 3D
Gaussians. Compared to the simple split and clone strategies used in 3DGS, our
method leverages the priors of the existing reconstructed geometries of the
scene and patch matching techniques to produce new Gaussians with accurate
positions and orientations. Experiments on both large-scale and small-scale
scenes validate the effectiveness of our method, where our method significantly
surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in
terms of PSNR.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14652" title="Abstract">arXiv:2402.14652</a> [<a href="/pdf/2402.14652" title="Download PDF">pdf</a>, <a href="/format/2402.14652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cleaner Pretraining Corpus Curation with Neural Web Scraping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhipeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The web contains large-scale, diverse, and abundant information to satisfy
the information-seeking needs of humans. Through meticulous data collection,
preprocessing, and curation, webpages can be used as a fundamental data
resource for language model pretraining. However, when confronted with the
progressively revolutionized and intricate nature of webpages,
rule-based/feature-based web scrapers are becoming increasingly inadequate.
This paper presents a simple, fast, and effective Neural web Scraper
(NeuScraper) to help extract primary and clean text contents from webpages.
Experimental results show that NeuScraper surpasses the baseline scrapers by
achieving more than a 20% improvement, demonstrating its potential in
extracting higher-quality data to facilitate the language model pretraining.
All of the code is available at https://github.com/OpenMatch/NeuScraper.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14654" title="Abstract">arXiv:2402.14654</a> [<a href="/pdf/2402.14654" title="Download PDF">pdf</a>, <a href="/format/2402.14654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baradel%2C+F">Fabien Baradel</a>, 
<a href="/search/cs?searchtype=author&query=Armando%2C+M">Matthieu Armando</a>, 
<a href="/search/cs?searchtype=author&query=Galaaoui%2C+S">Salma Galaaoui</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A9gier%2C+R">Romain Br&#xe9;gier</a>, 
<a href="/search/cs?searchtype=author&query=Weinzaepfel%2C+P">Philippe Weinzaepfel</a>, 
<a href="/search/cs?searchtype=author&query=Rogez%2C+G">Gr&#xe9;gory Rogez</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+T">Thomas Lucas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/naver/multi-hmr">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Multi-HMR, a strong single-shot model for multi-person 3D human
mesh recovery from a single RGB image. Predictions encompass the whole body,
i.e, including hands and facial expressions, using the SMPL-X parametric model
and spatial location in the camera coordinate system. Our model detects people
by predicting coarse 2D heatmaps of person centers, using features produced by
a standard Vision Transformer (ViT) backbone. It then predicts their whole-body
pose, shape and spatial location using a new cross-attention module called the
Human Prediction Head (HPH), with one query per detected center token,
attending to the entire set of features. As direct prediction of SMPL-X
parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames
of Full-Body Subjects dataset, containing humans close to the camera with
diverse hand poses. We show that incorporating this dataset into training
further enhances predictions, particularly for hands, enabling us to achieve
state-of-the-art performance. Multi-HMR also optionally accounts for camera
intrinsics, if available, by encoding camera ray directions for each image
token. This simple design achieves strong performance on whole-body and
body-only benchmarks simultaneously. We train models with various backbone
sizes and input resolutions. In particular, using a ViT-S backbone and
$448\times448$ input images already yields a fast and competitive model with
respect to state-of-the-art methods, while considering larger models and higher
resolutions further improve performance.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14657" title="Abstract">arXiv:2402.14657</a> [<a href="/pdf/2402.14657" title="Download PDF">pdf</a>, <a href="/format/2402.14657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilization of a matrix via a low rank-adaptive ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guglielmi%2C+N">Nicola Guglielmi</a>, 
<a href="/search/math?searchtype=author&query=Sicilia%2C+S">Stefano Sicilia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Let $A$ be a square matrix with a given structure (e.g. real matrix, sparsity
pattern, Toeplitz structure, etc.) and assume that it is unstable, i.e. at
least one of its eigenvalues lies in the complex right half-plane. The problem
of stabilizing $A$ consists in the computation of a matrix $B$, whose
eigenvalues have negative real part and such that the perturbation $\Delta=B-A$
has minimal norm. The structured stabilization further requires that the
perturbation preserves the structural pattern of $A$. We solve this non-convex
problem by a two-level procedure which involves the computation of the
stationary points of a matrix ODE. We exploit the low rank underlying features
of the problem by using an adaptive-rank integrator that follows slavishly the
rank of the solution. We show the benefits derived from the low rank setting in
several numerical examples, which also allow to deal with high dimensional
problems.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14658" title="Abstract">arXiv:2402.14658</a> [<a href="/pdf/2402.14658" title="Download PDF">pdf</a>, <a href="/format/2402.14658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenCodeInterpreter: Integrating Code Generation with Execution and  Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xueling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The introduction of large language models has significantly advanced code
generation. However, open-source models often lack the execution capabilities
and iterative refinement of advanced systems like the GPT-4 Code Interpreter.
To address this, we introduce OpenCodeInterpreter, a family of open-source code
systems designed for generating, executing, and iteratively refining code.
Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions,
OpenCodeInterpreter integrates execution and human feedback for dynamic code
refinement. Our comprehensive evaluation of OpenCodeInterpreter across key
benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus
reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves
an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and
MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)
with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap
between open-source code generation models and proprietary systems like GPT-4
Code Interpreter.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14660" title="Abstract">arXiv:2402.14660</a> [<a href="/pdf/2402.14660" title="Download PDF">pdf</a>, <a href="/format/2402.14660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+X">Xingyuan Bu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zhiqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haibin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tiezheng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wenbo Su</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The benchmark dataset will be released soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces ConceptMath, a bilingual (English and Chinese),
fine-grained benchmark that evaluates concept-wise mathematical reasoning of
Large Language Models (LLMs). Unlike traditional benchmarks that evaluate
general mathematical reasoning with an average accuracy, ConceptMath
systematically organizes math problems under a hierarchy of math concepts, so
that mathematical reasoning can be evaluated at different granularity with
concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range
of LLMs, and we observe existing LLMs, though achieving high average accuracies
on traditional benchmarks, exhibit significant performance variations across
different math concepts and may even fail catastrophically on the most basic
ones. Besides, we also introduce an efficient fine-tuning strategy to enhance
the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the
developers to understand the fine-grained mathematical abilities of their
models and facilitate the growth of foundation models.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14664" title="Abstract">arXiv:2402.14664</a> [<a href="/pdf/2402.14664" title="Download PDF">pdf</a>, <a href="/format/2402.14664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aouali%2C+I">Imad Aouali</a>, 
<a href="/search/cs?searchtype=author&query=Brunel%2C+V">Victor-Emmanuel Brunel</a>, 
<a href="/search/cs?searchtype=author&query=Rohde%2C+D">David Rohde</a>, 
<a href="/search/cs?searchtype=author&query=Korba%2C+A">Anna Korba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In interactive systems, actions are often correlated, presenting an
opportunity for more sample-efficient off-policy evaluation (OPE) and learning
(OPL) in large action spaces. We introduce a unified Bayesian framework to
capture these correlations through structured and informative priors. In this
framework, we propose sDM, a generic Bayesian approach designed for OPE and
OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM
leverages action correlations without compromising computational efficiency.
Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics
that assess the average performance of algorithms across multiple problem
instances, deviating from the conventional worst-case assessments. We analyze
sDM in OPE and OPL, highlighting the benefits of leveraging action
correlations. Empirical evidence showcases the strong performance of sDM.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14665" title="Abstract">arXiv:2402.14665</a> [<a href="/pdf/2402.14665" title="Download PDF">pdf</a>, <a href="/format/2402.14665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadruplet Loss For Improving the Robustness to Face Morphing Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medvedev%2C+I">Iurii Medvedev</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+N">Nuno Gon&#xe7;alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in deep learning have revolutionized technology and
security measures, necessitating robust identification methods. Biometric
approaches, leveraging personalized characteristics, offer a promising
solution. However, Face Recognition Systems are vulnerable to sophisticated
attacks, notably face morphing techniques, enabling the creation of fraudulent
documents. In this study, we introduce a novel quadruplet loss function for
increasing the robustness of face recognition systems against morphing attacks.
Our approach involves specific sampling of face image quadruplets, combined
with face morphs, for network training. Experimental results demonstrate the
efficiency of our strategy in improving the robustness of face recognition
networks against morphing attacks.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14666" title="Abstract">arXiv:2402.14666</a> [<a href="/pdf/2402.14666" title="Download PDF">pdf</a>, <a href="/format/2402.14666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability of P2P Networks Under Greedy Peering (Full Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiffer%2C+L">Lucianna Kiffer</a>, 
<a href="/search/cs?searchtype=author&query=Rajaraman%2C+R">Rajmohan Rajaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Major cryptocurrency networks have relied on random peering choice rules for
making connections in their peer-to-peer networks. Generally, these choices
have good properties, particularly for open, permissionless networks. Random
peering choices however do not take into account that some actors may choose to
optimize who they connect to such that they are quicker to hear about
information being propagated in the network. In this paper, we explore the
dynamics of such greedy strategies. We study a model in which nodes select
peers with the objective of minimizing their average distance to a designated
subset of nodes in the network, and consider the impact of several factors
including the peer selection process, degree constraints, and the size of the
designated subset. The latter is particularly interesting in the context of
blockchain networks as generally only a subset of nodes are the propagation
source for content.
<br />We first analyze an idealized version of the game where each node has full
knowledge of the current network and aims to select the $d$ best connections,
and prove the existence of equilibria under various model assumptions. Since in
reality nodes only have local knowledge based on their peers' behavior, we also
study a greedy protocol which runs in rounds, with each node replacing its
worst-performing edge with a new random edge. We exactly characterize stability
properties of networks that evolve with this peering rule and derive regimes
where stability is possible and even inevitable. We also run extensive
simulations with this peering rule examining both how the network evolves and
how different network parameters affect the stability properties of the
network. Our findings generally show that the only stable networks that arise
from greedy peering choices are low-diameter and result in disparate
performance for nodes in the network.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14667" title="Abstract">arXiv:2402.14667</a> [<a href="/pdf/2402.14667" title="Download PDF">pdf</a>, <a href="/format/2402.14667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConPA: A Contention-free Mechanism with Power Adaptation for Beyond  Listen-Before-Talk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilhelmi%2C+F">Francesc Wilhelmi</a>, 
<a href="/search/cs?searchtype=author&query=Baracca%2C+P">Paolo Baracca</a>, 
<a href="/search/cs?searchtype=author&query=Fontanesi%2C+G">Gianluca Fontanesi</a>, 
<a href="/search/cs?searchtype=author&query=Galati-Giordano%2C+L">Lorenzo Galati-Giordano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In view of the need to find novel means to utilize the unlicensed spectrum to
meet the rising latency and reliability requirements of new applications, we
propose a novel mechanism that allows devices to transmit anytime that a packet
has to be delivered. The proposed mechanism, Contention-free with Power
Adaptation (ConPA), aims to bypass the contention periods of current
Listen-Before-Talk (LBT) approaches, which are the main source of unreliability
in unlicensed technologies like Wi-Fi. To assess the feasibility of ConPA, we
provide an analytical method based on Markov chains, which allows deriving
relevant performance metrics, including throughput, airtime, and quality of
transmissions. Using such a model, we study the performance of ConPA in various
scenarios, and compare it to baseline channel access approaches like the
Distributed Coordination Function (DCF) and the IEEE 802.11ax Overlapping Basic
Service Set (OBSS) Packet Detect (PD)-based Spatial Reuse (SR). Our results
prove the effectiveness of ConPA in reusing the space to offer substantial
throughput gains with respect to the baselines (up to 76% improvement).
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14672" title="Abstract">arXiv:2402.14672</a> [<a href="/pdf/2402.14672" title="Download PDF">pdf</a>, <a href="/format/2402.14672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Middleware for LLMs: Tools Are Instrumental for Language Agents in  Complex Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yiheng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+J">Jayanth Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Latapie%2C+H">Hugo Latapie</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The applications of large language models (LLMs) have expanded well beyond
the confines of text processing, signaling a new era where LLMs are envisioned
as generalist language agents capable of operating within complex real-world
environments. These environments are often highly expansive, making it
impossible for the LLM to process them within its short-term memory. Motivated
by recent research on extending the capabilities of LLMs with tools, this paper
investigates the intriguing potential of tools to augment LLMs in handling such
complexity. To this end, we design customized tools to aid in the proactive
exploration within these massive environments. Such tools can serve as a
middleware layer shielding the LLM from environmental complexity. In two
representative complex environments -- knowledge bases (KBs) and databases --
we demonstrate the significant potential of augmenting language agents with
tools in complex environments. Notably, equipped with these tools, GPT-4
achieves 2.8X the performance of the best baseline in tasks requiring access to
database content and 2.2X in KB tasks. Our findings illuminate the path for
advancing language agents in complex real-world applications.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14674" title="Abstract">arXiv:2402.14674</a> [<a href="/pdf/2402.14674" title="Download PDF">pdf</a>, <a href="/ps/2402.14674" title="Download PostScript">ps</a>, <a href="/format/2402.14674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doing AI: Algorithmic decision support as a human activity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meyer%2C+J">Joachim Meyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Algorithmic decision support (ADS), using Machine-Learning-based AI, is
becoming a major part of many processes. Organizations introduce ADS to improve
decision-making and make optimal use of data, thereby possibly avoiding
deviations from the normative "homo economicus" and the biases that
characterize human decision-making. A closer look at the development process of
ADS systems reveals that ADS itself results from a series of largely
unspecified human decisions. They begin with deliberations for which decisions
to use ADS, continue with choices while developing the ADS, and end with using
the ADS output for decisions. Finally, conclusions are implemented in
organizational settings, often without analyzing the implications of the
decision support. The paper explores some issues in developing and using ADS,
pointing to behavioral aspects that should be considered when implementing ADS
in organizational settings. It points out directions for further research,
which is essential for gaining an informed understanding of the processes and
their vulnerabilities.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14679" title="Abstract">arXiv:2402.14679</a> [<a href="/pdf/2402.14679" title="Download PDF">pdf</a>, <a href="/format/2402.14679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Cognition and Action Consistent or Not: Investigating Large Language  Model&#x27;s Personality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ai%2C+Y">Yiming Ai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongkun Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lingjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In this study, we investigate the reliability of Large Language Models (LLMs)
in professing human-like personality traits through responses to personality
questionnaires. Our goal is to evaluate the consistency between LLMs' professed
personality inclinations and their actual "behavior", examining the extent to
which these models can emulate human-like personality patterns. Through a
comprehensive analysis of LLM outputs against established human benchmarks, we
seek to understand the cognition-action divergence in LLMs and propose
hypotheses for the observed results based on psychological theories and
metrics.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14683" title="Abstract">arXiv:2402.14683</a> [<a href="/pdf/2402.14683" title="Download PDF">pdf</a>, <a href="/format/2402.14683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Hallucinations of Multi-modal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines
incorrect details about an image in visual question answering. Existing studies
find VH instances only in existing image datasets, which results in biased
understanding of MLLMs' performance under VH due to limited diversity of such
VH instances. In this work, we propose a tool called VHTest to generate a
diverse set of VH instances. Specifically, VHTest finds some initial VH
instances in existing image datasets (e.g., COCO), generates a text description
for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to
generate VH images based on the text descriptions. We collect a benchmark
dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that
existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a
large fraction of the instances in our benchmark. Moreover, we find that
fine-tuning an MLLM using our benchmark dataset reduces its likelihood to
hallucinate without sacrificing its performance on other benchmarks. Our
benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14688" title="Abstract">arXiv:2402.14688</a> [<a href="/pdf/2402.14688" title="Download PDF">pdf</a>, <a href="/format/2402.14688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-Probe: A Lightweight Approach to Reward Maximization for Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenneth Li</a>, 
<a href="/search/cs?searchtype=author&query=Jelassi%2C+S">Samy Jelassi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hugh Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Wattenberg%2C+M">Martin Wattenberg</a>, 
<a href="/search/cs?searchtype=author&query=Brandfonbrener%2C+D">David Brandfonbrener</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present an approach called Q-probing to adapt a pre-trained language model
to maximize a task-specific reward function. At a high level, Q-probing sits
between heavier approaches such as finetuning and lighter approaches such as
few shot prompting, but can also be combined with either. The idea is to learn
a simple linear function on a model's embedding space that can be used to
reweight candidate completions. We theoretically show that this sampling
procedure is equivalent to a KL-constrained maximization of the Q-probe as the
number of samples increases. To train the Q-probes we consider either reward
modeling or a class of novel direct policy learning objectives based on
importance weighted policy gradients. With this technique, we see gains in
domains with ground-truth rewards (code generation) as well as implicit rewards
defined by preference data, even outperforming finetuning in data-limited
regimes. Moreover, a Q-probe can be trained on top of an API since it only
assumes access to sampling and embeddings. Code:
https://github.com/likenneth/q_probe .
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14690" title="Abstract">arXiv:2402.14690</a> [<a href="/pdf/2402.14690" title="Download PDF">pdf</a>, <a href="/format/2402.14690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFO: a Unified and Flexible Framework for Evaluating Factuality of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhaoheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) may generate text that lacks consistency with
human knowledge, leading to factual inaccuracies or \textit{hallucination}.
Existing research for evaluating the factuality of LLMs involves extracting
fact claims using an LLM and verifying them against a predefined fact source.
However, these evaluation metrics are task-specific, and not scalable, and the
substitutability of fact sources in different tasks is under-explored. To
address these challenges, we categorize four available fact sources:
human-written evidence, reference documents, search engine results, and LLM
knowledge, along with five text generation tasks containing six representative
datasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexible
evaluation framework to verify facts against plug-and-play fact sources. We
implement five evaluation scenarios based on this framework. Experimental
results show that for most QA tasks, human-written evidence and reference
documents are crucial, and they can substitute for each other in
retrieval-augmented QA tasks. In news fact generation tasks, search engine
results and LLM knowledge are essential. Our dataset and code are available at
\url{https://github.com/WaldenRUC/UFO}.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14691" title="Abstract">arXiv:2402.14691</a> [<a href="/pdf/2402.14691" title="Download PDF">pdf</a>, <a href="/format/2402.14691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Estimates for First- and Second-Order Lagrange-Galerkin Moving  Mesh Schemes for the One-Dimensional Convection-Diffusion Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Putri%2C+K+S">Kharisma Surya Putri</a>, 
<a href="/search/math?searchtype=author&query=Mizuochi%2C+T">Tatsuki Mizuochi</a>, 
<a href="/search/math?searchtype=author&query=Kolbe%2C+N">Niklas Kolbe</a>, 
<a href="/search/math?searchtype=author&query=Notsu%2C+H">Hirofumi Notsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A mass-conservative Lagrange--Galerkin scheme of second order in time for
convection-diffusion problems is presented, and convergence with optimal error
estimates is proved in the framework of $L^2$-theory. The introduced scheme
maintains the advantages of the Lagrange--Galerkin method, i.e., CFL-free
robustness for convection-dominated problems and a symmetric and positive
coefficient matrix resulting from the discretization. In addition, the scheme
conserves the mass on the discrete level. Unconditional stability and error
estimates of second order in time are proved by employing two new key lemmas on
the truncation error of the material derivative in conservative form and on a
discrete Gronwall inequality for multistep methods. The mass-conservation
property is achieved by the Jacobian multiplication technique introduced by Rui
and Tabata in 2010, and the accuracy of second order in time is obtained based
on the idea of the multistep Galerkin method along characteristics originally
introduced by Ewing and Russel in 1981. For the first time step, the
mass-conservative scheme of first order in time by Rui and Tabata in 2010 is
employed, which is efficient and does not cause any loss of convergence order
in the $\ell^\infty(L^2)$- and $\ell^2(H^1_0)$-norms. For the time
increment~$\Delta t$, the mesh size~$h$ and a conforming finite element space
of polynomial degree~$k \in \mathbb{N}$, the convergence order is of $O(\Delta
t^2 + h^k)$ in the $\ell^\infty(L^2)\cap \ell^2(H^1_0)$-norm and of $O(\Delta
t^2 + h^{k+1})$ in the $\ell^\infty(L^2)$-norm if the duality argument can be
employed. Error estimates of $O(\Delta t^{3/2}+h^k)$ in discrete versions of
the $L^\infty(H^1_0)$- and $H^1(L^2)$-norm are additionally proved. Numerical
results confirm the theoretical convergence orders in one, two and three
dimensions.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14693" title="Abstract">arXiv:2402.14693</a> [<a href="/pdf/2402.14693" title="Download PDF">pdf</a>, <a href="/ps/2402.14693" title="Download PostScript">ps</a>, <a href="/format/2402.14693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint AP-UE Association and Power Factor Optimization for Distributed  Massive MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+S+A">Mohd Saif Ali Khan</a>, 
<a href="/search/cs?searchtype=author&query=Agnihotri%2C+S">Samar Agnihotri</a>, 
<a href="/search/cs?searchtype=author&query=M%2C+K+R">Karthik R.M</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The uplink sum-throughput of distributed massive
multiple-input-multiple-output (mMIMO) networks depends majorly on Access point
(AP)-User Equipment (UE) association and power control. The AP-UE association
and power control both are important problems in their own right in distributed
mMIMO networks to improve scalability and reduce front-haul load of the
network, and to enhance the system performance by mitigating the interference
and boosting the desired signals, respectively. Unlike previous studies, which
focused primarily on addressing the AP-UE association or power control problems
separately, this work addresses the uplink sum-throughput maximization problem
in distributed mMIMO networks by solving the joint AP-UE association and power
control problem, while maintaining Quality-of-Service (QoS) requirements for
each UE. To improve scalability, we present an l1-penalty function that
delicately balances the trade-off between spectral efficiency (SE) and
front-haul signaling load. Our proposed methodology leverages fractional
programming, Lagrangian dual formation, and penalty functions to provide an
elegant and effective iterative solution with guaranteed convergence while
meeting strict QoS criteria. Extensive numerical simulations validate the
efficacy of the proposed technique for maximizing sum-throughput while
considering the joint AP-UE association and power control problem,
demonstrating its superiority over approaches that address these problems
individually. Furthermore, the results show that the introduced penalty
function can help us effectively control the maximum front-haul load for uplink
distributed mMIMO systems.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14695" title="Abstract">arXiv:2402.14695</a> [<a href="/pdf/2402.14695" title="Download PDF">pdf</a>, <a href="/format/2402.14695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QIS : Interactive Segmentation via Quasi-Conformal Mappings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Daoping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+L+M">Lok Ming Lui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image segmentation plays a crucial role in extracting important objects of
interest from images, enabling various applications. While existing methods
have shown success in segmenting clean images, they often struggle to produce
accurate segmentation results when dealing with degraded images, such as those
containing noise or occlusions. To address this challenge, interactive
segmentation has emerged as a promising approach, allowing users to provide
meaningful input to guide the segmentation process. However, an important
problem in interactive segmentation lies in determining how to incorporate
minimal yet meaningful user guidance into the segmentation model. In this
paper, we propose the quasi-conformal interactive segmentation (QIS) model,
which incorporates user input in the form of positive and negative clicks.
Users mark a few pixels belonging to the object region as positive clicks,
indicating that the segmentation model should include a region around these
clicks. Conversely, negative clicks are provided on pixels belonging to the
background, instructing the model to exclude the region near these clicks from
the segmentation mask. Additionally, the segmentation mask is obtained by
deforming a template mask with the same topology as the object of interest
using an orientation-preserving quasiconformal mapping. This approach helps to
avoid topological errors in the segmentation results. We provide a thorough
analysis of the proposed model, including theoretical support for the ability
of QIS to include or exclude regions of interest or disinterest based on the
user's indication. To evaluate the performance of QIS, we conduct experiments
on synthesized images, medical images, natural images and noisy natural images.
The results demonstrate the efficacy of our proposed method.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14696" title="Abstract">arXiv:2402.14696</a> [<a href="/pdf/2402.14696" title="Download PDF">pdf</a>, <a href="/ps/2402.14696" title="Download PostScript">ps</a>, <a href="/format/2402.14696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Schr&#xf6;dingerization based quantum algorithms for linear dynamical  systems with inhomogeneous terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jin%2C+S">S. Jin</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+N">N. Liu</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+C">C. Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We analyze the Schr\"odingerisation method for quantum simulation of a
general class of non-unitary dynamics with inhomogeneous source terms. The
Schr\"odingerisation technique, introduced in \cite{JLY22a,JLY23}, transforms
any linear ordinary and partial differential equations with non-unitary
dynamics into a system under unitary dynamics via a warped phase transition
that maps the equations into a higher dimension, making them suitable for
quantum simulation. This technique can also be applied to these equations with
inhomogeneous terms modeling source or forcing terms or boundary and interface
conditions, and discrete dynamical systems such as iterative methods in
numerical linear algebra, through extra equations in the system. Difficulty
airses with the presense of inhomogeneous terms since it can change the
stability of the original system.
<br />In this paper, we systematically study--both theoretically and
numerically--the important issue of recovering the original variables from the
Schr\"odingerized equations, even when the evolution operator contains unstable
modes. We show that even with unstable modes, one can still construct a stable
scheme, yet to recover the original variable one needs to use suitable data in
the extended space. We analyze and compare both the discrete and continuous
Fourier transforms used in the extended dimension, and derive corresponding
error estimates, which allows one to use the more appropriate transform for
specific equations. We also provide a smoother initialization for the
Schrod\"odingerized system to gain higher order accuracy in the extended space.
We homogenize the inhomogeneous terms with a stretch transformation, making it
easier to recover the original variable. Our recovering technique also provides
a simple and generic framework to solve general ill-posed problems in a
computationally stable way.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14698" title="Abstract">arXiv:2402.14698</a> [<a href="/pdf/2402.14698" title="Download PDF">pdf</a>, <a href="/format/2402.14698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big data analytics to classify earthwork-related locations: A Chengdu  study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Ke Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Air pollution has significantly intensified, leading to severe health
consequences worldwide. Earthwork-related locations (ERLs) constitute
significant sources of urban dust pollution. The effective management of ERLs
has long posed challenges for governmental and environmental agencies,
primarily due to their classification under different regulatory authorities,
information barriers, delays in data updating, and a lack of dust suppression
measures for various sources of dust pollution. To address these challenges, we
classified urban dust pollution sources using dump truck trajectory, urban
point of interest (POI), and land cover data. We compared several prediction
models and investigated the relationship between features and dust pollution
sources using real data. The results demonstrate that high-accuracy
classification can be achieved with a limited number of features. This method
was successfully implemented in the system called Alpha MAPS in Chengdu to
provide decision support for urban pollution control.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14700" title="Abstract">arXiv:2402.14700</a> [<a href="/pdf/2402.14700" title="Download PDF">pdf</a>, <a href="/format/2402.14700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Linguistic Regions in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated considerable cross-lingual
alignment and generalization ability. Current research primarily focuses on
improving LLMs' cross-lingual generalization capabilities. However, there is
still a lack of research on the intrinsic mechanisms of how LLMs achieve
cross-lingual alignment. From the perspective of region partitioning, this
paper conducts several investigations on the linguistic competence of LLMs. We
discover a core region in LLMs that corresponds to linguistic competence,
accounting for approximately 1% of the total model parameters. Removing this
core region by setting parameters to zero results in a significant performance
decrease across 30 different languages. Furthermore, this core region exhibits
significant dimensional dependency, perturbations to even a single parameter on
specific dimensions leading to a loss of linguistic competence. Moreover, we
discover that distinct regions exist for different monolingual families, and
disruption to these specific regions substantially reduces the LLMs'
proficiency in those corresponding languages. Our research also indicates that
freezing the core linguistic region during further pre-training can mitigate
the issue of catastrophic forgetting (CF), a common occurrence observed during
further pre-training of LLMs. Overall, exploring the LLMs' functional regions
provides insights into the foundation of their intelligence.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14701" title="Abstract">arXiv:2402.14701</a> [<a href="/pdf/2402.14701" title="Download PDF">pdf</a>, <a href="/format/2402.14701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies  with Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Baihan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bouneffouf%2C+D">Djallel Bouneffouf</a>, 
<a href="/search/cs?searchtype=author&query=Landa%2C+Y">Yulia Landa</a>, 
<a href="/search/cs?searchtype=author&query=Jespersen%2C+R">Rachel Jespersen</a>, 
<a href="/search/cs?searchtype=author&query=Corcoran%2C+C">Cheryl Corcoran</a>, 
<a href="/search/cs?searchtype=author&query=Cecchi%2C+G">Guillermo Cecchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work extends our research series in computational psychiatry (e.g auto annotation in <a href="/abs/2204.05522">arXiv:2204.05522</a>, topic extraction in <a href="/abs/2204.10189">arXiv:2204.10189</a>, and diagnosis in <a href="/abs/2210.15603">arXiv:2210.15603</a>) with the introduction of LLMs to complete the full cycle of interpreting and understanding psychotherapy strategies as a comprehensive analytical framework
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The therapeutic working alliance is a critical factor in predicting the
success of psychotherapy treatment. Traditionally, working alliance assessment
relies on questionnaires completed by both therapists and patients. In this
paper, we present COMPASS, a novel framework to directly infer the therapeutic
working alliance from the natural language used in psychotherapy sessions. Our
approach utilizes advanced large language models to analyze transcripts of
psychotherapy sessions and compare them with distributed representations of
statements in the working alliance inventory. Analyzing a dataset of over 950
sessions covering diverse psychiatric conditions, we demonstrate the
effectiveness of our method in microscopically mapping patient-therapist
alignment trajectories and providing interpretability for clinical psychiatry
and in identifying emerging patterns related to the condition being treated. By
employing various neural topic modeling techniques in combination with
generative language prompting, we analyze the topical characteristics of
different psychiatric conditions and incorporate temporal modeling to capture
the evolution of topics at a turn-level resolution. This combined framework
enhances the understanding of therapeutic interactions, enabling timely
feedback for therapists regarding conversation quality and providing
interpretable insights to improve the effectiveness of psychotherapy.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14702" title="Abstract">arXiv:2402.14702</a> [<a href="/pdf/2402.14702" title="Download PDF">pdf</a>, <a href="/format/2402.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfFeed: Influence Functions as a Feedback to Improve the Performance of  Subjective Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somnath Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+M">Maulindu Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Punyajoy Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mathew%2C+B">Binny Mathew</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024 (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, influence functions present an apparatus for achieving
explainability for deep neural models by quantifying the perturbation of
individual train instances that might impact a test prediction. Our objectives
in this paper are twofold. First we incorporate influence functions as a
feedback into the model to improve its performance. Second, in a dataset
extension exercise, using influence functions to automatically identify data
points that have been initially `silver' annotated by some existing method and
need to be cross-checked (and corrected) by annotators to improve the model
performance. To meet these objectives, in this paper, we introduce InfFeed,
which uses influence functions to compute the influential instances for a
target instance. Toward the first objective, we adjust the label of the target
instance based on its influencer(s) label. In doing this, InfFeed outperforms
the state-of-the-art baselines (including LLMs) by a maximum macro F1-score
margin of almost 4% for hate speech classification, 3.5% for stance
classification, and 3% for irony and 2% for sarcasm detection. Toward the
second objective we show that manually re-annotating only those silver
annotated data points in the extension set that have a negative influence can
immensely improve the model performance bringing it very close to the scenario
where all the data points in the extension set have gold labels. This allows
for huge reduction of the number of data points that need to be manually
annotated since out of the silver annotated extension dataset, the influence
function scheme picks up ~1/1000 points that need manual correction.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14703" title="Abstract">arXiv:2402.14703</a> [<a href="/pdf/2402.14703" title="Download PDF">pdf</a>, <a href="/ps/2402.14703" title="Download PostScript">ps</a>, <a href="/format/2402.14703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Curses of Future and History in Future-dependent Value Functions  for Off-policy Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study off-policy evaluation (OPE) in partially observable environments
with complex observations, with the goal of developing estimators whose
guarantee avoids exponential dependence on the horizon. While such estimators
exist for MDPs and POMDPs can be converted to history-based MDPs, their
estimation errors depend on the state-density ratio for MDPs which becomes
history ratios after conversion, an exponential object. Recently, Uehara et al.
(2022) proposed future-dependent value functions as a promising framework to
address this issue, where the guarantee for memoryless policies depends on the
density ratio over the latent state space. However, it also depends on the
boundedness of the future-dependent value function and other related
quantities, which we show could be exponential-in-length and thus erasing the
advantage of the method. In this paper, we discover novel coverage assumptions
tailored to the structure of POMDPs, such as outcome coverage and belief
coverage. These assumptions not only enable polynomial bounds on the
aforementioned quantities, but also lead to the discovery of new algorithms
with complementary properties.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14704" title="Abstract">arXiv:2402.14704</a> [<a href="/pdf/2402.14704" title="Download PDF">pdf</a>, <a href="/format/2402.14704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An LLM-Enhanced Adversarial Editing System for Lexical Simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+K">Keren Tan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kangyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+J">Jinlong Shu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Lexical Simplification (LS) aims to simplify text at the lexical level.
Existing methods rely heavily on annotated data, making it challenging to apply
in low-resource scenarios. In this paper, we propose a novel LS method without
parallel corpora. This method employs an Adversarial Editing System with
guidance from a confusion loss and an invariance loss to predict lexical edits
in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced
loss to enable the distillation of knowledge from Large Language Models (LLMs)
into a small-size LS system. From that, complex words within sentences are
masked and a Difficulty-aware Filling module is crafted to replace masked
positions with simpler words. At last, extensive experimental results and
analyses on three benchmark LS datasets demonstrate the effectiveness of our
proposed method.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14707" title="Abstract">arXiv:2402.14707</a> [<a href="/pdf/2402.14707" title="Download PDF">pdf</a>, <a href="/format/2402.14707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Cytopathological Image Synthesis for Augmenting Cervical  Abnormality Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhenrong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+M">Manman Fei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jiangdong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automatic thin-prep cytologic test (TCT) screening can assist pathologists in
finding cervical abnormality towards accurate and efficient cervical cancer
diagnosis. Current automatic TCT screening systems mostly involve abnormal
cervical cell detection, which generally requires large-scale and diverse
training data with high-quality annotations to achieve promising performance.
Pathological image synthesis is naturally raised to minimize the efforts in
data collection and annotation. However, it is challenging to generate
realistic large-size cytopathological images while simultaneously synthesizing
visually plausible appearances for small-size abnormal cervical cells. In this
paper, we propose a two-stage image synthesis framework to create synthetic
data for augmenting cervical abnormality screening. In the first Global Image
Generation stage, a Normal Image Generator is designed to generate
cytopathological images full of normal cervical cells. In the second Local Cell
Editing stage, normal cells are randomly selected from the generated images and
then are converted to different types of abnormal cells using the proposed
Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell
Synthesizer are built upon the pre-trained Stable Diffusion via
parameter-efficient fine-tuning methods for customizing cytopathological image
contents and extending spatial layout controllability, respectively. Our
experiments demonstrate the synthetic image quality, diversity, and
controllability of the proposed synthesis framework, and validate its data
augmentation effectiveness in enhancing the performance of abnormal cervical
cell detection.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14708" title="Abstract">arXiv:2402.14708</a> [<a href="/pdf/2402.14708" title="Download PDF">pdf</a>, <a href="/format/2402.14708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yifan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guibin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiaojiang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ziqi%2C+W">Wang Ziqi</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Junyuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinke Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">Credit card fraud poses a significant threat to the economy. While Graph
Neural Network (GNN)-based fraud detection methods perform well, they often
overlook the causal effect of a node's local structure on predictions. This
paper introduces a novel method for credit card fraud detection, the
\textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal
\textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork
(CaT-GNN), which leverages causal invariant learning to reveal inherent
correlations within transaction data. By decomposing the problem into discovery
and intervention phases, CaT-GNN identifies causal nodes within the transaction
graph and applies a causal mixup strategy to enhance the model's robustness and
interpretability. CaT-GNN consists of two key components: Causal-Inspector and
Causal-Intervener. The Causal-Inspector utilizes attention weights in the
temporal attention mechanism to identify causal and environment nodes without
introducing additional parameters. Subsequently, the Causal-Intervener performs
a causal mixup enhancement on environment nodes based on the set of nodes.
Evaluated on three datasets, including a private financial dataset and two
public datasets, CaT-GNN demonstrates superior performance over existing
state-of-the-art methods. Our findings highlight the potential of integrating
causal reasoning with graph neural networks to improve fraud detection
capabilities in financial transactions.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14710" title="Abstract">arXiv:2402.14710</a> [<a href="/pdf/2402.14710" title="Download PDF">pdf</a>, <a href="/format/2402.14710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IEPile: Unearthing Large-Scale Schema-Based Information Extraction  Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hongbin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengshu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 18 pages; Github: <a href="https://github.com/zjunlp/IEPile">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate remarkable potential across various
domains; however, they exhibit a significant performance gap in Information
Extraction (IE). Note that high-quality instruction data is the vital key for
enhancing the specific capabilities of LLMs, while current IE datasets tend to
be small in scale, fragmented, and lack standardized schema. To this end, we
introduce IEPile, a comprehensive bilingual (English and Chinese) IE
instruction corpus, which contains approximately 0.32B tokens. We construct
IEPile by collecting and cleaning 33 existing IE datasets, and introduce
schema-based instruction generation to unearth a large-scale corpus.
Experimental results on LLaMA and Baichuan demonstrate that using IEPile can
enhance the performance of LLMs for IE, especially the zero-shot
generalization. We open-source the resource and pre-trained models, hoping to
provide valuable support to the NLP community.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14711" title="Abstract">arXiv:2402.14711</a> [<a href="/pdf/2402.14711" title="Download PDF">pdf</a>, <a href="/format/2402.14711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Observability for Nonlinear Systems: Connecting Variational Dynamics,  Lyapunov Exponents, and Empirical Gramians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kazma%2C+M+H">Mohamad H. Kazma</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+A+F">Ahmad F. Taha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Observability is a key problem in dynamic network sciences. While it has been
thoroughly studied for linear systems, observability for nonlinear networks is
less intuitive and more cumbersome. One common approach to quantify
observability for nonlinear systems is via the Empirical Gramian (Empr-Gram) --
a generalized form of the Gramian of linear systems. In this technical note, we
produce three new results. First, we establish that a variational form of
nonlinear systems (computed via perturbing initial conditions) yields a
so-called Variational Gramian (Var-Gram) that is equivalent to the classic
Empr-Gram; the former being easier to compute than the latter. Via Lyapunov
exponents derived from Lyapunov's direct method, the technical note's second
result derives connections between vintage observability measures and Var-Gram.
The third result demonstrates the applicability of these new notions for sensor
selection/placement in nonlinear systems. Numerical case studies demonstrate
these three developments and their merits.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14712" title="Abstract">arXiv:2402.14712</a> [<a href="/pdf/2402.14712" title="Download PDF">pdf</a>, <a href="/format/2402.14712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gilbert-Varshamov Bound for Codes in $L_1$ Metric using Multivariate  Analytic Combinatorics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+K">Keshav Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+D+T">Duc Tu Dao</a>, 
<a href="/search/cs?searchtype=author&query=Kova%C4%8Devi%C4%87%2C+M">Mladen Kova&#x10d;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kiah%2C+H+M">Han Mao Kiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 3 figures, submitted to IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">Analytic combinatorics in several variables refers to a suite of tools that
provide sharp asymptotic estimates for certain combinatorial quantities. In
this paper, we apply these tools to determine the Gilbert--Varshamov lower
bound on the rate of optimal codes in $L_1$ metric. Several different code
spaces are analyzed, including the simplex and the hypercube in $\mathbb{Z^n}$,
all of which are inspired by concrete data storage and transmission models such
as the sticky insertion channel, the permutation channel, the adjacent
transposition (bit-shift) channel, the multilevel flash memory channel, etc.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14714" title="Abstract">arXiv:2402.14714</a> [<a href="/pdf/2402.14714" title="Download PDF">pdf</a>, <a href="/format/2402.14714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Effective Vocabulary Expansion Towards Multilingual Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungduk Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungtaek Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+M">Myeongho Jeong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of
large language models that exhibit remarkable capabilities across English and
Korean text understanding. Building on recent highly capable but
English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts
are inefficiently processed with English-centric tokenizers, we present an
efficient and effective vocabulary expansion (EEVE) method, which encompasses
parameter freezing and subword initialization. In contrast to previous efforts
that believe new embeddings require trillions of training tokens, we show that
our method can significantly boost non-English proficiency within just 2
billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM
Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0}
ranks as the leading Korean pre-trained model in the open-source community,
according to Hugging Face's leaderboard. We open-source our models on
Huggingface to empower the open research community in various languages.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14720" title="Abstract">arXiv:2402.14720</a> [<a href="/pdf/2402.14720" title="Download PDF">pdf</a>, <a href="/ps/2402.14720" title="Download PostScript">ps</a>, <a href="/format/2402.14720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Transformer Model for Boundary Detection in Continuous Sign Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rastgoo%2C+R">Razieh Rastgoo</a>, 
<a href="/search/cs?searchtype=author&query=Kiani%2C+K">Kourosh Kiani</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Sign Language Recognition (SLR) has garnered significant attention from
researchers in recent years, particularly the intricate domain of Continuous
Sign Language Recognition (CSLR), which presents heightened complexity compared
to Isolated Sign Language Recognition (ISLR). One of the prominent challenges
in CSLR pertains to accurately detecting the boundaries of isolated signs
within a continuous video stream. Additionally, the reliance on handcrafted
features in existing models poses a challenge to achieving optimal accuracy. To
surmount these challenges, we propose a novel approach utilizing a
Transformer-based model. Unlike traditional models, our approach focuses on
enhancing accuracy while eliminating the need for handcrafted features. The
Transformer model is employed for both ISLR and CSLR. The training process
involves using isolated sign videos, where hand keypoint features extracted
from the input video are enriched using the Transformer model. Subsequently,
these enriched features are forwarded to the final classification layer. The
trained model, coupled with a post-processing method, is then applied to detect
isolated sign boundaries within continuous sign videos. The evaluation of our
model is conducted on two distinct datasets, including both continuous signs
and their corresponding isolated signs, demonstrates promising results.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14723" title="Abstract">arXiv:2402.14723</a> [<a href="/pdf/2402.14723" title="Download PDF">pdf</a>, <a href="/format/2402.14723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Run Time Assurance for Simultaneous Constraint Satisfaction During  Spacecraft Attitude Maneuvering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=McQuinn%2C+C">Cassie-Kay McQuinn</a>, 
<a href="/search/eess?searchtype=author&query=Dunlap%2C+K">Kyle Dunlap</a>, 
<a href="/search/eess?searchtype=author&query=Hamilton%2C+N">Nathaniel Hamilton</a>, 
<a href="/search/eess?searchtype=author&query=Wilson%2C+J">Jabari Wilson</a>, 
<a href="/search/eess?searchtype=author&query=Hobbs%2C+K+L">Kerianne L. Hobbs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">A fundamental capability for On-orbit Servicing, Assembly, and Manufacturing
(OSAM) is inspection of the vehicle to be serviced, or the structure being
assembled. This research assumes autonomous slewing to maintain situational
awareness of multiple vehicles operating in close proximity where several
safety constraints must be satisfied. A variety of techniques may be used as
the primary controller. The focus of this research is developing Run Time
Assurance (RTA) filters that monitor system behavior and the output of the
primary controller to enforce safety constraint satisfaction. Specifically,
this research explores combining a subset of the constraints into an Active Set
Invariance Filter (ASIF) RTA defined using control barrier functions. This
method is minimally invasive to the primary control by minimizing deviation
from the desired control output of the primary controller, while simultaneously
enforcing all safety constraints. The RTA is designed to ensure the spacecraft
maintains attitude requirements for communication and data transfer with a
ground station during scheduled communication windows, adheres to conical
attitude keep out zones, limits thermally unfavorable attitude duration,
maintains attitude requirements for sufficient power generation, ensures
maneuvers are below threshold to cause structural damage, ensures maximum
angular velocity is below limits to maintain ability to respond quickly to new
slewing commands, and conserves actuator use to prevent wear when possible.
Slack variables are introduced into the ASIF controller to prioritize safety
constraints when a solution to all safety constraints is infeasible. Monte
Carlo simulation results as well as plots of example cases are shown and
evaluated for a three degree of freedom spacecraft with reaction wheel attitude
control.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14726" title="Abstract">arXiv:2402.14726</a> [<a href="/pdf/2402.14726" title="Download PDF">pdf</a>, <a href="/format/2402.14726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Expert Rules into Neural Networks in the Framework of  Concept-Based Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konstantinov%2C+A+V">Andrei V. Konstantinov</a>, 
<a href="/search/cs?searchtype=author&query=Utkin%2C+L+V">Lev V. Utkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">A problem of incorporating the expert rules into machine learning models for
extending the concept-based learning is formulated in the paper. It is proposed
how to combine logical rules and neural networks predicting the concept
probabilities. The first idea behind the combination is to form constraints for
a joint probability distribution over all combinations of concept values to
satisfy the expert rules. The second idea is to represent a feasible set of
probability distributions in the form of a convex polytope and to use its
vertices or faces. We provide several approaches for solving the stated problem
and for training neural networks which guarantee that the output probabilities
of concepts would not violate the expert rules. The solution of the problem can
be viewed as a way for combining the inductive and deductive learning. Expert
rules are used in a broader sense when any logical function that connects
concepts and class labels or just concepts with each other can be regarded as a
rule. This feature significantly expands the class of the proposed results.
Numerical examples illustrate the approaches. The code of proposed algorithms
is publicly available.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14728" title="Abstract">arXiv:2402.14728</a> [<a href="/pdf/2402.14728" title="Download PDF">pdf</a>, <a href="/ps/2402.14728" title="Download PostScript">ps</a>, <a href="/format/2402.14728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The European Commitment to Human-Centered Technology: The Integral Role  of HCI in the EU AI Act&#x27;s Success
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valdez%2C+A+C">Andr&#xe9; Calero Valdez</a>, 
<a href="/search/cs?searchtype=author&query=Heine%2C+M">Moreen Heine</a>, 
<a href="/search/cs?searchtype=author&query=Franke%2C+T">Thomas Franke</a>, 
<a href="/search/cs?searchtype=author&query=Jochems%2C+N">Nicole Jochems</a>, 
<a href="/search/cs?searchtype=author&query=Jetter%2C+H">Hans-Christian Jetter</a>, 
<a href="/search/cs?searchtype=author&query=Schrills%2C+T">Tim Schrills</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act,
regulating market access for AI-based systems. A salient feature of the Act is
to guard democratic and humanistic values by focusing regulation on
transparency, explainability, and the human ability to understand and control
AI systems. Hereby, the EU AI Act does not merely specify technological
requirements for AI systems. The EU issues a democratic call for human-centered
AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development. Without robust methods to assess
AI systems and their effect on individuals and society, the EU AI Act may lead
to repeating the mistakes of the General Data Protection Regulation of the EU
and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more
confusion than lending guidance. Moreover, determined research activities in
Human-AI interaction will be pivotal for both regulatory compliance and the
advancement of AI in a manner that is both ethical and effective. Such an
approach will ensure that AI development aligns with human values and needs,
fostering a technology landscape that is innovative, responsible, and an
integral part of our society.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14730" title="Abstract">arXiv:2402.14730</a> [<a href="/pdf/2402.14730" title="Download PDF">pdf</a>, <a href="/format/2402.14730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford-Steerable Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhdanov%2C+M">Maksim Zhdanov</a>, 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Weiler%2C+M">Maurice Weiler</a>, 
<a href="/search/cs?searchtype=author&query=Lucic%2C+A">Ana Lucic</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a
novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector
fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance,
$\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on
Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit
parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group
equivariant neural networks. We significantly and consistently outperform
baseline methods on fluid dynamics as well as relativistic electrodynamics
forecasting tasks.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14735" title="Abstract">arXiv:2402.14735</a> [<a href="/pdf/2402.14735" title="Download PDF">pdf</a>, <a href="/format/2402.14735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Transformers Learn Causal Structure with Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nichani%2C+E">Eshaan Nichani</a>, 
<a href="/search/cs?searchtype=author&query=Damian%2C+A">Alex Damian</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">The incredible success of transformers on sequence modeling tasks can be
largely attributed to the self-attention mechanism, which allows information to
be transferred between different parts of a sequence. Self-attention allows
transformers to encode causal structure which makes them particularly suitable
for sequence modeling. However, the process by which transformers learn such
causal structure via gradient-based training algorithms remains poorly
understood. To better understand this process, we introduce an in-context
learning task that requires learning latent causal structure. We prove that
gradient descent on a simplified two-layer transformer learns to solve this
task by encoding the latent causal graph in the first attention layer. The key
insight of our proof is that the gradient of the attention matrix encodes the
mutual information between tokens. As a consequence of the data processing
inequality, the largest entries of this gradient correspond to edges in the
latent causal graph. As a special case, when the sequences are generated from
in-context Markov chains, we prove that transformers learn an induction head
(Olsson et al., 2022). We confirm our theoretical findings by showing that
transformers trained on our in-context learning task are able to recover a wide
variety of causal structures.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14737" title="Abstract">arXiv:2402.14737</a> [<a href="/pdf/2402.14737" title="Download PDF">pdf</a>, <a href="/format/2402.14737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eavesdropping with Intelligent Reflective Surfaces: Near-Optimal  Configuration Cycling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malandrino%2C+F">Francesco Malandrino</a>, 
<a href="/search/cs?searchtype=author&query=Nordio%2C+A">Alessandro Nordio</a>, 
<a href="/search/cs?searchtype=author&query=Chiasserini%2C+C+F">Carla Fabiana Chiasserini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2108.00149">arXiv:2108.00149</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Networks, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Intelligent reflecting surfaces (IRSs) have several prominent advantages,
including improving the level of wireless communication security and privacy.
In this work, we focus on the latter aspect and introduce a strategy to
counteract the presence of passive eavesdroppers overhearing transmissions from
a base station towards legitimate users that are facilitated by the presence of
IRSs. Specifically, we envision a transmission scheme that cycles across a
number of IRS-to-user assignments, and we select them in a near-optimal
fashion, thus guaranteeing both a high data rate and a good secrecy rate.
Unlike most of the existing works addressing passive eavesdropping, the
strategy we envision has low complexity and is suitable for scenarios where
nodes are equipped with a limited number of antennas. Through our performance
evaluation, we highlight the trade-off between the legitimate users' data rate
and secrecy rate, and how the system parameters affect such a trade-off.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14739" title="Abstract">arXiv:2402.14739</a> [<a href="/pdf/2402.14739" title="Download PDF">pdf</a>, <a href="/format/2402.14739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samak%2C+C+V">Chinmay Vilas Samak</a>, 
<a href="/search/cs?searchtype=author&query=Samak%2C+T+V">Tanmay Vilas Samak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.12670">arXiv:2402.12670</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Modeling and simulation of autonomous vehicles plays a crucial role in
achieving enterprise-scale realization that aligns with technical, business and
regulatory requirements. Contemporary trends in digital lifecycle treatment
have proven beneficial to support SBD as well as V&amp;V of these complex systems.
Although, the development of appropriate fidelity simulation models capable of
capturing the intricate real-world physics and graphics (real2sim), while
enabling real-time interactivity for decision-making, has remained a challenge.
Nevertheless, recent advances in AI-based tools and workflows, such as online
deep-learning algorithms leveraging live-streaming data sources, offer the
tantalizing potential for real-time system-identification and adaptive modeling
to simulate vehicles, environments, as well as their interactions. This
transition from virtual prototypes to digital twins not only improves
simulation fidelity and real-time factor, but can also support the development
of online adaption/augmentation techniques that can help bridge the gap between
simulation and reality (sim2real). In such a milieu, this work focuses on
developing autonomy-oriented digital twins of vehicles across different scales
and configurations to help support the streamlined development and deployment
of Autoware stack, using a unified real2sim2real toolchain. Particularly, the
core deliverable for this project was to integrate the Autoware stack with
AutoDRIVE Ecosystem to demonstrate end-to-end task of map-based autonomous
navigation. This work discusses the development of vehicle and environment
digital twins using AutoDRIVE Ecosystem, along with various APIs and HMIs to
connect with the same, followed by a detailed section on AutoDRIVE-Autoware
integration. Furthermore, this study describes the first-ever off-road
deployment of the Autoware stack, expanding the ODD beyond on-road autonomous
navigation.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14740" title="Abstract">arXiv:2402.14740</a> [<a href="/pdf/2402.14740" title="Download PDF">pdf</a>, <a href="/format/2402.14740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to Basics: Revisiting REINFORCE Style Optimization for Learning  from Human Feedback in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadian%2C+A">Arash Ahmadian</a>, 
<a href="/search/cs?searchtype=author&query=Cremer%2C+C">Chris Cremer</a>, 
<a href="/search/cs?searchtype=author&query=Gall%C3%A9%2C+M">Matthias Gall&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Fadaee%2C+M">Marzieh Fadaee</a>, 
<a href="/search/cs?searchtype=author&query=Kreutzer%2C+J">Julia Kreutzer</a>, 
<a href="/search/cs?searchtype=author&query=%C3%9Cst%C3%BCn%2C+A">Ahmet &#xdc;st&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">AI alignment in the shape of Reinforcement Learning from Human Feedback
(RLHF) is increasingly treated as a crucial ingredient for high performance
large language models. \textsc{Proximal Policy Optimization} (PPO) has been
positioned by recent literature as the canonical method for the RL part of
RLHF. However, it involves both high computational cost and sensitive
hyperparameter tuning. We posit that most of the motivational principles that
led to the development of PPO are less of a practical concern in RLHF and
advocate for a less computationally expensive method that preserves and even
increases performance. We revisit the \textit{formulation} of alignment from
human preferences in the context of RL. Keeping simplicity as a guiding
principle, we show that many components of PPO are unnecessary in an RLHF
context and that far simpler REINFORCE-style optimization variants outperform
both PPO and newly proposed "RL-free" methods such as DPO and RAFT. Our work
suggests that careful adaptation to LLMs alignment characteristics enables
benefiting from online RL optimization at low cost.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14743" title="Abstract">arXiv:2402.14743</a> [<a href="/pdf/2402.14743" title="Download PDF">pdf</a>, <a href="/format/2402.14743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependency Annotation of Ottoman Turkish with Multilingual BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%96zate%C5%9F%2C+%C5%9E+B">&#x15e;aziye Bet&#xfc;l &#xd6;zate&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=T%C4%B1ra%C5%9F%2C+T+E">Tar&#x131;k Emre T&#x131;ra&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=Gen%C3%A7%2C+E+E">Efe Eren Gen&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=Ta%C5%9Fdemir%2C+E+F+B">Esma Fat&#x131;ma Bilgin Ta&#x15f;demir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures. Accepted to LAW-XVIII
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study introduces a pretrained large language model-based annotation
methodology for the first dependency treebank in Ottoman Turkish. Our
experimental results show that, iteratively, i) pseudo-annotating data using a
multilingual BERT-based parsing model, ii) manually correcting the
pseudo-annotations, and iii) fine-tuning the parsing model with the corrected
annotations, we speed up and simplify the challenging dependency annotation
process. The resulting treebank, that will be a part of the Universal
Dependencies (UD) project, will facilitate automated analysis of Ottoman
Turkish documents, unlocking the linguistic richness embedded in this
historical heritage.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14744" title="Abstract">arXiv:2402.14744</a> [<a href="/pdf/2402.14744" title="Download PDF">pdf</a>, <a href="/format/2402.14744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Urban Residents: An LLM Agent Framework for  Personal Mobility Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiawei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Renhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chuang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zengqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Onizuka%2C+M">Makoto Onizuka</a>, 
<a href="/search/cs?searchtype=author&query=Shibasaki%2C+R">Ryosuke Shibasaki</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chuan Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source codes will be released soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel approach using Large Language Models (LLMs)
integrated into an agent framework for flexible and efficient personal mobility
generation. LLMs overcome the limitations of previous models by efficiently
processing semantic data and offering versatility in modeling various tasks.
Our approach addresses the critical need to align LLMs with real-world urban
mobility data, focusing on three research questions: aligning LLMs with rich
activity data, developing reliable activity generation strategies, and
exploring LLM applications in urban mobility. The key technical contribution is
a novel LLM agent framework that accounts for individual activity patterns and
motivations, including a self-consistency approach to align LLMs with
real-world activity data and a retrieval-augmented strategy for interpretable
activity generation. In experimental studies, comprehensive validation is
performed using real-world data. This research marks the pioneering work of
designing an LLM agent framework for activity generation based on real-world
human activity data, offering a promising tool for urban mobility analysis.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14746" title="Abstract">arXiv:2402.14746</a> [<a href="/pdf/2402.14746" title="Download PDF">pdf</a>, <a href="/ps/2402.14746" title="Download PostScript">ps</a>, <a href="/format/2402.14746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Efficient LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kausik%2C+B+N">B.N. Kausik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Trained LLMs are typically sparse in that most of the parameters are zero,
raising questions on efficiency. In response, we inquire into efficient LLMs,
i.e. those with the fewest parameters that achieve the desired accuracy on a
training corpus. Specifically, we compare theoretical and empirical estimates
for training loss at current scale to obtain upper and lower bounds on the
number of unique sequences in a natural training corpus as a function of its
size. Our result implies (1) to double the number of skills represented in a
training corpus, the corpus must scale roughly between three and five fold (2)
for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural
training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of
an LLM is smaller than the number of unique sequences in the training corpus,
scaling up can uncover emergent skills.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14750" title="Abstract">arXiv:2402.14750</a> [<a href="/pdf/2402.14750" title="Download PDF">pdf</a>, <a href="/format/2402.14750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Spacecraft Formation Flying with Crazyflie Drones as Satellite  Surrogates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+la+Barcena%2C+A">Arturo de la Barcena</a>, 
<a href="/search/cs?searchtype=author&query=Rhodes%2C+C">Collin Rhodes</a>, 
<a href="/search/cs?searchtype=author&query=McCarroll%2C+J">John McCarroll</a>, 
<a href="/search/cs?searchtype=author&query=Cescon%2C+M">Marzia Cescon</a>, 
<a href="/search/cs?searchtype=author&query=Hobbs%2C+K+L">Kerianne L. Hobbs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">As the space domain becomes increasingly congested, autonomy is proposed as
one approach to enable small numbers of human ground operators to manage large
constellations of satellites and tackle more complex missions such as on-orbit
or in-space servicing, assembly, and manufacturing. One of the biggest
challenges in developing novel spacecraft autonomy is mechanisms to test and
evaluate their performance. Testing spacecraft autonomy on-orbit can be high
risk and prohibitively expensive. An alternative method is to test autonomy
terrestrially using satellite surrogates such as attitude test beds on air
bearings or drones for translational motion visualization. Against this
background, this work develops an approach to evaluate autonomous spacecraft
behavior using a surrogate platform, namely a micro-quadcopter drone developed
by the Bitcraze team, the Crazyflie 2.1. The Crazyflie drones are increasingly
becoming ubiquitous in flight testing labs because they are affordable, open
source, readily available, and include expansion decks which allow for features
such as positioning systems, distance and/or motion sensors, wireless charging,
and AI capabilities. In this paper, models of Crazyflie drones are used to
simulate the relative motion dynamics of spacecraft under linearized
Clohessy-Wiltshire dynamics in elliptical natural motion trajectories, in
pre-generated docking trajectories, and via trajectories output by neural
network control systems.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14751" title="Abstract">arXiv:2402.14751</a> [<a href="/pdf/2402.14751" title="Download PDF">pdf</a>, <a href="/ps/2402.14751" title="Download PostScript">ps</a>, <a href="/format/2402.14751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the communication complexity of finding a king in a tournament
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mande%2C+N+S">Nikhil S. Mande</a>, 
<a href="/search/cs?searchtype=author&query=Paraashar%2C+M">Manaswi Paraashar</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Swagato Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+N">Nitin Saurabh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">A tournament is a complete directed graph. A king in a tournament is a vertex
v such that every other vertex is reachable from v via a path of length at most
2. It is well known that every tournament has at least one king, one of which
is a maximum out-degree vertex. The tasks of finding a king, a maximum
out-degree vertex and a source in a tournament has been relatively well studied
in the context of query complexity. We study the communication complexity of
these tasks, where the edges are partitioned between two players. The following
are our main results for n-vertex tournaments:
<br />1) The deterministic communication complexity of finding whether a source
exists is tilde{Theta}(log^2 n).
<br />2) The deterministic and randomized communication complexities of finding a
king are Theta(n). The quantum communication complexity is
tilde{Theta}(sqrt{n}).
<br />3) The deterministic, randomized and quantum communication complexities of
finding a maximum out-degree vertex are Theta(n log n), tilde{Theta}(n) and
tilde{Theta}(sqrt{n}), respectively.
<br />Our upper bounds hold for all partitions of edges, and the lower bounds for a
specific partition of the edges. To show the first bullet above, we show,
perhaps surprisingly, that finding a source in a tournament is equivalent to
the well-studied Clique vs. Independent Set (CIS) problem on undirected graphs.
Our bounds for finding a source then follow from known bounds on the complexity
of the CIS problem. In view of this equivalence, we can view the task of
finding a king in a tournament to be a natural generalization of CIS.
<br />One of our lower bounds uses a fooling-set based argument, and all our other
lower bounds follow from carefully-constructed reductions from
Set-Disjointness.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14753" title="Abstract">arXiv:2402.14753</a> [<a href="/pdf/2402.14753" title="Download PDF">pdf</a>, <a href="/format/2402.14753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting a Pretrained Transformer Can Be a Universal Approximator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrov%2C+A">Aleksandar Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Functional Analysis (math.FA)

</div>
<p class="mathjax">Despite the widespread adoption of prompting, prompt tuning and prefix-tuning
of transformer models, our theoretical understanding of these fine-tuning
methods remains limited. A key question is whether one can arbitrarily modify
the behavior of pretrained model by prompting or prefix-tuning it. Formally,
whether prompting and prefix-tuning a pretrained model can universally
approximate sequence-to-sequence functions. This paper answers in the
affirmative and demonstrates that much smaller pretrained models than
previously thought can be universal approximators when prefixed. In fact, the
attention mechanism is uniquely suited for universal approximation with
prefix-tuning a single attention head being sufficient to approximate any
continuous function. Moreover, any sequence-to-sequence function can be
approximated by prefixing a transformer with depth linear in the sequence
length. Beyond these density-type results, we also offer Jackson-type bounds on
the length of the prefix needed to approximate a function to a desired
precision.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14757" title="Abstract">arXiv:2402.14757</a> [<a href="/pdf/2402.14757" title="Download PDF">pdf</a>, <a href="/format/2402.14757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHM-Traffic: DRL and Transfer learning based UAV Control for Structural  Health Monitoring of Bridges with Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadiraju%2C+D+S">Divija Swetha Gadiraju</a>, 
<a href="/search/cs?searchtype=author&query=Azam%2C+S+E">Saeed Eftekhar Azam</a>, 
<a href="/search/cs?searchtype=author&query=Khazanchi%2C+D">Deepak Khazanchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This work focuses on using advanced techniques for structural health
monitoring (SHM) for bridges with Traffic. We propose an approach using deep
reinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV).
Our approach conducts a concrete bridge deck survey while traffic is ongoing
and detects cracks. The UAV performs the crack detection, and the location of
cracks is initially unknown. We use two edge detection techniques. First, we
use canny edge detection for crack detection. We also use a Convolutional
Neural Network (CNN) for crack detection and compare it with canny edge
detection. Transfer learning is applied using CNN with pre-trained weights
obtained from a crack image dataset. This enables the model to adapt and
improve its performance in identifying and localizing cracks. Proximal Policy
Optimization (PPO) is applied for UAV control and bridge surveys. The
experimentation across various scenarios is performed to evaluate the
performance of the proposed methodology. Key metrics such as task completion
time and reward convergence are observed to gauge the effectiveness of the
approach. We observe that the Canny edge detector offers up to 40\% lower task
completion time, while the CNN excels in up to 12\% better damage detection and
1.8 times better rewards.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14759" title="Abstract">arXiv:2402.14759</a> [<a href="/pdf/2402.14759" title="Download PDF">pdf</a>, <a href="/ps/2402.14759" title="Download PostScript">ps</a>, <a href="/format/2402.14759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalising realisability in statistical learning theory under  epistemic uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cuzzolin%2C+F">Fabio Cuzzolin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2401.09435">arXiv:2401.09435</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST)

</div>
<p class="mathjax">The purpose of this paper is to look into how central notions in statistical
learning theory, such as realisability, generalise under the assumption that
train and test distribution are issued from the same credal set, i.e., a convex
set of probability distributions. This can be considered as a first step
towards a more general treatment of statistical learning under epistemic
uncertainty.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14760" title="Abstract">arXiv:2402.14760</a> [<a href="/pdf/2402.14760" title="Download PDF">pdf</a>, <a href="/format/2402.14760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Reward Modeling for Out-of-Distribution Preference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chen Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Preference learning (PL) with large language models (LLMs) aims to align the
LLMs' generations with human preferences. Previous work on reinforcement
learning from human feedback (RLHF) has demonstrated promising results in
in-distribution PL. However, due to the difficulty of obtaining human feedback,
discretely training reward models for every encountered distribution is
challenging. Thus, out-of-distribution (OOD) PL is practically useful for
enhancing the generalization ability of LLMs with limited preference feedback.
This work addresses OOD PL by optimizing a general reward model through a
meta-learning approach. During meta-training, a bilevel optimization algorithm
is utilized to learn a reward model capable of guiding policy learning to align
with human preferences across various distributions. When encountering a test
distribution, the meta-test procedure conducts regularized policy optimization
using the learned reward model for PL. We theoretically demonstrate the
convergence rate of the bilevel optimization algorithm under reasonable
assumptions. Additionally, we conduct experiments on two text generation tasks
across 20 held-out domains and outperform a variety of strong baselines across
various evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14762" title="Abstract">arXiv:2402.14762</a> [<a href="/pdf/2402.14762" title="Download PDF">pdf</a>, <a href="/format/2402.14762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language  Models in Multi-Turn Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+G">Ge Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+X">Xingyuan Bu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yancheng He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhuoran Lin</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wenbo Su</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tiezheng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first three authors contribute equally, 32 pages, repo at <a href="https://github.com/mtbench101/mt-bench-101">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has drastically enhanced dialogue
systems. However, comprehensively evaluating the dialogue abilities of LLMs
remains a challenge. Previous benchmarks have primarily focused on single-turn
dialogues or provided coarse-grained and incomplete assessments of multi-turn
dialogues, overlooking the complexity and fine-grained nuances of real-life
dialogues. To address this issue, we introduce MT-Bench-101, specifically
designed to evaluate the fine-grained abilities of LLMs in multi-turn
dialogues. By conducting a detailed analysis of real multi-turn dialogue data,
we construct a three-tier hierarchical ability taxonomy comprising 4208 turns
across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21
popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both
ability and task perspectives and observing differing trends in LLMs
performance across dialogue turns within various tasks. Further analysis
indicates that neither utilizing common alignment techniques nor chat-specific
designs has led to obvious enhancements in the multi-turn abilities of LLMs.
Extensive case studies suggest that our designed tasks accurately assess the
corresponding multi-turn abilities.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14766" title="Abstract">arXiv:2402.14766</a> [<a href="/pdf/2402.14766" title="Download PDF">pdf</a>, <a href="/format/2402.14766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Environment Semantic Communication: Enabling Distributed Sensing Aided  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+S">Shoaib Imran</a>, 
<a href="/search/cs?searchtype=author&query=Charan%2C+G">Gouranga Charan</a>, 
<a href="/search/cs?searchtype=author&query=Alkhateeb%2C+A">Ahmed Alkhateeb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code and dataset are available on the DeepSense website <a href="https://www.deepsense6g.net/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Millimeter-wave (mmWave) and terahertz (THz) communication systems require
large antenna arrays and use narrow directive beams to ensure sufficient
receive signal power. However, selecting the optimal beams for these large
antenna arrays incurs a significant beam training overhead, making it
challenging to support applications involving high mobility. In recent years,
machine learning (ML) solutions have shown promising results in reducing the
beam training overhead by utilizing various sensing modalities such as GPS
position and RGB images. However, the existing approaches are mainly limited to
scenarios with only a single object of interest present in the wireless
environment and focus only on co-located sensing, where all the sensors are
installed at the communication terminal. This brings key challenges such as the
limited sensing coverage compared to the coverage of the communication system
and the difficulty in handling non-line-of-sight scenarios. To overcome these
limitations, our paper proposes the deployment of multiple distributed sensing
nodes, each equipped with an RGB camera. These nodes focus on extracting
environmental semantics from the captured RGB images. The semantic data, rather
than the raw images, are then transmitted to the basestation. This strategy
significantly alleviates the overhead associated with the data storage and
transmission of the raw images. Furthermore, semantic communication enhances
the system's adaptability and responsiveness to dynamic environments, allowing
for prioritization and transmission of contextually relevant information.
Experimental results on the DeepSense 6G dataset demonstrate the effectiveness
of the proposed solution in reducing the sensing data transmission overhead
while accurately predicting the optimal beams in realistic communication
environments.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14767" title="Abstract">arXiv:2402.14767</a> [<a href="/pdf/2402.14767" title="Download PDF">pdf</a>, <a href="/format/2402.14767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuhang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present DualFocus, a novel framework for integrating macro and micro
perspectives within multi-modal large language models (MLLMs) to enhance
vision-language task performance. Current MLLMs typically singularly focus on
inputs at a predefined resolution, resulting in deficiencies in detailed
questions involving local regions. We introduced a DualFocus mechanism where
the model concentrates on the image from a macro perspective, responses to the
question, and identifies suitable sub-regions to zoom in for subsequent micro
perspective analysis. Via the integration of answers from both macro and micro
perspectives, the model is adept at addressing tasks that encompass global,
detailed, and combined considerations. To endows the DualFocus mechanism in
MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and
adapted it to align with the training regimen of DualFocus. Through comparative
studies across different model sizes and benchmarks, we demonstrate DualFocus's
superiority in balancing detailed examination with holistic insight,
significantly reducing hallucination instances in MLLMs and improving their
performance in various vision-language tasks.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14768" title="Abstract">arXiv:2402.14768</a> [<a href="/pdf/2402.14768" title="Download PDF">pdf</a>, <a href="/ps/2402.14768" title="Download PostScript">ps</a>, <a href="/format/2402.14768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Hybrid System Dynamics and Discrete Event Simulations to Identify  High Leverage Targets for Process Improvement in a Skill-based Organizational  Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Enos%2C+E">Eric Enos</a>, 
<a href="/search/eess?searchtype=author&query=Herber%2C+D">Daniel Herber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper is based on a case study of an IT organization in a large,
US-based healthcare provider, and develops simluation models to identify areas
for performance improvement. These organizations are often grouped into
departments by technical skill and support both operational work (tickets) and
project work (tasks) of various priorities. From a practical standpoint,
resource managers and staff regularly manage all work as queued and assign /
complete it based on the priorities of the day. Using project and operational
metrics from the case study organization, the hybrid model using both system
dynamics and discrete event simulation developed through this research depicts
the flow of work through a skill-based team as well as many of the key factors
that influence that workflow, both positive and negative. Experience indicates
that the interaction between project and operational work -- as well as between
teams with differing skills -- entangles work queues and wait times within
those queues in a way that rapidly scales in complexity as the number of
interacting individuals and teams increases. Results from model simulation bear
out this intuition. Scaling the models to accommodate multiple teams is a topic
of future research.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14776" title="Abstract">arXiv:2402.14776</a> [<a href="/pdf/2402.14776" title="Download PDF">pdf</a>, <a href="/format/2402.14776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2D Matryoshka Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Common approaches rely on fixed-length embedding vectors from language models
as sentence embeddings for downstream tasks such as semantic textual similarity
(STS). Such methods are limited in their flexibility due to unknown
computational constraints and budgets across various applications. Matryoshka
Representation Learning (MRL) (Kusupati et al., 2022) encodes information at
finer granularities, i.e., with lower embedding dimensions, to adaptively
accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller
embedding size, leading to speedups in downstream tasks. Despite its improved
efficiency, MRL still requires traversing all Transformer layers before
obtaining the embedding, which remains the dominant factor in time and memory
consumption. This prompts consideration of whether the fixed number of
Transformer layers affects representation quality and whether using
intermediate layers for sentence representation is feasible. In this paper, we
introduce a novel sentence embedding model called Two-dimensional Matryoshka
Sentence Embedding (2DMSE). It supports elastic settings for both embedding
sizes and Transformer layers, offering greater flexibility and efficiency than
MRL. We conduct extensive experiments on STS tasks and downstream applications.
The experimental results demonstrate the effectiveness of our proposed model in
dynamically supporting different embedding sizes and Transformer layers,
allowing it to be highly adaptable to various scenarios.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14778" title="Abstract">arXiv:2402.14778</a> [<a href="/pdf/2402.14778" title="Download PDF">pdf</a>, <a href="/format/2402.14778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot cross-lingual transfer in instruction tuning of large language  model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chirkova%2C+N">Nadezhda Chirkova</a>, 
<a href="/search/cs?searchtype=author&query=Nikoulina%2C+V">Vassilina Nikoulina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Instruction tuning (IT) is widely used to teach pretrained large language
models (LLMs) to follow arbitrary instructions, but is under-studied in
multilingual settings. In this work, we conduct a systematic study of zero-shot
cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only
data and then tested on user prompts in other languages. We investigate the
influence of model configuration choices and devise a multi-facet evaluation
strategy for multilingual instruction following. We find that cross-lingual
transfer does happen successfully in IT even if all stages of model training
are English-centric, but only if multiliguality is taken into account in
hyperparameter tuning and with large enough IT data. English-trained LLMs are
capable of generating correct-language, comprehensive and helpful responses in
the other languages, but suffer from low factuality and may occasionally have
fluency errors.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14780" title="Abstract">arXiv:2402.14780</a> [<a href="/pdf/2402.14780" title="Download PDF">pdf</a>, <a href="/format/2402.14780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Customize-A-Video: One-Shot Motion Customization of Text-to-Video  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yixuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jimei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Difan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+M">Mingi Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Abhinav Shrivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://anonymous-314.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image customization has been extensively studied in text-to-image (T2I)
diffusion models, leading to impressive outcomes and applications. With the
emergence of text-to-video (T2V) diffusion models, its temporal counterpart,
motion customization, has not yet been well investigated. To address the
challenge of one-shot motion customization, we propose Customize-A-Video that
models the motion from a single reference video and adapting it to new subjects
and scenes with both spatial and temporal varieties. It leverages low-rank
adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V
diffusion model for specific motion modeling from the reference videos. To
disentangle the spatial and temporal information during the training pipeline,
we introduce a novel concept of appearance absorbers that detach the original
appearance from the single reference video prior to motion learning. Our
proposed method can be easily extended to various downstream tasks, including
custom video generation and editing, video appearance customization, and
multiple motion combination, in a plug-and-play fashion. Our project page can
be found at https://anonymous-314.github.io.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14781" title="Abstract">arXiv:2402.14781</a> [<a href="/pdf/2402.14781" title="Download PDF">pdf</a>, <a href="/ps/2402.14781" title="Download PostScript">ps</a>, <a href="/format/2402.14781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rao-Blackwellising Bayesian Causal Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toth%2C+C">Christian Toth</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+C">Christian Knoll</a>, 
<a href="/search/cs?searchtype=author&query=Pernkopf%2C+F">Franz Pernkopf</a>, 
<a href="/search/cs?searchtype=author&query=Peharz%2C+R">Robert Peharz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages + references + appendices (19 pages total)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian causal inference, i.e., inferring a posterior over causal models for
the use in downstream causal reasoning tasks, poses a hard computational
inference problem that is little explored in literature. In this work, we
combine techniques from order-based MCMC structure learning with recent
advances in gradient-based graph learning into an effective Bayesian causal
inference framework. Specifically, we decompose the problem of inferring the
causal structure into (i) inferring a topological order over variables and (ii)
inferring the parent sets for each variable. When limiting the number of
parents per variable, we can exactly marginalise over the parent sets in
polynomial time. We further use Gaussian processes to model the unknown causal
mechanisms, which also allows their exact marginalisation. This introduces a
Rao-Blackwellization scheme, where all components are eliminated from the
model, except for the causal order, for which we learn a distribution via
gradient-based optimisation. The combination of Rao-Blackwellization with our
sequential inference procedure for causal orders yields state-of-the-art on
linear and non-linear additive noise benchmarks with scale-free and Erdos-Renyi
graph structures.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14789" title="Abstract">arXiv:2402.14789</a> [<a href="/pdf/2402.14789" title="Download PDF">pdf</a>, <a href="/format/2402.14789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Johnathan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+S">Annie S. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Self-supervised learning excels in learning representations from large
amounts of unlabeled data, demonstrating success across multiple data
modalities. Yet, extending self-supervised learning to new modalities is
non-trivial because the specifics of existing methods are tailored to each
domain, such as domain-specific augmentations which reflect the invariances in
the target task. While masked modeling is promising as a domain-agnostic
framework for self-supervised learning because it does not rely on input
augmentations, its mask sampling procedure remains domain-specific. We present
Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling
method. SMA trains an attention based model using a masked modeling objective,
by learning masks to sample without any domain-specific assumptions. We
evaluate SMA on three self-supervised learning benchmarks in protein biology,
chemical property prediction, and particle physics. We find SMA is capable of
learning representations without domain-specific knowledge and achieves
state-of-the-art performance on these three benchmarks.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14792" title="Abstract">arXiv:2402.14792</a> [<a href="/pdf/2402.14792" title="Download PDF">pdf</a>, <a href="/format/2402.14792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consolidating Attention Features for Multi-view Image Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patashnik%2C+O">Or Patashnik</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+R">Rinon Gal</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=De+la+Torre%2C+F">Fernando De la Torre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page at <a href="https://qnerf-consolidation.github.io/qnerf-consolidation/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14795" title="Abstract">arXiv:2402.14795</a> [<a href="/pdf/2402.14795" title="Download PDF">pdf</a>, <a href="/format/2402.14795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CyberDemo: Augmenting Simulated Human Demonstration for Real-World  Dexterous Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yuzhe Qin</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kaiming Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Korkmaz%2C+Y">Yigit Korkmaz</a>, 
<a href="/search/cs?searchtype=author&query=Gurumoorthy%2C+A">Akhilan Gurumoorthy</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We introduce CyberDemo, a novel approach to robotic imitation learning that
leverages simulated human demonstrations for real-world tasks. By incorporating
extensive data augmentation in a simulated environment, CyberDemo outperforms
traditional in-domain real-world demonstrations when transferred to the real
world, handling diverse physical and visual conditions. Regardless of its
affordability and convenience in data collection, CyberDemo outperforms
baseline methods in terms of success rates across various tasks and exhibits
generalizability with previously unseen objects. For example, it can rotate
novel tetra-valve and penta-valve, despite human demonstrations only involving
tri-valves. Our research demonstrates the significant potential of simulated
human demonstrations for real-world dexterous manipulation tasks. More details
can be found at https://cyber-demo.github.io
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14797" title="Abstract">arXiv:2402.14797</a> [<a href="/pdf/2402.14797" title="Download PDF">pdf</a>, <a href="/format/2402.14797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menapace%2C+W">Willi Menapace</a>, 
<a href="/search/cs?searchtype=author&query=Siarohin%2C+A">Aliaksandr Siarohin</a>, 
<a href="/search/cs?searchtype=author&query=Skorokhodov%2C+I">Ivan Skorokhodov</a>, 
<a href="/search/cs?searchtype=author&query=Deyneka%2C+E">Ekaterina Deyneka</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tsai-Shien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kag%2C+A">Anil Kag</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Stoliar%2C+A">Aleksei Stoliar</a>, 
<a href="/search/cs?searchtype=author&query=Ricci%2C+E">Elisa Ricci</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contemporary models for generating images show remarkable quality and
versatility. Swayed by these advantages, the research community repurposes them
to generate videos. Since video content is highly redundant, we argue that
naively bringing advances of image models to the video generation domain
reduces motion fidelity, visual quality and impairs scalability. In this work,
we build Snap Video, a video-first model that systematically addresses these
challenges. To do that, we first extend the EDM framework to take into account
spatially and temporally redundant pixels and naturally support video
generation. Second, we show that a U-Net - a workhorse behind image generation
- scales poorly when generating videos, requiring significant computational
overhead. Hence, we propose a new transformer-based architecture that trains
3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us
to efficiently train a text-to-video model with billions of parameters for the
first time, reach state-of-the-art results on a number of benchmarks, and
generate videos with substantially higher quality, temporal consistency, and
motion complexity. The user studies showed that our model was favored by a
large margin over the most recent methods. See our website at
https://snap-research.github.io/snapvideo/.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14798" title="Abstract">arXiv:2402.14798</a> [<a href="/pdf/2402.14798" title="Download PDF">pdf</a>, <a href="/format/2402.14798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Systematic Decompositional Natural Language Inference Using  Informal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weir%2C+N">Nathaniel Weir</a>, 
<a href="/search/cs?searchtype=author&query=Sanders%2C+K">Kate Sanders</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+O">Orion Weller</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shreya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dongwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+B+D">Bhavana Dalvi Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+P">Peter Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contemporary language models enable new opportunities for structured
reasoning with text, such as the construction and evaluation of intuitive,
proof-like textual entailment trees without relying on brittle formal logic.
However, progress in this direction has been hampered by a long-standing lack
of a clear protocol for determining what valid compositional entailment is.
This absence causes noisy datasets and limited performance gains by modern
neuro-symbolic engines. To address these problems, we formulate a consistent
and theoretically grounded approach to annotating decompositional entailment
datasets, and evaluate its impact on LLM-based textual inference. We find that
our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment),
has a substantially higher internal consistency (+9%) than prior
decompositional entailment datasets, suggesting that RDTE is a significant step
forward in the long-standing problem of forming a clear protocol for discerning
entailment. We also find that training an RDTE-oriented entailment classifier
via knowledge distillation and employing it in a modern neuro-symbolic
reasoning engine significantly improves results (both accuracy and proof
quality) over other entailment classifier baselines, illustrating the practical
benefit of this advance for textual inference.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14800" title="Abstract">arXiv:2402.14800</a> [<a href="/pdf/2402.14800" title="Download PDF">pdf</a>, <a href="/format/2402.14800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Experts are Equal: Efficient Expert Pruning and Skipping for  Mixture-of-Experts Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xudong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuhui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aojun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Mixture-of-Experts Large Language Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A pivotal advancement in the progress of large language models (LLMs) is the
emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,
MoE LLMs can achieve higher performance with fewer parameters, but it is still
hard to deploy them due to their immense parameter sizes. Different from
previous weight pruning methods that rely on specifically designed hardware,
this paper mainly aims to enhance the deployment efficiency of MoE LLMs by
introducing plug-and-play expert-level sparsification techniques. Specifically,
we propose, for the first time to our best knowledge, post-training approaches
for task-agnostic and task-specific expert pruning and skipping of MoE LLMs,
tailored to improve deployment efficiency while maintaining model performance
across a wide range of tasks. Extensive experiments show that our proposed
methods can simultaneously reduce model sizes and increase the inference speed,
while maintaining satisfactory performance. Data and code will be available at
https://github.com/Lucky-Lance/Expert_Sparsity.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14801" title="Abstract">arXiv:2402.14801</a> [<a href="/pdf/2402.14801" title="Download PDF">pdf</a>, <a href="/format/2402.14801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mochi: Fast \&amp; Exact Collision Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandarapu%2C+D+K">Durga Keerthi Mandarapu</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+N">Nicholas James</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+M">Milind Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Collision Detection (CD) has several applications across the domains such as
robotics, visual graphics, and fluid mechanics.
<br />Finding exact collisions between the objects in the scene is quite
computationally intensive.
<br />To quickly filter the object pairs that do not result in a collision,
bounding boxes are built on the objects, indexed using a Bounding Volume
Hierarchy(BVH), and tested for intersection before performing the expensive
object-object intersection tests.
<br />In state-of-the-art CD libraries, accelerators such as GPUs are used to
accelerate BVH traversal by building specialized data structures.
<br />The recent addition of ray tracing architecture to GPU hardware is designed
to do the same but in the context of implementing a Ray Tracing algorithm to
render a graphical scene in real-time.
<br />We present Mochi, a fast and exact collision detection engine that
accelerates both the broad and narrow phases by taking advantage of the
capabilities of Ray Tracing cores.
<br />We introduce multiple new reductions to perform generic CD to support three
types of objects for CD: simple spherical particles, objects describable by
mathematical equations, and complex objects composed of a triangle mesh.
<br />By implementing our reductions, Mochi achieves several orders of magnitude
speedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh
datasets.
<br />We further evaluate our reductions thoroughly and provide several
architectural insights on the ray tracing cores that are otherwise unknown due
to their proprietorship.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14802" title="Abstract">arXiv:2402.14802</a> [<a href="/pdf/2402.14802" title="Download PDF">pdf</a>, <a href="/format/2402.14802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link Prediction under Heterophily: A Physics-Inspired Graph Neural  Network Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Francesco%2C+A+G">Andrea Giuseppe Di Francesco</a>, 
<a href="/search/cs?searchtype=author&query=Caso%2C+F">Francesco Caso</a>, 
<a href="/search/cs?searchtype=author&query=Bucarelli%2C+M+S">Maria Sofia Bucarelli</a>, 
<a href="/search/cs?searchtype=author&query=Silvestri%2C+F">Fabrizio Silvestri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In the past years, Graph Neural Networks (GNNs) have become the `de facto'
standard in various deep learning domains, thanks to their flexibility in
modeling real-world phenomena represented as graphs. However, the
message-passing mechanism of GNNs faces challenges in learnability and
expressivity, hindering high performance on heterophilic graphs, where adjacent
nodes frequently have different labels. Most existing solutions addressing
these challenges are primarily confined to specific benchmarks focused on node
classification tasks. This narrow focus restricts the potential impact that
link prediction under heterophily could offer in several applications,
including recommender systems. For example, in social networks, two users may
be connected for some latent reason, making it challenging to predict such
connections in advance. Physics-Inspired GNNs such as GRAFF provided a
significant contribution to enhance node classification performance under
heterophily, thanks to the adoption of physics biases in the message-passing.
Drawing inspiration from these findings, we advocate that the methodology
employed by GRAFF can improve link prediction performance as well. To further
explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link
prediction. We evaluate its efficacy within a recent collection of heterophilic
graphs, establishing a new benchmark for link prediction under heterophily. Our
approach surpasses previous methods, in most of the datasets, showcasing a
strong flexibility in different contexts, and achieving relative AUROC
improvements of up to 26.7%.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14804" title="Abstract">arXiv:2402.14804</a> [<a href="/pdf/2402.14804" title="Download PDF">pdf</a>, <a href="/format/2402.14804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weikang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zimu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); History and Overview (math.HO)

</div>
<p class="mathjax">Recent advancements in Large Multimodal Models (LMMs) have shown promising
results in mathematical reasoning within visual contexts, with models
approaching human-level performance on existing benchmarks such as MathVista.
However, we observe significant limitations in the diversity of questions and
breadth of subjects covered by these benchmarks. To address this issue, we
present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of
3,040 high-quality mathematical problems with visual contexts sourced from real
math competitions. Spanning 16 distinct mathematical disciplines and graded
across 5 levels of difficulty, our dataset provides a comprehensive and diverse
set of challenges for evaluating the mathematical reasoning abilities of LMMs.
Through extensive experimentation, we unveil a notable performance gap between
current LMMs and human performance on MATH-V, underscoring the imperative for
further advancements in LMMs. Moreover, our detailed categorization allows for
a thorough error analysis of LMMs, offering valuable insights to guide future
research and development. The project is available at
https://mathvision-cuhk.github.io
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14805" title="Abstract">arXiv:2402.14805</a> [<a href="/pdf/2402.14805" title="Download PDF">pdf</a>, <a href="/format/2402.14805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Multiple Personalities in Large Language Models with  External Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Adachi%2C+Y">Yuta Adachi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jessie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Mouwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Linhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Frank Li</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Akshat Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+S">Simerjot Kaur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Large Language Models (LLMs) are integrated with human daily applications
rapidly, many societal and ethical concerns are raised regarding the behavior
of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their
personalities. Many recent studies quantify LLMs' personalities using
self-assessment tests that are created for humans. Yet many critiques question
the applicability and reliability of these self-assessment tests when applied
to LLMs. In this paper, we investigate LLM personalities using an alternate
personality measurement method, which we refer to as the external evaluation
method, where instead of prompting LLMs with multiple-choice questions in the
Likert scale, we evaluate LLMs' personalities by analyzing their responses
toward open-ended situational questions using an external machine learning
model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor
that outperforms the state-of-the-art models as the tool to analyze LLMs'
responses. Then, we prompt the LLMs with situational questions and ask them to
generate Twitter posts and comments, respectively, in order to assess their
personalities when playing two different roles. Using the external personality
evaluation method, we identify that the obtained personality types for LLMs are
significantly different when generating posts versus comments, whereas humans
show a consistent personality profile in these two different situations. This
shows that LLMs can exhibit different personalities based on different
scenarios, thus highlighting a fundamental difference between personality in
LLMs and humans. With our work, we call for a re-evaluation of personality
definition and measurement in LLMs.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14806" title="Abstract">arXiv:2402.14806</a> [<a href="/pdf/2402.14806" title="Download PDF">pdf</a>, <a href="/format/2402.14806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Difference Learning for Air Quality Forecasting Transport Emulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+R">Reed River Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ribaudo%2C+C">Christopher Ribaudo</a>, 
<a href="/search/cs?searchtype=author&query=Sleeman%2C+J">Jennifer Sleeman</a>, 
<a href="/search/cs?searchtype=author&query=Ashcraft%2C+C">Chace Ashcraft</a>, 
<a href="/search/cs?searchtype=author&query=Kofroth%2C+C">Collin Kofroth</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+M">Marisa Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Stajner%2C+I">Ivanka Stajner</a>, 
<a href="/search/cs?searchtype=author&query=Viner%2C+K">Kevin Viner</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Human health is negatively impacted by poor air quality including increased
risk for respiratory and cardiovascular disease. Due to a recent increase in
extreme air quality events, both globally and locally in the United States,
finer resolution air quality forecasting guidance is needed to effectively
adapt to these events. The National Oceanic and Atmospheric Administration
provides air quality forecasting guidance for the Continental United States.
Their air quality forecasting model is based on a 15 km spatial resolution;
however, the goal is to reach a three km spatial resolution. This is currently
not feasible due in part to prohibitive computational requirements for modeling
the transport of chemical species. In this work, we describe a deep learning
transport emulator that is able to reduce computations while maintaining skill
comparable with the existing numerical model. We show how this method maintains
skill in the presence of extreme air quality events, making it a potential
candidate for operational use. We also explore evaluating how well this model
maintains the physical properties of the modeled transport for a given set of
species.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14807" title="Abstract">arXiv:2402.14807</a> [<a href="/pdf/2402.14807" title="Download PDF">pdf</a>, <a href="/format/2402.14807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit  Tasks in Public Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behari%2C+N">Nikhil Behari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Edwin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aparna Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14808" title="Abstract">arXiv:2402.14808</a> [<a href="/pdf/2402.14808" title="Download PDF">pdf</a>, <a href="/format/2402.14808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RelayAttention for Efficient Large Language Model Serving with Long  System Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinjiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wayne Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+R+W+H">Rynson W.H. Lau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Practical large language model (LLM) services may involve a long system
prompt, which specifies the instructions, examples, and knowledge documents of
the task and is reused across numerous requests. However, the long system
prompt causes throughput/latency bottlenecks as the cost of generating the next
token grows w.r.t. the sequence length. This paper aims to improve the
efficiency of LLM services that involve long system prompts. Our key
observation is that handling these system prompts requires heavily redundant
memory accesses in existing causal attention computation algorithms.
Specifically, for batched requests, the cached hidden states (i.e., key-value
pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM
multiple times, each corresponding to an individual request. To eliminate such
a redundancy, we propose RelayAttention, an attention algorithm that allows
reading these hidden states from DRAM exactly once for a batch of input tokens.
RelayAttention is a free lunch: it maintains the generation quality while
requiring no model retraining, as it is based on a mathematical reformulation
of causal attention.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14809" title="Abstract">arXiv:2402.14809</a> [<a href="/pdf/2402.14809" title="Download PDF">pdf</a>, <a href="/format/2402.14809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zicheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Ruilin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability of Large Language Models (LLMs) to critique and refine their
reasoning is crucial for their application in evaluation, feedback provision,
and self-improvement. This paper introduces CriticBench, a comprehensive
benchmark designed to assess LLMs' abilities to critique and rectify their
reasoning across a variety of tasks. CriticBench encompasses five reasoning
domains: mathematical, commonsense, symbolic, coding, and algorithmic. It
compiles 15 datasets and incorporates responses from three LLM families.
Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in
generation, critique, and correction reasoning, i.e., GQC reasoning. Our
findings reveal: (1) a linear relationship in GQC capabilities, with
critique-focused training markedly enhancing performance; (2) a task-dependent
variation in correction effectiveness, with logic-oriented tasks being more
amenable to correction; (3) GQC knowledge inconsistencies that decrease as
model size increases; and (4) an intriguing inter-model critiquing dynamic,
where stronger models are better at critiquing weaker ones, while weaker models
can surprisingly surpass stronger ones in their self-critique. We hope these
insights into the nuanced critique-correct reasoning of LLMs will foster
further research in LLM critique and self-improvement.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14810" title="Abstract">arXiv:2402.14810</a> [<a href="/pdf/2402.14810" title="Download PDF">pdf</a>, <a href="/format/2402.14810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeneOH Diffusion: Towards Generalizable Hand-Object Interaction  Denoising via Denoising Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xueyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Li Yi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Project website: <a href="https://meowuu7.github.io/GeneOH-Diffusion/">this https URL</a>; Huggingface Demo: <a href="https://huggingface.co/spaces/xymeow7/gene-hoi-denoising">this https URL</a>; Code: <a href="https://github.com/Meowuu7/GeneOH-Diffusion">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we tackle the challenging problem of denoising hand-object
interactions (HOI). Given an erroneous interaction sequence, the objective is
to refine the incorrect hand trajectory to remove interaction artifacts for a
perceptually realistic sequence. This challenge involves intricate interaction
noise, including unnatural hand poses and incorrect hand-object relations,
alongside the necessity for robust generalization to new interactions and
diverse noise patterns. We tackle those challenges through a novel approach,
GeneOH Diffusion, incorporating two key designs: an innovative contact-centric
HOI representation named GeneOH and a new domain-generalizable denoising
scheme. The contact-centric representation GeneOH informatively parameterizes
the HOI process, facilitating enhanced generalization across various HOI
scenarios. The new denoising scheme consists of a canonical denoising model
trained to project noisy data samples from a whitened noise space to a clean
data manifold and a "denoising via diffusion" strategy which can handle input
trajectories with various noise patterns by first diffusing them to align with
the whitened noise space and cleaning via the canonical denoiser. Extensive
experiments on four benchmarks with significant domain variations demonstrate
the superior effectiveness of our method. GeneOH Diffusion also shows promise
for various downstream applications. Project website:
https://meowuu7.github.io/GeneOH-Diffusion/.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14811" title="Abstract">arXiv:2402.14811</a> [<a href="/pdf/2402.14811" title="Download PDF">pdf</a>, <a href="/format/2402.14811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prakash%2C+N">Nikhil Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Shaham%2C+T+R">Tamar Rott Shaham</a>, 
<a href="/search/cs?searchtype=author&query=Haklay%2C+T">Tal Haklay</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. 26 pages, 13 figures. Code and data at <a href="https://finetuning.baulab.info/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fine-tuning on generalized tasks such as instruction following, code
generation, and mathematics has been shown to enhance language models'
performance on a range of tasks. Nevertheless, explanations of how such
fine-tuning influences the internal computations in these models remain
elusive. We study how fine-tuning affects the internal mechanisms implemented
in language models. As a case study, we explore the property of entity
tracking, a crucial facet of language comprehension, where models fine-tuned on
mathematics have substantial performance gains. We identify the mechanism that
enables entity tracking and show that (i) in both the original model and its
fine-tuned versions primarily the same circuit implements entity tracking. In
fact, the entity tracking circuit of the original model on the fine-tuned
versions performs better than the full original model. (ii) The circuits of all
the models implement roughly the same functionality: Entity tracking is
performed by tracking the position of the correct entity in both the original
model and its fine-tuned versions. (iii) Performance boost in the fine-tuned
models is primarily attributed to its improved ability to handle the augmented
positional information. To uncover these findings, we employ: Patch Patching,
DCM, which automatically detects model components responsible for specific
semantics, and CMAP, a new approach for patching activations across models to
reveal improved mechanisms. Our findings suggest that fine-tuning enhances,
rather than fundamentally alters, the mechanistic operation of the model.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14812" title="Abstract">arXiv:2402.14812</a> [<a href="/pdf/2402.14812" title="Download PDF">pdf</a>, <a href="/format/2402.14812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WeakSAM: Segment Anything Meets Weakly-supervised Instance-level  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lianghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Junwei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xin Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/hustvl/WeakSAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Weakly supervised visual recognition using inexact supervision is a critical
yet challenging learning problem. It significantly reduces human labeling costs
and traditionally relies on multi-instance learning and pseudo-labeling. This
paper introduces WeakSAM and solves the weakly-supervised object detection
(WSOD) and segmentation by utilizing the pre-learned world knowledge contained
in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM
addresses two critical limitations in traditional WSOD retraining, i.e., pseudo
ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT
generation and Region of Interest (RoI) drop regularization. It also addresses
the SAM's problems of requiring prompts and category unawareness for automatic
object detection and segmentation. Our results indicate that WeakSAM
significantly surpasses previous state-of-the-art methods in WSOD and WSIS
benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,
respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14815" title="Abstract">arXiv:2402.14815</a> [<a href="/pdf/2402.14815" title="Download PDF">pdf</a>, <a href="/ps/2402.14815" title="Download PostScript">ps</a>, <a href="/format/2402.14815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demographic Bias of Expert-Level Vision-Language Foundation Models in  Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuzhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yujia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gulhane%2C+A">Avanti Gulhane</a>, 
<a href="/search/cs?searchtype=author&query=Mastrodicasa%2C+D">Domenico Mastrodicasa</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+E+J">Edward J Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sahani%2C+D+W">Dushyant W Sahani</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shwetak Patel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data are available at <a href="https://github.com/YyzHarry/vlm-fairness">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Advances in artificial intelligence (AI) have achieved expert-level
performance in medical imaging applications. Notably, self-supervised
vision-language foundation models can detect a broad spectrum of pathologies
without relying on explicit training annotations. However, it is crucial to
ensure that these AI models do not mirror or amplify human biases, thereby
disadvantaging historically marginalized groups such as females or Black
patients. The manifestation of such biases could systematically delay essential
medical care for certain patient subgroups. In this study, we investigate the
algorithmic fairness of state-of-the-art vision-language foundation models in
chest X-ray diagnosis across five globally-sourced datasets. Our findings
reveal that compared to board-certified radiologists, these foundation models
consistently underdiagnose marginalized groups, with even higher rates seen in
intersectional subgroups, such as Black female patients. Such demographic
biases present over a wide range of pathologies and demographic attributes.
Further analysis of the model embedding uncovers its significant encoding of
demographic information. Deploying AI systems with these biases in medical
imaging can intensify pre-existing care disparities, posing potential
challenges to equitable healthcare access and raising ethical questions about
their clinical application.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14817" title="Abstract">arXiv:2402.14817</a> [<a href="/pdf/2402.14817" title="Download PDF">pdf</a>, <a href="/format/2402.14817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cameras as Rays: Pose Estimation via Ray Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">Jason Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+A">Amy Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Moneish Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tzu-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Tulsiani%2C+S">Shubham Tulsiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ICLR 2024 (oral). Project webpage: <a href="https://jasonyzhang.com/RayDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Estimating camera poses is a fundamental task for 3D reconstruction and
remains challenging given sparse views (&lt;10). In contrast to existing
approaches that pursue top-down prediction of global parametrizations of camera
extrinsics, we propose a distributed representation of camera pose that treats
a camera as a bundle of rays. This representation allows for a tight coupling
with spatial image features improving pose precision. We observe that this
representation is naturally suited for set-level level transformers and develop
a regression-based approach that maps image patches to corresponding rays. To
capture the inherent uncertainties in sparse-view pose inference, we adapt this
approach to learn a denoising diffusion model which allows us to sample
plausible modes while improving performance. Our proposed methods, both
regression- and diffusion-based, demonstrate state-of-the-art performance on
camera pose estimation on CO3D while generalizing to unseen object categories
and in-the-wild captures.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14818" title="Abstract">arXiv:2402.14818</a> [<a href="/pdf/2402.14818" title="Download PDF">pdf</a>, <a href="/format/2402.14818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PALO: A Polyglot Large Multimodal Model for 5B People
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maaz%2C+M">Muhammad Maaz</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+H">Hanoona Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Shaker%2C+A">Abdelrahman Shaker</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Cholakal%2C+H">Hisham Cholakal</a>, 
<a href="/search/cs?searchtype=author&query=Anwer%2C+R+M">Rao M. Anwer</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Tim Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad S. Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report of PALO
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 23 Feb 24</h3>
<dl>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13794" title="Abstract">arXiv:2402.13794</a> (cross-list from math.OC) [<a href="/pdf/2402.13794" title="Download PDF">pdf</a>, <a href="/ps/2402.13794" title="Download PostScript">ps</a>, <a href="/format/2402.13794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Convergence of AdaGrad with Relaxed Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hong%2C+Y">Yusu Hong</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+J">Junhong Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this study, we revisit the convergence of AdaGrad with momentum (covering
AdaGrad as a special case) on non-convex smooth optimization problems. We
consider a general noise model where the noise magnitude is controlled by the
function value gap together with the gradient magnitude. This model encompasses
a broad range of noises including bounded noise, sub-Gaussian noise, affine
variance noise and the expected smoothness, and it has been shown to be more
realistic in many practical applications. Our analysis yields a probabilistic
convergence rate which, under the general noise, could reach at
(\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge
of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where
(T) denotes the total number iterations, when the noise parameters related to
the function value gap and noise level are sufficiently small. The convergence
rate thus matches the lower rate for stochastic first-order methods over
non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We
further derive a convergence bound for AdaGrad with mometum, considering the
generalized smoothness where the local smoothness is controlled by a
first-order function of the gradient norm.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14022" title="Abstract">arXiv:2402.14022</a> (cross-list from eess.IV) [<a href="/pdf/2402.14022" title="Download PDF">pdf</a>, <a href="/format/2402.14022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical validation of a deep learning algorithm for dental anomaly  detection in intraoral radiographs using paired data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Van+Leemput%2C+P">Pieter Van Leemput</a>, 
<a href="/search/eess?searchtype=author&query=Keustermans%2C+J">Johannes Keustermans</a>, 
<a href="/search/eess?searchtype=author&query=Mollemans%2C+W">Wouter Mollemans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">This article describes the clinical validation study setup, statistical
analysis and results for a deep learning algorithm which detects dental
anomalies in intraoral radiographic images, more specifically caries, apical
lesions, root canal treatment defects, marginal defects at crown restorations,
periodontal bone loss and calculus. The study compares the detection
performance of dentists using the deep learning algorithm to the prior
performance of these dentists evaluating the images without algorithmic
assistance. Calculating the marginal profit and loss of performance from the
annotated paired image data allows for a quantification of the hypothesized
change in sensitivity and specificity. The statistical significance of these
results is extensively proven using both McNemar's test and the binomial
hypothesis test. The average sensitivity increases from $60.7\%$ to $85.9\%$,
while the average specificity slightly decreases from $94.5\%$ to $92.7\%$. We
prove that the increase of the area under the localization ROC curve (AUC) is
significant (from $0.60$ to $0.86$ on average), while the average AUC is
bounded by the $95\%$ confidence intervals ${[}0.54, 0.65{]}$ and ${[}0.82,
0.90{]}$. When using the deep learning algorithm for diagnostic guidance, the
dentist can be $95\%$ confident that the average true population sensitivity is
bounded by the range $79.6\%$ to $91.9\%$. The proposed paired data setup and
statistical analysis can be used as a blueprint to thoroughly test the effect
of a modality change, like a deep learning based detection and/or segmentation,
on radiographic images.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14026" title="Abstract">arXiv:2402.14026</a> (cross-list from math.ST) [<a href="/pdf/2402.14026" title="Download PDF">pdf</a>, <a href="/ps/2402.14026" title="Download PostScript">ps</a>, <a href="/format/2402.14026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probability Tools for Sequential Random Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yingru Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Numerical Analysis (math.NA); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce the first probabilistic framework tailored for sequential random
projection, an approach rooted in the challenges of sequential decision-making
under uncertainty. The analysis is complicated by the sequential dependence and
high-dimensional nature of random variables, a byproduct of the adaptive
mechanisms inherent in sequential decision processes. Our work features a novel
construction of a stopped process, facilitating the analysis of a sequence of
concentration events that are interconnected in a sequential manner. By
employing the method of mixtures within a self-normalized process, derived from
the stopped process, we achieve a desired non-asymptotic probability bound.
This bound represents a non-trivial martingale extension of the
Johnson-Lindenstrauss (JL) lemma, marking a pioneering contribution to the
literature on random projection and sequential analysis.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14045" title="Abstract">arXiv:2402.14045</a> (cross-list from eess.IV) [<a href="/pdf/2402.14045" title="Download PDF">pdf</a>, <a href="/format/2402.14045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical  Imaging: A Systematic Literature Review and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hamlomo%2C+S">Sisipho Hamlomo</a>, 
<a href="/search/eess?searchtype=author&query=Atemkeng%2C+M">Marcellin Atemkeng</a>, 
<a href="/search/eess?searchtype=author&query=Brima%2C+Y">Yusuf Brima</a>, 
<a href="/search/eess?searchtype=author&query=Nunhokee%2C+C">Chuneeta Nunhokee</a>, 
<a href="/search/eess?searchtype=author&query=Baxter%2C+J">Jeremy Baxter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The large volume and complexity of medical imaging datasets are bottlenecks
for storage, transmission, and processing. To tackle these challenges, the
application of low-rank matrix approximation (LRMA) and its derivative, local
LRMA (LLRMA) has demonstrated potential.
<br />This paper conducts a systematic literature review to showcase works applying
LRMA and LLRMA in medical imaging. A detailed analysis of the literature
identifies LRMA and LLRMA methods applied to various imaging modalities. This
paper addresses the challenges and limitations associated with existing LRMA
and LLRMA methods.
<br />We note a significant shift towards a preference for LLRMA in the medical
imaging field since 2015, demonstrating its potential and effectiveness in
capturing complex structures in medical data compared to LRMA. Acknowledging
the limitations of shallow similarity methods used with LLRMA, we suggest
advanced semantic image segmentation for similarity measure, explaining in
detail how it can measure similar patches and their feasibility.
<br />We note that LRMA and LLRMA are mainly applied to unstructured medical data,
and we propose extending their application to different medical data types,
including structured and semi-structured. This paper also discusses how LRMA
and LLRMA can be applied to regular data with missing entries and the impact of
inaccuracies in predicting missing values and their effects. We discuss the
impact of patch size and propose the use of random search (RS) to determine the
optimal patch size. To enhance feasibility, a hybrid approach using Bayesian
optimization and RS is proposed, which could improve the application of LRMA
and LLRMA in medical imaging.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14053" title="Abstract">arXiv:2402.14053</a> (cross-list from math.CO) [<a href="/pdf/2402.14053" title="Download PDF">pdf</a>, <a href="/format/2402.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-adhesivity in lattices of abstract conditional independence models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boege%2C+T">Tobias Boege</a>, 
<a href="/search/math?searchtype=author&query=Bolt%2C+J+H">Janneke H. Bolt</a>, 
<a href="/search/math?searchtype=author&query=Studen%C3%BD%2C+M">Milan Studen&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We introduce an algebraic concept of the frame for abstract conditional
independence (CI) models, together with basic operations with respect to which
such a frame should be closed: copying and marginalization. Three standard
examples of such frames are (discrete) probabilistic CI structures,
semi-graphoids and structural semi-graphoids. We concentrate on those frames
which are closed under the operation of set-theoretical intersection because,
for these, the respective families of CI models are lattices. This allows one
to apply the results from lattice theory and formal concept analysis to
describe such families in terms of implications among CI statements.
<br />The central concept of this paper is that of self-adhesivity defined in
algebraic terms, which is a combinatorial reflection of the self-adhesivity
concept studied earlier in context of polymatroids and information theory. The
generalization also leads to a self-adhesivity operator defined on the
hyper-level of CI frames. We answer some of the questions related to this
approach and raise other open questions.
<br />The core of the paper is in computations. The combinatorial approach to
computation might overcome some memory and space limitation of software
packages based on polyhedral geometry, in particular, if SAT solvers are
utilized. We characterize some basic CI families over 4 variables in terms of
canonical implications among CI statements. We apply our method in
information-theoretical context to the task of entropic region demarcation over
5 variables.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14099" title="Abstract">arXiv:2402.14099</a> (cross-list from eess.IV) [<a href="/pdf/2402.14099" title="Download PDF">pdf</a>, <a href="/format/2402.14099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell  lung cancer radiotherapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hooshangnejad%2C+H">Hamed Hooshangnejad</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+X">Xue Feng</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+G">Gaofeng Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Q">Quan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+K">Kai Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Lung cancer is a devastating disease with the highest mortality rate among
cancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which
accounts for 87% of diagnoses, require radiation therapy. Rapid treatment
initiation significantly increases the patient's survival rate and reduces the
mortality rate. Accurate tumor segmentation is a critical step in the diagnosis
and treatment of NSCLC. Manual segmentation is time and labor-consuming and
causes delays in treatment initiation. Although many lung nodule detection
methods, including deep learning-based models, have been proposed, there is
still a long-standing problem of high false positives (FPs) with most of these
methods. Here, we developed an electronic health record (EHR) guided lung tumor
auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor
segmentation), where the extracted information from EHRs using a pre-trained
large language model (LLM), was used to remove the FPs and keep the TP nodules
only. The auto-segmentation model was trained on NSCLC patients' computed
tomography (CT), and the pre-trained LLM was used with the zero-shot learning
approach. Our approach resulted in a 250% boost in successful nodule detection
using the data from ten NSCLC patients treated in our institution.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14102" title="Abstract">arXiv:2402.14102</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.14102" title="Download PDF">pdf</a>, <a href="/format/2402.14102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning dynamic representations of the functional connectome in  neurobiological networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dyballa%2C+L">Luciano Dyballa</a>, 
<a href="/search/q-bio?searchtype=author&query=Lang%2C+S">Samuel Lang</a>, 
<a href="/search/q-bio?searchtype=author&query=Haslund-Gourley%2C+A">Alexandra Haslund-Gourley</a>, 
<a href="/search/q-bio?searchtype=author&query=Yemini%2C+E">Eviatar Yemini</a>, 
<a href="/search/q-bio?searchtype=author&query=Zucker%2C+S+W">Steven W. Zucker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The static synaptic connectivity of neuronal circuits stands in direct
contrast to the dynamics of their function. As in changing community
interactions, different neurons can participate actively in various
combinations to effect behaviors at different times. We introduce an
unsupervised approach to learn the dynamic affinities between neurons in live,
behaving animals, and to reveal which communities form among neurons at
different times. The inference occurs in two major steps. First, pairwise
non-linear affinities between neuronal traces from brain-wide calcium activity
are organized by non-negative tensor factorization (NTF). Each factor specifies
which groups of neurons are most likely interacting for an inferred interval in
time, and for which animals. Finally, a generative model that allows for
weighted community detection is applied to the functional motifs produced by
NTF to reveal a dynamic functional connectome. Since time codes the different
experimental variables (e.g., application of chemical stimuli), this provides
an atlas of neural motifs active during separate stages of an experiment (e.g.,
stimulus application or spontaneous behaviors). Results from our analysis are
experimentally validated, confirming that our method is able to robustly
predict causal interactions between neurons to generate behavior. Code is
available at https://github.com/dyballa/dynamic-connectomes.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14131" title="Abstract">arXiv:2402.14131</a> (cross-list from eess.SP) [<a href="/pdf/2402.14131" title="Download PDF">pdf</a>, <a href="/format/2402.14131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random forests for detecting weak signals and extracting physical  information: a case study of magnetic navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi%2C+M">Mohammadamin Moradi</a>, 
<a href="/search/eess?searchtype=author&query=Zhai%2C+Z">Zheng-Meng Zhai</a>, 
<a href="/search/eess?searchtype=author&query=Nielsen%2C+A">Aaron Nielsen</a>, 
<a href="/search/eess?searchtype=author&query=Lai%2C+Y">Ying-Cheng Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">It was recently demonstrated that two machine-learning architectures,
reservoir computing and time-delayed feed-forward neural networks, can be
exploited for detecting the Earth's anomaly magnetic field immersed in
overwhelming complex signals for magnetic navigation in a GPS-denied
environment. The accuracy of the detected anomaly field corresponds to a
positioning accuracy in the range of 10 to 40 meters. To increase the accuracy
and reduce the uncertainty of weak signal detection as well as to directly
obtain the position information, we exploit the machine-learning model of
random forests that combines the output of multiple decision trees to give
optimal values of the physical quantities of interest. In particular, from
time-series data gathered from the cockpit of a flying airplane during various
maneuvering stages, where strong background complex signals are caused by other
elements of the Earth's magnetic field and the fields produced by the
electronic systems in the cockpit, we demonstrate that the random-forest
algorithm performs remarkably well in detecting the weak anomaly field and in
filtering the position of the aircraft. With the aid of the conventional
inertial navigation system, the positioning error can be reduced to less than
10 meters. We also find that, contrary to the conventional wisdom, the classic
Tolles-Lawson model for calibrating and removing the magnetic field generated
by the body of the aircraft is not necessary and may even be detrimental for
the success of the random-forest method.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14145" title="Abstract">arXiv:2402.14145</a> (cross-list from stat.ML) [<a href="/pdf/2402.14145" title="Download PDF">pdf</a>, <a href="/format/2402.14145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiply Robust Estimation for Local Distribution Shifts with Multiple  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wilkins-Reeves%2C+S">Steven Wilkins-Reeves</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+Q">Qi Ma</a>, 
<a href="/search/stat?searchtype=author&query=Agarwal%2C+C">Christine Agarwal</a>, 
<a href="/search/stat?searchtype=author&query=Hofleitner%2C+A">Aude Hofleitner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Distribution shifts are ubiquitous in real-world machine learning
applications, posing a challenge to the generalization of models trained on one
data distribution to another. We focus on scenarios where data distributions
vary across multiple segments of the entire population and only make local
assumptions about the differences between training and test (deployment)
distributions within each segment. We propose a two-stage multiply robust
estimation method to improve model performance on each individual segment for
tabular data analysis. The method involves fitting a linear combination of the
based models, learned using clusters of training data from multiple segments,
followed by a refinement step for each segment. Our method is designed to be
implemented with commonly used off-the-shelf machine learning models. We
establish theoretical guarantees on the generalization bound of the method on
the test risk. With extensive experiments on synthetic and real datasets, we
demonstrate that the proposed method substantially improves over existing
alternatives in prediction accuracy and robustness on both regression and
classification tasks. We also assess its effectiveness on a user city
prediction dataset from a large technology company.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14148" title="Abstract">arXiv:2402.14148</a> (cross-list from physics.geo-ph) [<a href="/pdf/2402.14148" title="Download PDF">pdf</a>, <a href="/format/2402.14148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks and Friction: Slide, Hold, Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Garcia-Suarez%2C+J">Joaquin Garcia-Suarez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 paged, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, it is demonstrated that Recurrent Neural Networks (RNNs),
specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess
the capability to learn the complex dynamics of rate-and-state friction laws
from synthetic data. The data employed for training the network is generated
through the application of traditional rate-and-state friction equations
coupled with the aging law for state evolution. A novel aspect of our approach
is the formulation of a loss function that explicitly accounts for initial
conditions, the direct effect, and the evolution of state variables during
training. It is found that the RNN, with its GRU architecture, effectively
learns to predict changes in the friction coefficient resulting from velocity
jumps, thereby showcasing the potential of machine learning models in
understanding and simulating the physics of frictional processes.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14181" title="Abstract">arXiv:2402.14181</a> (cross-list from math.CO) [<a href="/pdf/2402.14181" title="Download PDF">pdf</a>, <a href="/format/2402.14181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid Minors and Products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dujmovi%C4%87%2C+V">Vida Dujmovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Morin%2C+P">Pat Morin</a>, 
<a href="/search/math?searchtype=author&query=Wood%2C+D">David Wood</a>, 
<a href="/search/math?searchtype=author&query=Worley%2C+D">David Worley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Motivated by recent developments regarding the product structure of planar
graphs, we study relationships between treewidth, grid minors, and graph
products. We show that the Cartesian product of any two connected $n$-vertex
graphs contains an $\Omega(\sqrt{n})\times\Omega(\sqrt{n})$ grid minor. This
result is tight: The lexicographic product (which includes the Cartesian
product as a subgraph) of a star and any $n$-vertex tree has no
$\omega(\sqrt{n})\times\omega(\sqrt{n})$ grid minor.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14189" title="Abstract">arXiv:2402.14189</a> (cross-list from econ.GN) [<a href="/pdf/2402.14189" title="Download PDF">pdf</a>, <a href="/format/2402.14189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal transmission expansion minimally reduces decarbonization costs  of U.S. electricity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Zheng%2C+R">Rangrang Zheng</a>, 
<a href="/search/econ?searchtype=author&query=Schivley%2C+G">Greg Schivley</a>, 
<a href="/search/econ?searchtype=author&query=Hidalgo-Gonzalez%2C+P">Patricia Hidalgo-Gonzalez</a>, 
<a href="/search/econ?searchtype=author&query=Fripp%2C+M">Matthias Fripp</a>, 
<a href="/search/econ?searchtype=author&query=Roberts%2C+M+J">Michael J. Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures in main paper. Additional 11 pages including 7 additional figures and one table in the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Solar and wind power are cost-competitive with fossil fuels, yet their
intermittent nature presents challenges. Significant temporal and geographic
differences in land, wind, and solar resources suggest that long-distance
transmission could be particularly beneficial. Using a detailed, open-source
model, we analyze optimal transmission expansion jointly with storage,
generation, and hourly operations across the three primary interconnects in the
United States. Transmission expansion offers far more benefits in a
high-renewable system than in a system with mostly conventional generation. Yet
while an optimal nationwide plan would have more than triple current
interregional transmission, transmission decreases the cost of a 100% clean
system by only 4% compared to a plan that relies solely on current
transmission. Expanding capacity only within existing interconnects can achieve
most of these savings. Adjustments to energy storage and generation mix can
leverage the current interregional transmission infrastructure to build a clean
power system at a reasonable cost.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14213" title="Abstract">arXiv:2402.14213</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.14213" title="Download PDF">pdf</a>, <a href="/ps/2402.14213" title="Download PostScript">ps</a>, <a href="/format/2402.14213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning of Shared Spatiotemporal EEG Representations Across  Individuals for Naturalistic Neuroscience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Shen%2C+X">Xinke Shen</a>, 
<a href="/search/q-bio?searchtype=author&query=Tao%2C+L">Lingyi Tao</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+X">Xuyang Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Song%2C+S">Sen Song</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Q">Quanying Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Neural representations induced by naturalistic stimuli offer insights into
how humans respond to peripheral stimuli in daily life. The key to
understanding the general neural mechanisms underlying naturalistic stimuli
processing involves aligning neural activities across individuals and
extracting inter-subject shared neural representations. Targeting the
Electroencephalogram (EEG) technique, known for its rich spatial and temporal
information, this study presents a general framework for Contrastive Learning
of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER).
Harnessing the representational capabilities of contrastive learning, CL-SSTER
utilizes a neural network to maximize the similarity of EEG representations
across individuals for identical stimuli, contrasting with those for varied
stimuli. The network employed spatial and temporal convolutions to
simultaneously learn the spatial and temporal patterns inherent in EEG. The
versatility of CL-SSTER was demonstrated on three EEG datasets, including a
synthetic dataset, a speech audio EEG dataset, and an emotional video EEG
dataset. CL-SSTER attained the highest inter-subject correlation (ISC) values
compared to the state-of-the-art ISC methods. The latent representations
generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can
be explained by specific aspects of the stimuli. CL-SSTER serves as an
interpretable and scalable foundational framework for the identification of
inter-subject shared neural representations in the realm of naturalistic
neuroscience.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14225" title="Abstract">arXiv:2402.14225</a> (cross-list from eess.AS) [<a href="/pdf/2402.14225" title="Download PDF">pdf</a>, <a href="/format/2402.14225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SICRN: Advancing Speech Enhancement through State Space Model and  Inplace Convolution Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Changjiang Zhao</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+S">Shulin He</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xueliang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Speech enhancement aims to improve speech quality and intelligibility,
especially in noisy environments where background noise degrades speech
signals. Currently, deep learning methods achieve great success in speech
enhancement, e.g. the representative convolutional recurrent neural network
(CRN) and its variants. However, CRN typically employs consecutive downsampling
and upsampling convolution for frequency modeling, which destroys the inherent
structure of the signal over frequency. Additionally, convolutional layers
lacks of temporal modelling abilities. To address these issues, we propose an
innovative module combing a State space model and Inplace Convolution (SIC),
and to replace the conventional convolution in CRN, called SICRN. Specifically,
a dual-path multidimensional State space model captures the global frequencies
dependency and long-term temporal dependencies. Meanwhile, the 2D-inplace
convolution is used to capture the local structure, which abandons the
downsampling and upsampling. Systematic evaluations on the public INTERSPEECH
2020 DNS challenge dataset demonstrate SICRN's efficacy. Compared to strong
baselines, SICRN achieves performance close to state-of-the-art while having
advantages in model parameters, computations, and algorithmic delay. The
proposed SICRN shows great promise for improved speech enhancement.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14229" title="Abstract">arXiv:2402.14229</a> (cross-list from math.ST) [<a href="/pdf/2402.14229" title="Download PDF">pdf</a>, <a href="/ps/2402.14229" title="Download PostScript">ps</a>, <a href="/format/2402.14229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Efficient Linear Regression with Self-Selection Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gaitonde%2C+J">Jason Gaitonde</a>, 
<a href="/search/math?searchtype=author&query=Mossel%2C+E">Elchanan Mossel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of linear regression with self-selection bias in the
unknown-index setting, as introduced in recent work by Cherapanamjeri,
Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$
i.i.d. samples $(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$ where
$z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$,
but the maximizing index $i_{\ell}$ is unobserved. Here, the
$\mathbf{x}_{\ell}$ are assumed to be $\mathcal{N}(0,I_n)$ and the noise
distribution $\mathbf{\eta}_{\ell}\sim \mathcal{D}$ is centered and independent
of $\mathbf{x}_{\ell}$. We provide a novel and near optimally sample-efficient
(in terms of $k$) algorithm to recover $\mathbf{w}_1,\ldots,\mathbf{w}_k\in
\mathbb{R}^n$ up to additive $\ell_2$-error $\varepsilon$ with polynomial
sample complexity $\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$ and
significantly improved time complexity
$\mathsf{poly}(n,k,1/\varepsilon)+O(\log(k)/\varepsilon)^{O(k)}$. When
$k=O(1)$, our algorithm runs in $\mathsf{poly}(n,1/\varepsilon)$ time,
generalizing the polynomial guarantee of an explicit moment matching algorithm
of Cherapanamjeri, et al. for $k=2$ and when it is known that
$\mathcal{D}=\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly
relaxed noise assumptions, and therefore also succeeds in the related setting
of max-linear regression where the added noise is taken outside the maximum.
For this problem, our algorithm is efficient in a much larger range of $k$ than
the state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran
[IEEE Trans. Inf. Theory 2022] for not too small $\varepsilon$, and leads to
improved algorithms for any $\varepsilon$ by providing a warm start for
existing local convergence methods.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14264" title="Abstract">arXiv:2402.14264</a> (cross-list from stat.ML) [<a href="/pdf/2402.14264" title="Download PDF">pdf</a>, <a href="/ps/2402.14264" title="Download PostScript">ps</a>, <a href="/format/2402.14264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-agnostic Optimality of Doubly Robust Learning for Treatment  Effect Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jin%2C+J">Jikai Jin</a>, 
<a href="/search/stat?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
<p class="mathjax">Average treatment effect estimation is the most central problem in causal
inference with application to numerous disciplines. While many estimation
strategies have been proposed in the literature, recently also incorporating
generic machine learning estimators, the statistical optimality of these
methods has still remained an open area of investigation. In this paper, we
adopt the recently introduced structure-agnostic framework of statistical lower
bounds, which poses no structural properties on the nuisance functions other
than access to black-box estimators that attain small errors; which is
particularly appealing when one is only willing to consider estimation
strategies that use non-parametric regression and classification oracles as a
black-box sub-process. Within this framework, we prove the statistical
optimality of the celebrated and widely used doubly robust estimators for both
the Average Treatment Effect (ATE) and the Average Treatment Effect on the
Treated (ATTE), as well as weighted variants of the former, which arise in
policy evaluation.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14276" title="Abstract">arXiv:2402.14276</a> (cross-list from eess.SP) [<a href="/pdf/2402.14276" title="Download PDF">pdf</a>, <a href="/format/2402.14276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bispectrum Unbiasing for Dilation-Invariant Multi-reference Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yin%2C+L">Liping Yin</a>, 
<a href="/search/eess?searchtype=author&query=Little%2C+A">Anna Little</a>, 
<a href="/search/eess?searchtype=author&query=Hirn%2C+M">Matthew Hirn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Motivated by modern data applications such as cryo-electron microscopy, the
goal of classic multi-reference alignment (MRA) is to recover an unknown signal
$f: \mathbb{R} \to \mathbb{R}$ from many observations that have been randomly
translated and corrupted by additive noise. We consider a generalization of
classic MRA where signals are also corrupted by a random scale change, i.e.
dilation. We propose a novel data-driven unbiasing procedure which can recover
an unbiased estimator of the bispectrum of the unknown signal, given knowledge
of the dilation distribution. Lastly, we invert the recovered bispectrum to
achieve full signal recovery, and validate our methodology on a set of
synthetic signals.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14315" title="Abstract">arXiv:2402.14315</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.14315" title="Download PDF">pdf</a>, <a href="/format/2402.14315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Based Drug Design via 3D Molecular Generative Pre-training and  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Y">Yuwei Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Ouyang%2C+S">Siqi Ouyang</a>, 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+X">Xueyu Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Dang%2C+M">Meihua Dang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zheng%2C+M">Mingyue Zheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Structure-based drug design aims at generating high affinity ligands with
prior knowledge of 3D target structures. Existing methods either use
conditional generative model to learn the distribution of 3D ligands given
target binding sites, or iteratively modify molecules to optimize a
structure-based activity estimator. The former is highly constrained by data
quantity and quality, which leaves optimization-based approaches more promising
in practical scenario. However, existing optimization-based approaches choose
to edit molecules in 2D space, and use molecular docking to estimate the
activity using docking predicted 3D target-ligand complexes. The misalignment
between the action space and the objective hinders the performance of these
models, especially for those employ deep learning for acceleration. In this
work, we propose MolEdit3D to combine 3D molecular generation with optimization
frameworks. We develop a novel 3D graph editing model to generate molecules
using fragments, and pre-train this model on abundant 3D ligands for learning
target-independent properties. Then we employ a target-guided self-learning
strategy to improve target-related properties using self-sampled molecules.
MolEdit3D achieves state-of-the-art performance on majority of the evaluation
metrics, and demonstrate strong capability of capturing both target-dependent
and -independent properties.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14343" title="Abstract">arXiv:2402.14343</a> (cross-list from math.CO) [<a href="/pdf/2402.14343" title="Download PDF">pdf</a>, <a href="/format/2402.14343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The expansion of half-integral polytopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cardinal%2C+J">Jean Cardinal</a>, 
<a href="/search/math?searchtype=author&query=Pournin%2C+L">Lionel Pournin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Metric Geometry (math.MG)

</div>
<p class="mathjax">The expansion of a polytope is an important parameter for the analysis of the
random walks on its graph. A conjecture of Mihai and Vazirani states that all
$0/1$-polytopes have expansion at least 1. We show that the generalization to
half-integral polytopes does not hold by constructing $d$-dimensional
half-integral polytopes whose expansion decreases exponentially fast with $d$.
We also prove that the expansion of half-integral zonotopes is uniformly
bounded away from $0$. As an intermediate result, we show that half-integral
zonotopes are always graphical.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14349" title="Abstract">arXiv:2402.14349</a> (cross-list from eess.IV) [<a href="/pdf/2402.14349" title="Download PDF">pdf</a>, <a href="/ps/2402.14349" title="Download PostScript">ps</a>, <a href="/format/2402.14349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-driven and Adversarial Calibration Learning for Epicardial  Adipose Tissue Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhiming Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jingbiao Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+B">Bihong Liao</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+H">Huifang Tang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiuyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chunquan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,7 figuers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete
large amounts of adipokines to affect the myocardium and coronary arteries. EAT
volume and density can be used as independent risk markers measurement of
volume by noninvasive magnetic resonance images is the best method of assessing
EAT. However, segmenting EAT is challenging due to the low contrast between EAT
and pericardial effusion and the presence of motion artifacts. we propose a
novel feature latent space multilevel supervision network (SPDNet) with
uncertainty-driven and adversarial calibration learning to enhance segmentation
for more accurate EAT volume estimation. The network first addresses the
blurring of EAT edges due to the medical images in the open medical
environments with low quality or out-of-distribution by modeling the
uncertainty as a Gaussian distribution in the feature latent space, which using
its Bayesian estimation as a regularization constraint to optimize SwinUNETR.
Second, an adversarial training strategy is introduced to calibrate the
segmentation feature map and consider the multi-scale feature differences
between the uncertainty-guided predictive segmentation and the ground truth
segmentation, synthesizing the multi-scale adversarial loss directly improves
the ability to discriminate the similarity between organizations. Experiments
on both the cardiac public MRI dataset (ACDC) and the real-world clinical
cohort EAT dataset show that the proposed network outperforms mainstream
models, validating that uncertainty-driven and adversarial calibration learning
can be used to provide additional information for modeling multi-scale
ambiguities.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14396" title="Abstract">arXiv:2402.14396</a> (cross-list from quant-ph) [<a href="/pdf/2402.14396" title="Download PDF">pdf</a>, <a href="/format/2402.14396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Circuit Optimization with AlphaTensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ruiz%2C+F+J+R">Francisco J. R. Ruiz</a>, 
<a href="/search/quant-ph?searchtype=author&query=Laakkonen%2C+T">Tuomas Laakkonen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bausch%2C+J">Johannes Bausch</a>, 
<a href="/search/quant-ph?searchtype=author&query=Balog%2C+M">Matej Balog</a>, 
<a href="/search/quant-ph?searchtype=author&query=Barekatain%2C+M">Mohammadamin Barekatain</a>, 
<a href="/search/quant-ph?searchtype=author&query=Heras%2C+F+J+H">Francisco J. H. Heras</a>, 
<a href="/search/quant-ph?searchtype=author&query=Novikov%2C+A">Alexander Novikov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fitzpatrick%2C+N">Nathan Fitzpatrick</a>, 
<a href="/search/quant-ph?searchtype=author&query=Romera-Paredes%2C+B">Bernardino Romera-Paredes</a>, 
<a href="/search/quant-ph?searchtype=author&query=van+de+Wetering%2C+J">John van de Wetering</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fawzi%2C+A">Alhussein Fawzi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Meichanetzidis%2C+K">Konstantinos Meichanetzidis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kohli%2C+P">Pushmeet Kohli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages main paper + 19 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A key challenge in realizing fault-tolerant quantum computers is circuit
optimization. Focusing on the most expensive gates in fault-tolerant quantum
computation (namely, the T gates), we address the problem of T-count
optimization, i.e., minimizing the number of T gates that are needed to
implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a
method based on deep reinforcement learning that exploits the relationship
between optimizing T-count and tensor decomposition. Unlike existing methods
for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific
knowledge about quantum computation and leverage gadgets, which significantly
reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms
the existing methods for T-count optimization on a set of arithmetic benchmarks
(even when compared without making use of gadgets). Remarkably, it discovers an
efficient algorithm akin to Karatsuba's method for multiplication in finite
fields. AlphaTensor-Quantum also finds the best human-designed solutions for
relevant arithmetic computations used in Shor's algorithm and for quantum
chemistry simulation, thus demonstrating it can save hundreds of hours of
research by optimizing relevant quantum circuits in a fully automated way.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14434" title="Abstract">arXiv:2402.14434</a> (cross-list from math.ST) [<a href="/pdf/2402.14434" title="Download PDF">pdf</a>, <a href="/format/2402.14434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallelized Midpoint Randomization for Langevin Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yu%2C+L">Lu Yu</a>, 
<a href="/search/math?searchtype=author&query=Dalalyana%2C+A">Arnak Dalalyana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2306.08494">arXiv:2306.08494</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Probability (math.PR); Computation (stat.CO)

</div>
<p class="mathjax">We explore the sampling problem within the framework where parallel
evaluations of the gradient of the log-density are feasible. Our investigation
focuses on target distributions characterized by smooth and strongly
log-concave densities. We revisit the parallelized randomized midpoint method
and employ proof techniques recently developed for analyzing its purely
sequential version. Leveraging these techniques, we derive upper bounds on the
Wasserstein distance between the sampling and target densities. These bounds
quantify the runtime improvement achieved by utilizing parallel processing
units, which can be considerable.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14446" title="Abstract">arXiv:2402.14446</a> (cross-list from math.OC) [<a href="/pdf/2402.14446" title="Download PDF">pdf</a>, <a href="/format/2402.14446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based Reinforcement Learning Control of Reaction-Diffusion  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schenk%2C+C">Christina Schenk</a>, 
<a href="/search/math?searchtype=author&query=Vasudevan%2C+A">Aditya Vasudevan</a>, 
<a href="/search/math?searchtype=author&query=Haranczyk%2C+M">Maciej Haranczyk</a>, 
<a href="/search/math?searchtype=author&query=Romero%2C+I">Ignacio Romero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY); Mathematical Physics (math-ph); Dynamical Systems (math.DS)

</div>
<p class="mathjax">Mathematical and computational tools have proven to be reliable in
decision-making processes. In recent times, in particular, machine
learning-based methods are becoming increasingly popular as advanced support
tools. When dealing with control problems, reinforcement learning has been
applied to decision-making in several applications, most notably in games. The
success of these methods in finding solutions to complex problems motivates the
exploration of new areas where they can be employed to overcome current
difficulties. In this paper, we explore the use of automatic control strategies
to initial boundary value problems in thermal and disease transport.
Specifically, in this work, we adapt an existing reinforcement learning
algorithm using a stochastic policy gradient method and we introduce two novel
reward functions to drive the flow of the transported field. The new
model-based framework exploits the interactions between a reaction-diffusion
model and the modified agent. The results show that certain controls can be
implemented successfully in these applications, although model simplifications
had to be assumed.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14459" title="Abstract">arXiv:2402.14459</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.14459" title="Download PDF">pdf</a>, <a href="/ps/2402.14459" title="Download PostScript">ps</a>, <a href="/format/2402.14459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on  Mediterranean Sea Water
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Trois%2C+C">Celio Trois</a>, 
<a href="/search/physics?searchtype=author&query=Del+Fabro%2C+L+D">Luciana Didonet Del Fabro</a>, 
<a href="/search/physics?searchtype=author&query=Baulin%2C+V+A">Vladimir A. Baulin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2204.13587">arXiv:2204.13587</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that
fosters biodiversity, stores carbon, releases oxygen, and provides habitat to
numerous sea organisms. Leveraging augmented research, we collected a
comprehensive dataset of 174 features compiled from diverse data sources.
Through machine learning analysis, we discovered the existence of a robust
correlation between the exact location of P. oceanica and water biogeochemical
properties. The model's feature importance, showed that carbon-related
variables as net biomass production and downward surface mass flux of carbon
dioxide have their values altered in the areas with P. oceanica, which in turn
can be used for indirect location of P. oceanica meadows. The study provides
the evidence of the plant's ability to exert a global impact on the environment
and underscores the crucial role of this plant in sea ecosystems, emphasizing
the need for its conservation and management.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14508" title="Abstract">arXiv:2402.14508</a> (cross-list from math.DS) [<a href="/pdf/2402.14508" title="Download PDF">pdf</a>, <a href="/ps/2402.14508" title="Download PostScript">ps</a>, <a href="/format/2402.14508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shifts on the lamplighter group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bartholdi%2C+L">Laurent Bartholdi</a>, 
<a href="/search/math?searchtype=author&query=Salo%2C+V">Ville Salo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Logic in Computer Science (cs.LO); Group Theory (math.GR)

</div>
<p class="mathjax">We prove that the lamplighter group admits strongly aperiodic SFTs, has
undecidable tiling problem, and the entropies of its SFTs are exactly the upper
semicomputable nonnegative real numbers, and some other results. These results
follow from two relatively general simulation theorems, which show that for a
large class of effective subshifts on the sea-level subgroup, their induction
to the lamplighter group is sofic; and the pullback of every effective Cantor
system on the integers admits an SFT cover. We exhibit a concrete strongly
aperiodic set with $1488$ tetrahedra. We show that metabelian Baumslag-Solitar
groups are intersimulable with lamplighter groups, and thus we obtain the same
characterization for their entropies.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14509" title="Abstract">arXiv:2402.14509</a> (cross-list from eess.IV) [<a href="/pdf/2402.14509" title="Download PDF">pdf</a>, <a href="/format/2402.14509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep vessel segmentation based on a new combination of vesselness  filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Garret%2C+G">Guillaume Garret</a>, 
<a href="/search/eess?searchtype=author&query=Vacavant%2C+A">Antoine Vacavant</a>, 
<a href="/search/eess?searchtype=author&query=Frindel%2C+C">Carole Frindel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures. This work has been submitted to the 21st IEEE International Symposium on Biomedical Imaging (ISBI 2024) for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Vascular segmentation represents a crucial clinical task, yet its automation
remains challenging. Because of the recent strides in deep learning, vesselness
filters, which can significantly aid the learning process, have been
overlooked. This study introduces an innovative filter fusion method crafted to
amplify the effectiveness of vessel segmentation models. Our investigation
seeks to establish the merits of a filter-based learning approach through a
comparative analysis. Specifically, we contrast the performance of a U-Net
model trained on CT images with an identical U-Net configuration trained on
vesselness hyper-volumes using matching parameters. Our findings, based on two
vascular datasets, highlight improved segmentations, especially for small
vessels, when the model's learning is exposed to vessel-enhanced inputs.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14515" title="Abstract">arXiv:2402.14515</a> (cross-list from quant-ph) [<a href="/pdf/2402.14515" title="Download PDF">pdf</a>, <a href="/format/2402.14515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral invariance and maximality properties of the frequency spectrum  of quantum neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Holzer%2C+P">Patrick Holzer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Turkalj%2C+I">Ivica Turkalj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine
Learning due to their close connection to Variational Quantum Circuits, making
them a promising candidate for practical applications on Noisy
Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite
Fourier series, where the set of frequencies is called the frequency spectrum.
We analyse this frequency spectrum and prove, for a large class of models,
various maximality results. Furthermore, we prove that under some mild
conditions there exists a bijection between classes of models with the same
area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the
number of qubits and $L$ the number of layers, which we consequently call
spectral invariance under area-preserving transformations. With this we explain
the symmetry in $R$ and $L$ in the results often observed in the literature and
show that the maximal frequency spectrum depends only on the area $A = RL$ and
not on the individual values of $R$ and $L$. Moreover, we extend existing
results and specify the maximum possible frequency spectrum of a QNN with
arbitrarily many layers as a function of the spectrum of its generators. If the
generators of the QNN can be further decomposed into 2-dimensional
sub-generators, then this specification follows from elementary
number-theoretical considerations. In the case of arbitrary dimensional
generators, we extend existing results based on the so-called Golomb ruler and
introduce a second novel approach based on a variation of the turnpike problem,
which we call the relaxed turnpike problem.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14565" title="Abstract">arXiv:2402.14565</a> (cross-list from eess.SP) [<a href="/pdf/2402.14565" title="Download PDF">pdf</a>, <a href="/format/2402.14565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Contact Acquisition of PPG Signal using Chest Movement-Modulated  Radio Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Filho%2C+I+J+S">Israel Jesus Santos Filho</a>, 
<a href="/search/eess?searchtype=author&query=Rahman%2C+M+M+U">Muhammad Mahboob Ur Rahman</a>, 
<a href="/search/eess?searchtype=author&query=Laleg-Kirati%2C+T">Taous-Meriem Laleg-Kirati</a>, 
<a href="/search/eess?searchtype=author&query=Al-Naffouri%2C+T">Tareq Al-Naffouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, under review with a conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We present for the first time a novel method that utilizes the chest
movement-modulated radio signals for non-contact acquisition of the
photoplethysmography (PPG) signal. Under the proposed method, a
software-defined radio (SDR) exposes the chest of a subject sitting nearby to
an orthogonal frequency division multiplexing signal with 64 sub-carriers at a
center frequency 5.24 GHz, while another SDR in the close vicinity collects the
modulated radio signal reflected off the chest. This way, we construct a custom
dataset by collecting 160 minutes of labeled data (both raw radio data as well
as the reference PPG signal) from 16 healthy young subjects. With this, we
first utilize principal component analysis for dimensionality reduction of the
radio data. Next, we denoise the radio signal and reference PPG signal using
wavelet technique, followed by segmentation and Z-score normalization. We then
synchronize the radio and PPG segments using cross-correlation method. Finally,
we proceed to the waveform translation (regression) task, whereby we first
convert the radio and PPG segments into frequency domain using discrete cosine
transform (DCT), and then learn the non-linear regression between them.
Eventually, we reconstruct the synthetic PPG signal by taking inverse DCT of
the output of regression block, with a mean absolute error of 8.1294. The
synthetic PPG waveform has a great clinical significance as it could be used
for non-contact performance assessment of cardiovascular and respiratory
systems of patients suffering from infectious diseases, e.g., covid19.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14578" title="Abstract">arXiv:2402.14578</a> (cross-list from stat.ML) [<a href="/pdf/2402.14578" title="Download PDF">pdf</a>, <a href="/format/2402.14578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Online Linear Regression for Hierarchical Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hihat%2C+M">Massil Hihat</a>, 
<a href="/search/stat?searchtype=author&query=Garrigos%2C+G">Guillaume Garrigos</a>, 
<a href="/search/stat?searchtype=author&query=Fermanian%2C+A">Adeline Fermanian</a>, 
<a href="/search/stat?searchtype=author&query=Bussy%2C+S">Simon Bussy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we consider a deterministic online linear regression model
where we allow the responses to be multivariate. To address this problem, we
introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth
algorithm to the multivariate setting, and show that it also enjoys logarithmic
regret in time. We apply our results to the online hierarchical forecasting
problem and recover an algorithm from this literature as a special case,
allowing us to relax the hypotheses usually made for its analysis.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14649" title="Abstract">arXiv:2402.14649</a> (cross-list from quant-ph) [<a href="/pdf/2402.14649" title="Download PDF">pdf</a>, <a href="/ps/2402.14649" title="Download PostScript">ps</a>, <a href="/format/2402.14649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Markov Decision Processes Part I: General Theory,  Approximations, and Classes of Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Saldi%2C+N">Naci Saldi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sanjari%2C+S">Sina Sanjari</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuksel%2C+S">Serdar Yuksel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this two part article, the aim is to develop a quantum counterpart to
classical Markov decision processes (MDPs). In Part I, we provide a very
general formulation of quantum MDPs with state and action spaces in the quantum
domain, quantum transitions, and cost functions. Once we formulate the quantum
MDP (q-MDP), our focus shifts to establishing the verification theorem that
proves the sufficiency of Markovian quantum control policies and provides a
dynamic programming principle. Subsequently, a comparison is drawn between our
q-MDP model and previously established quantum MDP models (referred to as
QOMDPs) found in the literature. Furthermore, approximations of q-MDPs are
obtained via finite-action models, which can be formulated as QOMDPs. Finally,
classes of open-loop and closed-loop policies for q-MDPs are introduced, along
with structural results for these policies. In summary, we present a novel
quantum MDP model aiming to introduce a new framework, algorithms, and future
research avenues. We believe that our approach will pave the way for a new
research direction in discrete-time quantum control.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14651" title="Abstract">arXiv:2402.14651</a> (cross-list from quant-ph) [<a href="/pdf/2402.14651" title="Download PDF">pdf</a>, <a href="/ps/2402.14651" title="Download PostScript">ps</a>, <a href="/format/2402.14651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Markov Decision Processes Part II: Optimal Solutions and  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Saldi%2C+N">Naci Saldi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sanjari%2C+S">Sina Sanjari</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuksel%2C+S">Serdar Yuksel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This two-part article aims to introduce a quantum analogue to classical
Markov decision processes (MDPs). In Part II, building on the formulation of
q-MDPs presented in Part I, our focus shifts to the development of algorithms
for computing optimal policies and value functions of both open-loop and
closed-loop policies. First, by using the duality between the dynamic
programming and the semi-definite programming formulations of any q-MDP with
open-loop policies, we establish an algorithm that enables us to efficiently
compute optimal open-loop quantum policies and value functions. Then, dynamic
programming and semi-definite programming formulations for closed-loop policies
is established, where duality of these two formulations similarly enables the
efficient computation of optimal closed-loop policies and value functions.
Finally, given that any q-MDP can be approximated by q-MDPs with classical
policies--potentially with higher-dimensional underlying Hilbert spaces than
the original model--and since any classical policy is an element of the set of
closed-loop policies, we conclude that any q-MDP can be approximated by q-MDPs
with closed-loop policies having higher-dimensional Hilbert spaces.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14684" title="Abstract">arXiv:2402.14684</a> (cross-list from stat.ML) [<a href="/pdf/2402.14684" title="Download PDF">pdf</a>, <a href="/format/2402.14684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive time series forecasting with markovian variance switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ab%C3%A9l%C3%A8s%2C+B">Baptiste Ab&#xe9;l&#xe8;s</a>, 
<a href="/search/stat?searchtype=author&query=de+Vilmarest%2C+J">Joseph de Vilmarest</a>, 
<a href="/search/stat?searchtype=author&query=Wintemberger%2C+O">Olivier Wintemberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">Adaptive time series forecasting is essential for prediction under regime
changes. Several classical methods assume linear Gaussian state space model
(LGSSM) with variances constant in time. However, there are many real-world
processes that cannot be captured by such models. We consider a state-space
model with Markov switching variances. Such dynamical systems are usually
intractable because of their computational complexity increasing exponentially
with time; Variational Bayes (VB) techniques have been applied to this problem.
In this paper, we propose a new way of estimating variances based on online
learning theory; we adapt expert aggregation methods to learn the variances
over time. We apply the proposed method to synthetic data and to the problem of
electricity load forecasting. We show that this method is robust to
misspecification and outperforms traditional expert aggregation.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14692" title="Abstract">arXiv:2402.14692</a> (cross-list from eess.AS) [<a href="/pdf/2402.14692" title="Download PDF">pdf</a>, <a href="/format/2402.14692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a  Diffusion Probabilistic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hono%2C+Y">Yukiya Hono</a>, 
<a href="/search/eess?searchtype=author&query=Hashimoto%2C+K">Kei Hashimoto</a>, 
<a href="/search/eess?searchtype=author&query=Nankaku%2C+Y">Yoshihiko Nankaku</a>, 
<a href="/search/eess?searchtype=author&query=Tokuda%2C+K">Keiichi Tokuda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, To appear in ICASSP 2024. Audio samples: <a href="https://www.sp.nitech.ac.jp/~hono/demos/icassp2024/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents a neural vocoder based on a denoising diffusion
probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary
conditioning signals. Recently, DDPM-based neural vocoders have gained
prominence as non-autoregressive models that can generate high-quality
waveforms. The neural vocoders based on DDPM have the advantage of training
with a simple time-domain loss. In practical applications, such as singing
voice synthesis, there is a demand for neural vocoders to generate
high-fidelity speech waveforms with flexible pitch control. However,
conventional DDPM-based neural vocoders struggle to generate speech waveforms
under such conditions. Our proposed model aims to accurately capture the
periodic structure of speech waveforms by incorporating explicit periodic
signals. Experimental results show that our model improves sound quality and
provides better pitch control than conventional DDPM-based neural vocoders.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14694" title="Abstract">arXiv:2402.14694</a> (cross-list from quant-ph) [<a href="/pdf/2402.14694" title="Download PDF">pdf</a>, <a href="/format/2402.14694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quick Introduction to Quantum Machine Learning for Non-Practitioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Evans%2C+E+N">Ethan N. Evans</a>, 
<a href="/search/quant-ph?searchtype=author&query=Byrne%2C+D">Dominic Byrne</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cook%2C+M+G">Matthew G. Cook</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a DTIC report under the title "Quantum Computing for Machine Learning - An Introduction". Distribution Statement A: Approved for Public Release. Distribution is Unlimited
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper provides an introduction to quantum machine learning, exploring
the potential benefits of using quantum computing principles and algorithms
that may improve upon classical machine learning approaches. Quantum computing
utilizes particles governed by quantum mechanics for computational purposes,
leveraging properties like superposition and entanglement for information
representation and manipulation. Quantum machine learning applies these
principles to enhance classical machine learning models, potentially reducing
network size and training time on quantum hardware. The paper covers basic
quantum mechanics principles, including superposition, phase space, and
entanglement, and introduces the concept of quantum gates that exploit these
properties. It also reviews classical deep learning concepts, such as
artificial neural networks, gradient descent, and backpropagation, before
delving into trainable quantum circuits as neural networks. An example problem
demonstrates the potential advantages of quantum neural networks, and the
appendices provide detailed derivations. The paper aims to help researchers new
to quantum mechanics and machine learning develop their expertise more
efficiently.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14736" title="Abstract">arXiv:2402.14736</a> (cross-list from stat.CO) [<a href="/pdf/2402.14736" title="Download PDF">pdf</a>, <a href="/format/2402.14736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grouped approximate control variate estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gorodetsky%2C+A+A">Alex A. Gorodetsky</a>, 
<a href="/search/stat?searchtype=author&query=Jakeman%2C+J+D">John D. Jakeman</a>, 
<a href="/search/stat?searchtype=author&query=Eldred%2C+M+S">Michael S. Eldred</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This paper analyzes the approximate control variate (ACV) approach to
multifidelity uncertainty quantification in the case where weighted estimators
are combined to form the components of the ACV. The weighted estimators enable
one to precisely group models that share input samples to achieve improved
variance reduction. We demonstrate that this viewpoint yields a generalized
linear estimator that can assign any weight to any sample. This generalization
shows that other linear estimators in the literature, particularly the
multilevel best linear unbiased estimator (ML-BLUE) of Schaden and Ullman in
2020, becomes a specific version of the ACV estimator of Gorodetsky, Geraci,
Jakeman, and Eldred, 2020. Moreover, this connection enables numerous
extensions and insights. For example, we empirically show that having
non-independent groups can yield better variance reduction compared to the
independent groups used by ML-BLUE. Furthermore, we show that such grouped
estimators can use arbitrary weighted estimators, not just the simple Monte
Carlo estimators used in ML-BLUE. Furthermore, the analysis enables the
derivation of ML-BLUE directly from a variance reduction perspective, rather
than a regression perspective.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14741" title="Abstract">arXiv:2402.14741</a> (cross-list from eess.IV) [<a href="/pdf/2402.14741" title="Download PDF">pdf</a>, <a href="/format/2402.14741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Capell%C3%A1n-Mart%C3%ADn%2C+D">Daniel Capell&#xe1;n-Mart&#xed;n</a>, 
<a href="/search/eess?searchtype=author&query=Parida%2C+A">Abhijeet Parida</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%B3mez-Valverde%2C+J+J">Juan J. G&#xf3;mez-Valverde</a>, 
<a href="/search/eess?searchtype=author&query=Sanchez-Jacob%2C+R">Ramon Sanchez-Jacob</a>, 
<a href="/search/eess?searchtype=author&query=Roshanitabrizi%2C+P">Pooneh Roshanitabrizi</a>, 
<a href="/search/eess?searchtype=author&query=Linguraru%2C+M+G">Marius G. Linguraru</a>, 
<a href="/search/eess?searchtype=author&query=Ledesma-Carbayo%2C+M+J">Mar&#xed;a J. Ledesma-Carbayo</a>, 
<a href="/search/eess?searchtype=author&query=Anwar%2C+S+M">Syed M. Anwar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 2 tables. This paper has been accepted at IEEE ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Tuberculosis (TB) remains a significant global health challenge, with
pediatric cases posing a major concern. The World Health Organization (WHO)
advocates for chest X-rays (CXRs) for TB screening. However, visual
interpretation by radiologists can be subjective, time-consuming and prone to
error, especially in pediatric TB. Artificial intelligence (AI)-driven
computer-aided detection (CAD) tools, especially those utilizing deep learning,
show promise in enhancing lung disease detection. However, challenges include
data scarcity and lack of generalizability. In this context, we propose a novel
self-supervised paradigm leveraging Vision Transformers (ViT) for improved TB
detection in CXR, enabling zero-shot pediatric TB detection. We demonstrate
improvements in TB detection performance ($\sim$12.7% and $\sim$13.4% top
AUC/AUPR gains in adults and children, respectively) when conducting
self-supervised pre-training when compared to fully-supervised (i.e., non
pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR
in adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB
detection. As a result, this work demonstrates that self-supervised learning on
adult CXRs effectively extends to challenging downstream tasks such as
pediatric TB detection, where data are scarce.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14742" title="Abstract">arXiv:2402.14742</a> (cross-list from math.CO) [<a href="/pdf/2402.14742" title="Download PDF">pdf</a>, <a href="/ps/2402.14742" title="Download PostScript">ps</a>, <a href="/format/2402.14742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New scattered quadrinomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Smaldore%2C+V">Valentino Smaldore</a>, 
<a href="/search/math?searchtype=author&query=Zanella%2C+C">Corrado Zanella</a>, 
<a href="/search/math?searchtype=author&query=Zullo%2C+F">Ferdinando Zullo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Let $1&lt;t&lt;n$ be integers, where $t$ is a divisor of $n$. An R-$q^t$-partially
scattered polynomial is a $\mathbb F_q$-linearized polynomial $f$ in $\mathbb
F_{q^n}[X]$ that satisfies the condition that for all $x,y\in\mathbb F_{q^n}^*$
such that $x/y\in\mathbb F_{q^t}$, if $f(x)/x=f(y)/y$, then $x/y\in\mathbb
F_q$; $f$ is called scattered if this implication holds for all $x,y\in\mathbb
F_{q^n}^*$. Two polynomials in $\mathbb F_{q^n}[X]$ are said to be equivalent
if their graphs are in the same orbit under the action of the group $\Gamma
L(2,q^n)$. For $n&gt;8$ only three families of scattered polynomials in $\mathbb
F_{q^n}[X]$ are known: $(i)$~monomials of pseudoregulus type, $(ii)$~binomials
of Lunardon-Polverino type, and $(iii)$~a family of quadrinomials defined in
[9] and extended in [7,12]. In this paper we prove that the polynomial
$\varphi_{m,q^J}=X^{q^{J(t-1)}}+X^{q^{J(2t-1)}}+m(X^{q^J}-X^{q^{J(t+1)}})\in\mathbb
F_{q^{2t}}[X]$, $q$ odd, $t\ge3$ is R-$q^t$-partially scattered for every value
of $m\in\mathbb F_{q^t}^*$ and $J$ coprime with $2t$. Moreover, for every $t&gt;4$
and $q&gt;5$ there exist values of $m$ for which $\varphi_{m,q}$ is scattered and
new with respect to the polynomials mentioned in $(i)$, $(ii)$ and $(iii)$
above. The related linear sets are of $\Gamma L$-class at least two.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14758" title="Abstract">arXiv:2402.14758</a> (cross-list from stat.ML) [<a href="/pdf/2402.14758" title="Download PDF">pdf</a>, <a href="/format/2402.14758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch and match: black-box variational inference with a score-based  divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cai%2C+D">Diana Cai</a>, 
<a href="/search/stat?searchtype=author&query=Modi%2C+C">Chirag Modi</a>, 
<a href="/search/stat?searchtype=author&query=Pillaud-Vivien%2C+L">Loucas Pillaud-Vivien</a>, 
<a href="/search/stat?searchtype=author&query=Margossian%2C+C+C">Charles C. Margossian</a>, 
<a href="/search/stat?searchtype=author&query=Gower%2C+R+M">Robert M. Gower</a>, 
<a href="/search/stat?searchtype=author&query=Blei%2C+D+M">David M. Blei</a>, 
<a href="/search/stat?searchtype=author&query=Saul%2C+L+K">Lawrence K. Saul</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Most leading implementations of black-box variational inference (BBVI) are
based on optimizing a stochastic evidence lower bound (ELBO). But such
approaches to BBVI often converge slowly due to the high variance of their
gradient estimates. In this work, we propose batch and match (BaM), an
alternative approach to BBVI based on a score-based divergence. Notably, this
score-based divergence can be optimized by a closed-form proximal update for
Gaussian variational families with full covariance matrices. We analyze the
convergence of BaM when the target distribution is Gaussian, and we prove that
in the limit of infinite batch size the variational parameter updates converge
exponentially quickly to the target mean and covariance. We also evaluate the
performance of BaM on Gaussian and non-Gaussian target distributions that arise
from posterior inference in hierarchical and deep generative models. In these
experiments, we find that BaM typically converges in fewer (and sometimes
significantly fewer) gradient evaluations than leading implementations of BBVI
based on ELBO maximization.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14777" title="Abstract">arXiv:2402.14777</a> (cross-list from stat.ML) [<a href="/pdf/2402.14777" title="Download PDF">pdf</a>, <a href="/format/2402.14777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent  Factor Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ribot%2C+A">Alvaro Ribot</a>, 
<a href="/search/stat?searchtype=author&query=Squires%2C+C">Chandler Squires</a>, 
<a href="/search/stat?searchtype=author&query=Uhler%2C+C">Caroline Uhler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the task of causal imputation, where we aim to predict the
outcomes of some set of actions across a wide range of possible contexts. As a
running example, we consider predicting how different drugs affect cells from
different cell types. We study the index-only setting, where the actions and
contexts are categorical variables with a finite number of possible values.
Even in this simple setting, a practical challenge arises, since often only a
small subset of possible action-context pairs have been studied. Thus, models
must extrapolate to novel action-context pairs, which can be framed as a form
of matrix completion with rows indexed by actions, columns indexed by contexts,
and matrix entries corresponding to outcomes. We introduce a novel SCM-based
model class, where the outcome is expressed as a counterfactual, actions are
expressed as interventions on an instrumental variable, and contexts are
defined based on the initial state of the system. We show that, under a
linearity assumption, this setup induces a latent factor model over the matrix
of outcomes, with an additional fixed effect term. To perform causal prediction
based on this model class, we introduce simple extension to the Synthetic
Interventions estimator (Agarwal et al., 2020). We evaluate several matrix
completion approaches on the PRISM drug repurposing dataset, showing that our
method outperforms all other considered matrix completion approaches.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14803" title="Abstract">arXiv:2402.14803</a> (cross-list from quant-ph) [<a href="/pdf/2402.14803" title="Download PDF">pdf</a>, <a href="/ps/2402.14803" title="Download PostScript">ps</a>, <a href="/format/2402.14803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudorandom unitaries with non-adaptive security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Metger%2C+T">Tony Metger</a>, 
<a href="/search/quant-ph?searchtype=author&query=Poremba%2C+A">Alexander Poremba</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sinha%2C+M">Makrand Sinha</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuen%2C+H">Henry Yuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Pseudorandom unitaries (PRUs) are ensembles of efficiently implementable
unitary operators that cannot be distinguished from Haar random unitaries by
any quantum polynomial-time algorithm with query access to the unitary. We
present a simple PRU construction that is a concatenation of a random Clifford
unitary, a pseudorandom binary phase operator, and a pseudorandom permutation
operator. We prove that this PRU construction is secure against non-adaptive
distinguishers assuming the existence of quantum-secure one-way functions. This
means that no efficient quantum query algorithm that is allowed a single
application of $U^{\otimes \mathrm{poly}(n)}$ can distinguish whether an
$n$-qubit unitary $U$ was drawn from the Haar measure or our PRU ensemble. We
conjecture that our PRU construction remains secure against adaptive
distinguishers, i.e. secure against distinguishers that can query the unitary
polynomially many times in sequence, not just in parallel.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 23 Feb 24</h3>
<dl>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.03546" title="Abstract">arXiv:2003.03546</a> (replaced) [<a href="/pdf/2003.03546" title="Download PDF">pdf</a>, <a href="/format/2003.03546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Machine Learning: Bayesian Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Insua%2C+D+R">David Rios Insua</a>, 
<a href="/search/cs?searchtype=author&query=Naveiro%2C+R">Roi Naveiro</a>, 
<a href="/search/cs?searchtype=author&query=Gallego%2C+V">Victor Gallego</a>, 
<a href="/search/cs?searchtype=author&query=Poulos%2C+J">Jason Poulos</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of the American Statistical Association. Volume 118, 2023
  - Issue 543
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.12091" title="Abstract">arXiv:2006.12091</a> (replaced) [<a href="/pdf/2006.12091" title="Download PDF">pdf</a>, <a href="/format/2006.12091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Parameter Tuning for Reachability Analysis of Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wetzlinger%2C+M">Mark Wetzlinger</a>, 
<a href="/search/math?searchtype=author&query=Kochdumper%2C+N">Niklas Kochdumper</a>, 
<a href="/search/math?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.06919" title="Abstract">arXiv:2007.06919</a> (replaced) [<a href="/pdf/2007.06919" title="Download PDF">pdf</a>, <a href="/format/2007.06919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AQD: Towards Accurate Fully-Quantized Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mingkui Tan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2021 Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.06029" title="Abstract">arXiv:2009.06029</a> (replaced) [<a href="/pdf/2009.06029" title="Download PDF">pdf</a>, <a href="/format/2009.06029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transition-Oriented Programming: Developing Provably Correct Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yepeng Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14978" title="Abstract">arXiv:2103.14978</a> (replaced) [<a href="/pdf/2103.14978" title="Download PDF">pdf</a>, <a href="/format/2103.14978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GateKeeper-GPU: Fast and Accurate Pre-Alignment Filtering in Short Read  Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bing%C3%B6l%2C+Z">Z&#xfc;lal Bing&#xf6;l</a>, 
<a href="/search/q-bio?searchtype=author&query=Alser%2C+M">Mohammed Alser</a>, 
<a href="/search/q-bio?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ozturk%2C+O">Ozcan Ozturk</a>, 
<a href="/search/q-bio?searchtype=author&query=Alkan%2C+C">Can Alkan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.10969" title="Abstract">arXiv:2107.10969</a> (replaced) [<a href="/pdf/2107.10969" title="Download PDF">pdf</a>, <a href="/format/2107.10969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Quadruped Locomotion Policies using Logical Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeFazio%2C+D">David DeFazio</a>, 
<a href="/search/cs?searchtype=author&query=Hayamizu%2C+Y">Yohei Hayamizu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.10241" title="Abstract">arXiv:2109.10241</a> (replaced) [<a href="/pdf/2109.10241" title="Download PDF">pdf</a>, <a href="/format/2109.10241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Life in a random universe: Sciama&#x27;s argument reconsidered
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+Z">Zhi-Wei Wang</a>, 
<a href="/search/physics?searchtype=author&query=Braunstein%2C+S+L">Samuel L. Braunstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, pulished on The Astrophysical Journal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ApJ 962 55 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">History and Philosophy of Physics (physics.hist-ph)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Artificial Intelligence (cs.AI); General Relativity and Quantum Cosmology (gr-qc); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03544" title="Abstract">arXiv:2110.03544</a> (replaced) [<a href="/pdf/2110.03544" title="Download PDF">pdf</a>, <a href="/format/2110.03544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Unsupervised Region-Aware Registration Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE International Conference on Image Processing (ICIP). IEEE, 2023. arXiv admin note: text overlap with <a href="/abs/2006.06200">arXiv:2006.06200</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.08021" title="Abstract">arXiv:2110.08021</a> (replaced) [<a href="/pdf/2110.08021" title="Download PDF">pdf</a>, <a href="/format/2110.08021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StreaMulT: Streaming Multimodal Transformer for Heterogeneous and  Arbitrary Long Sequential Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pellegrain%2C+V">Victor Pellegrain</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Tami%2C+M">Myriam Tami</a> (2), 
<a href="/search/cs?searchtype=author&query=Batteux%2C+M">Michel Batteux</a> (1), 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a> (2) ((1) Institut de Recherche Technologique SystemX, (2) Universit&#xe9; Paris-Saclay, CentraleSup&#xe9;lec, MICS)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.02164" title="Abstract">arXiv:2111.02164</a> (replaced) [<a href="/pdf/2111.02164" title="Download PDF">pdf</a>, <a href="/format/2111.02164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data structure &gt; labels? Unsupervised heuristics for SVM hyperparameter  estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cholewa%2C+M">Micha&#x142; Cholewa</a>, 
<a href="/search/cs?searchtype=author&query=Romaszewski%2C+M">Micha&#x142; Romaszewski</a>, 
<a href="/search/cs?searchtype=author&query=G%C5%82omb%2C+P">Przemys&#x142;aw G&#x142;omb</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.10936" title="Abstract">arXiv:2201.10936</a> (replaced) [<a href="/pdf/2201.10936" title="Download PDF">pdf</a>, <a href="/format/2201.10936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+R%C3%BCtte%2C+D">Dimitri von R&#xfc;tte</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+L">Luca Biggio</a>, 
<a href="/search/cs?searchtype=author&query=Kilcher%2C+Y">Yannic Kilcher</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.05560" title="Abstract">arXiv:2202.05560</a> (replaced) [<a href="/pdf/2202.05560" title="Download PDF">pdf</a>, <a href="/format/2202.05560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Adams%2C+R">Reuben Adams</a>, 
<a href="/search/stat?searchtype=author&query=Shawe-Taylor%2C+J">John Shawe-Taylor</a>, 
<a href="/search/stat?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07082" title="Abstract">arXiv:2202.07082</a> (replaced) [<a href="/pdf/2202.07082" title="Download PDF">pdf</a>, <a href="/format/2202.07082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Graphs with Heterophily: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.15402" title="Abstract">arXiv:2203.15402</a> (replaced) [<a href="/pdf/2203.15402" title="Download PDF">pdf</a>, <a href="/format/2203.15402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed deep-learning applications to experimental fluid  mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Eivazi%2C+H">Hamidreza Eivazi</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yuning Wang</a>, 
<a href="/search/physics?searchtype=author&query=Vinuesa%2C+R">Ricardo Vinuesa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06862" title="Abstract">arXiv:2204.06862</a> (replaced) [<a href="/pdf/2204.06862" title="Download PDF">pdf</a>, <a href="/format/2204.06862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Identity-Preserved Framework for Human Motion Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jingzhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shiqi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE Transactions on Information Forensics and Security (TIFS), see <a href="https://ieeexplore.ieee.org/document/10426775">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05396" title="Abstract">arXiv:2205.05396</a> (replaced) [<a href="/pdf/2205.05396" title="Download PDF">pdf</a>, <a href="/format/2205.05396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Fairness for Machine Learning on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laclau%2C+C">Charlotte Laclau</a>, 
<a href="/search/cs?searchtype=author&query=Largeron%2C+C">Christine Largeron</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+M">Manvi Choudhary</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05795" title="Abstract">arXiv:2205.05795</a> (replaced) [<a href="/pdf/2205.05795" title="Download PDF">pdf</a>, <a href="/format/2205.05795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic Machine Learning with an Application to Chemistry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sai%2C+E+E">Ezzeddine El Sai</a>, 
<a href="/search/math?searchtype=author&query=Gara%2C+P">Parker Gara</a>, 
<a href="/search/math?searchtype=author&query=Pflaum%2C+M+J">Markus J. Pflaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> minor revision with expanded section on preliminaries from real algebraic geometry and a revised section on related results, to appear in "Foundations of Data Science"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Computational Geometry (cs.CG); Machine Learning (cs.LG); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00333" title="Abstract">arXiv:2206.00333</a> (replaced) [<a href="/pdf/2206.00333" title="Download PDF">pdf</a>, <a href="/ps/2206.00333" title="Download PostScript">ps</a>, <a href="/format/2206.00333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\mathcal{S}$-adic characterization of minimal dendric shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gheeraert%2C+F">France Gheeraert</a>, 
<a href="/search/math?searchtype=author&query=Leroy%2C+J">Julien Leroy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12515" title="Abstract">arXiv:2207.12515</a> (replaced) [<a href="/pdf/2207.12515" title="Download PDF">pdf</a>, <a href="/format/2207.12515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Trustworthy Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yingqiang Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zuohui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Juntao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zelong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yikun Xian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Transactions on Recommender Systems (TORS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08011" title="Abstract">arXiv:2208.08011</a> (replaced) [<a href="/pdf/2208.08011" title="Download PDF">pdf</a>, <a href="/format/2208.08011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lingxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D">Dong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yujie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, accepted by WWW 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08199" title="Abstract">arXiv:2209.08199</a> (replaced) [<a href="/pdf/2209.08199" title="Download PDF">pdf</a>, <a href="/format/2209.08199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsiao%2C+Y">Yu-Chung Hsiao</a>, 
<a href="/search/cs?searchtype=author&query=Zubach%2C+F">Fedir Zubach</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maria Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09321" title="Abstract">arXiv:2209.09321</a> (replaced) [<a href="/pdf/2209.09321" title="Download PDF">pdf</a>, <a href="/format/2209.09321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully-Automated Verification of Linear Systems Using Inner- and  Outer-Approximations of Reachable Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wetzlinger%2C+M">Mark Wetzlinger</a>, 
<a href="/search/math?searchtype=author&query=Kochdumper%2C+N">Niklas Kochdumper</a>, 
<a href="/search/math?searchtype=author&query=Bak%2C+S">Stanley Bak</a>, 
<a href="/search/math?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00131" title="Abstract">arXiv:2210.00131</a> (replaced) [<a href="/pdf/2210.00131" title="Download PDF">pdf</a>, <a href="/format/2210.00131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underspecification in Language Modeling Tasks: A Causality-Informed  Study of Gendered Pronoun Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McMilin%2C+E">Emily McMilin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 41 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09023" title="Abstract">arXiv:2210.09023</a> (replaced) [<a href="/pdf/2210.09023" title="Download PDF">pdf</a>, <a href="/format/2210.09023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ratio convergence rates for Euclidean first-passage percolation:  Applications to the graph infinity Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bungert%2C+L">Leon Bungert</a>, 
<a href="/search/math?searchtype=author&query=Calder%2C+J">Jeff Calder</a>, 
<a href="/search/math?searchtype=author&query=Roith%2C+T">Tim Roith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13664" title="Abstract">arXiv:2210.13664</a> (replaced) [<a href="/pdf/2210.13664" title="Download PDF">pdf</a>, <a href="/format/2210.13664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher  Mixture Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conti%2C+J">Jean-R&#xe9;my Conti</a>, 
<a href="/search/cs?searchtype=author&query=Noiry%2C+N">Nathan Noiry</a>, 
<a href="/search/cs?searchtype=author&query=Despiegel%2C+V">Vincent Despiegel</a>, 
<a href="/search/cs?searchtype=author&query=Gentric%2C+S">St&#xe9;phane Gentric</a>, 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9men%C3%A7on%2C+S">St&#xe9;phan Cl&#xe9;men&#xe7;on</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:4344-4369, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15996" title="Abstract">arXiv:2210.15996</a> (replaced) [<a href="/pdf/2210.15996" title="Download PDF">pdf</a>, <a href="/format/2210.15996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalized Few-Shot Open-Set Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+B">Binyi Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhong Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12620" title="Abstract">arXiv:2211.12620</a> (replaced) [<a href="/pdf/2211.12620" title="Download PDF">pdf</a>, <a href="/format/2211.12620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promises and Pitfalls of Threshold-based Auto-labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vishwakarma%2C+H">Harit Vishwakarma</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Heguang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sala%2C+F">Frederic Sala</a>, 
<a href="/search/cs?searchtype=author&query=Vinayak%2C+R+K">Ramya Korlakai Vinayak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Thirty Seventh Conference on Neural Information Processing Systems
  (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03533" title="Abstract">arXiv:2212.03533</a> (replaced) [<a href="/pdf/2212.03533" title="Download PDF">pdf</a>, <a href="/format/2212.03533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Embeddings by Weakly-Supervised Contrastive Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+B">Binxing Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Daxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+R">Rangan Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, v2 fixes the SummEval numbers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08897" title="Abstract">arXiv:2212.08897</a> (replaced) [<a href="/pdf/2212.08897" title="Download PDF">pdf</a>, <a href="/format/2212.08897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolQA: Polish Question Answering Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rybak%2C+P">Piotr Rybak</a>, 
<a href="/search/cs?searchtype=author&query=Przyby%C5%82a%2C+P">Piotr Przyby&#x142;a</a>, 
<a href="/search/cs?searchtype=author&query=Ogrodniczuk%2C+M">Maciej Ogrodniczuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01488" title="Abstract">arXiv:2301.01488</a> (replaced) [<a href="/pdf/2301.01488" title="Download PDF">pdf</a>, <a href="/format/2301.01488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informed Down-Sampled Lexicase Selection: Identifying productive  training cases for efficient problem solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boldi%2C+R">Ryan Boldi</a>, 
<a href="/search/cs?searchtype=author&query=Briesch%2C+M">Martin Briesch</a>, 
<a href="/search/cs?searchtype=author&query=Sobania%2C+D">Dominik Sobania</a>, 
<a href="/search/cs?searchtype=author&query=Lalejini%2C+A">Alexander Lalejini</a>, 
<a href="/search/cs?searchtype=author&query=Helmuth%2C+T">Thomas Helmuth</a>, 
<a href="/search/cs?searchtype=author&query=Rothlauf%2C+F">Franz Rothlauf</a>, 
<a href="/search/cs?searchtype=author&query=Ofria%2C+C">Charles Ofria</a>, 
<a href="/search/cs?searchtype=author&query=Spector%2C+L">Lee Spector</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Publication in the Evolutionary Computation Journal by MIT Press
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03719" title="Abstract">arXiv:2302.03719</a> (replaced) [<a href="/pdf/2302.03719" title="Download PDF">pdf</a>, <a href="/format/2302.03719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persuading a Behavioral Agent: Approximately Best Responding and  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The main results in this draft have been subsumed by our later paper "Persuading a Learning Agent"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04124" title="Abstract">arXiv:2302.04124</a> (replaced) [<a href="/pdf/2302.04124" title="Download PDF">pdf</a>, <a href="/ps/2302.04124" title="Download PostScript">ps</a>, <a href="/format/2302.04124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An extended Gauss-Newton method for full waveform inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gholami%2C+A">Ali Gholami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07433" title="Abstract">arXiv:2302.07433</a> (replaced) [<a href="/pdf/2302.07433" title="Download PDF">pdf</a>, <a href="/format/2302.07433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Global LiDAR Localization: Challenges, Advances and Open  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Huan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuecheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sha Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Stachniss%2C+C">Cyrill Stachniss</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpeted by International Journal of Computer Vision (IJCV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09656" title="Abstract">arXiv:2302.09656</a> (replaced) [<a href="/pdf/2302.09656" title="Download PDF">pdf</a>, <a href="/format/2302.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credal Bayesian Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Souradeep Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+K+J">Kuk Jin Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Vivian Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+R">Radoslav Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Sokolsky%2C+O">Oleg Sokolsky</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12906" title="Abstract">arXiv:2302.12906</a> (replaced) [<a href="/pdf/2302.12906" title="Download PDF">pdf</a>, <a href="/format/2302.12906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Invertible Quantum Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Rousselot%2C+A">Armand Rousselot</a>, 
<a href="/search/hep-ph?searchtype=author&query=Spannowsky%2C+M">Michael Spannowsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures Changes in v2: Add references 49-51, provided gitlab link to code repository Changes in v3: Incorporate rebuttal from <a href="https://scipost.org/submissions/2302.12906v2/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02536" title="Abstract">arXiv:2303.02536</a> (replaced) [<a href="/pdf/2303.02536" title="Download PDF">pdf</a>, <a href="/format/2303.02536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Alignments Between Interpretable Causal Variables and  Distributed Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Atticus Geiger</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+T">Thomas Icard</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N+D">Noah D. Goodman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03060" title="Abstract">arXiv:2303.03060</a> (replaced) [<a href="/pdf/2303.03060" title="Download PDF">pdf</a>, <a href="/format/2303.03060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical analysis of a nonsmooth quasilinear elliptic control problem:  II. Finite element discretization and error estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Clason%2C+C">Christian Clason</a>, 
<a href="/search/math?searchtype=author&query=Nhu%2C+V+H">Vu Huu Nhu</a>, 
<a href="/search/math?searchtype=author&query=R%C3%B6sch%2C+A">Arnd R&#xf6;sch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Split out from <a href="/abs/2203.16865">arXiv:2203.16865</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04040" title="Abstract">arXiv:2303.04040</a> (replaced) [<a href="/pdf/2303.04040" title="Download PDF">pdf</a>, <a href="/format/2303.04040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification of Spatiotemporal Travel Demand with  Probabilistic Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+D">Dingyi Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Koutsopoulos%2C+H">Haris Koutsopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinhua Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04204" title="Abstract">arXiv:2303.04204</a> (replaced) [<a href="/pdf/2303.04204" title="Download PDF">pdf</a>, <a href="/format/2303.04204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep hybrid model with satellite imagery: how to combine demand modeling  and computer vision for behavior analysis?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yunhan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongzhou Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+J">Joan Walker</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transportation Research Part B: Methodological, Volume 179, 2024,
  102869
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10976" title="Abstract">arXiv:2303.10976</a> (replaced) [<a href="/pdf/2303.10976" title="Download PDF">pdf</a>, <a href="/format/2303.10976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Disturbance and Dual-Path Constraint Network for Occluded  Person Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jiaer Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Lei Tan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+P">Pingyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mingbo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongjian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liujuan Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11291" title="Abstract">arXiv:2303.11291</a> (replaced) [<a href="/pdf/2303.11291" title="Download PDF">pdf</a>, <a href="/format/2303.11291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobiprox: Supporting Dynamic Approximate Computing on Mobiles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabjan%C4%8Di%C4%8D%2C+M">Matev&#x17e; Fabjan&#x10d;i&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Machidon%2C+O">Octavian Machidon</a>, 
<a href="/search/cs?searchtype=author&query=Sharif%2C+H">Hashim Sharif</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Misailovi%C4%87%2C+S">Sa&#x161;a Misailovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Pejovi%C4%87%2C+V">Veljko Pejovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures. IEEE Internet of Things Journal (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12665" title="Abstract">arXiv:2303.12665</a> (replaced) [<a href="/pdf/2303.12665" title="Download PDF">pdf</a>, <a href="/format/2303.12665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Identify Stance Without Target Arguments? A Study for Rumour  Stance Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>, 
<a href="/search/cs?searchtype=author&query=Scarton%2C+C">Carolina Scarton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13672" title="Abstract">arXiv:2303.13672</a> (replaced) [<a href="/pdf/2303.13672" title="Download PDF">pdf</a>, <a href="/format/2303.13672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Level Set Topology Optimization Using Unfitted Finite Elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mallon%2C+C+N">Connor N. Mallon</a>, 
<a href="/search/cs?searchtype=author&query=Thornton%2C+A+W">Aaron W. Thornton</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+M+R">Matthew R. Hill</a>, 
<a href="/search/cs?searchtype=author&query=Badia%2C+S">Santiago Badia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages + refs, 10 figs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16133" title="Abstract">arXiv:2303.16133</a> (replaced) [<a href="/pdf/2303.16133" title="Download PDF">pdf</a>, <a href="/format/2303.16133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing and Addressing Cross-Task Inconsistency in Unified  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maharana%2C+A">Adyasha Maharana</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+A">Amita Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+C">Christopher Clark</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Kembhavi%2C+A">Aniruddha Kembhavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TMLR 2024; Project Website: <a href="https://adymaharana.github.io/cococon/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16156" title="Abstract">arXiv:2303.16156</a> (replaced) [<a href="/pdf/2303.16156" title="Download PDF">pdf</a>, <a href="/ps/2303.16156" title="Download PostScript">ps</a>, <a href="/format/2303.16156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remarks on on the derivatives of rational B&#xe9;zier curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Mao Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02558" title="Abstract">arXiv:2304.02558</a> (replaced) [<a href="/pdf/2304.02558" title="Download PDF">pdf</a>, <a href="/ps/2304.02558" title="Download PostScript">ps</a>, <a href="/format/2304.02558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple 1.5-Approximation Algorithm for a Wide Range of Max-SMTI  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cs%C3%A1ji%2C+G">Gergely Cs&#xe1;ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07999" title="Abstract">arXiv:2304.07999</a> (replaced) [<a href="/e-print/2304.07999" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everyone Can Be Picasso? A Computational Framework into the Myth of  Human versus AI Painting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yilin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wei Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The results in Figure 3 in Section 4 have error due to my mistakes in feature calculation. Particularly the error is in the classification accuracy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10031" title="Abstract">arXiv:2304.10031</a> (replaced) [<a href="/pdf/2304.10031" title="Download PDF">pdf</a>, <a href="/format/2304.10031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectures of Topological Deep Learning: A Survey of Message-Passing  Topological Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Sanborn%2C+S">Sophia Sanborn</a>, 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11625" title="Abstract">arXiv:2304.11625</a> (replaced) [<a href="/pdf/2304.11625" title="Download PDF">pdf</a>, <a href="/ps/2304.11625" title="Download PostScript">ps</a>, <a href="/format/2304.11625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaningful Causal Aggregation and Paradoxical Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuchen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Budhathoki%2C+K">Kailash Budhathoki</a>, 
<a href="/search/cs?searchtype=author&query=Kuebler%2C+J">Jonas Kuebler</a>, 
<a href="/search/cs?searchtype=author&query=Janzing%2C+D">Dominik Janzing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CLeaR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12033" title="Abstract">arXiv:2304.12033</a> (replaced) [<a href="/pdf/2304.12033" title="Download PDF">pdf</a>, <a href="/format/2304.12033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatial Calibration Method for Robust Cooperative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhiying Song</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tenghui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hailiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+F">Fuxi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00450" title="Abstract">arXiv:2305.00450</a> (replaced) [<a href="/pdf/2305.00450" title="Download PDF">pdf</a>, <a href="/format/2305.00450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMILE: Single-turn to Multi-turn Inclusive Language Expansion via  ChatGPT for Mental Health Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Huachuan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hongliang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01938" title="Abstract">arXiv:2305.01938</a> (replaced) [<a href="/pdf/2305.01938" title="Download PDF">pdf</a>, <a href="/format/2305.01938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text  Documents via Semantic-Oriented Hierarchical Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fengbin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zifeng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02299" title="Abstract">arXiv:2305.02299</a> (replaced) [<a href="/pdf/2305.02299" title="Download PDF">pdf</a>, <a href="/format/2305.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Sparse Training with Structured Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lasby%2C+M">Mike Lasby</a>, 
<a href="/search/cs?searchtype=author&query=Golubeva%2C+A">Anna Golubeva</a>, 
<a href="/search/cs?searchtype=author&query=Evci%2C+U">Utku Evci</a>, 
<a href="/search/cs?searchtype=author&query=Nica%2C+M">Mihai Nica</a>, 
<a href="/search/cs?searchtype=author&query=Ioannou%2C+Y">Yani Ioannou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024, 29 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05332" title="Abstract">arXiv:2305.05332</a> (replaced) [<a href="/pdf/2305.05332" title="Download PDF">pdf</a>, <a href="/ps/2305.05332" title="Download PostScript">ps</a>, <a href="/format/2305.05332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Multi-Message Private Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Ali Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jahani-Nezhad%2C+T">Tayyebeh Jahani-Nezhad</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mingyue Ji</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05415" title="Abstract">arXiv:2305.05415</a> (replaced) [<a href="/pdf/2305.05415" title="Download PDF">pdf</a>, <a href="/ps/2305.05415" title="Download PostScript">ps</a>, <a href="/format/2305.05415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheme-Theoretic Approach to Computational Complexity. III. SETH
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87ivril%2C+A">Ali &#xc7;ivril</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages. Updated the definitions according to the first paper in the series
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05676" title="Abstract">arXiv:2305.05676</a> (replaced) [<a href="/pdf/2305.05676" title="Download PDF">pdf</a>, <a href="/ps/2305.05676" title="Download PostScript">ps</a>, <a href="/format/2305.05676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheme-Theoretic Approach to Computational Complexity. IV. A New  Perspective on Hardness of Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87ivril%2C+A">Ali &#xc7;ivril</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages. Updated the definitions according to the first paper in the series
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06594" title="Abstract">arXiv:2305.06594</a> (replaced) [<a href="/pdf/2305.06594" title="Download PDF">pdf</a>, <a href="/format/2305.06594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V2Meow: Meowing to the Visual Beat via Video-to-Music Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+K">Kun Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+Y">Judith Yue Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingqing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kuzmin%2C+D">Dima Kuzmin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joonseok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Donahue%2C+C">Chris Donahue</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+A">Aren Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Verzetti%2C+M">Mauro Verzetti</a>, 
<a href="/search/cs?searchtype=author&query=Denk%2C+T+I">Timo I. Denk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at AAAI 2024, music samples available at <a href="https://tinyurl.com/v2meow">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13168" title="Abstract">arXiv:2305.13168</a> (replaced) [<a href="/pdf/2305.13168" title="Download PDF">pdf</a>, <a href="/format/2305.13168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities  and Future Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuqi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14961" title="Abstract">arXiv:2305.14961</a> (replaced) [<a href="/pdf/2305.14961" title="Download PDF">pdf</a>, <a href="/format/2305.14961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Survival Analysis: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wiegrebe%2C+S">Simon Wiegrebe</a>, 
<a href="/search/stat?searchtype=author&query=Kopper%2C+P">Philipp Kopper</a>, 
<a href="/search/stat?searchtype=author&query=Sonabend%2C+R">Raphael Sonabend</a>, 
<a href="/search/stat?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/stat?searchtype=author&query=Bender%2C+A">Andreas Bender</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 7 figures, 2 tables, 1 interactive table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artif Intell Rev 57, 65 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16202" title="Abstract">arXiv:2305.16202</a> (replaced) [<a href="/pdf/2305.16202" title="Download PDF">pdf</a>, <a href="/format/2305.16202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-SGD Without Clipping: The Lipschitz Neural Network Way
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bethune%2C+L">Louis Bethune</a>, 
<a href="/search/cs?searchtype=author&query=Massena%2C+T">Thomas Massena</a>, 
<a href="/search/cs?searchtype=author&query=Boissin%2C+T">Thibaut Boissin</a>, 
<a href="/search/cs?searchtype=author&query=Prudent%2C+Y">Yannick Prudent</a>, 
<a href="/search/cs?searchtype=author&query=Friedrich%2C+C">Corentin Friedrich</a>, 
<a href="/search/cs?searchtype=author&query=Mamalet%2C+F">Franck Mamalet</a>, 
<a href="/search/cs?searchtype=author&query=Bellet%2C+A">Aurelien Bellet</a>, 
<a href="/search/cs?searchtype=author&query=Serrurier%2C+M">Mathieu Serrurier</a>, 
<a href="/search/cs?searchtype=author&query=Vigouroux%2C+D">David Vigouroux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, published at International Conferences on Learning Representations (ICLR), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18903" title="Abstract">arXiv:2305.18903</a> (replaced) [<a href="/pdf/2305.18903" title="Download PDF">pdf</a>, <a href="/format/2305.18903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the performance of Learned Controllers in Behavior Trees using  Value Function Estimates at Switching Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kartasev%2C+M">Mart Kartasev</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96gren%2C+P">Petter &#xd6;gren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18970" title="Abstract">arXiv:2305.18970</a> (replaced) [<a href="/pdf/2305.18970" title="Download PDF">pdf</a>, <a href="/format/2305.18970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SENet: A Spectral Filtering Approach to Represent Exemplars for Few-shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03552" title="Abstract">arXiv:2306.03552</a> (replaced) [<a href="/pdf/2306.03552" title="Download PDF">pdf</a>, <a href="/format/2306.03552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Regularized Policy Optimization on Data with Dynamics Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhenghai Xue</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Q">Qingpeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05587" title="Abstract">arXiv:2306.05587</a> (replaced) [<a href="/pdf/2306.05587" title="Download PDF">pdf</a>, <a href="/format/2306.05587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MC-NN: An End-to-End Multi-Channel Neural Network Approach for  Predicting Influenza A Virus Hosts and Antigenic Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wojtczak%2C+D">Dominik Wojtczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version submitted to the SN Computer Science; Published in the SN Computer Science 2023; V2: minor updates were made to the Results section; V3: minor updates regarding data description; V4: correct the time stamps mentioned in the legends of Figures 1 and 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05843" title="Abstract">arXiv:2306.05843</a> (replaced) [<a href="/pdf/2306.05843" title="Download PDF">pdf</a>, <a href="/format/2306.05843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Batch Sizes for Active Learning A Probabilistic Numerics  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adachi%2C+M">Masaki Adachi</a>, 
<a href="/search/cs?searchtype=author&query=Hayakawa%2C+S">Satoshi Hayakawa</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B8rgensen%2C+M">Martin J&#xf8;rgensen</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Oberhauser%2C+H">Harald Oberhauser</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AISTATS 2024. 33 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06210" title="Abstract">arXiv:2306.06210</a> (replaced) [<a href="/pdf/2306.06210" title="Download PDF">pdf</a>, <a href="/format/2306.06210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Model Attribution of Generative Models Through Final-Layer  Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laszkiewicz%2C+M">Mike Laszkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Ricker%2C+J">Jonas Ricker</a>, 
<a href="/search/cs?searchtype=author&query=Lederer%2C+J">Johannes Lederer</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Asja Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06268" title="Abstract">arXiv:2306.06268</a> (replaced) [<a href="/pdf/2306.06268" title="Download PDF">pdf</a>, <a href="/ps/2306.06268" title="Download PostScript">ps</a>, <a href="/format/2306.06268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-stacked Generative Adversarial Network (AS-GAN)-empowered  Sensor Data Augmentation for Online Monitoring of Manufacturing System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09222" title="Abstract">arXiv:2306.09222</a> (replaced) [<a href="/pdf/2306.09222" title="Download PDF">pdf</a>, <a href="/format/2306.09222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ramnath Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Majmundar%2C+K">Kushal Majmundar</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Suggala%2C+A+S">Arun Sai Suggala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06520" title="Abstract">arXiv:2307.06520</a> (replaced) [<a href="/pdf/2307.06520" title="Download PDF">pdf</a>, <a href="/ps/2307.06520" title="Download PostScript">ps</a>, <a href="/format/2307.06520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Migrating to Post-Quantum Cryptography: Security  Dependency Analysis and Case Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+K+F">Khondokar Fida Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Simpson%2C+L">Leonie Simpson</a>, 
<a href="/search/cs?searchtype=author&query=Baee%2C+M+A+R">Mir Ali Rezazadeh Baee</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+C">Chadni Islam</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+Z">Ziaur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+W">Warren Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Gauravaram%2C+P">Praveen Gauravaram</a>, 
<a href="/search/cs?searchtype=author&query=McKague%2C+M">Matthew McKague</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07515" title="Abstract">arXiv:2307.07515</a> (replaced) [<a href="/pdf/2307.07515" title="Download PDF">pdf</a>, <a href="/ps/2307.07515" title="Download PostScript">ps</a>, <a href="/format/2307.07515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial intelligence is algorithmic mimicry: why artificial &quot;agents&quot;  are not (and won&#x27;t be) proper agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaeger%2C+J">Johannes Jaeger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07636" title="Abstract">arXiv:2307.07636</a> (replaced) [<a href="/pdf/2307.07636" title="Download PDF">pdf</a>, <a href="/format/2307.07636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissenting Explanations: Leveraging Disagreement to Reduce Model  Overreliance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reingold%2C+O">Omer Reingold</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J+H">Judy Hanwen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Talati%2C+A">Aditi Talati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2: AAAI 2024 V1: AI &amp; HCI Workshop at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09312" title="Abstract">arXiv:2307.09312</a> (replaced) [<a href="/pdf/2307.09312" title="Download PDF">pdf</a>, <a href="/format/2307.09312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Modal Discussion Transformer: Integrating Text, Images and Graph  Transformers to Detect Hate Speech on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hebert%2C+L">Liam Hebert</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+G">Gaurav Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuxuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Sreenivas%2C+N+K">Nanda Kishore Sreenivas</a>, 
<a href="/search/cs?searchtype=author&query=Golab%2C+L">Lukasz Golab</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+R">Robin Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024 (AI for Social Impact Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11603" title="Abstract">arXiv:2307.11603</a> (replaced) [<a href="/pdf/2307.11603" title="Download PDF">pdf</a>, <a href="/format/2307.11603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded multitask U-Net using topological loss for vessel segmentation  and centerline extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Roug%C3%A9%2C+P">Pierre Roug&#xe9;</a>, 
<a href="/search/eess?searchtype=author&query=Passat%2C+N">Nicolas Passat</a>, 
<a href="/search/eess?searchtype=author&query=Merveille%2C+O">Odyss&#xe9;e Merveille</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14642" title="Abstract">arXiv:2307.14642</a> (replaced) [<a href="/pdf/2307.14642" title="Download PDF">pdf</a>, <a href="/ps/2307.14642" title="Download PostScript">ps</a>, <a href="/format/2307.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Convergence of Black-Box Variational Inference: Should We Stick  the Landing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+K">Kyurae Kim</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+Y">Yian Ma</a>, 
<a href="/search/stat?searchtype=author&query=Gardner%2C+J+R">Jacob R. Gardner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16500" title="Abstract">arXiv:2307.16500</a> (replaced) [<a href="/pdf/2307.16500" title="Download PDF">pdf</a>, <a href="/ps/2307.16500" title="Download PostScript">ps</a>, <a href="/format/2307.16500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciding Linear Height and Linear Size-to-Height Increase for Macro Tree  Transducers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallot%2C+P">Paul Gallot</a>, 
<a href="/search/cs?searchtype=author&query=Maneth%2C+S">Sebastian Maneth</a>, 
<a href="/search/cs?searchtype=author&query=Nakano%2C+K">Keisuke Nakano</a>, 
<a href="/search/cs?searchtype=author&query=Peyrat%2C+C">Charles Peyrat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01574" title="Abstract">arXiv:2308.01574</a> (replaced) [<a href="/pdf/2308.01574" title="Download PDF">pdf</a>, <a href="/ps/2308.01574" title="Download PostScript">ps</a>, <a href="/format/2308.01574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Another Hamiltonian Cycle in Bipartite Pfaffian Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rklund%2C+A">Andreas Bj&#xf6;rklund</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+P">Petteri Kaski</a>, 
<a href="/search/cs?searchtype=author&query=Nederlof%2C+J">Jesper Nederlof</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Adds analysis of Thomason's lollipop method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04901" title="Abstract">arXiv:2308.04901</a> (replaced) [<a href="/pdf/2308.04901" title="Download PDF">pdf</a>, <a href="/format/2308.04901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards true discovery of the differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvatov%2C+A">Alexander Hvatov</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+R">Roman Titov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Knowledge and Logical Reasoning in the Era of Data-driven Learning workshop at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05962" title="Abstract">arXiv:2308.05962</a> (replaced) [<a href="/pdf/2308.05962" title="Download PDF">pdf</a>, <a href="/format/2308.05962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralised Governance-Driven Architecture for Designing Foundation  Model based Systems: Exploring the Role of Blockchain in Responsible AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qinghua Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Paik%2C+H">Hye-Young Paik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06032" title="Abstract">arXiv:2308.06032</a> (replaced) [<a href="/pdf/2308.06032" title="Download PDF">pdf</a>, <a href="/format/2308.06032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models in Cryptocurrency Securities Cases: Can a GPT  Model Meaningfully Assist Lawyers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trozze%2C+A">Arianna Trozze</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+T">Toby Davies</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+B">Bennett Kleinberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08241" title="Abstract">arXiv:2308.08241</a> (replaced) [<a href="/pdf/2308.08241" title="Download PDF">pdf</a>, <a href="/format/2308.08241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEST: Text Prototype Aligned Embedding to Activate LLM&#x27;s Ability for  Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenxi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Shenda Hong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08493" title="Abstract">arXiv:2308.08493</a> (replaced) [<a href="/pdf/2308.08493" title="Download PDF">pdf</a>, <a href="/ps/2308.08493" title="Download PostScript">ps</a>, <a href="/format/2308.08493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Travel in LLMs: Tracing Data Contamination in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Surdeanu%2C+M">Mihai Surdeanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024 as a Spotlight paper (notable top 5%)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10410" title="Abstract">arXiv:2308.10410</a> (replaced) [<a href="/pdf/2308.10410" title="Download PDF">pdf</a>, <a href="/format/2308.10410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models on Wikipedia-Style Survey Generation: an  Evaluation in NLP Concepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingcheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+M">Moritz Blum</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dairui Liu</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+T">Tianwei She</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10547" title="Abstract">arXiv:2308.10547</a> (replaced) [<a href="/pdf/2308.10547" title="Download PDF">pdf</a>, <a href="/format/2308.10547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Riemannian Conjugate Gradient Method on the Stiefel  Manifold
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/math?searchtype=author&query=Ye%2C+H">Haishan Ye</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+M">Mengmeng Wang</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+T">Tianxin Huang</a>, 
<a href="/search/math?searchtype=author&query=Dai%2C+G">Guang Dai</a>, 
<a href="/search/math?searchtype=author&query=Tsang%2C+I+W">Ivor W.Tsang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Learning Representations, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10561" title="Abstract">arXiv:2308.10561</a> (replaced) [<a href="/pdf/2308.10561" title="Download PDF">pdf</a>, <a href="/format/2308.10561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial Transform Decoupling for Oriented Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongtian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yunjie Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qixiang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunfan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11984" title="Abstract">arXiv:2308.11984</a> (replaced) [<a href="/pdf/2308.11984" title="Download PDF">pdf</a>, <a href="/ps/2308.11984" title="Download PostScript">ps</a>, <a href="/format/2308.11984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-ergodic linear convergence property of the delayed gradient descent  under the strongly convexity and the Polyak-&#x141;ojasiewicz condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Choi%2C+H+J">Hyung Jun Choi</a>, 
<a href="/search/math?searchtype=author&query=Choi%2C+W">Woocheol Choi</a>, 
<a href="/search/math?searchtype=author&query=Seok%2C+J">Jinmyoung Seok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A result for the delayed SGD was added. accepted in Analysis and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15246" title="Abstract">arXiv:2308.15246</a> (replaced) [<a href="/pdf/2308.15246" title="Download PDF">pdf</a>, <a href="/format/2308.15246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Classification-Guided Approach for Adversarial Attacks against Neural  Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadrizadeh%2C+S">Sahar Sadrizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Dolamic%2C+L">Ljiljana Dolamic</a>, 
<a href="/search/cs?searchtype=author&query=Frossard%2C+P">Pascal Frossard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16898" title="Abstract">arXiv:2308.16898</a> (replaced) [<a href="/pdf/2308.16898" title="Download PDF">pdf</a>, <a href="/format/2308.16898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers as Support Vector Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarzanagh%2C+D+A">Davoud Ataee Tarzanagh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingcong Li</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Oymak%2C+S">Samet Oymak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proof of global convergence for gradient descent in the equal score setting has been fixed, referring to Theorem 2 of [TLZO23], and the experimental results have been extended
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01669" title="Abstract">arXiv:2309.01669</a> (replaced) [<a href="/pdf/2309.01669" title="Download PDF">pdf</a>, <a href="/format/2309.01669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Donkii: Can Annotation Error Detection Methods Find Errors in  Instruction-Tuning Datasets?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber-Genzel%2C+L">Leon Weber-Genzel</a>, 
<a href="/search/cs?searchtype=author&query=Litschko%2C+R">Robert Litschko</a>, 
<a href="/search/cs?searchtype=author&query=Artemova%2C+E">Ekaterina Artemova</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version for LAW-XVIII
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02233" title="Abstract">arXiv:2309.02233</a> (replaced) [<a href="/pdf/2309.02233" title="Download PDF">pdf</a>, <a href="/ps/2309.02233" title="Download PostScript">ps</a>, <a href="/format/2309.02233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Black-box LLMs with Medical Textbooks for Clinical Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yubo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xueguang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03882" title="Abstract">arXiv:2309.03882</a> (replaced) [<a href="/pdf/2309.03882" title="Download PDF">pdf</a>, <a href="/format/2309.03882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Are Not Robust Multiple Choice Selectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chujie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05092" title="Abstract">arXiv:2309.05092</a> (replaced) [<a href="/pdf/2309.05092" title="Download PDF">pdf</a>, <a href="/format/2309.05092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive conformal classification with noisy labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sesia%2C+M">Matteo Sesia</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y+X+R">Y. X. Rachel Wang</a>, 
<a href="/search/stat?searchtype=author&query=Tong%2C+X">Xin Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages (127 pages including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08469" title="Abstract">arXiv:2309.08469</a> (replaced) [<a href="/pdf/2309.08469" title="Download PDF">pdf</a>, <a href="/format/2309.08469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Silver Retriever: Advancing Neural Passage Retrieval for Polish Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rybak%2C+P">Piotr Rybak</a>, 
<a href="/search/cs?searchtype=author&query=Ogrodniczuk%2C+M">Maciej Ogrodniczuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09546" title="Abstract">arXiv:2309.09546</a> (replaced) [<a href="/pdf/2309.09546" title="Download PDF">pdf</a>, <a href="/format/2309.09546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training dynamic models using early exits for automatic speech  recognition on resource-constrained devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wright%2C+G+A">George August Wright</a>, 
<a href="/search/eess?searchtype=author&query=Cappellazzo%2C+U">Umberto Cappellazzo</a>, 
<a href="/search/eess?searchtype=author&query=Zaiem%2C+S">Salah Zaiem</a>, 
<a href="/search/eess?searchtype=author&query=Raj%2C+D">Desh Raj</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+L+O">Lucas Ondel Yang</a>, 
<a href="/search/eess?searchtype=author&query=Falavigna%2C+D">Daniele Falavigna</a>, 
<a href="/search/eess?searchtype=author&query=Ali%2C+M+N">Mohamed Nabih Ali</a>, 
<a href="/search/eess?searchtype=author&query=Brutti%2C+A">Alessio Brutti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the ICASSP Workshop Self-supervision in Audio, Speech and Beyond 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13007" title="Abstract">arXiv:2309.13007</a> (replaced) [<a href="/pdf/2309.13007" title="Download PDF">pdf</a>, <a href="/format/2309.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReConcile: Round-Table Conference Improves Reasoning via Consensus among  Diverse LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+C">Justin Chih-Yao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swarnadeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13606" title="Abstract">arXiv:2309.13606</a> (replaced) [<a href="/pdf/2309.13606" title="Download PDF">pdf</a>, <a href="/format/2309.13606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating progressive failure in laminated glass beams with a  layer-wise randomized phase-field solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J">Jaroslav Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Zemanov%C3%A1%2C+A">Alena Zemanov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Zeman%2C+J">Jan Zeman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 18 figures, and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15507" title="Abstract">arXiv:2309.15507</a> (replaced) [<a href="/pdf/2309.15507" title="Download PDF">pdf</a>, <a href="/format/2309.15507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Message Passing with Rigorous Guarantees for Pooled Data and  Quantitative Group Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+N">Nelvin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Cobo%2C+P+P">Pablo Pascual Cobo</a>, 
<a href="/search/cs?searchtype=author&query=Scarlett%2C+J">Jonathan Scarlett</a>, 
<a href="/search/cs?searchtype=author&query=Venkataramanan%2C+R">Ramji Venkataramanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16044" title="Abstract">arXiv:2309.16044</a> (replaced) [<a href="/pdf/2309.16044" title="Download PDF">pdf</a>, <a href="/ps/2309.16044" title="Download PostScript">ps</a>, <a href="/format/2309.16044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Adaptive Online Learning Using Refined Discretization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Heng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cutkosky%2C+A">Ashok Cutkosky</a>, 
<a href="/search/cs?searchtype=author&query=Paschalidis%2C+I+C">Ioannis Ch. Paschalidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ALT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16060" title="Abstract">arXiv:2309.16060</a> (replaced) [<a href="/pdf/2309.16060" title="Download PDF">pdf</a>, <a href="/format/2309.16060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Single-channel Speech Enhancement Improve Keyword Spotting  Accuracy? A Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Brueggeman%2C+A">Avamarie Brueggeman</a>, 
<a href="/search/eess?searchtype=author&query=Higuchi%2C+T">Takuya Higuchi</a>, 
<a href="/search/eess?searchtype=author&query=Delfarah%2C+M">Masood Delfarah</a>, 
<a href="/search/eess?searchtype=author&query=Shum%2C+S">Stephen Shum</a>, 
<a href="/search/eess?searchtype=author&query=Garg%2C+V">Vineet Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16292" title="Abstract">arXiv:2309.16292</a> (replaced) [<a href="/pdf/2309.16292" title="Download PDF">pdf</a>, <a href="/format/2309.16292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Licheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Daocheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pinlong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+M">Min Dou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00313" title="Abstract">arXiv:2310.00313</a> (replaced) [<a href="/pdf/2310.00313" title="Download PDF">pdf</a>, <a href="/format/2310.00313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding In-Context Learning: Neuroscience-inspired Analysis of  Representations in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefi%2C+S">Safoora Yousefi</a>, 
<a href="/search/cs?searchtype=author&query=Betthauser%2C+L">Leo Betthauser</a>, 
<a href="/search/cs?searchtype=author&query=Hasanbeig%2C+H">Hosein Hasanbeig</a>, 
<a href="/search/cs?searchtype=author&query=Milli%C3%A8re%2C+R">Rapha&#xeb;l Milli&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03032" title="Abstract">arXiv:2310.03032</a> (replaced) [<a href="/pdf/2310.03032" title="Download PDF">pdf</a>, <a href="/format/2310.03032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-enhanced Optimizers for Structure-aware Recommendation Embedding  Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03774" title="Abstract">arXiv:2310.03774</a> (replaced) [<a href="/pdf/2310.03774" title="Download PDF">pdf</a>, <a href="/ps/2310.03774" title="Download PostScript">ps</a>, <a href="/format/2310.03774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Game Strategies for Social Networks with Self-Interested  Individuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jond%2C+H+B">Hossein B. Jond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Affiliation and Acknowledgment Correction to previous versions of "Differential Game Strategies for Social Networks with Self-Interested Individuals," <a href="/abs/2310.03774">arXiv:2310.03774</a>. Corrections were made by the author. arXiv admin note: substantial text overlap with <a href="/abs/2310.03095">arXiv:2310.03095</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03900" title="Abstract">arXiv:2310.03900</a> (replaced) [<a href="/pdf/2310.03900" title="Download PDF">pdf</a>, <a href="/ps/2310.03900" title="Download PostScript">ps</a>, <a href="/format/2310.03900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Game Approach to Multi-dimensional Opinion Dynamics in Social Networks  with Stubborn Strategist Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jond%2C+H+B">Hossein B. Jond</a>, 
<a href="/search/cs?searchtype=author&query=Y%C4%B1ld%C4%B1z%2C+A">Aykut Y&#x131;ld&#x131;z</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Affiliation and Acknowledgment Correction to previous versions of "A Game Approach to Multi-dimensional Opinion Dynamics in Social Networks with Stubborn Strategist Agents," <a href="/abs/2310.03900">arXiv:2310.03900</a>. Corrections correspond to the first author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04474" title="Abstract">arXiv:2310.04474</a> (replaced) [<a href="/pdf/2310.04474" title="Download PDF">pdf</a>, <a href="/format/2310.04474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinger Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xeirui Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yicheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Rui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jing Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05130" title="Abstract">arXiv:2310.05130</a> (replaced) [<a href="/pdf/2310.05130" title="Download PDF">pdf</a>, <a href="/format/2310.05130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text  via Conditional Probability Curvature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangsheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanbin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05231" title="Abstract">arXiv:2310.05231</a> (replaced) [<a href="/pdf/2310.05231" title="Download PDF">pdf</a>, <a href="/format/2310.05231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MindfulDiary: Harnessing Large Language Model to Support Psychiatric  Patients&#x27; Journaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taewan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Seolyeong Bae</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+A">Hyun Ah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Su-woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hwajung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chanmo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures, 4 tables. Accepted at ACM CHI 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05922" title="Abstract">arXiv:2310.05922</a> (replaced) [<a href="/pdf/2310.05922" title="Download PDF">pdf</a>, <a href="/format/2310.05922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video  editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+Y">Yuren Cong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengmeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+C">Christian Simon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shoufa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yanping Xie</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Rua%2C+J">Juan-Manuel Perez-Rua</a>, 
<a href="/search/cs?searchtype=author&query=Rosenhahn%2C+B">Bodo Rosenhahn</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sen He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR2024. Project page: <a href="https://flatten-video-editing.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09242" title="Abstract">arXiv:2310.09242</a> (replaced) [<a href="/pdf/2310.09242" title="Download PDF">pdf</a>, <a href="/format/2310.09242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multifaceted Look at Starlink Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohan%2C+N">Nitinder Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Ferguson%2C+A">Andrew Ferguson</a>, 
<a href="/search/cs?searchtype=author&query=Cech%2C+H">Hendrik Cech</a>, 
<a href="/search/cs?searchtype=author&query=Renatin%2C+P+R">Prakita Rayyan Renatin</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+R">Rohan Bose</a>, 
<a href="/search/cs?searchtype=author&query=Marina%2C+M">Mahesh Marina</a>, 
<a href="/search/cs?searchtype=author&query=Ott%2C+J">J&#xf6;rg Ott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACM Web Conference 2024 (WWW 24)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of ACM Web Conference 2024 (WWW 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09358" title="Abstract">arXiv:2310.09358</a> (replaced) [<a href="/pdf/2310.09358" title="Download PDF">pdf</a>, <a href="/format/2310.09358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bad Values but Good Behavior: Learning Highly Misspecified Bandits and  MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+D">Debangshu Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Aditya Gopalan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10642" title="Abstract">arXiv:2310.10642</a> (replaced) [<a href="/pdf/2310.10642" title="Download PDF">pdf</a>, <a href="/format/2310.10642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Photorealistic Dynamic Scene Representation and Rendering with  4D Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zeyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongye Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zijie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10690" title="Abstract">arXiv:2310.10690</a> (replaced) [<a href="/pdf/2310.10690" title="Download PDF">pdf</a>, <a href="/format/2310.10690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for In-Context Student Modeling: Synthesizing  Student&#x27;s Behavior in Visual Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M+H">Manh Hung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tschiatschek%2C+S">Sebastian Tschiatschek</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Adish Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12792" title="Abstract">arXiv:2310.12792</a> (replaced) [<a href="/pdf/2310.12792" title="Download PDF">pdf</a>, <a href="/format/2310.12792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Optimal Locality Sensitive Orderings in Euclidean Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhimeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Har-Peled%2C+S">Sariel Har-Peled</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SoCG 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13648" title="Abstract">arXiv:2310.13648</a> (replaced) [<a href="/pdf/2310.13648" title="Download PDF">pdf</a>, <a href="/format/2310.13648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT as a Software Development Bot: A Project-based Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waseem%2C+M">Muhammad Waseem</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+T">Teerath Das</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aakash Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Fehmideh%2C+M">Mahdi Fehmideh</a>, 
<a href="/search/cs?searchtype=author&query=Mikkonen%2C+T">Tommi Mikkonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 19th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13792" title="Abstract">arXiv:2310.13792</a> (replaced) [<a href="/pdf/2310.13792" title="Download PDF">pdf</a>, <a href="/format/2310.13792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Densest Subhypergraph: Negative Supermodular Functions and Strongly  Localized Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gleich%2C+D+F">David F. Gleich</a>, 
<a href="/search/cs?searchtype=author&query=Veldt%2C+N">Nate Veldt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14763" title="Abstract">arXiv:2310.14763</a> (replaced) [<a href="/pdf/2310.14763" title="Download PDF">pdf</a>, <a href="/format/2310.14763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Externally Valid Policy Evaluation Combining Trial and Observational  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ek%2C+S">Sofia Ek</a>, 
<a href="/search/stat?searchtype=author&query=Zachariah%2C+D">Dave Zachariah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15737" title="Abstract">arXiv:2310.15737</a> (replaced) [<a href="/pdf/2310.15737" title="Download PDF">pdf</a>, <a href="/format/2310.15737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-Preserving Image Coding based on Conditional Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pezone%2C+F">Francesco Pezone</a>, 
<a href="/search/cs?searchtype=author&query=Musa%2C+O">Osman Musa</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>, 
<a href="/search/cs?searchtype=author&query=Barbarossa%2C+S">Sergio Barbarossa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16022" title="Abstract">arXiv:2310.16022</a> (replaced) [<a href="/pdf/2310.16022" title="Download PDF">pdf</a>, <a href="/format/2310.16022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Measure on FDFAs Following Duo-Normalized Acceptance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fisman%2C+D">Dana Fisman</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+E">Emmanuel Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Zimerman%2C+O">Oded Zimerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16165" title="Abstract">arXiv:2310.16165</a> (replaced) [<a href="/pdf/2310.16165" title="Download PDF">pdf</a>, <a href="/format/2310.16165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Staircase Codes with Arbitrary Bit Degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shehadeh%2C+M">Mohannad Shehadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kschischang%2C+F+R">Frank R. Kschischang</a>, 
<a href="/search/cs?searchtype=author&query=Sukmadji%2C+A+Y">Alvin Y. Sukmadji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 Optical Fiber Communication Conference (OFC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16183" title="Abstract">arXiv:2310.16183</a> (replaced) [<a href="/pdf/2310.16183" title="Download PDF">pdf</a>, <a href="/format/2310.16183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLP-2023 Task 2: Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+A">Md. Arid Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>, 
<a href="/search/cs?searchtype=author&query=Anjum%2C+A">Anika Anjum</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Shudipta Das</a>, 
<a href="/search/cs?searchtype=author&query=Anjum%2C+A">Afiyat Anjum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in BLP Workshop at EMNLP-23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18285" title="Abstract">arXiv:2310.18285</a> (replaced) [<a href="/pdf/2310.18285" title="Download PDF">pdf</a>, <a href="/format/2310.18285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Prompt-Tuning in Bridging Generalized and  Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wenlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18297" title="Abstract">arXiv:2310.18297</a> (replaced) [<a href="/pdf/2310.18297" title="Download PDF">pdf</a>, <a href="/format/2310.18297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering Conditioned on Text Criteria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sehyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeseung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18574" title="Abstract">arXiv:2310.18574</a> (replaced) [<a href="/pdf/2310.18574" title="Download PDF">pdf</a>, <a href="/format/2310.18574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable  Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+G">Guangyao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziwei Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The Web Conference (WWW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19253" title="Abstract">arXiv:2310.19253</a> (replaced) [<a href="/pdf/2310.19253" title="Download PDF">pdf</a>, <a href="/format/2310.19253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow-based Distributionally Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Journal on Selected Areas in Information Theory (JSAIT). Accepted. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19608" title="Abstract">arXiv:2310.19608</a> (replaced) [<a href="/pdf/2310.19608" title="Download PDF">pdf</a>, <a href="/format/2310.19608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Feynman--Kac training of partial Bayesian neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+T+B">Thomas B. Sch&#xf6;n</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19651" title="Abstract">arXiv:2310.19651</a> (replaced) [<a href="/pdf/2310.19651" title="Download PDF">pdf</a>, <a href="/format/2310.19651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of Instruction Tuning: Each Ability of Large Language Models  Has Its Own Growth Pace
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chiyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanchao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yuejiao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01343" title="Abstract">arXiv:2311.01343</a> (replaced) [<a href="/pdf/2311.01343" title="Download PDF">pdf</a>, <a href="/format/2311.01343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Large Language Model for Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaochen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Liangjie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02856" title="Abstract">arXiv:2311.02856</a> (replaced) [<a href="/pdf/2311.02856" title="Download PDF">pdf</a>, <a href="/ps/2311.02856" title="Download PostScript">ps</a>, <a href="/format/2311.02856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Hotplug Caching Schemes Using PDAs and t-Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajput%2C+C">Charul Rajput</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+B+S">B. Sundar Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Two new sections (Sections VIII and IX) have been added. 20 pages, 12 figures and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03233" title="Abstract">arXiv:2311.03233</a> (replaced) [<a href="/pdf/2311.03233" title="Download PDF">pdf</a>, <a href="/format/2311.03233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Scaling Laws: Compute Optimality in Adaptive Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Schlag%2C+I">Imanol Schlag</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04131" title="Abstract">arXiv:2311.04131</a> (replaced) [<a href="/pdf/2311.04131" title="Download PDF">pdf</a>, <a href="/format/2311.04131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Shared Circuits for Ordered Sequence Prediction in a Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+M">Michael Lan</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05106" title="Abstract">arXiv:2311.05106</a> (replaced) [<a href="/pdf/2311.05106" title="Download PDF">pdf</a>, <a href="/format/2311.05106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A differentiable brain simulator bridging brain simulation and  brain-inspired computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianqiu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sichao He</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hongyaoxing Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Si Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 11 figures, ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06607" title="Abstract">arXiv:2311.06607</a> (replaced) [<a href="/pdf/2311.06607" title="Download PDF">pdf</a>, <a href="/format/2311.06607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monkey: Image Resolution and Text Label Are Important Things for Large  Multi-modal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Biao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiyin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingxu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yabo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06922" title="Abstract">arXiv:2311.06922</a> (replaced) [<a href="/pdf/2311.06922" title="Download PDF">pdf</a>, <a href="/format/2311.06922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Socially Aware Robot Navigation: Taxonomy and Future  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singamaneni%2C+P+T">Phani Teja Singamaneni</a>, 
<a href="/search/cs?searchtype=author&query=Bachiller-Burgos%2C+P">Pilar Bachiller-Burgos</a>, 
<a href="/search/cs?searchtype=author&query=Manso%2C+L+J">Luis J. Manso</a>, 
<a href="/search/cs?searchtype=author&query=Garrell%2C+A">Ana&#xed;s Garrell</a>, 
<a href="/search/cs?searchtype=author&query=Sanfeliu%2C+A">Alberto Sanfeliu</a>, 
<a href="/search/cs?searchtype=author&query=Spalanzani%2C+A">Anne Spalanzani</a>, 
<a href="/search/cs?searchtype=author&query=Alami%2C+R">Rachid Alami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06973" title="Abstract">arXiv:2311.06973</a> (replaced) [<a href="/pdf/2311.06973" title="Download PDF">pdf</a>, <a href="/ps/2311.06973" title="Download PostScript">ps</a>, <a href="/format/2311.06973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Verification of Performance of Deep Neural Network Based  Time-Synchronized Distribution System State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azimian%2C+B">Behrouz Azimian</a>, 
<a href="/search/cs?searchtype=author&query=Moshtagh%2C+S">Shiva Moshtagh</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Anamitra Pal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shanshan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08369" title="Abstract">arXiv:2311.08369</a> (replaced) [<a href="/pdf/2311.08369" title="Download PDF">pdf</a>, <a href="/format/2311.08369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How You Prompt Matters! Even Task-Oriented Constraints in Instructions  Affect LLM-Generated Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koike%2C+R">Ryuto Koike</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09606" title="Abstract">arXiv:2311.09606</a> (replaced) [<a href="/pdf/2311.09606" title="Download PDF">pdf</a>, <a href="/format/2311.09606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GistScore: Learning Better Representations for In-Context Example  Selection with Gist Bottlenecks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rosenbaum%2C+C">Clemens Rosenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Elenberg%2C+E+R">Ethan R. Elenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09808" title="Abstract">arXiv:2311.09808</a> (replaced) [<a href="/pdf/2311.09808" title="Download PDF">pdf</a>, <a href="/format/2311.09808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixT3: Pixel-based Table To Text generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+I">I&#xf1;igo Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10590" title="Abstract">arXiv:2311.10590</a> (replaced) [<a href="/pdf/2311.10590" title="Download PDF">pdf</a>, <a href="/format/2311.10590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EduGym: An Environment and Notebook Suite for Reinforcement Learning  Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moerland%2C+T+M">Thomas M. Moerland</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Brockhausen%2C+M">Matthias M&#xfc;ller-Brockhausen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bernatavicius%2C+A">Andrius Bernatavicius</a>, 
<a href="/search/cs?searchtype=author&query=Ponse%2C+K">Koen Ponse</a>, 
<a href="/search/cs?searchtype=author&query=Kouwenhoven%2C+T">Tom Kouwenhoven</a>, 
<a href="/search/cs?searchtype=author&query=Sauter%2C+A">Andreas Sauter</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Meer%2C+M">Michiel van der Meer</a>, 
<a href="/search/cs?searchtype=author&query=Renting%2C+B">Bram Renting</a>, 
<a href="/search/cs?searchtype=author&query=Plaat%2C+A">Aske Plaat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10879" title="Abstract">arXiv:2311.10879</a> (replaced) [<a href="/pdf/2311.10879" title="Download PDF">pdf</a>, <a href="/format/2311.10879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Osuala%2C+R">Richard Osuala</a>, 
<a href="/search/eess?searchtype=author&query=Joshi%2C+S">Smriti Joshi</a>, 
<a href="/search/eess?searchtype=author&query=Tsirikoglou%2C+A">Apostolia Tsirikoglou</a>, 
<a href="/search/eess?searchtype=author&query=Garrucho%2C+L">Lidia Garrucho</a>, 
<a href="/search/eess?searchtype=author&query=Pinaya%2C+W+H+L">Walter H. L. Pinaya</a>, 
<a href="/search/eess?searchtype=author&query=Diaz%2C+O">Oliver Diaz</a>, 
<a href="/search/eess?searchtype=author&query=Lekadir%2C+K">Karim Lekadir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as oral presentation at SPIE Medical Imaging 2024 (Image Processing)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11183" title="Abstract">arXiv:2311.11183</a> (replaced) [<a href="/pdf/2311.11183" title="Download PDF">pdf</a>, <a href="/format/2311.11183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deploying and Evaluating LLMs to Program Service Mobile Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zichao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lucchetti%2C+F">Francesca Lucchetti</a>, 
<a href="/search/cs?searchtype=author&query=Schlesinger%2C+C">Claire Schlesinger</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+Y">Yash Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+A">Anders Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Modak%2C+S">Sadanand Modak</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+A">Arjun Guha</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Accepted at IEEE Robotics and Automation Letters (RA-L)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters, vol. 9, no. 3, pp.
  2853-2860, March 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16163" title="Abstract">arXiv:2311.16163</a> (replaced) [<a href="/pdf/2311.16163" title="Download PDF">pdf</a>, <a href="/format/2311.16163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IODeep: an IOD for the introduction of deep learning in the DICOM  standard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Contino%2C+S">Salvatore Contino</a>, 
<a href="/search/eess?searchtype=author&query=Cruciata%2C+L">Luca Cruciata</a>, 
<a href="/search/eess?searchtype=author&query=Gambino%2C+O">Orazio Gambino</a>, 
<a href="/search/eess?searchtype=author&query=Pirrone%2C+R">Roberto Pirrone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17358" title="Abstract">arXiv:2311.17358</a> (replaced) [<a href="/pdf/2311.17358" title="Download PDF">pdf</a>, <a href="/format/2311.17358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenSense: An Open-World Sensing Framework for Incremental Learning and  Dynamic Sensor Scheduling on Embedded Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bukhari%2C+A">Abdulrahman Bukhari</a>, 
<a href="/search/eess?searchtype=author&query=Hosseinimotlagh%2C+S">Seyedmehdi Hosseinimotlagh</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+H">Hyoseung Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00824" title="Abstract">arXiv:2312.00824</a> (replaced) [<a href="/pdf/2312.00824" title="Download PDF">pdf</a>, <a href="/format/2312.00824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Self-Supervised Contrastive Learning Using Beta Divergence  For Face Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+M+C">Mehmet Can Yavuz</a>, 
<a href="/search/cs?searchtype=author&query=Yanikoglu%2C+B">Berrin Yanikoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03631" title="Abstract">arXiv:2312.03631</a> (replaced) [<a href="/pdf/2312.03631" title="Download PDF">pdf</a>, <a href="/format/2312.03631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Open-Vocabulary Caption Hallucinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-Kish%2C+A">Assaf Ben-Kish</a>, 
<a href="/search/cs?searchtype=author&query=Yanuka%2C+M">Moran Yanuka</a>, 
<a href="/search/cs?searchtype=author&query=Alper%2C+M">Morris Alper</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>, 
<a href="/search/cs?searchtype=author&query=Averbuch-Elor%2C+H">Hadar Averbuch-Elor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website Link: <a href="https://assafbk.github.io/mocha/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04161" title="Abstract">arXiv:2312.04161</a> (replaced) [<a href="/pdf/2312.04161" title="Download PDF">pdf</a>, <a href="/format/2312.04161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Numerical Analysis of Kangaroo Lower Body based on  Constrained Dynamics of Hybrid Serial-Parallel Floating-Base Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+E+M">Enrico Mingo Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Curti%2C+A">Andrea Curti</a>, 
<a href="/search/cs?searchtype=author&query=Miguel%2C+N">Narcis Miguel</a>, 
<a href="/search/cs?searchtype=author&query=Kothakota%2C+S+K">Sai Kishor Kothakota</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+A">Alberto Molina</a>, 
<a href="/search/cs?searchtype=author&query=Roig%2C+A">Adria Roig</a>, 
<a href="/search/cs?searchtype=author&query=Marchionni%2C+L">Luca Marchionni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04985" title="Abstract">arXiv:2312.04985</a> (replaced) [<a href="/pdf/2312.04985" title="Download PDF">pdf</a>, <a href="/format/2312.04985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparQ Attention: Bandwidth-Efficient LLM Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribar%2C+L">Luka Ribar</a>, 
<a href="/search/cs?searchtype=author&query=Chelombiev%2C+I">Ivan Chelombiev</a>, 
<a href="/search/cs?searchtype=author&query=Hudlass-Galley%2C+L">Luke Hudlass-Galley</a>, 
<a href="/search/cs?searchtype=author&query=Blake%2C+C">Charlie Blake</a>, 
<a href="/search/cs?searchtype=author&query=Luschi%2C+C">Carlo Luschi</a>, 
<a href="/search/cs?searchtype=author&query=Orr%2C+D">Douglas Orr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05767" title="Abstract">arXiv:2312.05767</a> (replaced) [<a href="/pdf/2312.05767" title="Download PDF">pdf</a>, <a href="/format/2312.05767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Teng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiangning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuzhen Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yabiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05928" title="Abstract">arXiv:2312.05928</a> (replaced) [<a href="/pdf/2312.05928" title="Download PDF">pdf</a>, <a href="/format/2312.05928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+J">Joonwoo Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sooyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuewei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+J">Jiook Cha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08656" title="Abstract">arXiv:2312.08656</a> (replaced) [<a href="/pdf/2312.08656" title="Download PDF">pdf</a>, <a href="/format/2312.08656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural  Networks Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hongwu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Shivdikar%2C+K">Kaustubh Shivdikar</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+A">MD Amit Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiahui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+O">Omer Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kaeli%2C+D">David Kaeli</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Caiwen Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASPLOS 2024 accepted publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09187" title="Abstract">arXiv:2312.09187</a> (replaced) [<a href="/pdf/2312.09187" title="Download PDF">pdf</a>, <a href="/format/2312.09187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Models as a Source of Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baumli%2C+K">Kate Baumli</a>, 
<a href="/search/cs?searchtype=author&query=Baveja%2C+S">Satinder Baveja</a>, 
<a href="/search/cs?searchtype=author&query=Behbahani%2C+F">Feryal Behbahani</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H">Harris Chan</a>, 
<a href="/search/cs?searchtype=author&query=Comanici%2C+G">Gheorghe Comanici</a>, 
<a href="/search/cs?searchtype=author&query=Flennerhag%2C+S">Sebastian Flennerhag</a>, 
<a href="/search/cs?searchtype=author&query=Gazeau%2C+M">Maxime Gazeau</a>, 
<a href="/search/cs?searchtype=author&query=Holsheimer%2C+K">Kristian Holsheimer</a>, 
<a href="/search/cs?searchtype=author&query=Horgan%2C+D">Dan Horgan</a>, 
<a href="/search/cs?searchtype=author&query=Laskin%2C+M">Michael Laskin</a>, 
<a href="/search/cs?searchtype=author&query=Lyle%2C+C">Clare Lyle</a>, 
<a href="/search/cs?searchtype=author&query=Masoom%2C+H">Hussain Masoom</a>, 
<a href="/search/cs?searchtype=author&query=McKinney%2C+K">Kay McKinney</a>, 
<a href="/search/cs?searchtype=author&query=Mnih%2C+V">Volodymyr Mnih</a>, 
<a href="/search/cs?searchtype=author&query=Neitz%2C+A">Alexander Neitz</a>, 
<a href="/search/cs?searchtype=author&query=Pardo%2C+F">Fabio Pardo</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+J">John Quan</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Sahni%2C+H">Himanshu Sahni</a>, 
<a href="/search/cs?searchtype=author&query=Schaul%2C+T">Tom Schaul</a>, 
<a href="/search/cs?searchtype=author&query=Schroecker%2C+Y">Yannick Schroecker</a>, 
<a href="/search/cs?searchtype=author&query=Spencer%2C+S">Stephen Spencer</a>, 
<a href="/search/cs?searchtype=author&query=Steigerwald%2C+R">Richie Steigerwald</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Luyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09236" title="Abstract">arXiv:2312.09236</a> (replaced) [<a href="/pdf/2312.09236" title="Download PDF">pdf</a>, <a href="/format/2312.09236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for conditional diffusion modelling with applications in  motif scaffolding for protein design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Didi%2C+K">Kieran Didi</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+F">Francisco Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+S+V">Simon V Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Dutordoir%2C+V">Vincent Dutordoir</a>, 
<a href="/search/cs?searchtype=author&query=Mathieu%2C+E">Emile Mathieu</a>, 
<a href="/search/cs?searchtype=author&query=Komorowska%2C+U+J">Urszula J Komorowska</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Pietro Lio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09843" title="Abstract">arXiv:2312.09843</a> (replaced) [<a href="/pdf/2312.09843" title="Download PDF">pdf</a>, <a href="/format/2312.09843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drivers and Barriers of AI Adoption and Use in Scientific Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bianchini%2C+S">Stefano Bianchini</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Moritz M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Pelletier%2C+P">Pierre Pelletier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09911" title="Abstract">arXiv:2312.09911</a> (replaced) [<a href="/pdf/2312.09911" title="Download PDF">pdf</a>, <a href="/format/2312.09911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amphion: An Open-Source Audio, Music and Speech Generation Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yicheng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuancheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haorui He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zihao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haopeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+T+Y">Tze Ying Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lexiao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Amphion Website: <a href="https://github.com/open-mmlab/Amphion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11194" title="Abstract">arXiv:2312.11194</a> (replaced) [<a href="/pdf/2312.11194" title="Download PDF">pdf</a>, <a href="/format/2312.11194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Human Intent from Imperfect Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+X">Xizhou Bu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiqiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengxiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Panfeng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14404" title="Abstract">arXiv:2312.14404</a> (replaced) [<a href="/pdf/2312.14404" title="Download PDF">pdf</a>, <a href="/format/2312.14404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Covariate Gait Recognition: A Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+S">Shinan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jianbo Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chuanfu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shiqi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jin Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15198" title="Abstract">arXiv:2312.15198</a> (replaced) [<a href="/pdf/2312.15198" title="Download PDF">pdf</a>, <a href="/format/2312.15198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do LLM Agents Exhibit Social Behavior?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leng%2C+Y">Yan Leng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Social and Information Networks (cs.SI); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15591" title="Abstract">arXiv:2312.15591</a> (replaced) [<a href="/pdf/2312.15591" title="Download PDF">pdf</a>, <a href="/format/2312.15591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Neural Graph Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaxin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16174" title="Abstract">arXiv:2312.16174</a> (replaced) [<a href="/pdf/2312.16174" title="Download PDF">pdf</a>, <a href="/format/2312.16174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Industrial Internet of Things Intelligence Empowering Smart  Manufacturing: A Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qingmin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mengjie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaomao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Renchao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">F. Richard Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IoTJ
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Internet of Things Journal,2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00523" title="Abstract">arXiv:2401.00523</a> (replaced) [<a href="/pdf/2401.00523" title="Download PDF">pdf</a>, <a href="/format/2401.00523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressing Deep Image Super-resolution Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Y">Yuxuan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Nawala%2C+J">Jakub Nawala</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01330" title="Abstract">arXiv:2401.01330</a> (replaced) [<a href="/pdf/2401.01330" title="Download PDF">pdf</a>, <a href="/format/2401.01330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aliannejadi%2C+M">Mohammad Aliannejadi</a>, 
<a href="/search/cs?searchtype=author&query=Abbasiantaeb%2C+Z">Zahra Abbasiantaeb</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Shubham Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Dalton%2C+J">Jeffery Dalton</a>, 
<a href="/search/cs?searchtype=author&query=Azzopardi%2C+L">Leif Azzopardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TREC iKAT 2023 Overview Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02330" title="Abstract">arXiv:2401.02330</a> (replaced) [<a href="/pdf/2401.02330" title="Download PDF">pdf</a>, <a href="/format/2401.02330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minjie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Z">Zhicai Ou</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+X">Xiaofeng Mou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The datasets were incomplete as they did not include all the necessary copyrights
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03374" title="Abstract">arXiv:2401.03374</a> (replaced) [<a href="/pdf/2401.03374" title="Download PDF">pdf</a>, <a href="/format/2401.03374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Powered Code Vulnerability Repair with Reinforcement Learning and  Semantic Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+N+T">Nafis Tanveer Islam</a>, 
<a href="/search/cs?searchtype=author&query=Khoury%2C+J">Joseph Khoury</a>, 
<a href="/search/cs?searchtype=author&query=Seong%2C+A">Andrew Seong</a>, 
<a href="/search/cs?searchtype=author&query=Karkevandi%2C+M+B">Mohammad Bahrami Karkevandi</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Torre+Parra%2C+G">Gonzalo De La Torre Parra</a>, 
<a href="/search/cs?searchtype=author&query=Bou-Harb%2C+E">Elias Bou-Harb</a>, 
<a href="/search/cs?searchtype=author&query=Najafirad%2C+P">Peyman Najafirad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05240" title="Abstract">arXiv:2401.05240</a> (replaced) [<a href="/pdf/2401.05240" title="Download PDF">pdf</a>, <a href="/format/2401.05240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupling Decision-Making in Fraud Prevention through Classifier  Calibration for Business Logic Action
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luzio%2C+E">Emanuele Luzio</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+M+A">Moacir Antonelli Ponti</a>, 
<a href="/search/cs?searchtype=author&query=Arevalo%2C+C+R">Christian Ramirez Arevalo</a>, 
<a href="/search/cs?searchtype=author&query=Argerich%2C+L">Luis Argerich</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Long version of the paper of ACM-SAC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05930" title="Abstract">arXiv:2401.05930</a> (replaced) [<a href="/pdf/2401.05930" title="Download PDF">pdf</a>, <a href="/format/2401.05930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kai%2C+J">Jushi Kai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06311" title="Abstract">arXiv:2401.06311</a> (replaced) [<a href="/pdf/2401.06311" title="Download PDF">pdf</a>, <a href="/format/2401.06311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuGI: Enhancing Information Retrieval through Multi-Text Generation  Integration with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Le Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06362" title="Abstract">arXiv:2401.06362</a> (replaced) [<a href="/pdf/2401.06362" title="Download PDF">pdf</a>, <a href="/format/2401.06362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention, Distillation, and Tabularization: Towards Practical Neural  Network-Based Prefetching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengmiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Neelesh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Rajgopal Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V+K">Viktor K. Prasanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG); Operating Systems (cs.OS)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06838" title="Abstract">arXiv:2401.06838</a> (replaced) [<a href="/pdf/2401.06838" title="Download PDF">pdf</a>, <a href="/format/2401.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPO: Advancing Multilingual Reasoning through Multilingual  Alignment-as-Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=She%2C+S">Shuaijie She</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The project is available at <a href="https://github.com/NJUNLP/MAPO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06855" title="Abstract">arXiv:2401.06855</a> (replaced) [<a href="/pdf/2401.06855" title="Download PDF">pdf</a>, <a href="/format/2401.06855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Hallucination Detection and Editing for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Abhika Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Asai%2C+A">Akari Asai</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code, data, and demo are available at <a href="https://fine-grained-hallucination.github.io.">this https URL</a> Expanded human annotations adding a new LM, as well as included more baselines for comparison
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06951" title="Abstract">arXiv:2401.06951</a> (replaced) [<a href="/pdf/2401.06951" title="Download PDF">pdf</a>, <a href="/format/2401.06951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E^2-LLM: Efficient and Extreme Length Extension of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zhiqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiakai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Que%2C+H">Haoran Que</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yukang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wenbo Su</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tiezheng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08025" title="Abstract">arXiv:2401.08025</a> (replaced) [<a href="/pdf/2401.08025" title="Download PDF">pdf</a>, <a href="/format/2401.08025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using  Self-Imagination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akter%2C+S+N">Syeda Nahida Akter</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangwu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nyberg%2C+E">Eric Nyberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08190" title="Abstract">arXiv:2401.08190</a> (replaced) [<a href="/pdf/2401.08190" title="Download PDF">pdf</a>, <a href="/format/2401.08190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible  Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Minpeng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+K">Kai Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10407" title="Abstract">arXiv:2401.10407</a> (replaced) [<a href="/pdf/2401.10407" title="Download PDF">pdf</a>, <a href="/format/2401.10407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning High-Quality and General-Purpose Phrase Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Varoquaux%2C+G">Ga&#xeb;l Varoquaux</a>, 
<a href="/search/cs?searchtype=author&query=Suchanek%2C+F+M">Fabian M. Suchanek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12713" title="Abstract">arXiv:2401.12713</a> (replaced) [<a href="/pdf/2401.12713" title="Download PDF">pdf</a>, <a href="/format/2401.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Zero-shot Abstractive Explanations for Rumour Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilal%2C+I+M">Iman Munire Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Procter%2C+R">Rob Procter</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version of the original
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12873" title="Abstract">arXiv:2401.12873</a> (replaced) [<a href="/pdf/2401.12873" title="Download PDF">pdf</a>, <a href="/format/2401.12873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Machine Translation with Human Feedback: An Exploration of  Quality Estimation as a Reward Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12874" title="Abstract">arXiv:2401.12874</a> (replaced) [<a href="/pdf/2401.12874" title="Download PDF">pdf</a>, <a href="/format/2401.12874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Understanding to Utilization: A Survey on Explainability for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haoyan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Specia%2C+L">Lucia Specia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12999" title="Abstract">arXiv:2401.12999</a> (replaced) [<a href="/pdf/2401.12999" title="Download PDF">pdf</a>, <a href="/format/2401.12999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum-Inspired Machine Learning for Molecular Docking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Shu%2C+R">Runqiu Shu</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+B">Bowen Liu</a>, 
<a href="/search/physics?searchtype=author&query=Xiong%2C+Z">Zhaoping Xiong</a>, 
<a href="/search/physics?searchtype=author&query=Cui%2C+X">Xiaopeng Cui</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Y">Yunting Li</a>, 
<a href="/search/physics?searchtype=author&query=Cui%2C+W">Wei Cui</a>, 
<a href="/search/physics?searchtype=author&query=Yung%2C+M">Man-Hong Yung</a>, 
<a href="/search/physics?searchtype=author&query=Qiao%2C+N">Nan Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13142" title="Abstract">arXiv:2401.13142</a> (replaced) [<a href="/pdf/2401.13142" title="Download PDF">pdf</a>, <a href="/ps/2401.13142" title="Download PostScript">ps</a>, <a href="/format/2401.13142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsocial Intelligence: a Pluralistic, Democratic, and Participatory  Investigation of AGI Discourse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blili-Hamelin%2C+B">Borhane Blili-Hamelin</a>, 
<a href="/search/cs?searchtype=author&query=Hancox-Li%2C+L">Leif Hancox-Li</a>, 
<a href="/search/cs?searchtype=author&query=Smart%2C+A">Andrew Smart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13223" title="Abstract">arXiv:2401.13223</a> (replaced) [<a href="/pdf/2401.13223" title="Download PDF">pdf</a>, <a href="/format/2401.13223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAT-LLM: A Specialized Language Model for Discrete Reasoning over  Tabular and Textual Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fengbin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2024 (Under Review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13552" title="Abstract">arXiv:2401.13552</a> (replaced) [<a href="/pdf/2401.13552" title="Download PDF">pdf</a>, <a href="/ps/2401.13552" title="Download PostScript">ps</a>, <a href="/format/2401.13552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Constrained CAV Platoon Control Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bahavarnia%2C+M">MirSaleh Bahavarnia</a>, 
<a href="/search/eess?searchtype=author&query=Ji%2C+J">Junyi Ji</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+A+F">Ahmad F. Taha</a>, 
<a href="/search/eess?searchtype=author&query=Work%2C+D+B">Daniel B. Work</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14505" title="Abstract">arXiv:2401.14505</a> (replaced) [<a href="/e-print/2401.14505" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified KKL Interval Observer for Nonlinear Discrete-time Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dinh%2C+T+N">Thach Ngoc Dinh</a>, 
<a href="/search/eess?searchtype=author&query=Tran%2C+G+Q+B">Gia Quoc Bao Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A conflict of interest involving another group of authors who also worked on the same topic as this paper, <a href="/abs/2401.14505">arXiv:2401.14505</a>. Following discussions among our team, we have collectively decided to withdraw the aforementioned paper until a suitable solution can be found to address the concerns of all authors involved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15475" title="Abstract">arXiv:2401.15475</a> (replaced) [<a href="/pdf/2401.15475" title="Download PDF">pdf</a>, <a href="/format/2401.15475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epidemic Population Games And Perturbed Best Response Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+S">Shinkyu Park</a>, 
<a href="/search/eess?searchtype=author&query=Certorio%2C+J">Jair Certorio</a>, 
<a href="/search/eess?searchtype=author&query=Martins%2C+N+C">Nuno C. Martins</a>, 
<a href="/search/eess?searchtype=author&query=La%2C+R+J">Richard J. La</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16637" title="Abstract">arXiv:2401.16637</a> (replaced) [<a href="/pdf/2401.16637" title="Download PDF">pdf</a>, <a href="/format/2401.16637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRCoCo: Immediate Rewards-Guided Deep Reinforcement Learning for Code  Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bolun Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhihong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Chen Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the 32nd ACM Symposium on the Foundations of Software Engineering (FSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17010" title="Abstract">arXiv:2401.17010</a> (replaced) [<a href="/pdf/2401.17010" title="Download PDF">pdf</a>, <a href="/ps/2401.17010" title="Download PostScript">ps</a>, <a href="/format/2401.17010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finetuning Large Language Models for Vulnerability Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shestov%2C+A">Alexey Shestov</a>, 
<a href="/search/cs?searchtype=author&query=Levichev%2C+R">Rodion Levichev</a>, 
<a href="/search/cs?searchtype=author&query=Mussabayev%2C+R">Ravil Mussabayev</a>, 
<a href="/search/cs?searchtype=author&query=Cheshkov%2C+A">Anton Cheshkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17270" title="Abstract">arXiv:2401.17270</a> (replaced) [<a href="/pdf/2401.17270" title="Download PDF">pdf</a>, <a href="/format/2401.17270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-World: Real-Time Open-Vocabulary Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Tianheng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lin Song</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yixiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work still in progress. Code &amp; models are available at: <a href="https://github.com/AILab-CVC/YOLO-World">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01696" title="Abstract">arXiv:2402.01696</a> (replaced) [<a href="/pdf/2402.01696" title="Download PDF">pdf</a>, <a href="/format/2402.01696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vidit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rungta%2C+M">Mukund Rungta</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Skolnick%2C+J">Jeffrey Skolnick</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01719" title="Abstract">arXiv:2402.01719</a> (replaced) [<a href="/pdf/2402.01719" title="Download PDF">pdf</a>, <a href="/format/2402.01719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Moral Inconsistencies in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonagiri%2C+V+K">Vamshi Krishna Bonagiri</a>, 
<a href="/search/cs?searchtype=author&query=Vennam%2C+S">Sreeram Vennam</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+M">Manas Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BlackBoxNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02001" title="Abstract">arXiv:2402.02001</a> (replaced) [<a href="/pdf/2402.02001" title="Download PDF">pdf</a>, <a href="/ps/2402.02001" title="Download PostScript">ps</a>, <a href="/format/2402.02001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANDA: Query Evaluation in Submodular Width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khamis%2C+M+A">Mahmoud Abo Khamis</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+Q">Hung Q. Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Suciu%2C+D">Dan Suciu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02439" title="Abstract">arXiv:2402.02439</a> (replaced) [<a href="/pdf/2402.02439" title="Download PDF">pdf</a>, <a href="/format/2402.02439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanghe Li</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Yixiang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Ting Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02543" title="Abstract">arXiv:2402.02543</a> (replaced) [<a href="/pdf/2402.02543" title="Download PDF">pdf</a>, <a href="/format/2402.02543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safeguarding the Truth of High-Value Price Oracle Task: A Dynamically  Adjusted Truth Discovery Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Youquan Xian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongcheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xueying Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Engineering, Finance, and Science (cs.CE); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02611" title="Abstract">arXiv:2402.02611</a> (replaced) [<a href="/pdf/2402.02611" title="Download PDF">pdf</a>, <a href="/format/2402.02611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial  Reasoning Problems?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+C">Chinmay Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Kartik%2C+K">Krishna Kartik</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03367" title="Abstract">arXiv:2402.03367</a> (replaced) [<a href="/pdf/2402.03367" title="Download PDF">pdf</a>, <a href="/format/2402.03367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAG-Fusion: a New Take on Retrieval-Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rackauckas%2C+Z">Zackary Rackauckas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03686" title="Abstract">arXiv:2402.03686</a> (replaced) [<a href="/pdf/2402.03686" title="Download PDF">pdf</a>, <a href="/format/2402.03686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Machines Better at Complex Reasoning? Unveiling Human-Machine  Inference Gaps in Entailment Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Soumya Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tianyi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03776" title="Abstract">arXiv:2402.03776</a> (replaced) [<a href="/pdf/2402.03776" title="Download PDF">pdf</a>, <a href="/ps/2402.03776" title="Download PostScript">ps</a>, <a href="/format/2402.03776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models As MOOCs Graders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Garuda%2C+N">Nikhil Garuda</a>, 
<a href="/search/cs?searchtype=author&query=Impey%2C+C">Christopher Impey</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+M">Matthew Wenger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1.2 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03927" title="Abstract">arXiv:2402.03927</a> (replaced) [<a href="/pdf/2402.03927" title="Download PDF">pdf</a>, <a href="/format/2402.03927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in  Closed-Source LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balloccu%2C+S">Simone Balloccu</a>, 
<a href="/search/cs?searchtype=author&query=Schmidtov%C3%A1%2C+P">Patr&#xed;cia Schmidtov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Lango%2C+M">Mateusz Lango</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024 - main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05391" title="Abstract">arXiv:2402.05391</a> (replaced) [<a href="/pdf/2402.05391" title="Download PDF">pdf</a>, <a href="/format/2402.05391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yuxia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yushan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13 Figures, 617 citations; Paper list is available at <a href="https://github.com/zjukg/KG-MM-Survey">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05644" title="Abstract">arXiv:2402.05644</a> (replaced) [<a href="/pdf/2402.05644" title="Download PDF">pdf</a>, <a href="/format/2402.05644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single  Annotated Example Object
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanzhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Binbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Leutenegger%2C+S">Stefan Leutenegger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06216" title="Abstract">arXiv:2402.06216</a> (replaced) [<a href="/pdf/2402.06216" title="Download PDF">pdf</a>, <a href="/format/2402.06216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large  Language Model-based Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhangchi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07610" title="Abstract">arXiv:2402.07610</a> (replaced) [<a href="/pdf/2402.07610" title="Download PDF">pdf</a>, <a href="/format/2402.07610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guozheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Ziqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zeyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08441" title="Abstract">arXiv:2402.08441</a> (replaced) [<a href="/pdf/2402.08441" title="Download PDF">pdf</a>, <a href="/format/2402.08441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent space configuration for improved generalization in supervised  autoencoder neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabdullin%2C+N">Nikita Gabdullin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages,18 figures, 2 tables, 15 equations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08953" title="Abstract">arXiv:2402.08953</a> (replaced) [<a href="/pdf/2402.08953" title="Download PDF">pdf</a>, <a href="/ps/2402.08953" title="Download PostScript">ps</a>, <a href="/format/2402.08953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy Jump and Entropic Central Limit Theorem for Independent Sum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yao%2C+L">Liuquan Yao</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09664" title="Abstract">arXiv:2402.09664</a> (replaced) [<a href="/pdf/2402.09664" title="Download PDF">pdf</a>, <a href="/format/2402.09664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeMind: A Framework to Challenge Large Language Models for Code  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changshu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S+D">Shizhuo Dylan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jabbarvand%2C+R">Reyhaneh Jabbarvand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09721" title="Abstract">arXiv:2402.09721</a> (replaced) [<a href="/pdf/2402.09721" title="Download PDF">pdf</a>, <a href="/ps/2402.09721" title="Download PostScript">ps</a>, <a href="/format/2402.09721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persuading a Learning Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09808" title="Abstract">arXiv:2402.09808</a> (replaced) [<a href="/pdf/2402.09808" title="Download PDF">pdf</a>, <a href="/format/2402.09808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge of Pretrained Language Models on Surface Information of Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiraoka%2C+T">Tatsuya Hiraoka</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10002" title="Abstract">arXiv:2402.10002</a> (replaced) [<a href="/pdf/2402.10002" title="Download PDF">pdf</a>, <a href="/format/2402.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D  Point Cloud Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai-Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mofei Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10059" title="Abstract">arXiv:2402.10059</a> (replaced) [<a href="/pdf/2402.10059" title="Download PDF">pdf</a>, <a href="/format/2402.10059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial synchrony for free? New bounds for Byzantine agreement via a  generic transformation across network models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Civit%2C+P">Pierre Civit</a>, 
<a href="/search/cs?searchtype=author&query=Dzulfikar%2C+M+A">Muhammad Ayaz Dzulfikar</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S">Seth Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Komatovic%2C+J">Jovan Komatovic</a>, 
<a href="/search/cs?searchtype=author&query=Vidigueira%2C+M">Manuel Vidigueira</a>, 
<a href="/search/cs?searchtype=author&query=Zablotchi%2C+I">Igor Zablotchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10251" title="Abstract">arXiv:2402.10251</a> (replaced) [<a href="/pdf/2402.10251" title="Download PDF">pdf</a>, <a href="/format/2402.10251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brant-2: Foundation Model for Brain Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yuan%2C+Z">Zhizhang Yuan</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+D">Daoze Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+J">Junru Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Gu%2C+G">Geifei Gu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10797" title="Abstract">arXiv:2402.10797</a> (replaced) [<a href="/pdf/2402.10797" title="Download PDF">pdf</a>, <a href="/format/2402.10797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BlackJAX: Composable Bayesian inference in JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabezas%2C+A">Alberto Cabezas</a>, 
<a href="/search/cs?searchtype=author&query=Corenflos%2C+A">Adrien Corenflos</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+J">Junpeng Lao</a>, 
<a href="/search/cs?searchtype=author&query=Louf%2C+R">R&#xe9;mi Louf</a>, 
<a href="/search/cs?searchtype=author&query=Carnec%2C+A">Antoine Carnec</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+K">Kaustubh Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Cohn-Gordon%2C+R">Reuben Cohn-Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Coullon%2C+J">Jeremie Coullon</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Duffield%2C+S">Sam Duffield</a>, 
<a href="/search/cs?searchtype=author&query=Dur%C3%A1n-Mart%C3%ADn%2C+G">Gerardo Dur&#xe1;n-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Elantkowski%2C+M">Marcin Elantkowski</a>, 
<a href="/search/cs?searchtype=author&query=Foreman-Mackey%2C+D">Dan Foreman-Mackey</a>, 
<a href="/search/cs?searchtype=author&query=Gregori%2C+M">Michele Gregori</a>, 
<a href="/search/cs?searchtype=author&query=Iguaran%2C+C">Carlos Iguaran</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ravin Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Lysy%2C+M">Martin Lysy</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+K">Kevin Murphy</a>, 
<a href="/search/cs?searchtype=author&query=Orduz%2C+J+C">Juan Camilo Orduz</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+K">Karm Patel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zinkov%2C+R">Rob Zinkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Companion paper for the library <a href="https://github.com/blackjax-devs/blackjax">this https URL</a> Update: minor changes and updated the list of authors to include technical contributors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10991" title="Abstract">arXiv:2402.10991</a> (replaced) [<a href="/pdf/2402.10991" title="Download PDF">pdf</a>, <a href="/ps/2402.10991" title="Download PostScript">ps</a>, <a href="/format/2402.10991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Semi-Asynchronous Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fanghao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jize Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11005" title="Abstract">arXiv:2402.11005</a> (replaced) [<a href="/pdf/2402.11005" title="Download PDF">pdf</a>, <a href="/format/2402.11005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Value Biases: How LLMs Deviate Towards the Ideal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+P">Pramod Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Abdelnabi%2C+S">Sahar Abdelnabi</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11084" title="Abstract">arXiv:2402.11084</a> (replaced) [<a href="/pdf/2402.11084" title="Download PDF">pdf</a>, <a href="/ps/2402.11084" title="Download PostScript">ps</a>, <a href="/format/2402.11084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Competition Complexity of Prophet Inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brustle%2C+J">Johannes Brustle</a>, 
<a href="/search/cs?searchtype=author&query=Correa%2C+J">Jos&#xe9; Correa</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCtting%2C+P">Paul D&#xfc;tting</a>, 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Michal Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Verdugo%2C+V">Victor Verdugo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11200" title="Abstract">arXiv:2402.11200</a> (replaced) [<a href="/pdf/2402.11200" title="Download PDF">pdf</a>, <a href="/format/2402.11200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contraction of Markovian Operators in Orlicz Spaces and Error Bounds for  Markov Chain Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esposito%2C+A+R">Amedeo Roberto Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11209" title="Abstract">arXiv:2402.11209</a> (replaced) [<a href="/pdf/2402.11209" title="Download PDF">pdf</a>, <a href="/format/2402.11209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Simple is Near-Optimal in Security Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalota%2C+D">Devansh Jalota</a>, 
<a href="/search/cs?searchtype=author&query=Ostrovsky%2C+M">Michael Ostrovsky</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC); Theoretical Economics (econ.TH); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11329" title="Abstract">arXiv:2402.11329</a> (replaced) [<a href="/pdf/2402.11329" title="Download PDF">pdf</a>, <a href="/format/2402.11329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a recent extension of a family of biprojective APN functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%B6lsch%2C+L">Lukas K&#xf6;lsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11525" title="Abstract">arXiv:2402.11525</a> (replaced) [<a href="/pdf/2402.11525" title="Download PDF">pdf</a>, <a href="/format/2402.11525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Translation Preference Modeling with RLHF: A Step Towards  Cost-Effective Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+C">Can Zu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11753" title="Abstract">arXiv:2402.11753</a> (replaced) [<a href="/pdf/2402.11753" title="Download PDF">pdf</a>, <a href="/format/2402.11753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fengqing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhangchen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Luyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhen Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+B">Bhaskar Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11773" title="Abstract">arXiv:2402.11773</a> (replaced) [<a href="/pdf/2402.11773" title="Download PDF">pdf</a>, <a href="/format/2402.11773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Multi-Network Mining of Tensor Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Obata%2C+K">Kohei Obata</a>, 
<a href="/search/cs?searchtype=author&query=Kawabata%2C+K">Koki Kawabata</a>, 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+Y">Yasuko Matsubara</a>, 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+Y">Yasushi Sakurai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12011" title="Abstract">arXiv:2402.12011</a> (replaced) [<a href="/pdf/2402.12011" title="Download PDF">pdf</a>, <a href="/format/2402.12011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Comparison of Contextualized Word Embeddings for Lexical  Semantic Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Periti%2C+F">Francesco Periti</a>, 
<a href="/search/cs?searchtype=author&query=Tahmasebi%2C+N">Nina Tahmasebi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NAACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12052" title="Abstract">arXiv:2402.12052</a> (replaced) [<a href="/pdf/2402.12052" title="Download PDF">pdf</a>, <a href="/format/2402.12052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When  and What to Retrieve for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiejun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Peidong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kun Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12424" title="Abstract">arXiv:2402.12424</a> (replaced) [<a href="/pdf/2402.12424" title="Download PDF">pdf</a>, <a href="/format/2402.12424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tables as Images? Exploring the Strengths and Limitations of LLMs on  Multimodal Representations of Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhenjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruiqi He</a>, 
<a href="/search/cs?searchtype=author&query=Sikka%2C+A">Aman Sikka</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12532" title="Abstract">arXiv:2402.12532</a> (replaced) [<a href="/pdf/2402.12532" title="Download PDF">pdf</a>, <a href="/format/2402.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Human-Machine Point Cloud Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ulhaq%2C+M">Mateen Ulhaq</a>, 
<a href="/search/cs?searchtype=author&query=Baji%C4%87%2C+I+V">Ivan V. Baji&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 2024 Picture Coding Symposium (PCS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12744" title="Abstract">arXiv:2402.12744</a> (replaced) [<a href="/pdf/2402.12744" title="Download PDF">pdf</a>, <a href="/format/2402.12744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Models for Vibrational Entropy Based on a Spatial  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Torabi%2C+T">Tina Torabi</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yangshuai Wang</a>, 
<a href="/search/physics?searchtype=author&query=Ortner%2C+C">Christoph Ortner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12762" title="Abstract">arXiv:2402.12762</a> (replaced) [<a href="/pdf/2402.12762" title="Download PDF">pdf</a>, <a href="/ps/2402.12762" title="Download PostScript">ps</a>, <a href="/format/2402.12762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning under Singularity: An Information Criterion improving WBIC and  sBIC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+L">Lirui Liu</a>, 
<a href="/search/stat?searchtype=author&query=Suzuki%2C+J">Joe Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12997" title="Abstract">arXiv:2402.12997</a> (replaced) [<a href="/pdf/2402.12997" title="Download PDF">pdf</a>, <a href="/format/2402.12997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Trustworthy Reranking: A Simple yet Effective Abstention  Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gisserot-Boukhlef%2C+H">Hippolyte Gisserot-Boukhlef</a>, 
<a href="/search/cs?searchtype=author&query=Faysse%2C+M">Manuel Faysse</a>, 
<a href="/search/cs?searchtype=author&query=Malherbe%2C+E">Emmanuel Malherbe</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13185" title="Abstract">arXiv:2402.13185</a> (replaced) [<a href="/pdf/2402.13185" title="Download PDF">pdf</a>, <a href="/format/2402.13185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jianhongbai.github.io/UniEdit/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13250" title="Abstract">arXiv:2402.13250</a> (replaced) [<a href="/pdf/2402.13250" title="Download PDF">pdf</a>, <a href="/format/2402.13250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video ReCap: Recursive Captioning of Hour-Long Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Mohaiminul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Ngan Ho</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+T">Tushar Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Torresani%2C+L">Lorenzo Torresani</a>, 
<a href="/search/cs?searchtype=author&query=Bertasius%2C+G">Gedas Bertasius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13349" title="Abstract">arXiv:2402.13349</a> (replaced) [<a href="/pdf/2402.13349" title="Download PDF">pdf</a>, <a href="/format/2402.13349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aria Everyday Activities Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhaoyang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Charron%2C+N">Nicholas Charron</a>, 
<a href="/search/cs?searchtype=author&query=Moulon%2C+P">Pierre Moulon</a>, 
<a href="/search/cs?searchtype=author&query=Gamino%2C+A">Alexander Gamino</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Cheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sweeney%2C+C">Chris Sweeney</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+E">Edward Miller</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+J">Jeff Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+K">Kiran Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Pesqueira%2C+L">Luis Pesqueira</a>, 
<a href="/search/cs?searchtype=author&query=Schwesinger%2C+M">Mark Schwesinger</a>, 
<a href="/search/cs?searchtype=author&query=Parkhi%2C+O">Omkar Parkhi</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qiao Gu</a>, 
<a href="/search/cs?searchtype=author&query=De+Nardi%2C+R">Renzo De Nardi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shangyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Saarinen%2C+S">Steve Saarinen</a>, 
<a href="/search/cs?searchtype=author&query=Baiyya%2C+V">Vijay Baiyya</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuyang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Newcombe%2C+R">Richard Newcombe</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+J+J">Jakob Julian Engel</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaqing Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Carl Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset website: <a href="https://www.projectaria.com/datasets/aea/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13352" title="Abstract">arXiv:2402.13352</a> (replaced) [<a href="/pdf/2402.13352" title="Download PDF">pdf</a>, <a href="/format/2402.13352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Apak%2C+B">Boran Apak</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bandic%2C+M">Medina Bandic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sarkar%2C+A">Aritra Sarkar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Feld%2C+S">Sebastian Feld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13372" title="Abstract">arXiv:2402.13372</a> (replaced) [<a href="/pdf/2402.13372" title="Download PDF">pdf</a>, <a href="/format/2402.13372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J+H">Jing Han Sun</a>, 
<a href="/search/cs?searchtype=author&query=Emami%2C+A">Ali Emami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in main proceedings of LREC-COLING 2024, 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13388" title="Abstract">arXiv:2402.13388</a> (replaced) [<a href="/pdf/2402.13388" title="Download PDF">pdf</a>, <a href="/format/2402.13388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer tricks: Precomputing the first layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graef%2C+N">Nils Graef</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13399" title="Abstract">arXiv:2402.13399</a> (replaced) [<a href="/pdf/2402.13399" title="Download PDF">pdf</a>, <a href="/format/2402.13399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning and Sustaining Shared Normative Systems via Bayesian Rule  Induction in Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oldenburg%2C+N">Ninell Oldenburg</a>, 
<a href="/search/cs?searchtype=author&query=Zhi-Xuan%2C+T">Tan Zhi-Xuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 23rd International Conference on Autonomous Agents and Multi-Agent Systems, 8 pages (excl. references), 6 figures/tables, (Appendix: 7 pages, 6 figures/tables). Code available at: <a href="https://github.com/ninell-oldenburg/social-contracts">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13463" title="Abstract">arXiv:2402.13463</a> (replaced) [<a href="/pdf/2402.13463" title="Download PDF">pdf</a>, <a href="/format/2402.13463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13528" title="Abstract">arXiv:2402.13528</a> (replaced) [<a href="/pdf/2402.13528" title="Download PDF">pdf</a>, <a href="/format/2402.13528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrastructure Ombudsman: Mining Future Failure Concerns from Structural  Disaster Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+M+T+A">Md Towhidul Absar Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Soumyajit Datta</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Naveen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R">Ashiqur R. KhudaBukhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13545" title="Abstract">arXiv:2402.13545</a> (replaced) [<a href="/pdf/2402.13545" title="Download PDF">pdf</a>, <a href="/ps/2402.13545" title="Download PostScript">ps</a>, <a href="/format/2402.13545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Two-Stage Dual-Path Framework for Text Tampering Detection and  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guandong Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenpin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13583" title="Abstract">arXiv:2402.13583</a> (replaced) [<a href="/pdf/2402.13583" title="Download PDF">pdf</a>, <a href="/format/2402.13583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongWanjuan: Towards Systematic Measurement for Long Text Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kai Lv</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13586" title="Abstract">arXiv:2402.13586</a> (replaced) [<a href="/pdf/2402.13586" title="Download PDF">pdf</a>, <a href="/format/2402.13586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delay-Aware Semantic Sampling in Power Electronic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gupta%2C+K">Kirti Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Sahoo%2C+S">Subham Sahoo</a>, 
<a href="/search/eess?searchtype=author&query=Panigrahi%2C+B+K">Bijaya Ketan Panigrahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Transactions on Smart Grid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13605" title="Abstract">arXiv:2402.13605</a> (replaced) [<a href="/pdf/2402.13605" title="Download PDF">pdf</a>, <a href="/format/2402.13605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jiyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junghwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seunghyun Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13617" title="Abstract">arXiv:2402.13617</a> (replaced) [<a href="/pdf/2402.13617" title="Download PDF">pdf</a>, <a href="/format/2402.13617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Monolithic Cybersecurity Architecture for Power Electronic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gupta%2C+K">Kirti Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Sahoo%2C+S">Subham Sahoo</a>, 
<a href="/search/eess?searchtype=author&query=Panigrahi%2C+B+K">Bijaya Ketan Panigrahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Transactions on Smart Grid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13651" title="Abstract">arXiv:2402.13651</a> (replaced) [<a href="/pdf/2402.13651" title="Download PDF">pdf</a>, <a href="/format/2402.13651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czerkawski%2C+M">Mikolaj Czerkawski</a>, 
<a href="/search/cs?searchtype=author&query=Clemente%2C+C">Carmine Clemente</a>, 
<a href="/search/cs?searchtype=author&query=Michie%2C+C">Craig Michie</a>, 
<a href="/search/cs?searchtype=author&query=Tachtatzis%2C+C">Christos Tachtatzis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Radar Symposium 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13662" title="Abstract">arXiv:2402.13662</a> (replaced) [<a href="/pdf/2402.13662" title="Download PDF">pdf</a>, <a href="/ps/2402.13662" title="Download PostScript">ps</a>, <a href="/format/2402.13662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Method For Bounding Tail Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zlatanov%2C+N">Nikola Zlatanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13711" title="Abstract">arXiv:2402.13711</a> (replaced) [<a href="/pdf/2402.13711" title="Download PDF">pdf</a>, <a href="/format/2402.13711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based  Graph Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungyoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjoong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM TheWebConf 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13718" title="Abstract">arXiv:2402.13718</a> (replaced) [<a href="/pdf/2402.13718" title="Download PDF">pdf</a>, <a href="/format/2402.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingfa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+M+K">Moo Khai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+Z+L">Zhen Leng Thai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023.12.15ARR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13764" title="Abstract">arXiv:2402.13764</a> (replaced) [<a href="/pdf/2402.13764" title="Download PDF">pdf</a>, <a href="/format/2402.13764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CriticBench: Evaluating Large Language Models as Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xian-ling Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13777" title="Abstract">arXiv:2402.13777</a> (replaced) [<a href="/pdf/2402.13777" title="Download PDF">pdf</a>, <a href="/format/2402.13777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+B">Bhargav Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yongsheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13823" title="Abstract">arXiv:2402.13823</a> (replaced) [<a href="/pdf/2402.13823" title="Download PDF">pdf</a>, <a href="/format/2402.13823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models for Natural Language Processing Tasks in  Requirements Engineering: A Systematic Guideline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vogelsang%2C+A">Andreas Vogelsang</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13948" title="Abstract">arXiv:2402.13948</a> (replaced) [<a href="/pdf/2402.13948" title="Download PDF">pdf</a>, <a href="/ps/2402.13948" title="Download PostScript">ps</a>, <a href="/format/2402.13948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Syndrome-based Neural Decoder for Linear Block Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Boni+Rovella%2C+G">Gast&#xf3;n De Boni Rovella</a>, 
<a href="/search/cs?searchtype=author&query=Benammar%2C+M">Meryem Benammar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures. To be published in Proc. IEEE Global Communications Conference (GLOBECOM 2023), Kuala Lumpur, Malaysia, December 4-8, 2023. \{copyright} 2023 IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item379">Cross-lists</a></li>
<li><a href="#item419">Replacements</a></li>
</ul>
<small>[ total of 662 entries:  <b>1-662</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
