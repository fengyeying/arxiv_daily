<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 14 Feb 24  to  Thu 15 Feb 24, announced Fri, 16 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item367">Cross-lists</a></li>
<li><a href="#item451">Replacements</a></li>
</ul>
<small>[ total of 692 entries:  <b>1-692</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 16 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09406" title="Abstract">arXiv:2402.09406</a> [<a href="/pdf/2402.09406" title="Download PDF">pdf</a>, <a href="/ps/2402.09406" title="Download PostScript">ps</a>, <a href="/format/2402.09406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VR-CAD Framework for Parametric Data Modification with a 3D Shape-based  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okuya%2C+Y">Yujiro Okuya</a> (LRI, LIMSI, EX-SITU), 
<a href="/search/cs?searchtype=author&query=Ladeveze%2C+N">Nicolas Ladeveze</a> (LIMSI), 
<a href="/search/cs?searchtype=author&query=Fleury%2C+C">C&#xe9;dric Fleury</a> (LRI, EX-SITU), 
<a href="/search/cs?searchtype=author&query=Bourdot%2C+P">Patrick Bourdot</a> (LIMSI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In this poster, we present a new VR-CAD framework, allowing user to modify
parametric CAD data with 3D interaction in an immersive environment. With this
framework, users can implicitly modify parameter values of CAD data with
co-localized 3D shape-based interaction. This poster describes the system
architecture and the interaction technique based on it.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09407" title="Abstract">arXiv:2402.09407</a> [<a href="/pdf/2402.09407" title="Download PDF">pdf</a>, <a href="/ps/2402.09407" title="Download PostScript">ps</a>, <a href="/format/2402.09407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Large-Scale Publication and Citation Data Tell Us About  International Research Collaboration in Europe: Changing National Patterns in  Global Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwiek%2C+M">Marek Kwiek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Studies in Higher Education, 46:12, 2629-2649
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This study analyzes the unprecedented growth of international research
collaboration (IRC) in Europe during the period 2009-2018 in terms of
coauthorship and citation distribution of globally indexed publications. The
results reveal the dynamics of this change, as growing IRC moves European
systems away from institutional collaboration, with stable and strong national
collaboration. Domestic output has remained flat. The growth in publications in
major European systems is almost entirely attributable to internationally
coauthored papers. A comparison of trends within the four complementary
collaboration modes clearly reveals that the growth of European science is
driven solely by internationally co-authored papers. With the emergence of
global network science, which diminishes the role of national policies in IRC
and foregrounds the role of scientists, the individual scientists willingness
to collaborate internationally is central to advancing IRC in Europe.
Scientists collaborate internationally when it enhances their academic
prestige, scientific recognition, and access to research funding, as indicated
by the credibility cycle, prestige maximization, and global science models. The
study encompassed 5.5 million Scopus-indexed articles, including 2.2 million
involving international collaboration.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09409" title="Abstract">arXiv:2402.09409</a> [<a href="/pdf/2402.09409" title="Download PDF">pdf</a>, <a href="/ps/2402.09409" title="Download PostScript">ps</a>, <a href="/format/2402.09409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seasons&#x27;s Greetings by AD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naumann%2C+U">Uwe Naumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available on github
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">We use Algorithmic Differentiation (AD) to implement type-generic tangent and
adjoint versions of $$ y=\sum_{i=0}^{n-1} x_{2 i} \cdot x_{2 i+1} $$ in C++. We
run an instantiation for char-arithmetic and we print the gradient at
$(101~77~114~114~32~121~109~88~115~97)^T$ to std::cout, yielding the output
``Merry Xmas''.
<br />Similar instantiations of type-generic second-order tangent and second-order
adjoint versions of $$
<br />y=\frac{1}{6} \cdot \sum_{i=0}^{n-1} x^3_{i} $$ yield ``Happy 2024'' at
$(72~97~112~112~121~32~50~48~50~52)^T.$ Prepend a sufficiently large number of
zeros to the input vector to explore the varying run times of the different
derivative codes. The entire source code can be found on
https://github.com/un110076/SeasonsGreetings.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09410" title="Abstract">arXiv:2402.09410</a> [<a href="/pdf/2402.09410" title="Download PDF">pdf</a>, <a href="/format/2402.09410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computability of Computable Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaliq%2C+A">Asad Khaliq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 Pages,4 figures, and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">Computational problems are classified into computable and uncomputable
problems.If there exists an effective procedure (algorithm) to compute a
problem then the problem is computable otherwise it is uncomputable.Turing
machines can execute any algorithm therefore every computable problem is Turing
computable.There are some variants of Turing machine that appear
computationally more powerful but all these variants have been proven equally
powerful.The main objective of this work is to revisit and examine the
computational power of different variants of Turing machines at very fine-grain
level.We achieve this objective by constructing a transform technique for
Turing computable problems that transforms computable problems into another
type of problems, and then we try to compute the transformed problems through
different variants of Turing machine.This paper shows the existence of a
realizable computational scheme that can establish a framework to analyze
computational characteristics of different variants of Turing machine at
infinitesimal scale.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09413" title="Abstract">arXiv:2402.09413</a> [<a href="/pdf/2402.09413" title="Download PDF">pdf</a>, <a href="/ps/2402.09413" title="Download PostScript">ps</a>, <a href="/format/2402.09413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halpern%2C+J+Y">Joseph Y. Halpern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A definition of what counts as an explanation of mathematical statement, and
when one explanation is better than another, is given. Since all mathematical
facts must be true in all causal models, and hence known by an agent,
mathematical facts cannot be part of an explanation (under the standard notion
of explanation). This problem is solved using impossible possible worlds.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09432" title="Abstract">arXiv:2402.09432</a> [<a href="/pdf/2402.09432" title="Download PDF">pdf</a>, <a href="/ps/2402.09432" title="Download PostScript">ps</a>, <a href="/format/2402.09432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Enhanced Analysis of Traffic Intelligence in Smart Cities Using  Sustainable Deep Radial Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ismaeel%2C+A+G">Ayad Ghany Ismaeel</a>, 
<a href="/search/cs?searchtype=author&query=Mary%2C+S+J+J">S.J. Jereesha Mary</a>, 
<a href="/search/cs?searchtype=author&query=Anitha%2C+C">C. Anitha</a>, 
<a href="/search/cs?searchtype=author&query=Logeshwaran%2C+J">Jaganathan Logeshwaran</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+S+N">Sarmad Nozad Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Alani%2C+S">Sameer Alani</a>, 
<a href="/search/cs?searchtype=author&query=Shather%2C+A+H">Akram H. Shather</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures, and 3 Tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sustainability 2023, 15, x FOR PEER REVIEW
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Smart cities have revolutionized urban living by incorporating sophisticated
technologies to optimize various aspects of urban infrastructure, such as
transportation systems. Effective traffic management is a crucial component of
smart cities, as it has a direct impact on the quality of life of residents and
tourists. Utilizing deep radial basis function (RBF) networks, this paper
describes a novel strategy for enhancing traffic intelligence in smart cities.
Traditional methods of traffic analysis frequently rely on simplistic models
that are incapable of capturing the intricate patterns and dynamics of urban
traffic systems. Deep learning techniques, such as deep RBF networks, have the
potential to extract valuable insights from traffic data and enable more
precise predictions and decisions. In this paper, we propose an RBF based
method for enhancing smart city traffic intelligence. Deep RBF networks combine
the adaptability and generalization capabilities of deep learning with the
discriminative capability of radial basis functions. The proposed method can
effectively learn intricate relationships and nonlinear patterns in traffic
data by leveraging the hierarchical structure of deep neural networks. The deep
RBF model can learn to predict traffic conditions, identify congestion
patterns, and make informed recommendations for optimizing traffic management
strategies by incorporating these rich and diverse data To evaluate the
efficacy of our proposed method, extensive experiments and comparisons with
real world traffic datasets from a smart city environment were conducted. In
terms of prediction accuracy and efficiency, the results demonstrate that the
deep RBF based approach outperforms conventional traffic analysis methods.
Smart city traffic intelligence is enhanced by the model capacity to capture
nonlinear relationships and manage large scale data sets.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09437" title="Abstract">arXiv:2402.09437</a> [<a href="/pdf/2402.09437" title="Download PDF">pdf</a>, <a href="/format/2402.09437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Managing Household Waste through Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunwar%2C+S">Suman Kunwar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As the world continues to face the challenges of climate change, it is
crucial to consider the environmental impact of the technologies we use. In
this study, we investigate the performance and computational carbon emissions
of various transfer learning models for garbage classification. We examine the
MobileNet, ResNet50, ResNet101, and EfficientNetV2S and EfficientNetV2M models.
Our findings indicate that the EfficientNetV2 family achieves the highest
accuracy, recall, f1-score, and IoU values. However, the EfficientNetV2M model
requires more time and produces higher carbon emissions. ResNet50 outperforms
ResNet110 in terms of accuracy, recall, f1-score, and IoU, but it has a larger
carbon footprint. We conclude that EfficientNetV2S is the most sustainable and
accurate model with 96.41% accuracy. Our research highlights the significance
of considering the ecological impact of machine learning models in garbage
classification.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09446" title="Abstract">arXiv:2402.09446</a> [<a href="/pdf/2402.09446" title="Download PDF">pdf</a>, <a href="/format/2402.09446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeshAC: A 3D Mesh Generation and Adaptation Package for Multiscale  Coupling Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Kejie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Mingjie Liao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yangshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">This paper introduces the MeshAC package, which generates three-dimensional
adaptive meshes tailored for the efficient and robust implementation of
multiscale coupling methods. While Delaunay triangulation is commonly used for
mesh generation across the entire computational domain, generating meshes for
multiscale coupling methods is more challenging due to intrinsic discrete
structures such as defects, and the need to match these structures to the
continuum domain at the interface. The MeshAC package tackles these challenges
by generating meshes that align with fine-level discrete structures. It also
incorporates localized modification and reconstruction operations specifically
designed for interfaces. These enhancements improve both the implementation
efficiency and the quality of the coupled mesh. Furthermore, MeshAC introduces
a novel adaptive feature that utilizes gradient-based a posteriori error
estimation, which automatically adjusts the atomistic region and continuum
mesh, ensuring an optimal balance between accuracy and efficiency. This package
can be directly applied to the geometry optimization problems of a/c coupling
in static mechanics, with potential extensions to many other scenarios. Its
capabilities are demonstrated for complex material defects, including straight
edge dislocation in BCC W and double voids in FCC Cu. These results suggest
that MeshAC can be a valuable tool for researchers and practitioners in
computational mechanics.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09456" title="Abstract">arXiv:2402.09456</a> [<a href="/pdf/2402.09456" title="Download PDF">pdf</a>, <a href="/format/2402.09456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingru Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+W">Wenqiang Pi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Many real-world problems involving multiple decision-makers can be modeled as
an unknown game characterized by partial observations. Addressing the
challenges posed by partial information and the curse of multi-agency, we
developed Thompson sampling-type algorithms, leveraging information about
opponent's action and reward structures. Our approach significantly reduces
experimental budgets, achieving a more than tenfold reduction compared to
baseline algorithms in practical applications like traffic routing and radar
sensing. We demonstrate that, under certain assumptions about the reward
structure, the regret bound exhibits merely a logarithmic dependence on the
total action space size, effectively mitigating the curse of multi-agency.
Additionally, this research introduces the Optimism-then-NoRegret framework, a
novel contribution that integrates both our proposed methodologies and existing
algorithms in the field.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09469" title="Abstract">arXiv:2402.09469</a> [<a href="/pdf/2402.09469" title="Download PDF">pdf</a>, <a href="/format/2402.09469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier Circuits in Neural Networks: Unlocking the Potential of Large  Language Models in Mathematical Reasoning and Modular Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenmei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In the evolving landscape of machine learning, a pivotal challenge lies in
deciphering the internal representations harnessed by neural networks and
Transformers. Building on recent progress toward comprehending how networks
execute distinct target functions, our study embarks on an exploration of the
underlying reasons behind networks adopting specific computational strategies.
We direct our focus to the complex algebraic learning task of modular addition
involving $k$ inputs. Our research presents a thorough analytical
characterization of the features learned by stylized one-hidden layer neural
networks and one-layer Transformers in addressing this task.
<br />A cornerstone of our theoretical framework is the elucidation of how the
principle of margin maximization shapes the features adopted by one-hidden
layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of
modular arithmetic with $k$ inputs and $m$ denote the network width. We
demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these
networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $.
Furthermore, we establish that each hidden-layer neuron aligns with a specific
Fourier spectrum, integral to solving modular addition problems.
<br />By correlating our findings with the empirical observations of similar
studies, we contribute to a deeper comprehension of the intrinsic computational
mechanisms of neural networks. Furthermore, we observe similar computational
mechanisms in the attention matrix of the Transformer. This research stands as
a significant stride in unraveling their operation complexities, particularly
in the realm of complex algebraic tasks.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09470" title="Abstract">arXiv:2402.09470</a> [<a href="/pdf/2402.09470" title="Download PDF">pdf</a>, <a href="/format/2402.09470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rolling Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Heek%2C+J">Jonathan Heek</a>, 
<a href="/search/cs?searchtype=author&query=Salimans%2C+T">Tim Salimans</a>, 
<a href="/search/cs?searchtype=author&query=Hoogeboom%2C+E">Emiel Hoogeboom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models have recently been increasingly applied to temporal data
such as video, fluid mechanics simulations, or climate data. These methods
generally treat subsequent frames equally regarding the amount of noise in the
diffusion process. This paper explores Rolling Diffusion: a new approach that
uses a sliding window denoising process. It ensures that the diffusion process
progressively corrupts through time by assigning more noise to frames that
appear later in a sequence, reflecting greater uncertainty about the future as
the generation process unfolds. Empirically, we show that when the temporal
dynamics are complex, Rolling Diffusion is superior to standard diffusion. In
particular, this result is demonstrated in a video prediction task using the
Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting
experiment.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09471" title="Abstract">arXiv:2402.09471</a> [<a href="/pdf/2402.09471" title="Download PDF">pdf</a>, <a href="/format/2402.09471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Stochastic Parametrisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christensen%2C+H+M">Hannah M. Christensen</a>, 
<a href="/search/cs?searchtype=author&query=Kouhen%2C+S">Salah Kouhen</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+G">Greta Miller</a>, 
<a href="/search/cs?searchtype=author&query=Parthipan%2C+R">Raghul Parthipan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Climate Informatics 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Atmospheric models used for weather and climate prediction are traditionally
formulated in a deterministic manner. In other words, given a particular state
of the resolved scale variables, the most likely forcing from the sub-grid
scale processes is estimated and used to predict the evolution of the
large-scale flow. However, the lack of scale-separation in the atmosphere means
that this approach is a large source of error in forecasts. Over recent years,
an alternative paradigm has developed: the use of stochastic techniques to
characterise uncertainty in small-scale processes. These techniques are now
widely used across weather, sub-seasonal, seasonal, and climate timescales. In
parallel, recent years have also seen significant progress in replacing
parametrisation schemes using machine learning (ML). This has the potential to
both speed up and improve our numerical models. However, the focus to date has
largely been on deterministic approaches. In this position paper, we bring
together these two key developments, and discuss the potential for data-driven
approaches for stochastic parametrisation. We highlight early studies in this
area, and draw attention to the novel challenges that remain.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09473" title="Abstract">arXiv:2402.09473</a> [<a href="/pdf/2402.09473" title="Download PDF">pdf</a>, <a href="/format/2402.09473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-for-many Counterfactual Explanations by Column Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lodi%2C+A">Andrea Lodi</a>, 
<a href="/search/cs?searchtype=author&query=Ram%C3%ADrez-Ayerbe%2C+J">Jasone Ram&#xed;rez-Ayerbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we consider the problem of generating a set of counterfactual
explanations for a group of instances, with the one-for-many allocation rule,
where one explanation is allocated to a subgroup of the instances. For the
first time, we solve the problem of minimizing the number of explanations
needed to explain all the instances, while considering sparsity by limiting the
number of features allowed to be changed collectively in each explanation. A
novel column generation framework is developed to efficiently search for the
explanations. Our framework can be applied to any black-box classifier, like
neural networks. Compared with a simple adaptation of a mixed-integer
programming formulation from the literature, the column generation framework
dominates in terms of scalability, computational performance and quality of the
solutions.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09475" title="Abstract">arXiv:2402.09475</a> [<a href="/pdf/2402.09475" title="Download PDF">pdf</a>, <a href="/ps/2402.09475" title="Download PostScript">ps</a>, <a href="/format/2402.09475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiveDocs: Crafting Interactive Development Environments From Research  Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klein%2C+P+C">Pedro Costa Klein</a>, 
<a href="/search/cs?searchtype=author&query=Lehrenfeld%2C+C">Christoph Lehrenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Osterhoff%2C+M">Markus Osterhoff</a>, 
<a href="/search/cs?searchtype=author&query=Uecker%2C+M">Martin Uecker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; History and Overview (math.HO)

</div>
<p class="mathjax">Open Science is a recurrent topic in scientific discussion, and there is a
current effort to make research more accessible to a broader audience. A focus
on delivering research findings that are reproducible, or even re-usable has
been proposed as one way of achieving such accessibility goals. In this work,
we present the LiveDocs initiative, an effort of the ``Collaborative Research
Center 1456 - Mathematics of Experiment'' on tackling common issues of
reproducibility and re-usability in scientific publications. The LiveDocs
initiative is proposed as a concept alongside a collection of methods that
enable scientists to provide research findings under an interactive development
environment. This environment allows users from a broader audience to easily
reproduce research findings by re-running scripts, for instance, those that
generate figures, tables, and other elements from scientific publications.
Moreover, LiveDocs also allow the audience to interact with code and data in
such environments, thus allowing users to explore algorithms, datasets and
software interfaces. This directly lowers the barriers to access and comprehend
research methods and findings, which facilitates more scientific exchange and
fosters knowledge advancement.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09477" title="Abstract">arXiv:2402.09477</a> [<a href="/pdf/2402.09477" title="Download PDF">pdf</a>, <a href="/format/2402.09477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANORAMIA: Privacy Auditing of Machine Learning Models without  Retraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazmi%2C+M">Mishaal Kazmi</a>, 
<a href="/search/cs?searchtype=author&query=Lautraite%2C+H">Hadrien Lautraite</a>, 
<a href="/search/cs?searchtype=author&query=Akbari%2C+A">Alireza Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Soroco%2C+M">Mauricio Soroco</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiaoyue Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gambs%2C+S">S&#xe9;bastien Gambs</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A9cuyer%2C+M">Mathias L&#xe9;cuyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a privacy auditing scheme for ML models that relies on
membership inference attacks using generated data as "non-members". This
scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale
ML models without control of the training process or model re-training and only
requires access to a subset of the training data. To demonstrate its
applicability, we evaluate our auditing scheme across multiple ML domains,
ranging from image and tabular data classification to large-scale language
models.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09478" title="Abstract">arXiv:2402.09478</a> [<a href="/pdf/2402.09478" title="Download PDF">pdf</a>, <a href="/format/2402.09478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Reconstruction Attacks and Defenses: A Systematic Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Q">Qi Lei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reconstruction attacks and defenses are essential in understanding the data
leakage problem in machine learning. However, prior work has centered around
empirical observations of gradient inversion attacks, lacks theoretical
groundings, and was unable to disentangle the usefulness of defending methods
versus the computational limitation of attacking methods. In this work, we
propose a strong reconstruction attack in the setting of federated learning.
The attack reconstructs intermediate features and nicely integrates with and
outperforms most of the previous methods. On this stronger attack, we
thoroughly investigate both theoretically and empirically the effect of the
most common defense methods. Our findings suggest that among various defense
mechanisms, such as gradient clipping, dropout, additive noise, local
aggregation, etc., gradient pruning emerges as the most effective strategy to
defend against state-of-the-art attacks.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09480" title="Abstract">arXiv:2402.09480</a> [<a href="/pdf/2402.09480" title="Download PDF">pdf</a>, <a href="/ps/2402.09480" title="Download PostScript">ps</a>, <a href="/format/2402.09480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptoanalysis of a key exchange protocol based on a congruence-simple  semiring action
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alvaro%2C+O+S">Otero Sanchez Alvaro</a>, 
<a href="/search/cs?searchtype=author&query=Antonio%2C+L+R+J">Lopez Ramos Juan Antonio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We show that a previously introduced key exchange based on a
congruence-simple semiring action is not secure by providing an attack that
reveals the shared key from the distributed public information for any of such
semirings
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09481" title="Abstract">arXiv:2402.09481</a> [<a href="/pdf/2402.09481" title="Download PDF">pdf</a>, <a href="/format/2402.09481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptomite: A versatile and user-friendly library of randomness  extractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foreman%2C+C">Cameron Foreman</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+R">Richie Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Edgington%2C+A">Alec Edgington</a>, 
<a href="/search/cs?searchtype=author&query=Curchod%2C+F+J">Florian J. Curchod</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 + 10 pages, including figures and examples with code
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">We present Cryptomite, a Python library of randomness extractor
implementations. The library offers a range of two-source, seeded and
deterministic randomness extractors, together with parameter calculation
modules, making it easy to use and suitable for a variety of applications. We
also present theoretical results, including new extractor constructions and
improvements to existing extractor parameters. The extractor implementations
are efficient in practice and tolerate input sizes of up to $2^{40} &gt; 10^{12}$
bits. They are also numerically precise (implementing convolutions using the
Number Theoretic Transform to avoid floating point arithmetic), making them
well suited to cryptography. The algorithms and parameter calculation are
described in detail, including illustrative code examples and performance
benchmarking.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09486" title="Abstract">arXiv:2402.09486</a> [<a href="/pdf/2402.09486" title="Download PDF">pdf</a>, <a href="/format/2402.09486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMOEA/D: A Multiobjective Evolutionary Algorithm for Uniform Pareto  Objectives based on Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multiobjective optimization (MOO) is prevalent in numerous applications, in
which a Pareto front (PF) is constructed to display optima under various
preferences. Previous methods commonly utilize the set of Pareto objectives
(particles on the PF) to represent the entire PF. However, the empirical
distribution of the Pareto objectives on the PF is rarely studied, which
implicitly impedes the generation of diverse and representative Pareto
objectives in previous methods. To bridge the gap, we suggest in this paper
constructing \emph{uniformly distributed} Pareto objectives on the PF, so as to
alleviate the limited diversity found in previous MOO approaches. We are the
first to formally define the concept of ``uniformity" for an MOO problem. We
optimize the maximal minimal distances on the Pareto front using a neural
network, resulting in both asymptotically and non-asymptotically uniform Pareto
objectives. Our proposed method is validated through experiments on real-world
and synthetic problems, which demonstrates the efficacy in generating
high-quality uniform Pareto objectives and the encouraging performance
exceeding existing state-of-the-art methods.
<br />The detailed model implementation and the code are scheduled to be
open-sourced upon publication.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09488" title="Abstract">arXiv:2402.09488</a> [<a href="/pdf/2402.09488" title="Download PDF">pdf</a>, <a href="/format/2402.09488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Agricultural Greenhouse Control System Based on Internet of  Things and Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Cangqing Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study endeavors to conceptualize and execute a sophisticated
agricultural greenhouse control system grounded in the amalgamation of the
Internet of Things (IoT) and machine learning. Through meticulous monitoring of
intrinsic environmental parameters within the greenhouse and the integration of
machine learning algorithms, the conditions within the greenhouse are aptly
modulated. The envisaged outcome is an enhancement in crop growth efficiency
and yield, accompanied by a reduction in resource wastage. In the backdrop of
escalating global population figures and the escalating exigencies of climate
change, agriculture confronts unprecedented challenges. Conventional
agricultural paradigms have proven inadequate in addressing the imperatives of
food safety and production efficiency. Against this backdrop, greenhouse
agriculture emerges as a viable solution, proffering a controlled milieu for
crop cultivation to augment yields, refine quality, and diminish reliance on
natural resources [b1]. Nevertheless, greenhouse agriculture contends with a
gamut of challenges. Traditional greenhouse management strategies, often
grounded in experiential knowledge and predefined rules, lack targeted
personalized regulation, thereby resulting in resource inefficiencies. The
exigencies of real-time monitoring and precise control of the greenhouse's
internal environment gain paramount importance with the burgeoning scale of
agriculture. To redress this challenge, the study introduces IoT technology and
machine learning algorithms into greenhouse agriculture, aspiring to institute
an intelligent agricultural greenhouse control system conducive to augmenting
the efficiency and sustainability of agricultural production.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09489" title="Abstract">arXiv:2402.09489</a> [<a href="/pdf/2402.09489" title="Download PDF">pdf</a>, <a href="/format/2402.09489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pearson Correlations on Networks: Corrigendum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coscia%2C+M">Michele Coscia</a>, 
<a href="/search/cs?searchtype=author&query=Devriendt%2C+K">Karel Devriendt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Analysis, Statistics and Probability (physics.data-an); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Recently, the first author proposed a measure to calculate Pearson
correlations for node values expressed in a network, by taking into account
distances or metrics defined on the network. In this technical note, we show
that using an arbitrary choice of distances might result in imaginary or
unbounded correlation values, which is undesired. We prove that this problem is
solved by restricting to a special class of distances: negative type metrics.
We also discuss two natural classes of negative type metrics on graphs, for
which the network correlations are properly defined.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09491" title="Abstract">arXiv:2402.09491</a> [<a href="/pdf/2402.09491" title="Download PDF">pdf</a>, <a href="/format/2402.09491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualization Requirements for Business Intelligence Analytics: A  Goal-Based, Iterative Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lavalle%2C+A">Ana Lavalle</a>, 
<a href="/search/cs?searchtype=author&query=Mat%C3%A9%2C+A">Alejandro Mat&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Trujillo%2C+J">Juan Trujillo</a>, 
<a href="/search/cs?searchtype=author&query=Rizzi%2C+S">Stefano Rizzi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2019 IEEE 27th International Requirements Engineering Conference
  (RE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Information visualization plays a key role in business intelligence
analytics. With ever larger amounts of data that need to be interpreted, using
the right visualizations is crucial in order to understand the underlying
patterns and results obtained by analysis algorithms. Despite its importance,
defining the right visualization is still a challenging task. Business users
are rarely experts in information visualization, and they may not exactly know
the most adequate visualization tools or patterns for their goals.
Consequently, misinterpreted graphs and wrong results can be obtained, leading
to missed opportunities and significant losses for companies. The main problem
underneath is a lack of tools and methodologies that allow non-expert users to
define their visualization and data analysis goals in business terms. In order
to tackle this problem, we present an iterative goal-oriented approach based on
the i* language for the automatic derivation of data visualizations. Our
approach links non-expert user requirements to the data to be analyzed,
choosing the most suited visualization techniques in a semi-automatic way. The
great advantage of our proposal is that we provide non-expert users with the
best suited visualizations according to their information needs and their data
with little effort and without requiring expertise in information
visualization.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09492" title="Abstract">arXiv:2402.09492</a> [<a href="/pdf/2402.09492" title="Download PDF">pdf</a>, <a href="/format/2402.09492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMGDA: A Preference-based Multiple Gradient Descent Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It is desirable in many multi-objective machine learning applications, such
as multi-task learning and multi-objective reinforcement learning, to find a
Pareto optimal solution that can exactly match a given preference of
decision-makers. These problems are often large-scale with available gradient
information but cannot be handled very well by the existing algorithms. To
tackle this critical issue, this paper proposes a novel predict-and-correct
framework for locating the exact Pareto optimal solutions required by a
decision maker. In the proposed framework, a constraint function is introduced
in the search progress to align the solution with a user-specific preference,
which can be optimized simultaneously with multiple objective functions.
Experimental results show that our proposed method can efficiently find exact
Pareto optimal solutions for standard benchmarks, multi-task, and
multi-objective reinforcement learning problems with more than thousands of
decision variables.
<br />Code is available at: \url{https://github.com/xzhang2523/pmgda}.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09493" title="Abstract">arXiv:2402.09493</a> [<a href="/pdf/2402.09493" title="Download PDF">pdf</a>, <a href="/format/2402.09493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic modeling and predictive control of a microfluidic system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Martinez%2C+J+V">Jorge Vicente Martinez</a> (1), 
<a href="/search/eess?searchtype=author&query=Ramirez-Laboreo%2C+E">Edgar Ramirez-Laboreo</a> (1), 
<a href="/search/eess?searchtype=author&query=Gil%2C+P+C">Pablo Calderon Gil</a> (2) ((1) Universidad de Zaragoza, (2) Instituto Tecnologico de Aragon)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 17 figures. This is an author-approved English translation of a paper accepted for publication (see Related DOI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Microfluidics, the study of fluids in microscopic channels, has led to
important advances in fields as diverse as microelectronics, biotechnology and
chemistry. Microfluidic research is primarily based on the use of microfluidic
chips, low-cost devices that can be used to perform laboratory experiments
using small amounts of fluid. These systems, however, require advanced control
mechanisms in order to accurately achieve the flow rates and pressures required
in the experiments. In this paper, we present the design of a model predictive
controller intended to regulate the fluid flows in one of these systems. The
results obtained, both through simulations and real experiments performed on
the device, show that predictive control is an ideal technique to control these
systems, especially taking into account all the existing constraints.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09494" title="Abstract">arXiv:2402.09494</a> [<a href="/pdf/2402.09494" title="Download PDF">pdf</a>, <a href="/ps/2402.09494" title="Download PostScript">ps</a>, <a href="/format/2402.09494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can AI and humans genuinely communicate?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonard%2C+C">Constant Bonard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> January 2024 manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Can AI and humans genuinely communicate? In this article, after giving some
background and motivating my proposal (sections 1 to 3), I explore a way to
answer this question that I call the "mental-behavioral methodology" (sections
4 and 5). This methodology follows the following three steps: First, spell out
what mental capacities are sufficient for human communication (as opposed to
communication more generally). Second, spell out the experimental paradigms
required to test whether a behavior exhibits these capacities. Third, apply or
adapt these paradigms to test whether an AI displays the relevant behaviors. If
the first two steps are successfully completed, and if the AI passes the tests
with human-like results, this constitutes evidence that this AI and humans can
genuinely communicate. This mental-behavioral methodology has the advantage
that we don't need to understand the workings of black-box algorithms, such as
standard deep neural networks. This is comparable to the fact that we don't
need to understand how human brains work to know that humans can genuinely
communicate. This methodology also has its disadvantages and I will discuss
some of them (section 6).
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09497" title="Abstract">arXiv:2402.09497</a> [<a href="/pdf/2402.09497" title="Download PDF">pdf</a>, <a href="/format/2402.09497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Tuning for Secure Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingxuan He</a>, 
<a href="/search/cs?searchtype=author&query=Vero%2C+M">Mark Vero</a>, 
<a href="/search/cs?searchtype=author&query=Krasnopolska%2C+G">Gabriela Krasnopolska</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Modern language models (LMs) have gained widespread acceptance in everyday
and professional contexts, particularly in programming. An essential procedure
enabling this adoption is instruction tuning, which substantially enhances LMs'
practical utility by training them to follow user instructions and human
preferences. However, existing instruction tuning schemes overlook a crucial
aspect: the security of generated code. As a result, even the state-of-the-art
instruction-tuned LMs frequently produce unsafe code, posing significant
security risks. In this work, we introduce SafeCoder to address this gap.
SafeCoder performs security-centric fine-tuning using a diverse and
high-quality dataset that we collected using an automated pipeline. We
integrate the security fine-tuning with standard instruction tuning, to
facilitate a joint optimization of both security and utility. Despite its
simplicity, we show that SafeCoder is effective across a variety of popular LMs
and datasets. It is able to drastically improve security (by about 30%), while
preserving utility.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09498" title="Abstract">arXiv:2402.09498</a> [<a href="/pdf/2402.09498" title="Download PDF">pdf</a>, <a href="/ps/2402.09498" title="Download PostScript">ps</a>, <a href="/format/2402.09498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of the most influential variables for preventing postpartum  urinary incontinence using machine learning techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez-Gonz%C3%A1lez%2C+M">Mar&#xed;a &#xc1;lvarez-Gonz&#xe1;lez</a>, 
<a href="/search/cs?searchtype=author&query=Leir%C3%B3s-Rodr%C3%ADguez%2C+R">Raquel Leir&#xf3;s-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+A+F+L">Ana F L&#xf3;pez Rodr&#xed;guez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Digital Health, Volume 8, 2022, 20552076221111289
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Background: Postpartum urinary incontinence (PUI) is a common issue among
postnatal women. Previous studies identified potential related variables, but
lacked analysis on certain intrinsic and extrinsic patient variables during
pregnancy.
<br />Objective: The study aims to evaluate the most influential variables in PUI
using machine learning, focusing on intrinsic, extrinsic, and combined variable
groups.
<br />Methods: Data from 93 pregnant women were analyzed using machine learning and
oversampling techniques. Four key variables were predicted: occurrence,
frequency, intensity of urinary incontinence, and stress urinary incontinence.
<br />Results: Models using extrinsic variables were most accurate, with 70%
accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and
93% for stress urinary incontinence.
<br />Conclusions: The study highlights extrinsic variables as significant
predictors of PUI issues. This suggests that PUI prevention might be achievable
through healthy habits during pregnancy, although further research is needed
for confirmation.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09499" title="Abstract">arXiv:2402.09499</a> [<a href="/pdf/2402.09499" title="Download PDF">pdf</a>, <a href="/ps/2402.09499" title="Download PostScript">ps</a>, <a href="/format/2402.09499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enriched multi-agent middleware for building rule-based distributed  security solutions for IoT environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguayo-Canela%2C+F+J">Francisco Jos&#xe9; Aguayo-Canela</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Journal of Supercomputing, Volume 77, pages 13046 - 13068,
  2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The increasing number of connected devices and the complexity of Internet of
Things (IoT) ecosystems are demanding new architectures for managing and
securing these networked environments. Intrusion Detection Systems (IDS) are
security solutions that help to detect and mitigate the threats that IoT
systems face, but there is a need for new IDS strategies and architectures.
This paper describes a development environment that allows the programming and
debugging of distributed, rule-based multi-agent IDS solutions. The proposed
solution consists in the integration of a rule engine into the agent, the use
of a specialized, wrapping agent class with a graphical user interface for
programming and debugging purposes, and a mechanism for the incremental
composition of behaviors. A comparative study and an example IDS are used to
test and show the suitability and validity of the approach. The JADE
multi-agent middleware has been used for the practical implementations.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09500" title="Abstract">arXiv:2402.09500</a> [<a href="/pdf/2402.09500" title="Download PDF">pdf</a>, <a href="/ps/2402.09500" title="Download PostScript">ps</a>, <a href="/format/2402.09500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Formally Undecidable Traits of Intelligent Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fox%2C+M">Matthew Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Building on work by Alfonseca et al. (2021), we study the conditions
necessary for it to be logically possible to prove that an arbitrary
artificially intelligent machine will exhibit certain behavior. To do this, we
develop a formalism like -- but mathematically distinct from -- the theory of
formal languages and their properties. Our formalism affords a precise means
for not only talking about the traits we desire of machines (such as them being
intelligent, contained, moral, and so forth), but also for detailing the
conditions necessary for it to be logically possible to decide whether a given
arbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s
(2021) results, we find that Rice's theorem from computability theory cannot in
general be used to determine whether an arbitrary machine possesses a given
trait or not. Therefore, it is not necessarily the case that deciding whether
an arbitrary machine is intelligent, contained, moral, and so forth is
logically impossible.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09508" title="Abstract">arXiv:2402.09508</a> [<a href="/pdf/2402.09508" title="Download PDF">pdf</a>, <a href="/format/2402.09508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation  and Editing via Content-based Controls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junyan Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Controllable music generation plays a vital role in human-AI music
co-creation. While Large Language Models (LLMs) have shown promise in
generating high-quality music, their focus on autoregressive generation limits
their utility in music editing tasks. To bridge this gap, we introduce a novel
Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables
autoregressive language models to seamlessly address music inpainting tasks.
Additionally, our PEFT method integrates frame-level content-based controls,
facilitating track-conditioned music refinement and score-conditioned music
arrangement. We apply this method to fine-tune MusicGen, a leading
autoregressive music generation model. Our experiments demonstrate promising
results across multiple music editing tasks, offering more flexible controls
for future AI-driven music editing tools. A demo
page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and
source codes\footnote{\url{https://github.com/Kikyo-16/airgen}.} are available
online.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09527" title="Abstract">arXiv:2402.09527</a> [<a href="/pdf/2402.09527" title="Download PDF">pdf</a>, <a href="/format/2402.09527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jasper: Scalable and Fair Multicast for Financial Exchanges in the Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haseeb%2C+M">Muhammad Haseeb</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jinkun Geng</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+U">Ulysses Butler</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xiyu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Duclos-Cavalcanti%2C+D">Daniel Duclos-Cavalcanti</a>, 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+A">Anirudh Sivaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Financial exchanges have recently shown an interest in migrating to the
public cloud for scalability, elasticity, and cost savings. However, financial
exchanges often have strict network requirements that can be difficult to meet
on the cloud. Notably, market participants (MPs) trade based on market data
about different activities in the market. Exchanges often use switch multicast
to disseminate market data to MPs. However, if one MP receives market data
earlier than another, that MP would have an unfair advantage. To prevent this,
financial exchanges often equalize exchange-to-MP cable lengths to provide
near-simultaneous reception of market data at MPs.
<br />As a cloud tenant, however, building a fair multicast service is challenging
because of the lack of switch support for multicast, high latency variance, and
the lack of native mechanisms for simultaneous data delivery in the cloud.
Jasper introduces a solution that creates an overlay multicast tree within a
cloud region that minimizes latency and latency variations through hedging,
leverages recent advancements in clock synchronization to achieve simultaneous
delivery, and addresses various sources of latency through an optimized
DPDK/eBPF implementation -- while scaling to 1000+ emulated receivers. Jasper
outperforms a prior system CloudEx and a commercial multicast solution provided
by Amazon Web Services.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09528" title="Abstract">arXiv:2402.09528</a> [<a href="/pdf/2402.09528" title="Download PDF">pdf</a>, <a href="/format/2402.09528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Help Me to Understand this Commit! -- A Vision for Contextualized Code  Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Badampudi%2C+D">Deepika Badampudi</a>, 
<a href="/search/cs?searchtype=author&query=Britto%2C+R">Ricardo Britto</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+N+b">Nauman bin Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First IDE Workshop (IDE '24), April 20, 2024, Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Background: Modern Code Review (MCR) is a key component for delivering
high-quality software and sharing knowledge among developers. Effective reviews
require an in-depth understanding of the code and demand from the reviewers to
contextualize the change from different perspectives. Aim: While there is a
plethora of research on solutions that support developers to understand changed
code, we have observed that many provide only narrow, specialized insights and
very few aggregate information in a meaningful manner. Therefore, we aim to
provide a vision of improving code understanding in MCR. Method: We classified
53 research papers suggesting proposals to improve MCR code understanding. We
use this classification, the needs expressed by code reviewers from previous
research, and the information we have not found in the literature for
extrapolation. Results: We identified four major types of support systems and
suggest an environment for contextualized code reviews. Furthermore, we
illustrate with a set of scenarios how such an environment would improve the
effectiveness of code reviews. Conclusions: Current research focuses mostly on
providing narrow support for developers. We outline a vision for how MCR can be
improved by using context and reducing the cognitive load on developers. We
hope our vision can foster future advancements in development environments.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09529" title="Abstract">arXiv:2402.09529</a> [<a href="/pdf/2402.09529" title="Download PDF">pdf</a>, <a href="/format/2402.09529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Manifold Density Function: An Intrinsic Method for the Validation of  Manifold Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holmgren%2C+B">Benjamin Holmgren</a>, 
<a href="/search/cs?searchtype=author&query=Quist%2C+E">Eli Quist</a>, 
<a href="/search/cs?searchtype=author&query=Schupbach%2C+J">Jordan Schupbach</a>, 
<a href="/search/cs?searchtype=author&query=Fasy%2C+B+T">Brittany Terese Fasy</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">We introduce the manifold density function, which is an intrinsic method to
validate manifold learning techniques. Our approach adapts and extends Ripley's
$K$-function, and categorizes in an unsupervised setting the extent to which an
output of a manifold learning algorithm captures the structure of a latent
manifold. Our manifold density function generalizes to broad classes of
Riemannian manifolds. In particular, we extend the manifold density function to
general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the
manifold density function for hypersurfaces is well approximated using the
first Laplacian eigenvalue. We prove desirable convergence and robustness
properties.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09530" title="Abstract">arXiv:2402.09530</a> [<a href="/pdf/2402.09530" title="Download PDF">pdf</a>, <a href="/format/2402.09530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Texture Bias of Deep Neural Networks via Edge Enhancing  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heinert%2C+E">Edgar Heinert</a>, 
<a href="/search/cs?searchtype=author&query=Rottmann%2C+M">Matthias Rottmann</a>, 
<a href="/search/cs?searchtype=author&query=Maag%2C+K">Kira Maag</a>, 
<a href="/search/cs?searchtype=author&query=Kahl%2C+K">Karsten Kahl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Convolutional neural networks (CNNs) for image processing tend to focus on
localized texture patterns, commonly referred to as texture bias. While most of
the previous works in the literature focus on the task of image classification,
we go beyond this and study the texture bias of CNNs in semantic segmentation.
In this work, we propose to train CNNs on pre-processed images with less
texture to reduce the texture bias. Therein, the challenge is to suppress image
texture while preserving shape information. To this end, we utilize edge
enhancing diffusion (EED), an anisotropic image diffusion method initially
introduced for image compression, to create texture reduced duplicates of
existing datasets. Extensive numerical studies are performed with both CNNs and
vision transformer models trained on original data and EED-processed data from
the Cityscapes dataset and the CARLA driving simulator. We observe strong
texture-dependence of CNNs and moderate texture-dependence of transformers.
Training CNNs on EED-processed images enables the models to become completely
ignorant with respect to texture, demonstrating resilience with respect to
texture re-introduction to any degree. Additionally we analyze the performance
reduction in depth on a level of connected components in the semantic
segmentation and study the influence of EED pre-processing on domain
generalization as well as adversarial robustness.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09531" title="Abstract">arXiv:2402.09531</a> [<a href="/pdf/2402.09531" title="Download PDF">pdf</a>, <a href="/format/2402.09531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The dimension weighted fast multipole method for scattered data  approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harbrecht%2C+H">Helmut Harbrecht</a>, 
<a href="/search/math?searchtype=author&query=Multerer%2C+M">Michael Multerer</a>, 
<a href="/search/math?searchtype=author&query=Quizi%2C+J">Jacopo Quizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The present article is concerned scattered data approximation for higher
dimensional data sets which exhibit an anisotropic behavior in the different
dimensions. Tailoring sparse polynomial interpolation to this specific
situation, we derive very efficient degenerate kernel approximations which we
then use in a dimension weighted fast multipole method. This dimension weighted
fast multipole method enables to deal with many more dimensions than the
standard black-box fast multipole method based on interpolation. A thorough
analysis of the method is provided including rigorous error estimates. The
accuracy and the cost of the approach are validated by extensive numerical
results. As a relevant application, we apply the approach to a shape
uncertainty quantification problem.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09538" title="Abstract">arXiv:2402.09538</a> [<a href="/pdf/2402.09538" title="Download PDF">pdf</a>, <a href="/ps/2402.09538" title="Download PostScript">ps</a>, <a href="/format/2402.09538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Lessons Learned: Preliminary Findings From a Study of  Learning From Failure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sillito%2C+J">Jonathan Sillito</a>, 
<a href="/search/cs?searchtype=author&query=Pope%2C+M">Matt Pope</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Due to various sources of uncertainty, emergent behavior, and ongoing
changes, the reliability of many socio-technical systems depends on an
iterative and collaborative process in which organizations (1) analyze and
learn from system failures, and then (2) co-evolve both the technical and human
parts of their systems based on what they learn. Many organizations have
defined processes for learning from failure, often involving postmortem
analyses conducted after any system failures that are judged to be sufficiently
severe. Despite established processes and tool support, our preliminary
research, and professional experience, suggest that it is not straightforward
to take what was learned from a failure and successfully improve the
reliability of the socio-technical system. To better understand this
collaborative process and the associated challenges, we are conducting a study
of how teams learn from failure. We are gathering incident reports from
multiple organizations and conducting interviews with engineers and managers
with relevant experience. Our analytic interest is in what is learned by teams
as they reflect on failures, the learning processes involved, and how they use
what is learned. Our data collection and analysis are not yet complete, but we
have so far analyzed 13 incident reports and seven interviews. In this short
paper we (1) present our preliminary findings, and (2) outline our broader
research plans.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09540" title="Abstract">arXiv:2402.09540</a> [<a href="/pdf/2402.09540" title="Download PDF">pdf</a>, <a href="/format/2402.09540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Does Differential Privacy with Large Epsilon Defend Against  Practical Membership Inference Attacks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lowy%2C+A">Andrew Lowy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Koike-Akino%2C+T">Toshiaki Koike-Akino</a>, 
<a href="/search/cs?searchtype=author&query=Parsons%2C+K">Kieran Parsons</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at PPAI-24: AAAI Workshop on Privacy-Preserving Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP)
provides a strong worst-case guarantee that no membership inference attack
(MIA) can succeed at determining whether a person's data was used to train a
machine learning model. The guarantee of DP is worst-case because: a) it holds
even if the attacker already knows the records of all but one person in the
data set; and b) it holds uniformly over all data sets. In practical
applications, such a worst-case guarantee may be overkill: practical attackers
may lack exact knowledge of (nearly all of) the private data, and our data set
might be easier to defend, in some sense, than the worst-case data set. Such
considerations have motivated the industrial deployment of DP models with large
privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed
empirically that DP with large $\epsilon$ can successfully defend against
state-of-the-art MIAs. Existing DP theory cannot explain these empirical
findings: e.g., the theoretical privacy guarantees of $\epsilon \geq 7$ are
essentially vacuous. In this paper, we aim to close this gap between theory and
practice and understand why a large DP parameter can prevent practical MIAs. To
tackle this problem, we propose a new privacy notion called practical
membership privacy (PMP). PMP models a practical attacker's uncertainty about
the contents of the private data. The PMP parameter has a natural
interpretation in terms of the success rate of a practical MIA on a given data
set. We quantitatively analyze the PMP parameter of two fundamental DP
mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis
reveals that a large DP parameter often translates into a much smaller PMP
parameter, which guarantees strong privacy against practical MIAs. Using our
findings, we offer principled guidance for practitioners in choosing the DP
parameter.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09541" title="Abstract">arXiv:2402.09541</a> [<a href="/pdf/2402.09541" title="Download PDF">pdf</a>, <a href="/format/2402.09541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing test artifact quality -- A tertiary study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+H+K+V">Huynh Khanh Vi Tran</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6rstler%2C+J">J&#xfc;rgen B&#xf6;rstler</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+N+b">Nauman bin Ali</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information and Software Technology 139 (2021): 106620
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Modern software development increasingly relies on software testing
for an ever more frequent delivery of high quality software. This puts high
demands on the quality of the central artifacts in software testing, test
suites and test cases. Objective: We aim to develop a comprehensive model for
capturing the dimensions of test case/suite quality, which are relevant for a
variety of perspectives. Method: We have carried out a systematic literature
review to identify and analyze existing secondary studies on quality aspects of
software testing artifacts. Results: We identified 49 relevant secondary
studies. Of these 49 studies, less than half did some form of quality appraisal
of the included primary studies and only 3 took into account the quality of the
primary study when synthesizing the results. We present an aggregation of the
context dimensions and factors that can be used to characterize the environment
in which the test case/suite quality is investigated. We also provide a
comprehensive model of test case/suite quality with definitions for the quality
attributes and measurements based on findings in the literature and ISO/IEC
25010:2011. Conclusion: The test artifact quality model presented in the paper
can be used to support test artifact quality assessment and improvement
initiatives in practice. Furtherm Information and Software Technology 139
(2021): 106620ore, the model can also be used as a framework for documenting
context characteristics to make research results more accessible for research
and practice.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09542" title="Abstract">arXiv:2402.09542</a> [<a href="/pdf/2402.09542" title="Download PDF">pdf</a>, <a href="/format/2402.09542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Layerwise Proximal Replay: A Proximal Point Method for Online Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jason Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunpeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+F">Frank Wood</a>, 
<a href="/search/cs?searchtype=author&query=Pleiss%2C+G">Geoff Pleiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In online continual learning, a neural network incrementally learns from a
non-i.i.d. data stream. Nearly all online continual learning methods employ
experience replay to simultaneously prevent catastrophic forgetting and
underfitting on past data. Our work demonstrates a limitation of this approach:
networks trained with experience replay tend to have unstable optimization
trajectories, impeding their overall accuracy. Surprisingly, these
instabilities persist even when the replay buffer stores all previous training
examples, suggesting that this issue is orthogonal to catastrophic forgetting.
We minimize these instabilities through a simple modification of the
optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances
learning from new and replay data while only allowing for gradual changes in
the hidden activation of past data. We demonstrate that LPR consistently
improves replay-based online continual learning methods across multiple problem
settings, regardless of the amount of available replay memory.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09543" title="Abstract">arXiv:2402.09543</a> [<a href="/pdf/2402.09543" title="Download PDF">pdf</a>, <a href="/format/2402.09543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Large Language Model Architectures for Sequential  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaorui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kini%2C+V">Venkataramana Kini</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+D">Devendra Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recently, sequential recommendation has been adapted to the LLM paradigm to
enjoy the power of LLMs. LLM-based methods usually formulate recommendation
information into natural language and the model is trained to predict the next
item in an auto-regressive manner. Despite their notable success, the
substantial computational overhead of inference poses a significant obstacle to
their real-world applicability. In this work, we endeavor to streamline
existing LLM-based recommendation models and propose a simple yet highly
effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve
efficient inference for the sequential recommendation task. Lite-LLM4Rec
circumvents the beam search decoding by using a straight item projection head
for ranking scores generation. This design stems from our empirical observation
that beam search decoding is ultimately unnecessary for sequential
recommendations. Additionally, Lite-LLM4Rec introduces a hierarchical LLM
structure tailored to efficiently handle the extensive contextual information
associated with items, thereby reducing computational overhead while enjoying
the capabilities of LLMs. Experiments on three publicly available datasets
corroborate the effectiveness of Lite-LLM4Rec in both performance and inference
efficiency (notably 46.8% performance improvement and 97.28% efficiency
improvement on ML-1m) over existing LLM-based methods. Our implementations will
be open sourced.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09545" title="Abstract">arXiv:2402.09545</a> [<a href="/pdf/2402.09545" title="Download PDF">pdf</a>, <a href="/ps/2402.09545" title="Download PostScript">ps</a>, <a href="/format/2402.09545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 3D Memristor Architecture for In-Memory Computing Demonstrated with  SHA3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aljafar%2C+M+J">Muayad J. Aljafar</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+R">Rasika Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Acken%2C+J+M">John M. Acken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 tables, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Security is a growing problem that needs hardware support. Memristors provide
an alternative technology for hardware-supported security implementation. This
paper presents a specific technique that utilizes the benefits of hybrid
CMOS-memristors technology demonstrated with SHA3 over implementations that use
only memristor technology. In the proposed technique, SHA3 is implemented in a
set of perpendicular crossbar arrays structured to facilitate logic
implementation and circular bit rotation (Rho operation), which is perhaps the
most complex operation in SHA3 when carried out in memristor arrays. The Rho
operation itself is implemented with CMOS multiplexers (MUXs). The proposed
accelerator is standby power-free and circumvents the memory access bottleneck
in conventional computers. In addition, our design obscures the intermediate
values from the I/O interface and outperforms the state-of-the-art
memristor-based designs in terms of size and energy. Demonstrating the
memristor implementation of SHA3 provides an impetus for utilizing memristors
in information security applications.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09546" title="Abstract">arXiv:2402.09546</a> [<a href="/pdf/2402.09546" title="Download PDF">pdf</a>, <a href="/format/2402.09546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Secure Are Large Language Models (LLMs) for Navigation in Urban  Environments?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Congcong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiazhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuaihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of robotics and automation, navigation systems based on Large
Language Models (LLMs) have recently shown impressive performance. However, the
security aspects of these systems have received relatively less attention. This
paper pioneers the exploration of vulnerabilities in LLM-based navigation
models in urban outdoor environments, a critical area given the technology's
widespread application in autonomous driving, logistics, and emergency
services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)
Attack that manipulates LLM-based navigation models by appending
gradient-derived suffixes to the original navigational prompt, leading to
incorrect actions. We conducted comprehensive experiments on an LLMs-based
navigation model that employs various LLMs for reasoning. Our results, derived
from the Touchdown and Map2Seq street-view datasets under both few-shot
learning and fine-tuning configurations, demonstrate notable performance
declines across three metrics in the face of both white-box and black-box
attacks. These results highlight the generalizability and transferability of
the NPS Attack, emphasizing the need for enhanced security in LLM-based
navigation systems. As an initial countermeasure, we propose the Navigational
Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant
keywords to reduce the impact of adversarial suffixes. While initial findings
indicate that this strategy enhances navigational safety, there remains a
critical need for the wider research community to develop stronger defense
methods to effectively tackle the real-world challenges faced by these systems.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09548" title="Abstract">arXiv:2402.09548</a> [<a href="/pdf/2402.09548" title="Download PDF">pdf</a>, <a href="/format/2402.09548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does bilevel optimization result in more competitive racing behavior?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cinar%2C+A">Andrew Cinar</a>, 
<a href="/search/cs?searchtype=author&query=Laine%2C+F">Forrest Laine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Two-vehicle racing is natural example of a competitive dynamic game. As with
most dynamic games, there are many ways in which the underlying information
pattern can be structured, resulting in different equilibrium concepts. For
racing in particular, the information pattern assumed plays a large impact in
the type of behaviors that can emerge from the two interacting players. For
example, blocking behavior is something that cannot emerge from static Nash
play, but could presumably emerge from leader-follower play. In this work, we
develop a novel model for competitive two-player vehicle racing, complete with
simplified aerodynamic drag and drafting effects, as well as position-dependent
collision-avoidance responsibility. We use this model to explore the impact
that different information patterns have on the resulting competitiveness of
the players. A solution approach for solving bilevel optimization problems is
developed, which allows us to run a large-scale empirical study comparing how
bilevel strategy generation (both as leader and as follower) compares with Nash
equilibrium strategy generation as well as a single-player, constant velocity
prediction baseline. Each of these choices are evaluated against different
combinations of opponent strategy selection method. The somewhat surprising
results of this study are discussed throughout.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09549" title="Abstract">arXiv:2402.09549</a> [<a href="/pdf/2402.09549" title="Download PDF">pdf</a>, <a href="/format/2402.09549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pareto-Optimal Algorithms for Learning in Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arunachaleswaran%2C+E+R">Eshwar Ram Arunachaleswaran</a>, 
<a href="/search/cs?searchtype=author&query=Collina%2C+N">Natalie Collina</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jon Schneider</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the problem of characterizing optimal learning algorithms for
playing repeated games against an adversary with unknown payoffs. In this
problem, the first player (called the learner) commits to a learning algorithm
against a second player (called the optimizer), and the optimizer best-responds
by choosing the optimal dynamic strategy for their (unknown but well-defined)
payoff. Classic learning algorithms (such as no-regret algorithms) provide some
counterfactual guarantees for the learner, but might perform much more poorly
than other learning algorithms against particular optimizer payoffs.
<br />In this paper, we introduce the notion of asymptotically Pareto-optimal
learning algorithms. Intuitively, if a learning algorithm is Pareto-optimal,
then there is no other algorithm which performs asymptotically at least as well
against all optimizers and performs strictly better (by at least $\Omega(T)$)
against some optimizer. We show that well-known no-regret algorithms such as
Multiplicative Weights and Follow The Regularized Leader are Pareto-dominated.
However, while no-regret is not enough to ensure Pareto-optimality, we show
that a strictly stronger property, no-swap-regret, is a sufficient condition
for Pareto-optimality.
<br />Proving these results requires us to address various technical challenges
specific to repeated play, including the fact that there is no simple
characterization of how optimizers who are rational in the long-term
best-respond against a learning algorithm over multiple rounds of play. To
address this, we introduce the idea of the asymptotic menu of a learning
algorithm: the convex closure of all correlated distributions over strategy
profiles that are asymptotically implementable by an adversary. We show that
all no-swap-regret algorithms share the same asymptotic menu, implying that all
no-swap-regret algorithms are ``strategically equivalent''.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09550" title="Abstract">arXiv:2402.09550</a> [<a href="/pdf/2402.09550" title="Download PDF">pdf</a>, <a href="/format/2402.09550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Clustering for Improved Offline Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yixin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+F+R">Francisco Roldan Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keru Wang</a>, 
<a href="/search/cs?searchtype=author&query=McGuinness%2C+K">Kevin McGuinness</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+N">Noel O&#x27;Connor</a>, 
<a href="/search/cs?searchtype=author&query=Redmond%2C+S+J">Stephen J. Redmond</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Offline policy learning aims to discover decision-making policies from
previously-collected datasets without additional online interactions with the
environment. As the training dataset is fixed, its quality becomes a crucial
determining factor in the performance of the learned policy. This paper studies
a dataset characteristic that we refer to as multi-behavior, indicating that
the dataset is collected using multiple policies that exhibit distinct
behaviors. In contrast, a uni-behavior dataset would be collected solely using
one policy. We observed that policies learned from a uni-behavior dataset
typically outperform those learned from multi-behavior datasets, despite the
uni-behavior dataset having fewer examples and less diversity. Therefore, we
propose a behavior-aware deep clustering approach that partitions
multi-behavior datasets into several uni-behavior subsets, thereby benefiting
downstream policy learning. Our approach is flexible and effective; it can
adaptively estimate the number of clusters while demonstrating high clustering
accuracy, achieving an average Adjusted Rand Index of 0.987 across various
continuous control task datasets. Finally, we present improved policy learning
examples using dataset clustering and discuss several potential scenarios where
our approach might benefit the offline policy learning community.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09551" title="Abstract">arXiv:2402.09551</a> [<a href="/pdf/2402.09551" title="Download PDF">pdf</a>, <a href="/ps/2402.09551" title="Download PostScript">ps</a>, <a href="/format/2402.09551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zak-OTFS and LDPC Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabak%2C+B">Beyza Dabak</a>, 
<a href="/search/cs?searchtype=author&query=Khammammetti%2C+V">Venkatesh Khammammetti</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+S+K">Saif Khan Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Calderbank%2C+R">Robert Calderbank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages (double column), 6 figures, accepted at 2024 IEEE International Conference on Communications (ICC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Orthogonal Time Frequency Space (OTFS) is a framework for communications and
active sensing that processes signals in the delay-Doppler (DD) domain. It is
informed by 6G propagation environments, where Doppler spreads measured in kHz
make it more and more difficult to estimate channels, and the standard
model-dependent approach to wireless communication is starting to break down.
We consider Zak-OTFS where inverse Zak transform converts information symbols
mounted on DD domain pulses to the time domain for transmission. Zak-OTFS
modulation is parameterized by a delay period $\tau_{p}$ and a Doppler period
$\nu_{p}$, where the product $\tau_{p}\nu_{p}=1$. When the channel spread is
less than the delay period, and the Doppler spread is less than the Doppler
period, the Zak-OTFS input-output relation can be predicted from the response
to a single pilot symbol. The highly reliable channel estimates concentrate
around the pilot location, and we configure low-density parity-check (LDPC)
codes that take advantage of this prior information about reliability. It is
advantageous to allocate information symbols to more reliable bins in the DD
domain. We report simulation results for a Veh-A channel model where it is not
possible to resolve all the paths, showing that LDPC coding extends the range
of Doppler spreads for which reliable model-free communication is possible. We
show that LDPC coding reduces sensitivity to the choice of transmit filter,
making bandwidth expansion less necessary. Finally, we compare BER performance
of Zak-OTFS to that of a multicarrier approximation (MC-OTFS), showing LDPC
coding amplifies the gains previously reported for uncoded transmission.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09552" title="Abstract">arXiv:2402.09552</a> [<a href="/pdf/2402.09552" title="Download PDF">pdf</a>, <a href="/format/2402.09552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rationality Report Cards: Assessing the Economic Rationality of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+N">Narun Raman</a>, 
<a href="/search/cs?searchtype=author&query=Lundy%2C+T">Taylor Lundy</a>, 
<a href="/search/cs?searchtype=author&query=Amouyal%2C+S">Samuel Amouyal</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+Y">Yoav Levine</a>, 
<a href="/search/cs?searchtype=author&query=Leyton-Brown%2C+K">Kevin Leyton-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+M">Moshe Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">There is increasing interest in using LLMs as decision-making "agents." Doing
so includes many degrees of freedom: which model should be used; how should it
be prompted; should it be asked to introspect, conduct chain-of-thought
reasoning, etc? Settling these questions -- and more broadly, determining
whether an LLM agent is reliable enough to be trusted -- requires a methodology
for assessing such an agent's economic rationality. In this paper, we provide
one. We begin by surveying the economic literature on rational decision making,
taxonomizing a large set of fine-grained "elements" that an agent should
exhibit, along with dependencies between them. We then propose a benchmark
distribution that quantitatively scores an LLMs performance on these elements
and, combined with a user-provided rubric, produces a "rationality report
card." Finally, we describe the results of a large-scale empirical experiment
with 14 different LLMs, characterizing the both current state of the art and
the impact of different model sizes on models' ability to exhibit rational
behavior.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09553" title="Abstract">arXiv:2402.09553</a> [<a href="/pdf/2402.09553" title="Download PDF">pdf</a>, <a href="/format/2402.09553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical and Machine Learning Models for Predicting Fire and Other  Emergency Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D+P">Dilli Prasad Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Beigi-Mohammadi%2C+N">Nasim Beigi-Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Hongxiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+D">Dawn Dixon</a>, 
<a href="/search/cs?searchtype=author&query=Madro%2C+R">Rob Madro</a>, 
<a href="/search/cs?searchtype=author&query=Emmenegger%2C+P">Phil Emmenegger</a>, 
<a href="/search/cs?searchtype=author&query=Tobar%2C+C">Carlos Tobar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jeff Li</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Emergency events in a city cause considerable economic loss to individuals,
their families, and the community. Accurate and timely prediction of events can
help the emergency fire and rescue services in preparing for and mitigating the
consequences of emergency events. In this paper, we present a systematic
development of predictive models for various types of emergency events in the
City of Edmonton, Canada. We present methods for (i) data collection and
dataset development; (ii) descriptive analysis of each event type and its
characteristics at different spatiotemporal levels; (iii) feature analysis and
selection based on correlation coefficient analysis and feature importance
analysis; and (iv) development of prediction models for the likelihood of
occurrence of each event type at different temporal and spatial resolutions. We
analyze the association of event types with socioeconomic and demographic data
at the neighborhood level, identify a set of predictors for each event type,
and develop predictive models with negative binomial regression. We conduct
evaluations at neighborhood and fire station service area levels. Our results
show that the models perform well for most of the event types with acceptable
prediction errors for weekly and monthly periods. The evaluation shows that the
prediction accuracy is consistent at the level of the fire station, so the
predictions can be used in management by fire rescue service departments for
planning resource allocation for these time periods. We also examine the impact
of the COVID-19 pandemic on the occurrence of events and on the accuracy of
event predictor models. Our findings show that COVID-19 had a significant
impact on the performance of the event prediction models.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09555" title="Abstract">arXiv:2402.09555</a> [<a href="/pdf/2402.09555" title="Download PDF">pdf</a>, <a href="/format/2402.09555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Use of Agile Practices in Start-ups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klotins%2C+E">Eriks Klotins</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Chatzipetrou%2C+P">Panagiota Chatzipetrou</a>, 
<a href="/search/cs?searchtype=author&query=Gorschek%2C+T">Tony Gorschek</a>, 
<a href="/search/cs?searchtype=author&query=Prikladnicki%2C+R">Rafael Prikladnicki</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+N">Nirnaya Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Pompermaier%2C+L+B">Leandro Bento Pompermaier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2309.12434">arXiv:2309.12434</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> e-Informatica Software Engineering Journal, 2021, 15.1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context Software start-ups have shown their ability to develop and launch
innovative software products and services. Small, motivated teams and uncertain
project scope makes start-ups good candidates for adopting Agile practices.
Objective We explore how start-ups use Agile practices and what effects can be
associated with the use of those practices. Method We use a case survey to
analyze 84 start-up cases and 56 Agile practices. We apply statistical methods
to test for statistically significant associations between the use of Agile
practices, team, and product factors. Results Our results suggest that
development of the backlog, use of version control, code refactoring, and
development of user stories are the most frequently reported practices. We
identify 22 associations between the use of Agile practices, team, and product
factors. The use of Agile practices is associated with effects on source code
and overall product quality. A team's positive or negative attitude towards
best engineering practices is a significant indicator for either adoption or
rejection of certain Agile practices. To explore the relationships in our
findings, we set forth a number of propositions that can be investigated in
future research. Conclusions We conclude that start-ups use Agile practices,
however without following any specific methodology. We identify the opportunity
for more fine-grained studies into the adoption and effects of individual Agile
practices. Start-up practitioners could benefit from Agile practices in terms
of better overall quality, tighter control over team performance, and resource
utilization.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09557" title="Abstract">arXiv:2402.09557</a> [<a href="/pdf/2402.09557" title="Download PDF">pdf</a>, <a href="/ps/2402.09557" title="Download PostScript">ps</a>, <a href="/format/2402.09557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Source Code Representations for Deep Learning with Static  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xueting Guan</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Deep learning techniques applied to program analysis tasks such as code
classification, summarization, and bug detection have seen widespread interest.
Traditional approaches, however, treat programming source code as natural
language text, which may neglect significant structural or semantic details.
Additionally, most current methods of representing source code focus solely on
the code, without considering beneficial additional context. This paper
explores the integration of static analysis and additional context such as bug
reports and design patterns into source code representations for deep learning
models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and
augment it with additional context information obtained from bug reports and
design patterns, creating an enriched source code representation that
significantly enhances the performance of common software engineering tasks
such as code classification and code clone detection. Utilizing existing
open-source code data, our approach improves the representation and processing
of source code, thereby improving task performance.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09558" title="Abstract">arXiv:2402.09558</a> [<a href="/pdf/2402.09558" title="Download PDF">pdf</a>, <a href="/format/2402.09558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Generative Pre-training for Improving Time Series  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qincheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">He Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning time-series representations for discriminative tasks has been a
long-standing challenge. Current pre-training methods are limited in either
unidirectional next-token prediction or randomly masked token prediction. We
propose a novel architecture called Bidirectional Timely Generative Pre-trained
Transformer (BiTimelyGPT), which pre-trains on time-series data by both
next-token and previous-token predictions in alternating transformer layers.
This pre-training task preserves original distribution and data shapes of the
time-series. Additionally, the full-rank forward and backward attention
matrices exhibit more expressive representation capabilities. Using biosignal
data, BiTimelyGPT demonstrates superior performance in predicting neurological
functionality, disease diagnosis, and physiological signs. By visualizing the
attention heatmap, we observe that the pre-trained BiTimelyGPT can identify
discriminative segments from time-series sequences, even more so after
fine-tuning on the task.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09560" title="Abstract">arXiv:2402.09560</a> [<a href="/pdf/2402.09560" title="Download PDF">pdf</a>, <a href="/format/2402.09560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Free Rates in Neyman-Pearson Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalan%2C+M+M">Mohammadreza M. Kalan</a>, 
<a href="/search/cs?searchtype=author&query=Kpotufe%2C+S">Samory Kpotufe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of Neyman-Pearson classification which models
unbalanced classification settings where error w.r.t. a distribution $\mu_1$ is
to be minimized subject to low error w.r.t. a different distribution $\mu_0$.
Given a fixed VC class $\mathcal{H}$ of classifiers to be minimized over, we
provide a full characterization of possible distribution-free rates, i.e.,
minimax rates over the space of all pairs $(\mu_0, \mu_1)$. The rates involve a
dichotomy between hard and easy classes $\mathcal{H}$ as characterized by a
simple geometric condition, a three-points-separation condition, loosely
related to VC dimension.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09561" title="Abstract">arXiv:2402.09561</a> [<a href="/pdf/2402.09561" title="Download PDF">pdf</a>, <a href="/format/2402.09561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-based adaptive temporal filter and residual evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weiying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Riot%2C+P">Paul Riot</a>, 
<a href="/search/cs?searchtype=author&query=Deledalle%2C+C">Charles-Alban Deledalle</a>, 
<a href="/search/cs?searchtype=author&query=Ma%C3%AEtre%2C+H">Henri Ma&#xee;tre</a>, 
<a href="/search/cs?searchtype=author&query=Nicolas%2C+J">Jean-Marie Nicolas</a>, 
<a href="/search/cs?searchtype=author&query=Tupin%2C+F">Florence Tupin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In coherent imaging systems, speckle is a signal-dependent noise that
visually strongly degrades images' appearance. A huge amount of SAR data has
been acquired from different sensors with different wavelengths, resolutions,
incidences and polarizations. We extend the nonlocal filtering strategy to the
temporal domain and propose a patch-based adaptive temporal filter (PATF) to
take advantage of well-registered multi-temporal SAR images. A patch-based
generalised likelihood ratio test is processed to suppress the changed object
effects on the multitemporal denoising results. Then, the similarities are
transformed into corresponding weights with an exponential function. The
denoised value is calculated with a temporal weighted average. Spatial adaptive
denoising methods can improve the patch-based weighted temporal average image
when the time series is limited. The spatial adaptive denoising step is
optional when the time series is large enough. Without reference image, we
propose using a patch-based auto-covariance residual evaluation method to
examine the ratio image between the noisy and denoised images and look for
possible remaining structural contents. It can process automatically and does
not rely on a supervised selection of homogeneous regions. It also provides a
global score for the whole image. Numerous results demonstrate the
effectiveness of the proposed time series denoising method and the usefulness
of the residual evaluation method.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09563" title="Abstract">arXiv:2402.09563</a> [<a href="/pdf/2402.09563" title="Download PDF">pdf</a>, <a href="/format/2402.09563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ABIDES-Economist: Agent-Based Simulation of Economic Systems with  Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwarakanath%2C+K">Kshama Dwarakanath</a>, 
<a href="/search/cs?searchtype=author&query=Vyetrenko%2C+S">Svitlana Vyetrenko</a>, 
<a href="/search/cs?searchtype=author&query=Tavallali%2C+P">Peyman Tavallali</a>, 
<a href="/search/cs?searchtype=author&query=Balch%2C+T">Tucker Balch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">We introduce a multi-agent simulator for economic systems comprised of
heterogeneous Households, heterogeneous Firms, Central Bank and Government
agents, that could be subjected to exogenous, stochastic shocks. The
interaction between agents defines the production and consumption of goods in
the economy alongside the flow of money. Each agent can be designed to act
according to fixed, rule-based strategies or learn their strategies using
interactions with others in the simulator. We ground our simulator by choosing
agent heterogeneity parameters based on economic literature, while designing
their action spaces in accordance with real data in the United States. Our
simulator facilitates the use of reinforcement learning strategies for the
agents via an OpenAI Gym style environment definition for the economic system.
We demonstrate the utility of our simulator by simulating and analyzing two
hypothetical (yet interesting) economic scenarios. The first scenario
investigates the impact of heterogeneous household skills on their learned
preferences to work at different firms. The second scenario examines the impact
of a positive production shock to one of two firms on its pricing strategy in
comparison to the second firm. We aspire that our platform sets a stage for
subsequent research at the intersection of artificial intelligence and
economics.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09564" title="Abstract">arXiv:2402.09564</a> [<a href="/pdf/2402.09564" title="Download PDF">pdf</a>, <a href="/format/2402.09564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile-Informed Action Primitives Mitigate Jamming in Dense Clutter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brouwer%2C+D">Dane Brouwer</a>, 
<a href="/search/cs?searchtype=author&query=Citron%2C+J">Joshua Citron</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hojung Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lepert%2C+M">Marion Lepert</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Michael Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>, 
<a href="/search/cs?searchtype=author&query=Cutkosky%2C+M">Mark Cutkosky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of paper accepted to IEEE ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">It is difficult for robots to retrieve objects in densely cluttered lateral
access scenes with movable objects as jamming against adjacent objects and
walls can inhibit progress. We propose the use of two action primitives --
burrowing and excavating -- that can fluidize the scene to un-jam obstacles and
enable continued progress. Even when these primitives are implemented in an
open loop manner at clock-driven intervals, we observe a decrease in the final
distance to the target location. Furthermore, we combine the primitives into a
closed loop hybrid control strategy using tactile and proprioceptive
information to leverage the advantages of both primitives without being overly
disruptive. In doing so, we achieve a 10-fold increase in success rate above
the baseline control strategy and significantly improve completion times as
compared to the primitives alone or a naive combination of them.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09565" title="Abstract">arXiv:2402.09565</a> [<a href="/pdf/2402.09565" title="Download PDF">pdf</a>, <a href="/format/2402.09565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale  Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Linfeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haoran Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024 (WWW'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Due to the ubiquity of graph data on the web, web graph mining has become a
hot research spot. Nonetheless, the prevalence of large-scale web graphs in
real applications poses significant challenges to storage, computational
capacity and graph model design. Despite numerous studies to enhance the
scalability of graph models, a noticeable gap remains between academic research
and practical web graph mining applications. One major cause is that in most
industrial scenarios, only a small part of nodes in a web graph are actually
required to be analyzed, where we term these nodes as target nodes, while
others as background nodes. In this paper, we argue that properly fetching and
condensing the background nodes from massive web graph data might be a more
economical shortcut to tackle the obstacles fundamentally. To this end, we make
the first attempt to study the problem of massive background nodes compression
for target nodes classification. Through extensive experiments, we reveal two
critical roles played by the background nodes in target node classification:
enhancing structural connectivity between target nodes, and feature correlation
with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model,
which properly fetches the background nodes, and further condenses the semantic
and topological information of background nodes within similar
target-background local structures. Extensive experiments on various web graph
datasets demonstrate the effectiveness and efficiency of the proposed method.
In particular, for MAG240M dataset with 0.24 billion nodes, our generated
skeleton graph achieves highly comparable performance while only containing
1.8% nodes of the original graph.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09568" title="Abstract">arXiv:2402.09568</a> [<a href="/pdf/2402.09568" title="Download PDF">pdf</a>, <a href="/format/2402.09568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Irreducible Markov Chains on spaces of graphs with fixed degree-color  sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almendra-Hern%C3%A1ndez%2C+F">F&#xe9;lix Almendra-Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=De+Loera%2C+J+A">Jes&#xfa;s A. De Loera</a>, 
<a href="/search/cs?searchtype=author&query=Petrovi%C4%87%2C+S">Sonja Petrovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Commutative Algebra (math.AC); Combinatorics (math.CO)

</div>
<p class="mathjax">We study a colored generalization of the famous simple-switch Markov chain
for sampling the set of graphs with a fixed degree sequence. Here we consider
the space of graphs with colored vertices, in which we fix the degree sequence
and another statistic arising from the vertex coloring, and prove that the set
can be connected with simple color-preserving switches or moves. These moves
form a basis for defining an irreducible Markov chain necessary for testing
statistical model fit to block-partitioned network data. Our methods further
generalize well-known algebraic results from the 1990s: namely, that the
corresponding moves can be used to construct a regular triangulation for a
generalization of the second hypersimplex. On the other hand, in contrast to
the monochromatic case, we show that for simple graphs, the 1-norm of the moves
necessary to connect the space increases with the number of colors.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09569" title="Abstract">arXiv:2402.09569</a> [<a href="/pdf/2402.09569" title="Download PDF">pdf</a>, <a href="/format/2402.09569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Plaque Detection and Agatston Score Estimation on Non-Contrast  CT Scans: A Multicenter Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A+M">Andrew M. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mathai%2C+T+S">Tejas Sudharshan Mathai</a>, 
<a href="/search/cs?searchtype=author&query=Grayson%2C+P+C">Peter C. Grayson</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+R+M">Ronald M. Summers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Coronary artery calcification (CAC) is a strong and independent predictor of
cardiovascular disease (CVD). However, manual assessment of CAC often requires
radiological expertise, time, and invasive imaging techniques. The purpose of
this multicenter study is to validate an automated cardiac plaque detection
model using a 3D multiclass nnU-Net for gated and non-gated non-contrast chest
CT volumes. CT scans were performed at three tertiary care hospitals and
collected as three datasets, respectively. Heart, aorta, and lung segmentations
were determined using TotalSegmentator, while plaques in the coronary arteries
and heart valves were manually labeled for 801 volumes. In this work we
demonstrate how the nnU-Net semantic segmentation pipeline may be adapted to
detect plaques in the coronary arteries and valves. With a linear correction,
nnU-Net deep learning methods may also accurately estimate Agatston scores on
chest non-contrast CT scans. Compared to manual Agatson scoring, automated
Agatston scoring indicated a slope of the linear regression of 0.841 with an
intercept of +16 HU (R2 = 0.97). These results are an improvement over previous
work assessing automated Agatston score computation in non-gated CT scans.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09573" title="Abstract">arXiv:2402.09573</a> [<a href="/pdf/2402.09573" title="Download PDF">pdf</a>, <a href="/format/2402.09573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Changes by Butterflies: Farsighted Forecasting with Group Reservoir  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowsher%2C+M">Md Kowsher</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jia Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In Chaos, a minor divergence between two initial conditions exhibits
exponential amplification over time, leading to far-away outcomes, known as the
butterfly effect. Thus, the distant future is full of uncertainty and hard to
forecast. We introduce Group Reservoir Transformer to predict long-term events
more accurately and robustly by overcoming two challenges in Chaos: (1) the
extensive historical sequences and (2) the sensitivity to initial conditions. A
reservoir is attached to a Transformer to efficiently handle arbitrarily long
historical lengths, with an extension of a group of reservoirs to reduce the
uncertainty due to the initialization variations. Our architecture consistently
outperforms state-of-the-art DNN models in multivariate time series, including
NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an
error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air
quality, demonstrating that an ensemble of butterfly learning, the prediction
can be improved to a more adequate and certain one, despite of the traveling
time to the unknown future.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09579" title="Abstract">arXiv:2402.09579</a> [<a href="/pdf/2402.09579" title="Download PDF">pdf</a>, <a href="/ps/2402.09579" title="Download PostScript">ps</a>, <a href="/format/2402.09579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Building Energy Modeling with Large Language Models:  Exploration and Case Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhelun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ford%2C+V">Vitaly Ford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid progression in artificial intelligence has facilitated the
emergence of large language models like ChatGPT, offering potential
applications extending into specialized engineering modeling, especially
physics-based building energy modeling. This paper investigates the innovative
integration of large language models with building energy modeling software,
focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature
review is first conducted to reveal a growing trend of incorporating of large
language models in engineering modeling, albeit limited research on their
application in building energy modeling. We underscore the potential of large
language models in addressing building energy modeling challenges and outline
potential applications including 1) simulation input generation, 2) simulation
output analysis and visualization, 3) conducting error analysis, 4)
co-simulation, 5) simulation knowledge extraction and training, and 6)
simulation optimization. Three case studies reveal the transformative potential
of large language models in automating and optimizing building energy modeling
tasks, underscoring the pivotal role of artificial intelligence in advancing
sustainable building practices and energy efficiency. The case studies
demonstrate that selecting the right large language model techniques is
essential to enhance performance and reduce engineering efforts. Besides direct
use of large language models, three specific techniques were utilized: 1)
prompt engineering, 2) retrieval-augmented generation, and 3) multi-agent large
language models. The findings advocate a multidisciplinary approach in future
artificial intelligence research, with implications extending beyond building
energy modeling to other specialized engineering modeling.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09580" title="Abstract">arXiv:2402.09580</a> [<a href="/pdf/2402.09580" title="Download PDF">pdf</a>, <a href="/format/2402.09580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity Reduction in Machine Learning-Based Wireless Positioning:  Minimum Description Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+M+S">Myeung Suk Oh</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A+B">Anindya Bijoy Das</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taejoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Love%2C+D+J">David J. Love</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted in IEEE International Conference on Communications (ICC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A recent line of research has been investigating deep learning approaches to
wireless positioning (WP). Although these WP algorithms have demonstrated high
accuracy and robust performance against diverse channel conditions, they also
have a major drawback: they require processing high-dimensional features, which
can be prohibitive for mobile applications. In this work, we design a
positioning neural network (P-NN) that substantially reduces the complexity of
deep learning-based WP through carefully crafted minimum description features.
Our feature selection is based on maximum power measurements and their temporal
locations to convey information needed to conduct WP. We also develop a novel
methodology for adaptively selecting the size of feature space, which optimizes
over balancing the expected amount of useful information and classification
capability, quantified using information-theoretic measures on the signal bin
selection. Numerical results show that P-NN achieves a significant advantage in
performance-complexity tradeoff over deep learning baselines that leverage the
full power delay profile (PDP).
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09581" title="Abstract">arXiv:2402.09581</a> [<a href="/pdf/2402.09581" title="Download PDF">pdf</a>, <a href="/format/2402.09581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combatting deepfakes: Policies to address national security threats and  rights violations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miotti%2C+A">Andrea Miotti</a>, 
<a href="/search/cs?searchtype=author&query=Wasil%2C+A">Akash Wasil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper provides policy recommendations to address threats from deepfakes.
First, we provide background information about deepfakes and review the harms
they pose. We describe how deepfakes are currently used to proliferate sexual
abuse material, commit fraud, manipulate voter behavior, and pose threats to
national security. Second, we review previous legislative proposals designed to
address deepfakes. Third, we present a comprehensive policy proposal that
focuses on addressing multiple parts of the deepfake supply chain. The deepfake
supply chain begins with a small number of model developers, model providers,
and compute providers, and it expands to include billions of potential deepfake
creators. We describe this supply chain in greater detail and describe how
entities at each step of the supply chain ought to take reasonable measures to
prevent the creation and proliferation of deepfakes. Finally, we address
potential counterpoints of our proposal. Overall, deepfakes will present
increasingly severe threats to global security and individual liberties. To
address these threats, we call on policymakers to enact legislation that
addresses multiple parts of the deepfake supply chain.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09582" title="Abstract">arXiv:2402.09582</a> [<a href="/pdf/2402.09582" title="Download PDF">pdf</a>, <a href="/ps/2402.09582" title="Download PostScript">ps</a>, <a href="/format/2402.09582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finnish primary school students&#x27; conceptions of machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mertala%2C+P">Pekka Mertala</a>, 
<a href="/search/cs?searchtype=author&query=Fagerlund%2C+J">Janne Fagerlund</a>, 
<a href="/search/cs?searchtype=author&query=Lehtoranta%2C+J">Jukka Lehtoranta</a>, 
<a href="/search/cs?searchtype=author&query=Mattila%2C+E">Emilia Mattila</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+T">Tiina Korhonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Objective This study investigates what kind of conceptions primary school
students have about ML if they are not conceptually "primed" with the idea that
in ML, humans teach computers. Method Qualitative survey responses from 197
Finnish primary schoolers were analyzed via an abductive method. Findings We
identified three partly overlapping ML conception categories, starting from the
most accurate one: ML is about teaching machines (34%), ML is about coding
(7.6%), and ML is about learning via or about machines (37.1%). Implications
The findings suggest that without conceptual clues, children's conceptions of
ML are varied and may include misconceptions such as ML is about learning via
or about machines. The findings underline the importance of clear and
systematic use of key concepts in computer science education. Besides
researchers, this study offers insights for teachers, teacher educators,
curriculum developers, and policymakers. Method Qualitative survey responses
from 197 Finnish primary schoolers were analyzed via an abductive method.
Findings We identified three partly overlapping ML conception categories,
starting from the most accurate one: ML is about teaching machines (34%), ML is
about coding (7.6%), and ML is about learning via or about machines (37.1%).
Implications The findings suggest that without conceptual clues, children's
conceptions of ML are varied and may include misconceptions such as ML is about
learning via or about machines. The findings underline the importance of clear
and systematic use of key concepts in computer science education. Besides
researchers, this study offers insights for teachers, teacher educators,
curriculum developers, and policymakers.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09584" title="Abstract">arXiv:2402.09584</a> [<a href="/pdf/2402.09584" title="Download PDF">pdf</a>, <a href="/ps/2402.09584" title="Download PostScript">ps</a>, <a href="/format/2402.09584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-Based Interpretable Machine Learning Control in  Building Energy Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhelun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The potential of Machine Learning Control (MLC) in HVAC systems is hindered
by its opaque nature and inference mechanisms, which is challenging for users
and modelers to fully comprehend, ultimately leading to a lack of trust in
MLC-based decision-making. To address this challenge, this paper investigates
and explores Interpretable Machine Learning (IML), a branch of Machine Learning
(ML) that enhances transparency and understanding of models and their
inferences, to improve the credibility of MLC and its industrial application in
HVAC systems. Specifically, we developed an innovative framework that combines
the principles of Shapley values and the in-context learning feature of Large
Language Models (LLMs). While the Shapley values are instrumental in dissecting
the contributions of various features in ML models, LLM provides an in-depth
understanding of rule-based parts in MLC; combining them, LLM further packages
these insights into a coherent, human-understandable narrative. The paper
presents a case study to demonstrate the feasibility of the developed IML
framework for model predictive control-based precooling under demand response
events in a virtual testbed. The results indicate that the developed framework
generates and explains the control signals in accordance with the rule-based
rationale.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09585" title="Abstract">arXiv:2402.09585</a> [<a href="/pdf/2402.09585" title="Download PDF">pdf</a>, <a href="/format/2402.09585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptation for Contrastive Audio-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio-Language Models (ALM) aim to be general-purpose audio models by
providing zero-shot capabilities at test time. The zero-shot performance of ALM
improves by using suitable text prompts for each domain. The text prompts are
usually hand-crafted through an ad-hoc process and lead to a drop in ALM
generalization and out-of-distribution performance. Existing approaches to
improve domain performance, like few-shot learning or fine-tuning, require
access to annotated data and iterations of training. Therefore, we propose a
test-time domain adaptation method for ALMs that does not require access to
annotations. Our method learns a domain vector by enforcing consistency across
augmented views of the testing audio. We extensively evaluate our approach on
12 downstream tasks across domains. With just one example, our domain
adaptation method leads to 3.2% (max 8.4%) average zero-shot performance
improvement. After adaptation, the model still retains the generalization
property of ALMs.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09586" title="Abstract">arXiv:2402.09586</a> [<a href="/pdf/2402.09586" title="Download PDF">pdf</a>, <a href="/format/2402.09586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WERank: Towards Rank Degradation Prevention for Self-Supervised Learning  Using Weight Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasand%2C+A+S">Ali Saheb Pasand</a>, 
<a href="/search/cs?searchtype=author&query=Moravej%2C+R">Reza Moravej</a>, 
<a href="/search/cs?searchtype=author&query=Biparva%2C+M">Mahdi Biparva</a>, 
<a href="/search/cs?searchtype=author&query=Ghodsi%2C+A">Ali Ghodsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A common phenomena confining the representation quality in Self-Supervised
Learning (SSL) is dimensional collapse (also known as rank degeneration), where
the learned representations are mapped to a low dimensional subspace of the
representation space. The State-of-the-Art SSL methods have shown to suffer
from dimensional collapse and fall behind maintaining full rank. Recent
approaches to prevent this problem have proposed using contrastive losses,
regularization techniques, or architectural tricks. We propose WERank, a new
regularizer on the weight parameters of the network to prevent rank
degeneration at different layers of the network. We provide empirical evidence
and mathematical justification to demonstrate the effectiveness of the proposed
regularization method in preventing dimensional collapse. We verify the impact
of WERank on graph SSL where dimensional collapse is more pronounced due to the
lack of proper data augmentation. We empirically demonstrate that WERank is
effective in helping BYOL to achieve higher rank during SSL pre-training and
consequently downstream accuracy during evaluation probing. Ablation studies
and experimental analysis shed lights on the underlying factors behind the
performance gains of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09587" title="Abstract">arXiv:2402.09587</a> [<a href="/pdf/2402.09587" title="Download PDF">pdf</a>, <a href="/format/2402.09587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepATLAS: One-Shot Localization for Biomedical Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+P+D">Peter D. Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper introduces the DeepATLAS foundational model for localization tasks
in the domain of high-dimensional biomedical data. Upon convergence of the
proposed self-supervised objective, a pretrained model maps an input to an
anatomically-consistent embedding from which any point or set of points (e.g.,
boxes or segmentations) may be identified in a one-shot or few-shot approach.
As a representative benchmark, a DeepATLAS model pretrained on a comprehensive
cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot
segmentation performance on over 50 anatomic structures across four different
external test sets, either matching or exceeding the performance of a standard
supervised learning model. Further improvements in accuracy can be achieved by
adding a small amount of labeled data using either a semisupervised or more
conventional fine-tuning strategy.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09588" title="Abstract">arXiv:2402.09588</a> [<a href="/pdf/2402.09588" title="Download PDF">pdf</a>, <a href="/format/2402.09588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emerging Opportunities of Using Large Language Language Models for  Translation Between Drug Molecules and Indications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oniani%2C+D">David Oniani</a>, 
<a href="/search/cs?searchtype=author&query=Hilsman%2C+J">Jordan Hilsman</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+C">Chengxi Zang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junmei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Lianjin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zawala%2C+J">Jan Zawala</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanshan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">A drug molecule is a substance that changes the organism's mental or physical
state. Every approved drug has an indication, which refers to the therapeutic
use of that drug for treating a particular medical condition. While the Large
Language Model (LLM), a generative Artificial Intelligence (AI) technique, has
recently demonstrated effectiveness in translating between molecules and their
textual descriptions, there remains a gap in research regarding their
application in facilitating the translation between drug molecules and
indications, or vice versa, which could greatly benefit the drug discovery
process. The capability of generating a drug from a given indication would
allow for the discovery of drugs targeting specific diseases or targets and
ultimately provide patients with better treatments. In this paper, we first
propose a new task, which is the translation between drug molecules and
corresponding indications, and then test existing LLMs on this new task.
Specifically, we consider nine variations of the T5 LLM and evaluate them on
two public datasets obtained from ChEMBL and DrugBank. Our experiments show the
early results of using LLMs for this task and provide a perspective on the
state-of-the-art. We also emphasize the current limitations and discuss future
work that has the potential to improve the performance on this task. The
creation of molecules from indications, or vice versa, will allow for more
efficient targeting of diseases and significantly reduce the cost of drug
discovery, with the potential to revolutionize the field of drug discovery in
the era of generative AI.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09589" title="Abstract">arXiv:2402.09589</a> [<a href="/pdf/2402.09589" title="Download PDF">pdf</a>, <a href="/format/2402.09589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLTCP: Congestion Control for DNN Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajasekaran%2C+S">Sudarsanan Rajasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+S">Sanjoli Narang</a>, 
<a href="/search/cs?searchtype=author&query=Zabreyko%2C+A+A">Anton A. Zabreyko</a>, 
<a href="/search/cs?searchtype=author&query=Ghobadi%2C+M">Manya Ghobadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present MLTCP, a technique to augment today's congestion control
algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP
enables the communication phases of jobs that compete for network bandwidth to
interleave with each other, thereby utilizing the network efficiently. At the
heart of MLTCP lies a very simple principle based on a key conceptual insight:
DNN training flows should scale their congestion window size based on the
number of bytes sent at each training iteration. We show that integrating this
principle into today's congestion control protocols is straightforward: by
adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of
different jobs into an interleaved state within a few training iterations,
regardless of the number of competing flows or the start time of each flow. Our
experiments with popular DNN training jobs demonstrate that enabling MLTCP
accelerates the average and 99th percentile training iteration time by up to 2x
and 4x, respectively.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09591" title="Abstract">arXiv:2402.09591</a> [<a href="/pdf/2402.09591" title="Download PDF">pdf</a>, <a href="/ps/2402.09591" title="Download PostScript">ps</a>, <a href="/format/2402.09591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing the Geometry of Random Geometric Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Han Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiradilok%2C+P">Pakawut Jiradilok</a>, 
<a href="/search/cs?searchtype=author&query=Mossel%2C+E">Elchanan Mossel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
<p class="mathjax">Random geometric graphs are random graph models defined on metric spaces.
Such a model is defined by first sampling points from a metric space and then
connecting each pair of sampled points with probability that depends on their
distance, independently among pairs. In this work, we show how to efficiently
reconstruct the geometry of the underlying space from the sampled graph under
the manifold assumption, i.e., assuming that the underlying space is a low
dimensional manifold and that the connection probability is a strictly
decreasing function of the Euclidean distance between the points in a given
embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body
of work on manifold learning, where the goal is to recover a manifold from
sampled points sampled in the manifold along with their (approximate)
distances.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09592" title="Abstract">arXiv:2402.09592</a> [<a href="/pdf/2402.09592" title="Download PDF">pdf</a>, <a href="/ps/2402.09592" title="Download PostScript">ps</a>, <a href="/format/2402.09592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Web-Based Tool for Automatic Data Collection, Curation, and  Visualization of Complex Healthcare Survey Studies including Social Network  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Labra%2C+J+E">Jos&#xe9; Emilio Labra</a>, 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+E">Enedina Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn%2C+V">Vicente Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+I">Isa&#xed;as Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computation and Mathematical Methods in Medicine, Volume 2017,
  Article ID 2579848
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">There is a great concern nowadays regarding alcohol consumption and drug
abuse, especially in young people. Analyzing the social environment where these
adolescents are immersed, as well as a series of measures determining the
alcohol abuse risk or personal situation and perception using a number of
questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain
insight into the current situation of a given individual regarding his/her
consumption behavior. But this analysis, in order to be achieved, requires the
use of tools that can ease the process of questionnaire creation, data
gathering, curation and representation, and later analysis and visualization to
the user. This research presents the design and construction of a web-based
platform able to facilitate each of the mentioned processes by integrating the
different phases into an intuitive system with a graphical user interface that
hides the complexity underlying each of the questionnaires and techniques used
and presenting the results in a flexible and visual way, avoiding any manual
handling of data during the process. Advantages of this approach are shown and
compared to the previous situation where some of the tasks were accomplished by
time consuming and error prone manipulations of data.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09595" title="Abstract">arXiv:2402.09595</a> [<a href="/pdf/2402.09595" title="Download PDF">pdf</a>, <a href="/format/2402.09595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correctly Communicating Software: Distributed, Asynchronous, and Beyond  (extended version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+den+Heuvel%2C+B">Bas van den Heuvel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis. arXiv admin note: text overlap with <a href="/abs/2111.13091">arXiv:2111.13091</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Much of the software we use in everyday life consists of distributed
components (running on separate cores or even computers) that collaborate
through communication (by exchanging messages). It is crucial to develop robust
methods that can give reliable guarantees about the behavior of such
message-passing software. With a focus on session types as communication
protocols and their foundations in logic, this thesis revolves around the
following question: How can we push the boundaries of the logical foundations
of session types (binary and multiparty), extending their expressiveness and
applicability, while preserving fundamental correctness properties? In this
context, this thesis studies several intertwined aspects of message-passing.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09596" title="Abstract">arXiv:2402.09596</a> [<a href="/pdf/2402.09596" title="Download PDF">pdf</a>, <a href="/format/2402.09596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pulmonologists-Level lung cancer detection based on standard blood test  results and smoking status using an explainable machine learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flyckt%2C+R+N+H">Ricco Noel Hansen Flyckt</a>, 
<a href="/search/cs?searchtype=author&query=Sjodsholm%2C+L">Louise Sjodsholm</a>, 
<a href="/search/cs?searchtype=author&query=Henriksen%2C+M+H+B">Margrethe H&#xf8;stgaard Bang Henriksen</a>, 
<a href="/search/cs?searchtype=author&query=Brasen%2C+C+L">Claus Lohman Brasen</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+A">Ali Ebrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Hilberg%2C+O">Ole Hilberg</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+T+F">Torben Fr&#xf8;strup Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Wiil%2C+U+K">Uffe Kock Wiil</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+L+H">Lars Henrik Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Peimankar%2C+A">Abdolrahman Peimankar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Lung cancer (LC) remains the primary cause of cancer-related mortality,
largely due to late-stage diagnoses. Effective strategies for early detection
are therefore of paramount importance. In recent years, machine learning (ML)
has demonstrated considerable potential in healthcare by facilitating the
detection of various diseases. In this retrospective development and validation
study, we developed an ML model based on dynamic ensemble selection (DES) for
LC detection. The model leverages standard blood sample analysis and smoking
history data from a large population at risk in Denmark. The study includes all
patients examined on suspicion of LC in the Region of Southern Denmark from
2009 to 2018. We validated and compared the predictions by the DES model with
diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had
complete data of which 2,505 (25\%) had LC. The DES model achieved an area
under the roc curve of 0.77$\pm$0.01, sensitivity of 76.2\%$\pm$2.4\%,
specificity of 63.8\%$\pm$2.3\%, positive predictive value of 41.6\%$\pm$1.2\%,
and F\textsubscript{1}-score of 53.8\%$\pm$1.1\%. The DES model outperformed
all five pulmonologists, achieving a sensitivity 9\% higher than their average.
The model identified smoking status, age, total calcium levels, neutrophil
count, and lactate dehydrogenase as the most important factors for the
detection of LC. The results highlight the successful application of the ML
approach in detecting LC, surpassing pulmonologists' performance. Incorporating
clinical and laboratory data in future risk assessment models can improve
decision-making and facilitate timely referrals.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09600" title="Abstract">arXiv:2402.09600</a> [<a href="/pdf/2402.09600" title="Download PDF">pdf</a>, <a href="/format/2402.09600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Graph Contrastive Learning for Node Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yancheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yingzhen Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.14109">arXiv:2205.14109</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs revealed by recent
studies. In this work, we propose a novel and robust GNN encoder, Low-Rank
Graph Contrastive Learning (LR-GCL). Our method performs transductive node
classification in two steps. First, a low-rank GCL encoder named LR-GCL is
trained by prototypical contrastive learning with low-rank regularization.
Next, using the features produced by LR-GCL, a linear transductive
classification algorithm is used to classify the unlabeled nodes in the graph.
Our LR-GCL is inspired by the low frequency property of the graph data and its
labels, and it is also theoretically motivated by our sharp generalization
bound for transductive learning. To the best of our knowledge, our theoretical
result is among the first to theoretically demonstrate the advantage of
low-rank learning in graph contrastive learning supported by strong empirical
performance. Extensive experiments on public benchmarks demonstrate the
superior performance of LR-GCL and the robustness of the learned node
representations. The code of LR-GCL is available at
\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09603" title="Abstract">arXiv:2402.09603</a> [<a href="/pdf/2402.09603" title="Download PDF">pdf</a>, <a href="/format/2402.09603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Graph Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasand%2C+A+S">Ali Saheb Pasand</a>, 
<a href="/search/cs?searchtype=author&query=Moravej%2C+R">Reza Moravej</a>, 
<a href="/search/cs?searchtype=author&query=Biparva%2C+M">Mahdi Biparva</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+R">Raika Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Ghodsi%2C+A">Ali Ghodsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In regularization Self-Supervised Learning (SSL) methods for graphs,
computational complexity increases with the number of nodes in graphs and
embedding dimensions. To mitigate the scalability of non-contrastive graph SSL,
we propose a novel approach to reduce the cost of computing the covariance
matrix for the pre-training loss function with volume-maximization terms. Our
work focuses on reducing the cost associated with the loss computation via
graph node or dimension sampling. We provide theoretical insight into why
dimension sampling would result in accurate loss computations and support it
with mathematical derivation of the novel approach. We develop our experimental
setup on the node-level graph prediction tasks, where SSL pre-training has
shown to be difficult due to the large size of real world graphs. Our
experiments demonstrate that the cost associated with the loss computation can
be reduced via node or dimension sampling without lowering the downstream
performance. Our results demonstrate that sampling mostly results in improved
downstream performance. Ablation studies and experimental analysis are provided
to untangle the role of the different factors in the experimental setup.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09604" title="Abstract">arXiv:2402.09604</a> [<a href="/pdf/2402.09604" title="Download PDF">pdf</a>, <a href="/format/2402.09604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for  Single Image Test-Time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Haoyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hanxue Gu</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Test-time adaptation (TTA) refers to adapting a trained model to a new domain
during testing. Existing TTA techniques rely on having multiple test images
from the same domain, yet this may be impractical in real-world applications
such as medical imaging, where data acquisition is expensive and imaging
conditions vary frequently. Here, we approach such a task, of adapting a
medical image segmentation model with only a single unlabeled test image. Most
TTA approaches, which directly minimize the entropy of predictions, fail to
improve performance significantly in this setting, in which we also observe the
choice of batch normalization (BN) layer statistics to be a highly important
yet unstable factor due to only having a single test domain example. To
overcome this, we propose to instead \textit{integrate} over predictions made
with various estimates of target domain statistics between the training and
test statistics, weighted based on their entropy statistics.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09607" title="Abstract">arXiv:2402.09607</a> [<a href="/pdf/2402.09607" title="Download PDF">pdf</a>, <a href="/format/2402.09607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Study of a Strongly Coupled Two-scale System with Nonlinear  Dispersion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nepal%2C+S">Surendra Nepal</a>, 
<a href="/search/math?searchtype=author&query=Raveendran%2C+V">Vishnu Raveendran</a>, 
<a href="/search/math?searchtype=author&query=Eden%2C+M">Michael Eden</a>, 
<a href="/search/math?searchtype=author&query=Lyons%2C+R">Rainey Lyons</a>, 
<a href="/search/math?searchtype=author&query=Muntean%2C+A">Adrian Muntean</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">Thinking of flows crossing through regular porous media, we numerically
explore the behavior of weak solutions to a two-scale elliptic-parabolic system
that is strongly coupled by means of a suitable nonlinear dispersion term. The
two-scale system of interest originates from the fast-drift periodic
homogenization of a nonlinear convective-diffusion-reaction problem, where the
structure of the non-linearity in the drift fits to the hydrodynamic limit of a
totally asymmetric simple exclusion process for a population of particles. In
this article, we focus exclusively on numerical simulations that employ two
decoupled approximation schemes, viz. 'scheme 1' - a Picard-type iteration -
and 'scheme 2' - a time discretization decoupling. Additionally, we describe a
computational strategy which helps to drastically improve computation times.
Finally, we provide several numerical experiments to illustrate what dispersion
effects are introduced by a specific choice of microstructure and model
ingredients.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09608" title="Abstract">arXiv:2402.09608</a> [<a href="/pdf/2402.09608" title="Download PDF">pdf</a>, <a href="/format/2402.09608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact, Fast and Expressive Poisson Point Processes via Squared Neural  Families
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsuchida%2C+R">Russell Tsuchida</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+C+S">Cheng Soon Ong</a>, 
<a href="/search/cs?searchtype=author&query=Sejdinovic%2C+D">Dino Sejdinovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024 camera ready submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce squared neural Poisson point processes (SNEPPPs) by
parameterising the intensity function by the squared norm of a two layer neural
network. When the hidden layer is fixed and the second layer has a single
neuron, our approach resembles previous uses of squared Gaussian process or
kernel methods, but allowing the hidden layer to be learnt allows for
additional flexibility. In many cases of interest, the integrated intensity
function admits a closed form and can be computed in quadratic time in the
number of hidden neurons. We enumerate a far more extensive number of such
cases than has previously been discussed. Our approach is more memory and time
efficient than naive implementations of squared or exponentiated kernel methods
or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in
a reparameterisation of the final layer of the intensity function can be
obtained by solving a (strongly) convex optimisation problem using projected
gradient descent. We demonstrate SNEPPPs on real, and synthetic benchmarks, and
provide a software implementation. https://github.com/RussellTsuchida/snefy
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09609" title="Abstract">arXiv:2402.09609</a> [<a href="/pdf/2402.09609" title="Download PDF">pdf</a>, <a href="/format/2402.09609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogicPrpBank: A Corpus for Logical Implication and Equivalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhexiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaying Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenjing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+J+C">Joyce C Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the 5th AI4ED Workshop, held in conjunction with The 38th AAAI Conference on Artificial Intelligence, February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Logic reasoning has been critically needed in problem-solving and
decision-making. Although Language Models (LMs) have demonstrated capabilities
of handling multiple reasoning tasks (e.g., commonsense reasoning), their
ability to reason complex mathematical problems, specifically propositional
logic, remains largely underexplored. This lack of exploration can be
attributed to the limited availability of annotated corpora. Here, we present a
well-labeled propositional logic corpus, LogicPrpBank, containing 7093
Propositional Logic Statements (PLSs) across six mathematical subjects, to
study a brand-new task of reasoning logical implication and equivalence. We
benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a
useful resource for this challenging task and there is ample room for model
improvement.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09611" title="Abstract">arXiv:2402.09611</a> [<a href="/pdf/2402.09611" title="Download PDF">pdf</a>, <a href="/format/2402.09611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Privacy-Aware Sign Language Translation at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rust%2C+P">Phillip Rust</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Bowen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Skyler Wang</a>, 
<a href="/search/cs?searchtype=author&query=Camg%C3%B6z%2C+N+C">Necati Cihan Camg&#xf6;z</a>, 
<a href="/search/cs?searchtype=author&query=Maillard%2C+J">Jean Maillard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">A major impediment to the advancement of sign language translation (SLT) is
data scarcity. Much of the sign language data currently available on the web
cannot be used for training supervised models due to the lack of aligned
captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears
privacy risks due to the presence of biometric information, which the
responsible development of SLT technologies should account for. In this work,
we propose a two-stage framework for privacy-aware SLT at scale that addresses
both of these issues. We introduce SSVP-SLT, which leverages self-supervised
video pretraining on anonymized and unannotated videos, followed by supervised
SLT finetuning on a curated parallel dataset. SSVP-SLT achieves
state-of-the-art finetuned and zero-shot gloss-free SLT performance on the
How2Sign dataset, outperforming the strongest respective baselines by over 3
BLEU-4. Based on controlled experiments, we further discuss the advantages and
limitations of self-supervised pretraining and anonymization via facial
obfuscation for SLT.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09613" title="Abstract">arXiv:2402.09613</a> [<a href="/pdf/2402.09613" title="Download PDF">pdf</a>, <a href="/format/2402.09613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantified Task Misalignment to Inform PEFT: An Exploration of Domain  Generalization and Catastrophic Forgetting in CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niss%2C+L">Laura Niss</a>, 
<a href="/search/cs?searchtype=author&query=Vogt-Lowell%2C+K">Kevin Vogt-Lowell</a>, 
<a href="/search/cs?searchtype=author&query=Tsiligkaridis%2C+T">Theodoros Tsiligkaridis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Foundations models are presented as generalists that often perform well over
a myriad of tasks. Fine-tuning these models, even on limited data, provides an
additional boost in task-specific performance but often at the cost of their
wider generalization, an effect termed catastrophic forgetting. In this paper,
we analyze the relation between task difficulty in the CLIP model and the
performance of several simple parameter-efficient fine-tuning methods through
the lens of domain generalization and catastrophic forgetting. We provide
evidence that the silhouette score of the zero-shot image and text embeddings
is a better measure of task difficulty than the average cosine similarity of
correct image/label embeddings, and discuss observable relationships between
task difficulty, fine-tuning method, domain generalization, and catastrophic
forgetting. Additionally, the averaged results across tasks and performance
measures demonstrate that a simplified method that trains only a subset of
attention weights, which we call A-CLIP, yields a balance between domain
generalization and catastrophic forgetting.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09614" title="Abstract">arXiv:2402.09614</a> [<a href="/pdf/2402.09614" title="Download PDF">pdf</a>, <a href="/format/2402.09614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Reasoning in Generative Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nafar%2C+A">Aliakbar Nafar</a>, 
<a href="/search/cs?searchtype=author&query=Venable%2C+K+B">Kristen Brent Venable</a>, 
<a href="/search/cs?searchtype=author&query=Kordjamshidi%2C+P">Parisa Kordjamshidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper considers the challenges that Large Language Models (LLMs) face
when reasoning over text that includes information involving uncertainty
explicitly quantified via probability values. This type of reasoning is
relevant to a variety of contexts ranging from everyday conversations to
medical decision-making. Despite improvements in the mathematical reasoning
capabilities of LLMs, they still exhibit significant difficulties when it comes
to probabilistic reasoning. To deal with this problem, we first introduce the
Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically
designed to test the probabilistic reasoning capabilities of LLMs. We then
leverage this new dataset to thoroughly illustrate the specific limitations of
LLMs for tasks involving probabilistic reasoning and present several strategies
that map the problem to different formal representations, including Python
code, probabilistic inference algorithms, and probabilistic logical
programming. We conclude by providing an evaluation of our methods on BLInD and
on an adaptation of a causal reasoning question-answering dataset, which
further shows their practical effectiveness.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09615" title="Abstract">arXiv:2402.09615</a> [<a href="/pdf/2402.09615" title="Download PDF">pdf</a>, <a href="/format/2402.09615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> API Pack: A Massive Multilingual Dataset for API Call Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Soria%2C+A+M">Adriana Meza Soria</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce API Pack, a multilingual dataset featuring over one million
instruction-API call pairs aimed at advancing large language models' API call
generation capabilities. Through experiments, we demonstrate API Pack's
efficacy in enhancing models for this specialized task while maintaining their
overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000
Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4
respectively in generating unseen API calls. Scaling to 100k examples improves
generalization to new APIs not seen during training. In addition, cross-lingual
API call generation is achieved without needing extensive data per language.
The dataset, fine-tuned models, and overall code base are publicly available at
https://github.com/anonymous_url.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09617" title="Abstract">arXiv:2402.09617</a> [<a href="/pdf/2402.09617" title="Download PDF">pdf</a>, <a href="/format/2402.09617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Enhanced User-Item Interactions: Leveraging Edge Information for  Optimized Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Liangjie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanjie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The extraordinary performance of large language models has not only reshaped
the research landscape in the field of NLP but has also demonstrated its
exceptional applicative potential in various domains. However, the potential of
these models in mining relationships from graph data remains under-explored.
Graph neural networks, as a popular research area in recent years, have
numerous studies on relationship mining. Yet, current cutting-edge research in
graph neural networks has not been effectively integrated with large language
models, leading to limited efficiency and capability in graph relationship
mining tasks. A primary challenge is the inability of LLMs to deeply exploit
the edge information in graphs, which is critical for understanding complex
node relationships. This gap limits the potential of LLMs to extract meaningful
insights from graph structures, limiting their applicability in more complex
graph-based analysis. We focus on how to utilize existing LLMs for mining and
understanding relationships in graph data, applying these techniques to
recommendation tasks. We propose an innovative framework that combines the
strong contextual representation capabilities of LLMs with the relationship
extraction and analysis functions of GNNs for mining relationships in graph
data. Specifically, we design a new prompt construction framework that
integrates relational information of graph data into natural language
expressions, aiding LLMs in more intuitively grasping the connectivity
information within graph data. Additionally, we introduce graph relationship
understanding and analysis functions into LLMs to enhance their focus on
connectivity information in graph data. Our evaluation on real-world datasets
demonstrates the framework's ability to understand connectivity information in
graph data.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09621" title="Abstract">arXiv:2402.09621</a> [<a href="/pdf/2402.09621" title="Download PDF">pdf</a>, <a href="/format/2402.09621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schnorr Approval-Based Secure and Privacy-Preserving IoV Data  Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jianping Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Secure and privacy-preserving data aggregation in the Internet of Vehicles
(IoV) continues to be a focal point of interest in both the industry and
academia. Aiming at tackling the challenges and solving the remaining
limitations of existing works, this paper introduces a novel Schnorr
approval-based IoV data aggregation framework based on a two-layered
architecture. In this framework, a server can aggregate the IoV data from
clusters without inferring the raw data, real identity and trajectories of
vehicles. Notably, we avoid incorporating the widely-accepted techniques such
as homomorphic encryption and digital pseudonym to avoid introducing high
computation cost to vehicles. We propose a novel concept, data approval, based
on the Schnorr signature scheme. With the approval, the fake data injection
attack carried out by a cluster head can be defended against. The separation of
liability is achieved as well. The evaluation shows that the framework is
secure and lightweight for vehicles in terms of the computation and
communication costs.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09629" title="Abstract">arXiv:2402.09629</a> [<a href="/pdf/2402.09629" title="Download PDF">pdf</a>, <a href="/format/2402.09629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Information Exchange for Unsupervised Federated Learning via  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seohyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A+B">Anindya Bijoy Das</a>, 
<a href="/search/cs?searchtype=author&query=Wagle%2C+S">Satyavrat Wagle</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">One of the main challenges of decentralized machine learning paradigms such
as Federated Learning (FL) is the presence of local non-i.i.d. datasets.
Device-to-device transfers (D2D) between distributed devices has been shown to
be an effective tool for dealing with this problem and robust to stragglers. In
an unsupervised case, however, it is not obvious how data exchanges should take
place due to the absence of labels. In this paper, we propose an approach to
create an optimal graph for data transfer using Reinforcement Learning. The
goal is to form links that will provide the most benefit considering the
environment's constraints and improve convergence speed in an unsupervised FL
environment. Numerical analysis shows the advantages in terms of convergence
speed and straggler resilience of the proposed method to different available FL
schemes and benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09631" title="Abstract">arXiv:2402.09631</a> [<a href="/pdf/2402.09631" title="Download PDF">pdf</a>, <a href="/format/2402.09631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiMiC: Minimally Modified Counterfactuals in the Representation Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shashwat Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Language models often exhibit undesirable behaviors, such as gender bias or
toxic language. Interventions in the representation space were shown effective
in mitigating such issues by altering the LM behavior. We first show that two
prominent intervention techniques, Linear Erasure and Steering Vectors, do not
enable a high degree of control and are limited in expressivity.
<br />We then propose a novel intervention methodology for generating expressive
counterfactuals in the representation space, aiming to make representations of
a source class (e.g., ``toxic'') resemble those of a target class (e.g.,
``non-toxic''). This approach, generalizing previous linear intervention
techniques, utilizes a closed-form solution for the Earth Mover's problem under
Gaussian assumptions and provides theoretical guarantees on the representation
space's geometric organization. We further build on this technique and derive a
nonlinear intervention that enables controlled generation. We demonstrate the
effectiveness of the proposed approaches in mitigating bias in multiclass
classification and in reducing the generation of toxic language, outperforming
strong baselines.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09635" title="Abstract">arXiv:2402.09635</a> [<a href="/pdf/2402.09635" title="Download PDF">pdf</a>, <a href="/format/2402.09635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image  Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozer%2C+S">Sedat Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Ndigande%2C+A+P">Alain P. Ndigande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes a deep learning based solution for multi-modal image
alignment regarding UAV-taken images. Many recently proposed state-of-the-art
alignment techniques rely on using Lucas-Kanade (LK) based solutions for a
successful alignment. However, we show that we can achieve state of the art
results without using LK-based methods. Our approach carefully utilizes a
two-branch based convolutional neural network (CNN) based on feature embedding
blocks. We propose two variants of our approach, where in the first variant
(ModelA), we directly predict the new coordinates of only the four corners of
the image to be aligned; and in the second one (ModelB), we predict the
homography matrix directly. Applying alignment on the image corners forces
algorithm to match only those four corners as opposed to computing and matching
many (key)points, since the latter may cause many outliers, yielding less
accurate alignment. We test our proposed approach on four aerial datasets and
obtain state of the art results, when compared to the existing recent deep
LK-based architectures.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09637" title="Abstract">arXiv:2402.09637</a> [<a href="/pdf/2402.09637" title="Download PDF">pdf</a>, <a href="/format/2402.09637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal Time Frequency Space for Integrated Sensing and  Communication: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shtaiwi%2C+E">Eyad Shtaiwi</a>, 
<a href="/search/cs?searchtype=author&query=Abdelhadi%2C+A">Ahmed Abdelhadi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Husheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Sixth-generation (6G) wireless communication systems, as stated in the
European 6G flagship project Hexa-X, are anticipated to feature the integration
of intelligence, communication, sensing, positioning, and computation. An
important aspect of this integration is integrated sensing and communication
(ISAC), in which the same waveform is used for both systems both sensing and
communication, to address the challenge of spectrum scarcity. Recently, the
orthogonal time frequency space (OTFS) waveform has been proposed to address
OFDM's limitations due to the high Doppler spread in some future wireless
communication systems. In this paper, we review existing OTFS waveforms for
ISAC systems and provide some insights into future research. Firstly, we
introduce the basic principles and a system model of OTFS and provide a
foundational understanding of this innovative technology's core concepts and
architecture. Subsequently, we present an overview of OTFS-based ISAC system
frameworks. We provide a comprehensive review of recent research developments
and the current state of the art in the field of OTFS-assisted ISAC systems to
gain a thorough understanding of the current landscape and advancements.
Furthermore, we perform a thorough comparison between OTFS-enabled ISAC
operations and traditional OFDM, highlighting the distinctive advantages of
OTFS, especially in high Doppler spread scenarios. Subsequently, we address the
primary challenges facing OTFS-based ISAC systems, identifying potential
limitations and drawbacks. Then, finally, we suggest future research
directions, aiming to inspire further innovation in the 6G wireless
communication landscape.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09638" title="Abstract">arXiv:2402.09638</a> [<a href="/pdf/2402.09638" title="Download PDF">pdf</a>, <a href="/format/2402.09638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Fidelity Methods for Optimization: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Real-world black-box optimization often involves time-consuming or costly
experiments and simulations. Multi-fidelity optimization (MFO) stands out as a
cost-effective strategy that balances high-fidelity accuracy with computational
efficiency through a hierarchical fidelity approach. This survey presents a
systematic exploration of MFO, underpinned by a novel text mining framework
based on a pre-trained language model. We delve deep into the foundational
principles and methodologies of MFO, focusing on three core components --
multi-fidelity surrogate models, fidelity management strategies, and
optimization techniques. Additionally, this survey highlights the diverse
applications of MFO across several key domains, including machine learning,
engineering design optimization, and scientific discovery, showcasing the
adaptability and effectiveness of MFO in tackling complex computational
challenges. Furthermore, we also envision several emerging challenges and
prospects in the MFO landscape, spanning scalability, the composition of lower
fidelities, and the integration of human-in-the-loop approaches at the
algorithmic level. We also address critical issues related to benchmarking and
the advancement of open science within the MFO community. Overall, this survey
aims to catalyze further research and foster collaborations in MFO, setting the
stage for future innovations and breakthroughs in the field.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09639" title="Abstract">arXiv:2402.09639</a> [<a href="/pdf/2402.09639" title="Download PDF">pdf</a>, <a href="/format/2402.09639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Misinformation Regulation in the Presence of Competition between Social  Media Platforms (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sasaki%2C+S">So Sasaki</a>, 
<a href="/search/cs?searchtype=author&query=Langbort%2C+C">C&#xe9;dric Langbort</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version extends the article submitted to the IEEE Transactions on Control of Network Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Social media platforms have diverse content moderation policies, with many
prominent actors hesitant to impose strict regulations. A key reason for this
reluctance could be the competitive advantage that comes with lax regulation. A
popular platform that starts enforcing content moderation rules may fear that
it could lose users to less-regulated alternative platforms. Moreover, if users
continue harmful activities on other platforms, regulation ends up being
futile. This article examines the competitive aspect of content moderation by
considering the motivations of all involved players (platformer, news source,
and social media users), identifying the regulation policies sustained in
equilibrium, and evaluating the information quality available on each platform.
Applied to simple yet relevant social networks such as stochastic block models,
our model reveals the conditions for a popular platform to enforce strict
regulation without losing users. Effectiveness of regulation depends on the
diffusive property of news posts, friend interaction qualities in social media,
the sizes and cohesiveness of communities, and how much sympathizers appreciate
surprising news from influencers.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09642" title="Abstract">arXiv:2402.09642</a> [<a href="/pdf/2402.09642" title="Download PDF">pdf</a>, <a href="/format/2402.09642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Answer is All You Need: Instruction-following Text Embedding via  Answering the Question
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Letian Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+J">Jayanth Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gaowen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This work aims to build a text embedder that can capture characteristics of
texts specified by user instructions. Despite its tremendous potential to
deploy user-oriented embeddings, none of previous approaches provides a
concrete solution for it. This paper offers a new viewpoint, which treats the
instruction as a question about the input text and encodes the expected answers
to obtain the representation accordingly. Intuitively, texts with the same
(implicit) semantics would share similar answers following the instruction,
thus leading to more similar embeddings. Specifically, we propose InBedder that
instantiates this embed-via-answering idea by only fine-tuning language models
on abstractive question answering tasks. InBedder demonstrates significantly
improved instruction-following capabilities according to our proposed
instruction awareness tests and instruction robustness tests, when applied to
both large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based
LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering
outcomes, achieved by applying different instructions to the same corpus,
demonstrates a high degree of interpretability.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09644" title="Abstract">arXiv:2402.09644</a> [<a href="/pdf/2402.09644" title="Download PDF">pdf</a>, <a href="/format/2402.09644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing the Modification Space of Signature IDS Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guide%2C+R">Ryan Guide</a>, 
<a href="/search/cs?searchtype=author&query=Pauley%2C+E">Eric Pauley</a>, 
<a href="/search/cs?searchtype=author&query=Beugin%2C+Y">Yohan Beugin</a>, 
<a href="/search/cs?searchtype=author&query=Sheatsley%2C+R">Ryan Sheatsley</a>, 
<a href="/search/cs?searchtype=author&query=McDaniel%2C+P">Patrick McDaniel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in: MILCOM 2023 - 2023 IEEE Military Communications Conference (MILCOM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Signature-based Intrusion Detection Systems (SIDSs) are traditionally used to
detect malicious activity in networks. A notable example of such a system is
Snort, which compares network traffic against a series of rules that match
known exploits. Current SIDS rules are designed to minimize the amount of
legitimate traffic flagged incorrectly, reducing the burden on network
administrators. However, different use cases than the traditional one--such as
researchers studying trends or analyzing modified versions of known
exploits--may require SIDSs to be less constrained in their operation. In this
paper, we demonstrate that applying modifications to real-world SIDS rules
allow for relaxing some constraints and characterizing the performance space of
modified rules. We develop an iterative approach for exploring the space of
modifications to SIDS rules. By taking the modifications that expand the ROC
curve of performance and altering them further, we show how to modify rules in
a directed manner. Using traffic collected and identified as benign or
malicious from a cloud telescope, we find that the removal of a single
component from SIDS rules has the largest impact on the performance space.
Effectively modifying SIDS rules to reduce constraints can enable a broader
range of detection for various objectives, from increased security to research
purposes.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09649" title="Abstract">arXiv:2402.09649</a> [<a href="/pdf/2402.09649" title="Download PDF">pdf</a>, <a href="/format/2402.09649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProtChatGPT: Towards Understanding Proteins with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hehe Fan</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+R">Ruijie Quan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Protein research is crucial in various fundamental disciplines, but
understanding their intricate structure-function relationships remains
challenging. Recent Large Language Models (LLMs) have made significant strides
in comprehending task-specific knowledge, suggesting the potential for
ChatGPT-like systems specialized in protein to facilitate basic research. In
this work, we introduce ProtChatGPT, which aims at learning and understanding
protein structures via natural languages. ProtChatGPT enables users to upload
proteins, ask questions, and engage in interactive conversations to produce
comprehensive answers. The system comprises protein encoders, a
Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and
an LLM. The protein first undergoes protein encoders and PLP-former to produce
protein embeddings, which are then projected by the adapter to conform with the
LLM. The LLM finally combines user questions with projected embeddings to
generate informative answers. Experiments show that ProtChatGPT can produce
promising responses to proteins and their corresponding questions. We hope that
ProtChatGPT could form the basis for further exploration and application in
protein research. Code and our pre-trained model will be publicly available.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09650" title="Abstract">arXiv:2402.09650</a> [<a href="/pdf/2402.09650" title="Download PDF">pdf</a>, <a href="/format/2402.09650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foul prediction with estimated poses from soccer broadcast video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiale Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+C">Calvin Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+K">Keisuke Fujii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in computer vision have made significant progress in tracking
and pose estimation of sports players. However, there have been fewer studies
on behavior prediction with pose estimation in sports, in particular, the
prediction of soccer fouls is challenging because of the smaller image size of
each player and of difficulty in the usage of e.g., the ball and pose
information. In our research, we introduce an innovative deep learning approach
for anticipating soccer fouls. This method integrates video data, bounding box
positions, image details, and pose information by curating a novel soccer foul
dataset. Our model utilizes a combination of convolutional and recurrent neural
networks (CNNs and RNNs) to effectively merge information from these four
modalities. The experimental results show that our full model outperformed the
ablated models, and all of the RNN modules, bounding box position and image,
and estimated pose were useful for the foul prediction. Our findings have
important implications for a deeper understanding of foul play in soccer and
provide a valuable reference for future research and practice in this area.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09651" title="Abstract">arXiv:2402.09651</a> [<a href="/pdf/2402.09651" title="Download PDF">pdf</a>, <a href="/format/2402.09651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practitioners&#x27; Challenges and Perceptions of CI Build Failure  Predictions at Atlassian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yang Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Pasuksmit%2C+J">Jirat Pasuksmit</a>, 
<a href="/search/cs?searchtype=author&query=Thongtanunam%2C+P">Patanamon Thongtanunam</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+A">Arik Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Krasikov%2C+A">Anton Krasikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Continuous Integration (CI) build failures could significantly impact the
software development process and teams, such as delaying the release of new
features and reducing developers' productivity. In this work, we report on an
empirical study that investigates CI build failures throughout product
development at Atlassian. Our quantitative analysis found that the repository
dimension is the key factor influencing CI build failures. In addition, our
qualitative survey revealed that Atlassian developers perceive CI build
failures as challenging issues in practice. Furthermore, we found that the CI
build prediction can not only provide proactive insight into CI build failures
but also facilitate the team's decision-making. Our study sheds light on the
challenges and expectations involved in integrating CI build prediction tools
into the Bitbucket environment, providing valuable insights for enhancing CI
processes.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09654" title="Abstract">arXiv:2402.09654</a> [<a href="/pdf/2402.09654" title="Download PDF">pdf</a>, <a href="/format/2402.09654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4&#x27;s assessment of its performance in a USMLE-based case study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhakal%2C+U">Uttam Dhakal</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Aniket Kumar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Devkota%2C+S">Suman Devkota</a>, 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+Y">Yogesh Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Lamichhane%2C+B">Bishal Lamichhane</a>, 
<a href="/search/cs?searchtype=author&query=Paudyal%2C+S">Suprinsa Paudyal</a>, 
<a href="/search/cs?searchtype=author&query=Dhakal%2C+C">Chandra Dhakal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This study investigates GPT-4's assessment of its performance in healthcare
applications. A simple prompting technique was used to prompt the LLM with
questions taken from the United States Medical Licensing Examination (USMLE)
questionnaire and it was tasked to evaluate its confidence score before posing
the question and after asking the question. The questionnaire was categorized
into two groups-questions with feedback (WF) and questions with no feedback(NF)
post-question. The model was asked to provide absolute and relative confidence
scores before and after each question. The experimental findings were analyzed
using statistical tools to study the variability of confidence in WF and NF
groups. Additionally, a sequential analysis was conducted to observe the
performance variation for the WF and NF groups. Results indicate that feedback
influences relative confidence but doesn't consistently increase or decrease
it. Understanding the performance of LLM is paramount in exploring its utility
in sensitive areas like healthcare. This study contributes to the ongoing
discourse on the reliability of AI, particularly of LLMs like GPT-4, within
healthcare, offering insights into how feedback mechanisms might be optimized
to enhance AI-assisted medical education and decision support.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09656" title="Abstract">arXiv:2402.09656</a> [<a href="/pdf/2402.09656" title="Download PDF">pdf</a>, <a href="/format/2402.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Butterfly Effect of Model Editing: Few Edits Can Trigger Large  Language Models Collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanli Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Although model editing has shown promise in revising knowledge in Large
Language Models (LLMs), its impact on the inherent capabilities of LLMs is
often overlooked. In this work, we reveal a critical phenomenon: even a single
edit can trigger model collapse, manifesting as significant performance
degradation in various benchmark tasks. However, benchmarking LLMs after each
edit, while necessary to prevent such collapses, is impractically
time-consuming and resource-intensive. To mitigate this, we propose using
perplexity as a surrogate metric, validated by extensive experiments
demonstrating its strong correlation with downstream task performance. We
further conduct an in-depth study on sequential editing, a practical setting
for real-world scenarios, across various editing methods and LLMs, focusing on
hard cases from our previous single edit studies. The results indicate that
nearly all examined editing methods result in model collapse after only few
edits. To facilitate further research, we have utilized ChatGPT to develop a
new dataset, HardCF, based on those hard cases. This dataset aims to establish
the foundation for pioneering research in reliable model editing and the
mechanisms underlying editing-induced model collapse. We hope this work can
draw the community's attention to the potential risks inherent in model editing
practices.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09657" title="Abstract">arXiv:2402.09657</a> [<a href="/pdf/2402.09657" title="Download PDF">pdf</a>, <a href="/ps/2402.09657" title="Download PostScript">ps</a>, <a href="/format/2402.09657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital versus Analog Transmissions for Federated Learning over Wireless  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiacheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xiaohu You</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In this paper, we quantitatively compare these two effective communication
schemes, i.e., digital and analog ones, for wireless federated learning (FL)
over resource-constrained networks, highlighting their essential differences as
well as their respective application scenarios. We first examine both digital
and analog transmission methods, together with a unified and fair comparison
scheme under practical constraints. A universal convergence analysis under
various imperfections is established for FL performance evaluation in wireless
networks. These analytical results reveal that the fundamental difference
between the two paradigms lies in whether communication and computation are
jointly designed or not. The digital schemes decouple the communication design
from specific FL tasks, making it difficult to support simultaneous uplink
transmission of massive devices with limited bandwidth. In contrast, the analog
communication allows over-the-air computation (AirComp), thus achieving
efficient spectrum utilization. However, computation-oriented analog
transmission reduces power efficiency, and its performance is sensitive to
computational errors. Finally, numerical simulations are conducted to verify
these theoretical observations.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09660" title="Abstract">arXiv:2402.09660</a> [<a href="/pdf/2402.09660" title="Download PDF">pdf</a>, <a href="/format/2402.09660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Modeling and User Profiling: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purificato%2C+E">Erasmo Purificato</a> (1), 
<a href="/search/cs?searchtype=author&query=Boratto%2C+L">Ludovico Boratto</a> (2), 
<a href="/search/cs?searchtype=author&query=De+Luca%2C+E+W">Ernesto William De Luca</a> (1) ((1) Otto von Guericke University Magdeburg, Germany, (2) University of Cagliari, Italy)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The integration of artificial intelligence (AI) into daily life, particularly
through information retrieval and recommender systems, has necessitated
advanced user modeling and profiling techniques to deliver personalized
experiences. These techniques aim to construct accurate user representations
based on the rich amounts of data generated through interactions with these
systems. This paper presents a comprehensive survey of the current state,
evolution, and future directions of user modeling and profiling research. We
provide a historical overview, tracing the development from early stereotype
models to the latest deep learning techniques, and propose a novel taxonomy
that encompasses all active topics in this research area, including recent
trends. Our survey highlights the paradigm shifts towards more sophisticated
user profiling methods, emphasizing implicit data collection, multi-behavior
modeling, and the integration of graph data structures. We also address the
critical need for privacy-preserving techniques and the push towards
explainability and fairness in user modeling approaches. By examining the
definitions of core terminology, we aim to clarify ambiguities and foster a
clearer understanding of the field by proposing two novel encyclopedic
definitions of the main terms. Furthermore, we explore the application of user
modeling in various domains, such as fake news detection, cybersecurity, and
personalized education. This survey serves as a comprehensive resource for
researchers and practitioners, offering insights into the evolution of user
modeling and profiling and guiding the development of more personalized,
ethical, and effective AI systems.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09662" title="Abstract">arXiv:2402.09662</a> [<a href="/pdf/2402.09662" title="Download PDF">pdf</a>, <a href="/format/2402.09662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanzim%2C+S">Syed Tanzim</a>, 
<a href="/search/cs?searchtype=author&query=Mubarrat">Mubarrat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA 2024). 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This article introduces GeoBotsVR, an easily accessible virtual reality game
that combines elements of puzzle-solving with robotics learning and aims to
cultivate interest and motivation in robotics, programming, and electronics
among individuals with limited experience in these domains. The game allows
players to build and customize a two-wheeled mobile robot using various robotic
components and use their robot to solve various procedurally-generated puzzles
in a diverse range of environments. An innovative aspect is the inclusion of a
repair feature, requiring players to address randomly generated electronics and
programming issues with their robot through hands-on manipulation. GeoBotsVR is
designed to be immersive, replayable, and practical application-based, offering
an enjoyable and accessible tool for beginners to acquaint themselves with
robotics. The game simulates a hands-on learning experience and does not
require prior technical knowledge, making it a potentially valuable resource
for beginners to get an engaging introduction to the field of robotics.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09663" title="Abstract">arXiv:2402.09663</a> [<a href="/pdf/2402.09663" title="Download PDF">pdf</a>, <a href="/format/2402.09663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hand Shape and Gesture Recognition using Multiscale Template Matching,  Background Subtraction and Binary Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saichandran%2C+K+S">Ketan Suhaas Saichandran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a hand shape classification approach employing multiscale
template matching. The integration of background subtraction is utilized to
derive a binary image of the hand object, enabling the extraction of key
features such as centroid and bounding box. The methodology, while simple,
demonstrates effectiveness in basic hand shape classification tasks, laying the
foundation for potential applications in straightforward human-computer
interaction scenarios. Experimental results highlight the system's capability
in controlled environments.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09664" title="Abstract">arXiv:2402.09664</a> [<a href="/pdf/2402.09664" title="Download PDF">pdf</a>, <a href="/format/2402.09664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeMind: A Framework to Challenge Large Language Models for Code  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changshu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S+D">Shizhuo Dylan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jabbarvand%2C+R">Reyhaneh Jabbarvand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">Solely relying on test passing to evaluate Large Language Models (LLMs) for
code synthesis may result in unfair assessment or promoting models with data
leakage. As an alternative, we introduce CodeMind, a framework designed to
gauge the code reasoning abilities of LLMs. CodeMind currently supports three
code reasoning tasks: Independent Execution Reasoning (IER), Dependent
Execution Reasoning (DER), and Specification Reasoning (SR). The first two
evaluate models to predict the execution output of an arbitrary code or code
the model could correctly synthesize. The third one evaluates the extent to
which LLMs implement the specified expected behavior. Our extensive evaluation
of nine LLMs across five benchmarks in two different programming languages
using CodeMind shows that LLMs fairly understand control flow constructs and,
in general, are capable of reasoning how inputs evolve to output, specifically
for simple programs and the ones they can correctly synthesize. However, their
performance drops for code with higher complexity, non-trivial logical and
arithmetic operators, non-primitive types, and API calls. Furthermore, we
observe that, while correlated, specification reasoning (essential for code
synthesis) does not imply execution reasoning (essential for broader
programming tasks such as testing and debugging): ranking LLMs based on test
passing can be different compared to code reasoning.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09666" title="Abstract">arXiv:2402.09666</a> [<a href="/pdf/2402.09666" title="Download PDF">pdf</a>, <a href="/format/2402.09666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph  Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Ying Su</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Huiru Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Commonsense knowledge graph completion is a new challenge for commonsense
knowledge graph construction and application. In contrast to factual knowledge
graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g.,
ConceptNet) utilize free-form text to represent named entities, short phrases,
and events as their nodes. Such a loose structure results in large and sparse
CSKGs, which makes the semantic understanding of these nodes more critical for
learning rich commonsense knowledge graph embedding. While current methods
leverage semantic similarities to increase the graph density, the semantic
plausibility of the nodes and their relations are under-explored. Previous
works adopt conceptual abstraction to improve the consistency of modeling
(event) plausibility, but they are not scalable enough and still suffer from
data sparsity. In this paper, we propose to adopt textual entailment to find
implicit entailment relations between CSKG nodes, to effectively densify the
subgraph connecting nodes within the same conceptual class, which indicates a
similar level of plausibility. Each node in CSKG finds its top entailed nodes
using a finetuned transformer over natural language inference (NLI) tasks,
which sufficiently capture textual entailment signals. The entailment relation
between these nodes are further utilized to: 1) build new connections between
source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the
generalization ability of node representations by comparing the node embeddings
with a contrastive loss. Experiments on two standard CSKGs demonstrate that our
proposed framework EntailE can improve the performance of CSKG completion tasks
under both transductive and inductive settings.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09667" title="Abstract">arXiv:2402.09667</a> [<a href="/pdf/2402.09667" title="Download PDF">pdf</a>, <a href="/ps/2402.09667" title="Download PostScript">ps</a>, <a href="/format/2402.09667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbalanced Random Matching Markets with Partial Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Potukuchi%2C+A">Aditya Potukuchi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shikha Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Properties of stable matchings in the popular random-matching-market model
have been studied for over 50 years. In a random matching market, each agent
has complete preferences drawn uniformly and independently at random. Wilson
(1972), Knuth (1976) and Pittel (1989) proved that in balanced random matching
markets, the proposers are matched to their $\ln n$th choice on average. In
this paper, we consider markets where agents have partial (truncated)
preferences, that is, the proposers only rank their top $d$ partners. Despite
the long history of the problem, the following fundamental question remained
unanswered: \emph{what is the smallest value of $d$ that results in a perfect
stable matching with high probability?} In this paper, we answer this question
exactly -- we prove that a degree of $\ln^2 n$ is necessary and sufficient.
That is, we show that if $d &lt; (1-\epsilon) \ln^2 n$ then no stable matching is
perfect and if $d &gt; (1+ \epsilon) \ln^2 n$, then every stable matching is
perfect with high probability. This settles a recent conjecture by Kanoria, Min
and Qian (2021).
<br />We generalize this threshold for unbalanced markets: we consider a matching
market with $n$ agents on the shorter side and $n(\alpha+1)$ agents on the
longer side. We show that for markets with $\alpha =o(1)$, the sharp threshold
characterizing the existence of perfect stable matching occurs when $d$ is $\ln
n \cdot \ln \left(\frac{1 + \alpha}{\alpha + (1/n(\alpha+1))} \right)$.
<br />Finally, we extend the line of work studying the effect of imbalance on the
expected rank of the proposers (termed the ``stark effect of competition''). We
establish the regime in unbalanced markets that forces this stark effect to
take shape in markets with partial preferences.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09668" title="Abstract">arXiv:2402.09668</a> [<a href="/pdf/2402.09668" title="Download PDF">pdf</a>, <a href="/format/2402.09668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Train Data-Efficient LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
<a href="/search/cs?searchtype=author&query=Coleman%2C+B">Benjamin Coleman</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wang-Cheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jianmo Ni</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D+Z">Derek Zhiyuan Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. 44 pages, 30 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The training of large language models (LLMs) is expensive. In this paper, we
study data-efficient approaches for pre-training LLMs, i.e., techniques that
aim to optimize the Pareto frontier of model quality and training resource/data
consumption. We seek to understand the tradeoffs associated with data selection
routines based on (i) expensive-to-compute data-quality estimates, and (ii)
maximization of coverage and diversity-based measures in the feature space. Our
first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of
instruction-tuned LLMs to directly assess the quality of a training example. To
target coverage, we propose Density sampling, which models the data
distribution to select a diverse sample. In our comparison of 19 samplers,
involving hundreds of evaluation tasks and pre-training runs, we find that
Ask-LLM and Density are the best methods in their respective categories.
Coverage sampling can recover the performance of the full data, while models
trained on Ask-LLM data consistently outperform full-data training -- even when
we reject 90% of the original dataset, while converging up to 70% faster.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09670" title="Abstract">arXiv:2402.09670</a> [<a href="/pdf/2402.09670" title="Download PDF">pdf</a>, <a href="/ps/2402.09670" title="Download PostScript">ps</a>, <a href="/format/2402.09670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient $&#x3a6;$-Regret Minimization with Low-Degree Swap Deviations in  Extensive-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Recent breakthrough results by Dagan, Daskalakis, Fishelson and Golowich
[2023] and Peng and Rubinstein [2023] established an efficient algorithm
attaining at most $\epsilon$ swap regret over extensive-form strategy spaces of
dimension $N$ in $N^{\tilde O(1/\epsilon)}$ rounds. On the other extreme,
Farina and Pipis [2023] developed an efficient algorithm for minimizing the
weaker notion of linear-swap regret in $\mathsf{poly}(N)/\epsilon^2$ rounds. In
this paper, we take a step toward bridging the gap between those two results.
We introduce the set of $k$-mediator deviations, which generalize the untimed
communication deviations recently introduced by Zhang, Farina and Sandholm
[2024] to the case of having multiple mediators. We develop parameterized
algorithms for minimizing the regret with respect to this set of deviations in
$N^{O(k)}/\epsilon^2$ rounds. This closes the gap in the sense that $k=1$
recovers linear swap regret, while $k=N$ recovers swap regret. Moreover, by
relating $k$-mediator deviations to low-degree polynomials, we show that regret
minimization against degree-$k$ polynomial swap deviations is achievable in
$N^{O(kd)^3}/\epsilon^2$ rounds, where $d$ is the depth of the game, assuming
constant branching factor. For a fixed degree $k$, this is polynomial for
Bayesian games and quasipolynomial more broadly when $d = \mathsf{polylog} N$
-- the usual balancedness assumption on the game tree.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09671" title="Abstract">arXiv:2402.09671</a> [<a href="/pdf/2402.09671" title="Download PDF">pdf</a>, <a href="/ps/2402.09671" title="Download PostScript">ps</a>, <a href="/format/2402.09671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Alpha Transparency In Language And Vision-Based AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noever%2C+D">David Noever</a>, 
<a href="/search/cs?searchtype=author&query=McKee%2C+F">Forrest McKee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This investigation reveals a novel exploit derived from PNG image file
formats, specifically their alpha transparency layer, and its potential to fool
multiple AI vision systems. Our method uses this alpha layer as a clandestine
channel invisible to human observers but fully actionable by AI image
processors. The scope tested for the vulnerability spans representative vision
systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook,
highlighting the attack's potential breadth. This vulnerability challenges the
security protocols of existing and fielded vision systems, from medical imaging
to autonomous driving technologies. Our experiments demonstrate that the
affected systems, which rely on convolutional neural networks or the latest
multimodal language models, cannot quickly mitigate these vulnerabilities
through simple patches or updates. Instead, they require retraining and
architectural changes, indicating a persistent hole in multimodal technologies
without some future adversarial hardening against such vision-language
exploits.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09673" title="Abstract">arXiv:2402.09673</a> [<a href="/pdf/2402.09673" title="Download PDF">pdf</a>, <a href="/format/2402.09673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace Decomposition of Coset Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hunn%2C+D">David Hunn</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+W">Willie Harrison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 2 figures, submitted to Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A new method is explored for analyzing the performance of coset codes over
the binary erasure wiretap channel (BEWC) by decomposing the code over
subspaces of the code space. This technique leads to an improved algorithm for
calculating equivocation loss. It also provides a continuous-valued function
for equivocation loss, permitting proofs of local optimality for certain
finite-blocklength code constructions, including a code formed by excluding
from the generator matrix all columns which lie within a particular subspace.
Subspace decomposition is also used to explore the properties of an alternative
secrecy code metric, the chi squared divergence. The chi squared divergence is
shown to be far simpler to calculate than equivocation loss. Additionally, the
codes which are shown to be locally optimal in terms of equivocation are also
proved to be globally optimal in terms of chi squared divergence.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09674" title="Abstract">arXiv:2402.09674</a> [<a href="/pdf/2402.09674" title="Download PDF">pdf</a>, <a href="/format/2402.09674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAL: Proxy-Guided Black-Box Attack on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sitawarin%2C+C">Chawin Sitawarin</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+N">Norman Mu</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+D">David Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+A">Alexandre Araujo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have surged in popularity in recent months, but
they have demonstrated concerning capabilities to generate harmful content when
manipulated. While techniques like safety fine-tuning aim to minimize harmful
use, recent works have shown that LLMs remain vulnerable to attacks that elicit
toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs
(PAL), the first optimization-based attack on LLMs in a black-box query-only
setting. In particular, it relies on a surrogate model to guide the
optimization and a sophisticated loss designed for real-world LLM APIs. Our
attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on
Llama-2-7B, compared to 4% for the current state of the art. We also propose
GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box
Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple
baseline for query-based attacks. We believe the techniques proposed in this
work will enable more comprehensive safety testing of LLMs and, in the long
term, the development of better security guardrails. The code can be found at
https://github.com/chawins/pal.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09675" title="Abstract">arXiv:2402.09675</a> [<a href="/pdf/2402.09675" title="Download PDF">pdf</a>, <a href="/format/2402.09675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repurposing Coal Power Plants into Thermal Energy Storage for Supporting  Zero-carbon Data Centers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ding%2C+Y">Yifu Ding</a>, 
<a href="/search/eess?searchtype=author&query=Patel%2C+S">Serena Patel</a>, 
<a href="/search/eess?searchtype=author&query=Mallapragada%2C+D">Dharik Mallapragada</a>, 
<a href="/search/eess?searchtype=author&query=Stoner%2C+R+J">Robert James Stoner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Coal power plants will need to be phased out and face stranded asset risks
under the net-zero energy system transition. Repurposing coal power plants
could recoup profits and reduce carbon emissions using the existing
infrastructure and grid connections. This paper investigates a retrofitting
strategy that turns coal power plants into thermal energy storage (TES) and
zero-carbon data centers (DCs). The proposed capacity expansion model considers
the co-locations of DCs, local renewablewith the system-generation, andlevel
coal retir energy storage ement and retrofitting. We optimize the DC system
configurations under the hourly-matching carbon policy and flexible operations.
Results show that under hourly-matching carbon constraints, the retrofitted TES
could complement the operations of lithium-ion batteries (LIBs) to reduce
system costs. This could render DCs with optimal co-located renewable
generations and energy storage more cost-effective than unconstrained DCs.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09676" title="Abstract">arXiv:2402.09676</a> [<a href="/pdf/2402.09676" title="Download PDF">pdf</a>, <a href="/format/2402.09676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benko%2C+T">Tatyana Benko</a>, 
<a href="/search/cs?searchtype=author&query=Buck%2C+M">Martin Buck</a>, 
<a href="/search/cs?searchtype=author&query=Amburg%2C+I">Ilya Amburg</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+S+J">Stephen J. Young</a>, 
<a href="/search/cs?searchtype=author&query=Aksoy%2C+S+G">Sinan G. Aksoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In data science, hypergraphs are natural models for data exhibiting multi-way
relations, whereas graphs only capture pairwise. Nonetheless, many proposed
hypergraph neural networks effectively reduce hypergraphs to undirected graphs
via symmetrized matrix representations, potentially losing important
information. We propose an alternative approach to hypergraph neural networks
in which the hypergraph is represented as a non-reversible Markov chain. We use
this Markov chain to construct a complex Hermitian Laplacian matrix - the
magnetic Laplacian - which serves as the input to our proposed hypergraph
neural network. We study HyperMagNet for the task of node classification, and
demonstrate its effectiveness over graph-reduction based hypergraph neural
networks.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09677" title="Abstract">arXiv:2402.09677</a> [<a href="/pdf/2402.09677" title="Download PDF">pdf</a>, <a href="/format/2402.09677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-based Personalized Federated Learning for Medical Visual Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">He Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Togo%2C+R">Ren Togo</a>, 
<a href="/search/cs?searchtype=author&query=Ogawa%2C+T">Takahiro Ogawa</a>, 
<a href="/search/cs?searchtype=author&query=Haseyama%2C+M">Miki Haseyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept by ICASSP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a novel prompt-based personalized federated learning (pFL) method
to address data heterogeneity and privacy concerns in traditional medical
visual question answering (VQA) methods. Specifically, we regard medical
datasets from different organs as clients and use pFL to train personalized
transformer-based VQA models for each client. To address the high computational
complexity of client-to-client communication in previous pFL methods, we
propose a succinct information sharing system by introducing prompts that are
small learnable parameters. In addition, the proposed method introduces a
reliability parameter to prevent the negative effects of low performance and
irrelevant clients. Finally, extensive evaluations on various heterogeneous
medical datasets attest to the effectiveness of our proposed method.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09679" title="Abstract">arXiv:2402.09679</a> [<a href="/pdf/2402.09679" title="Download PDF">pdf</a>, <a href="/format/2402.09679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Visual Servoing Control of a Hybrid Dual-Segment Flexible  Neurosurgical Robot for Intraventricular Biopsy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingcong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Ying Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+D+T+M">Danny Tat Ming Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+K+T+L">Kam Tong Leo Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+D+Y+C">David Yuen Chung Chan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Robotics and Automation (ICRA) 2024, 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Traditional rigid endoscopes have challenges in flexibly treating tumors
located deep in the brain, and low operability and fixed viewing angles limit
its development. This study introduces a novel dual-segment flexible robotic
endoscope MicroNeuro, designed to perform biopsies with dexterous surgical
manipulation deep in the brain. Taking into account the uncertainty of the
control model, an image-based visual servoing with online robot Jacobian
estimation has been implemented to enhance motion accuracy. Furthermore, the
application of model predictive control with constraints significantly bolsters
the flexible robot's ability to adaptively track mobile objects and resist
external interference. Experimental results underscore that the proposed
control system enhances motion stability and precision. Phantom testing
substantiates its considerable potential for deployment in neurosurgery.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09683" title="Abstract">arXiv:2402.09683</a> [<a href="/pdf/2402.09683" title="Download PDF">pdf</a>, <a href="/format/2402.09683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring a Behavioral Model of &quot;Positive Friction&quot; in Human-AI  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeya Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+R">Ruth Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint has not undergone peer review or any post-submission corrections. The Version of Record of this contribution will be published in Springer Nature Computer Science book series in Volume HCI International 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> DESIGN, USER EXPERIENCE AND USABILITY. HCII 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Designing seamless, frictionless user experiences has long been a dominant
trend in both applied behavioral science and artificial intelligence (AI), in
which the goal of making desirable actions easy and efficient informs efforts
to minimize friction in user experiences. However, in some settings, friction
can be genuinely beneficial, such as the insertion of deliberate delays to
increase reflection, preventing individuals from resorting to automatic or
biased behaviors, and enhancing opportunities for unexpected discoveries. More
recently, the popularization and availability of AI on a widespread scale has
only increased the need to examine how friction can help or hinder users of AI;
it also suggests a need to consider how positive friction can benefit AI
practitioners, both during development processes (e.g., working with diverse
teams) and to inform how AI is designed into offerings. This paper first
proposes a "positive friction" model that can help characterize how friction is
currently beneficial in user and developer experiences with AI, diagnose the
potential need for friction where it may not yet exist in these contexts, and
inform how positive friction can be used to generate solutions, especially as
advances in AI continue to be progress and new opportunities emerge. It then
explores this model in the context of AI users and developers by proposing the
value of taking a hybrid "AI+human" lens, and concludes by suggesting questions
for further exploration.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09685" title="Abstract">arXiv:2402.09685</a> [<a href="/pdf/2402.09685" title="Download PDF">pdf</a>, <a href="/format/2402.09685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pheno-Robot: An Auto-Digital Modelling System for In-Situ Phenotyping in  the Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yaoqiang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kewei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Hanwen Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurate reconstruction of plant models for phenotyping analysis is critical
for optimising sustainable agricultural practices in precision agriculture.
Traditional laboratory-based phenotyping, while valuable, falls short of
understanding how plants grow under uncontrolled conditions. Robotic
technologies offer a promising avenue for large-scale, direct phenotyping in
real-world environments. This study explores the deployment of emerging
robotics and digital technology in plant phenotyping to improve performance and
efficiency. Three critical functional modules: environmental understanding,
robotic motion planning, and in-situ phenotyping, are introduced to automate
the entire process. Experimental results demonstrate the effectiveness of the
system in agricultural environments. The pheno-robot system autonomously
collects high-quality data by navigating around plants. In addition, the
in-situ modelling model reconstructs high-quality plant models from the data
collected by the robot. The developed robotic system shows high efficiency and
robustness, demonstrating its potential to advance plant science in real-world
agricultural environments.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09687" title="Abstract">arXiv:2402.09687</a> [<a href="/pdf/2402.09687" title="Download PDF">pdf</a>, <a href="/format/2402.09687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning-Augmented Dictionaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeynali%2C+A">Ali Zeynali</a>, 
<a href="/search/cs?searchtype=author&query=Kamali%2C+S">Shahin Kamali</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages plus 4 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present the first learning-augmented data structure for implementing
dictionaries with optimal consistency and robustness. Our data structure, named
RobustSL, is a skip list augmented by predictions of access frequencies of
elements in a data sequence. With proper predictions, RobustSL has optimal
consistency (achieves static optimality). At the same time, it maintains a
logarithmic running time for each operation, ensuring optimal robustness, even
if predictions are generated adversarially. Therefore, RobustSL has all the
advantages of the recent learning-augmented data structures of Lin, Luo, and
Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness
guarantees that are absent in the previous work. Numerical experiments show
that RobustSL outperforms alternative data structures using both synthetic and
real datasets.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09688" title="Abstract">arXiv:2402.09688</a> [<a href="/pdf/2402.09688" title="Download PDF">pdf</a>, <a href="/ps/2402.09688" title="Download PostScript">ps</a>, <a href="/format/2402.09688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A System-Level Dynamic Binary Translator using Automatically-Learned  Translation Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jinhu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chaoyi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Rongchao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhongjun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yew%2C+P">Pen-Chung Yew</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weihua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 19 figures, to be published in International Symposium on Code Generation and Optimization (CGO) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>; Performance (cs.PF)

</div>
<p class="mathjax">System-level emulators have been used extensively for system design,
debugging and evaluation. They work by providing a system-level virtual machine
to support a guest operating system (OS) running on a platform with the same or
different native OS that uses the same or different instruction-set
architecture. For such system-level emulation, dynamic binary translation (DBT)
is one of the core technologies. A recently proposed learning-based DBT
approach has shown a significantly improved performance with a higher quality
of translated code using automatically learned translation rules. However, it
has only been applied to user-level emulation, and not yet to system-level
emulation. In this paper, we explore the feasibility of applying this approach
to improve system-level emulation, and use QEMU to build a prototype. ... To
achieve better performance, we leverage several optimizations that include
coordination overhead reduction to reduce the overhead of each coordination,
and coordination elimination and code scheduling to reduce the coordination
frequency. Experimental results show that it can achieve an average of 1.36X
speedup over QEMU 6.1 with negligible coordination overhead in the system
emulation mode using SPEC CINT2006 as application benchmarks and 1.15X on
real-world applications.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09694" title="Abstract">arXiv:2402.09694</a> [<a href="/pdf/2402.09694" title="Download PDF">pdf</a>, <a href="/format/2402.09694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seed Optimization with Frozen Generator for Superior Zero-shot Low-light  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuxuan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Ben Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhixiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+P">Pengyang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we observe that the generators, which are pre-trained on
massive natural images, inherently hold the promising potential for superior
low-light image enhancement against varying scenarios.Specifically, we embed a
pre-trained generator to Retinex model to produce reflectance maps with
enhanced detail and vividness, thereby recovering features degraded by
low-light conditions.Taking one step further, we introduce a novel optimization
strategy, which backpropagates the gradients to the input seeds rather than the
parameters of the low-light enhancement model, thus intactly retaining the
generative knowledge learned from natural images and achieving faster
convergence speed. Benefiting from the pre-trained knowledge and
seed-optimization strategy, the low-light enhancement model can significantly
regularize the realness and fidelity of the enhanced result, thus rapidly
generating high-quality images without training on any low-light dataset.
Extensive experiments on various benchmarks demonstrate the superiority of the
proposed method over numerous state-of-the-art methods qualitatively and
quantitatively.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09695" title="Abstract">arXiv:2402.09695</a> [<a href="/pdf/2402.09695" title="Download PDF">pdf</a>, <a href="/format/2402.09695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Poisoning Attack Against Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinglun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gumaste%2C+R">Rohan Gumaste</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gagandeep Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We study the problem of reward poisoning attacks against general offline
reinforcement learning with deep neural networks for function approximation. We
consider a black-box threat model where the attacker is completely oblivious to
the learning algorithm and its budget is limited by constraining both the
amount of corruption at each data point, and the total perturbation. We propose
an attack strategy called `policy contrast attack'. The high-level idea is to
make some low-performing policies appear as high-performing while making
high-performing policies appear as low-performing. To the best of our
knowledge, we propose the first black-box reward poisoning attack in the
general offline RL setting. We provide theoretical insights on the attack
design and empirically show that our attack is efficient against current
state-of-the-art offline RL algorithms in different kinds of learning datasets.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09696" title="Abstract">arXiv:2402.09696</a> [<a href="/pdf/2402.09696" title="Download PDF">pdf</a>, <a href="/format/2402.09696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Langauge Frequency and Error Correction for Esperanto
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junhong Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Current Grammar Error Correction (GEC) initiatives tend to focus on major
languages, with less attention given to low-resource languages like Esperanto.
In this article, we begin to bridge this gap by first conducting a
comprehensive frequency analysis using the Eo-GP dataset, created explicitly
for this purpose. We then introduce the Eo-GEC dataset, derived from authentic
user cases and annotated with fine-grained linguistic details for error
identification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4
outperforms GPT-3.5 in both automated and human evaluations, highlighting its
efficacy in addressing Esperanto's grammatical peculiarities and illustrating
the potential of advanced language models to enhance GEC strategies for less
commonly studied languages.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09699" title="Abstract">arXiv:2402.09699</a> [<a href="/pdf/2402.09699" title="Download PDF">pdf</a>, <a href="/ps/2402.09699" title="Download PostScript">ps</a>, <a href="/format/2402.09699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-Drazin inverse combined with inner inverse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maharanaa%2C+G">G. Maharanaa</a>, 
<a href="/search/math?searchtype=author&query=Sahooa%2C+J+K">J. K. Sahooa</a>, 
<a href="/search/math?searchtype=author&query=Thome%2C+N">Nestor Thome</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, Linear and Multilinear Algebra (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper introduces new classes of generalized inverses for square matrices
named GD1, and the dual, called 1GD inverse. In addition, we discuss a few
characterizations and representations of these inverses. The explicit
expressions of these inverses have been established via core-nilpotent
decomposition. Further, we introduce a binary relation for GD1 inverse and 1GD
inverse, along with a few derived properties.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09701" title="Abstract">arXiv:2402.09701</a> [<a href="/pdf/2402.09701" title="Download PDF">pdf</a>, <a href="/format/2402.09701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart  Trojan Attacks in COTS Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+T">Tanvir Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Showers%2C+M">Matthew Showers</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M">Mahmudul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+T">Tamzidul Hoque</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Commercial-off-the-shelf (COTS) components are often preferred over custom
Integrated Circuits (ICs) to achieve reduced system development time and cost,
easy adoption of new technologies, and replaceability. Unfortunately, the
integration of COTS components introduces serious security concerns. None of
the entities in the COTS IC supply chain are trusted from a consumer's
perspective, leading to a ''zero trust'' threat model. Any of these entities
could introduce hidden malicious circuits or hardware Trojans within the
component, allowing an attacker in the field to extract secret information
(e.g., cryptographic keys) or cause a functional failure. Existing solutions to
counter hardware Trojans are inapplicable in such a zero-trust scenario as they
assume either the design house or the foundry to be trusted and consider the
design to be available for either analysis or modification. In this work, we
have proposed a software-oriented countermeasure to ensure the confidentiality
of secret assets against hardware Trojans that can be seamlessly integrated in
existing COTS microprocessors. The proposed solution does not require any
supply chain entity to be trusted and does not require analysis or modification
of the IC design. To protect secret assets in an untrusted microprocessor, the
proposed method leverages the concept of residue number coding (RNC) to
transform the software functions operating on the asset to be fully
homomorphic. We have implemented the proposed solution to protect the secret
key within the Advanced Encryption Standard (AES) program and presented a
detailed security analysis. We also have developed a plugin for the LLVM
compiler toolchain that automatically integrates the solution in AES. Finally,
we compare the execution time overhead of the operations in the RNC-based
technique with comparable homomorphic solutions and demonstrate significant
improvement.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09702" title="Abstract">arXiv:2402.09702</a> [<a href="/pdf/2402.09702" title="Download PDF">pdf</a>, <a href="/format/2402.09702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse and Faithful Explanations Without Sparse Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Orlandi%2C+V">Vittorio Orlandi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Even if a model is not globally sparse, it is possible for decisions made
from that model to be accurately and faithfully described by a small number of
features. For instance, an application for a large loan might be denied to
someone because they have no credit history, which overwhelms any evidence
towards their creditworthiness. In this work, we introduce the Sparse
Explanation Value (SEV), a new way of measuring sparsity in machine learning
models. In the loan denial example above, the SEV is 1 because only one factor
is needed to explain why the loan was denied. SEV is a measure of decision
sparsity rather than overall model sparsity, and we are able to show that many
machine learning models -- even if they are not sparse -- actually have low
decision sparsity, as measured by SEV. SEV is defined using movements over a
hypercube, allowing SEV to be defined consistently over various model classes,
with movement restrictions reflecting real-world constraints. We proposed the
algorithms that reduce SEV without sacrificing accuracy, providing sparse and
completely faithful explanations, even without globally sparse models.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09707" title="Abstract">arXiv:2402.09707</a> [<a href="/pdf/2402.09707" title="Download PDF">pdf</a>, <a href="/format/2402.09707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the adversarial robustness of Locality-Sensitive Hashing in Hamming  space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapralov%2C+M">Michael Kapralov</a>, 
<a href="/search/cs?searchtype=author&query=Makarov%2C+M">Mikhail Makarov</a>, 
<a href="/search/cs?searchtype=author&query=Sohler%2C+C">Christian Sohler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure
for approximate nearest neighbor search. It allows, after a close to linear
time preprocessing of the input dataset, to find an approximately nearest
neighbor of any fixed query in sublinear time in the dataset size. The
resulting data structure is randomized and succeeds with high probability for
every fixed query.
<br />In many modern applications of nearest neighbor search the queries are chosen
adaptively. In this paper, we study the robustness of the locality-sensitive
hashing to adaptive queries in Hamming space. We present a simple adversary
that can, under mild assumptions on the initial point set, provably find a
query to the approximate near neighbor search data structure that the data
structure fails on. Crucially, our adaptive algorithm finds the hard query
exponentially faster than random sampling.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09710" title="Abstract">arXiv:2402.09710</a> [<a href="/pdf/2402.09710" title="Download PDF">pdf</a>, <a href="/ps/2402.09710" title="Download PostScript">ps</a>, <a href="/format/2402.09710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Data Privacy for ML-driven Applications in Open Radio Access  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gajjar%2C+P">Pranshav Gajjar</a>, 
<a href="/search/cs?searchtype=author&query=Chiejina%2C+A">Azuka Chiejina</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Deep learning offers a promising solution to improve spectrum access
techniques by utilizing data-driven approaches to manage and share limited
spectrum resources for emerging applications. For several of these
applications, the sensitive wireless data (such as spectrograms) are stored in
a shared database or multistakeholder cloud environment and are therefore prone
to privacy leaks. This paper aims to address such privacy concerns by examining
the representative case study of shared database scenarios in 5G Open Radio
Access Network (O-RAN) networks where we have a shared database within the
near-real-time (near-RT) RAN intelligent controller. We focus on securing the
data that can be used by machine learning (ML) models for spectrum sharing and
interference mitigation applications without compromising the model and network
performances. The underlying idea is to leverage a (i) Shuffling-based
learnable encryption technique to encrypt the data, following which, (ii)
employ a custom Vision transformer (ViT) as the trained ML model that is
capable of performing accurate inferences on such encrypted data. The paper
offers a thorough analysis and comparisons with analogous convolutional neural
networks (CNN) as well as deeper architectures (such as ResNet-50) as
baselines. Our experiments showcase that the proposed approach significantly
outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the
percent accuracy and F1-Score respectively when operated on encrypted data.
Though deeper ResNet-50 architecture is obtained as a slightly more accurate
model, with an increase of 4.4%, the proposed approach boasts a reduction of
parameters by 99.32%, and thus, offers a much-improved prediction time by
nearly 60%.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09711" title="Abstract">arXiv:2402.09711</a> [<a href="/pdf/2402.09711" title="Download PDF">pdf</a>, <a href="/format/2402.09711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node Duplication Improves Cold-start Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhichun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yozen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kaiwen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shiao%2C+W">William Shiao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are prominent in graph machine learning and have
shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless,
recent studies show that GNNs struggle to produce good results on low-degree
nodes despite their overall strong performance. In practical applications of
LP, like recommendation systems, improving performance on low-degree nodes is
critical, as it amounts to tackling the cold-start problem of improving the
experiences of users with few observed interactions. In this paper, we
investigate improving GNNs' LP performance on low-degree nodes while preserving
their performance on high-degree nodes and propose a simple yet surprisingly
effective augmentation technique called NodeDup. Specifically, NodeDup
duplicates low-degree nodes and creates links between nodes and their own
duplicates before following the standard supervised LP training scheme. By
leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows
significant LP performance improvements on low-degree nodes without
compromising any performance on high-degree nodes. Additionally, as a
plug-and-play augmentation module, NodeDup can be easily applied to existing
GNNs with very light computational cost. Extensive experiments show that
NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated,
low-degree, and warm nodes, respectively, on average across all datasets
compared to GNNs and state-of-the-art cold-start methods.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09712" title="Abstract">arXiv:2402.09712</a> [<a href="/pdf/2402.09712" title="Download PDF">pdf</a>, <a href="/format/2402.09712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model with Cross Attention as an Inductive Bias for  Disentanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Cuiling Lan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>, 
<a href="/search/cs?searchtype=author&query=zheng%2C+N">Nanning zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Disentangled representation learning strives to extract the intrinsic factors
within observed data. Factorizing these representations in an unsupervised
manner is notably challenging and usually requires tailored loss functions or
specific structural designs. In this paper, we introduce a new perspective and
framework, demonstrating that diffusion models with cross-attention can serve
as a powerful inductive bias to facilitate the learning of disentangled
representations. We propose to encode an image to a set of concept tokens and
treat them as the condition of the latent diffusion for image reconstruction,
where cross-attention over the concept tokens is used to bridge the interaction
between the encoder and diffusion. Without any additional regularization, this
framework achieves superior disentanglement performance on the benchmark
datasets, surpassing all previous methods with intricate designs. We have
conducted comprehensive ablation studies and visualization analysis, shedding
light on the functioning of this model. This is the first work to reveal the
potent disentanglement capability of diffusion models with cross-attention,
requiring no complex designs. We anticipate that our findings will inspire more
investigation on exploring diffusion for disentangled representation learning
towards more sophisticated data analysis and understanding.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09715" title="Abstract">arXiv:2402.09715</a> [<a href="/pdf/2402.09715" title="Download PDF">pdf</a>, <a href="/format/2402.09715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated  Learning as a Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Computer Communications (INFOCOM '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning (FL) has emerged as a prevalent distributed machine
learning scheme that enables collaborative model training without aggregating
raw data. Cloud service providers further embrace Federated Learning as a
Service (FLaaS), allowing data analysts to execute their FL training pipelines
over differentially-protected data. Due to the intrinsic properties of
differential privacy, the enforced privacy level on data blocks can be viewed
as a privacy budget that requires careful scheduling to cater to diverse
training pipelines. Existing privacy budget scheduling studies prioritize
either efficiency or fairness individually. In this paper, we propose
DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes
both efficiency and fairness. We first develop a comprehensive utility function
incorporating data analyst-level dominant shares and FL-specific performance
metrics. A sequential allocation mechanism is then designed using the Lagrange
multiplier method and effective greedy heuristics. We theoretically prove that
DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and
Weak Strategy Proofness. We also theoretically prove the existence of a
fairness-efficiency tradeoff in privacy budgeting. Extensive experiments
demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an
average efficiency improvement of $1.44\times \sim 3.49 \times$, and an average
fairness improvement of $1.37\times \sim 24.32 \times$.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09716" title="Abstract">arXiv:2402.09716</a> [<a href="/pdf/2402.09716" title="Download PDF">pdf</a>, <a href="/format/2402.09716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Privacy Harms and Risks in Conversational AI: A Proposed Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gumusel%2C+E">Ece Gumusel</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K+Z">Kyrie Zhixuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sanfilippo%2C+M+R">Madelyn Rose Sanfilippo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This study presents a unique framework that applies and extends Solove
(2006)'s taxonomy to address privacy concerns in interactions with text-based
AI chatbots. As chatbot prevalence grows, concerns about user privacy have
heightened. While existing literature highlights design elements compromising
privacy, a comprehensive framework is lacking. Through semi-structured
interviews with 13 participants interacting with two AI chatbots, this study
identifies 9 privacy harms and 9 privacy risks in text-based interactions.
Using a grounded theory approach for interview and chatlog analysis, the
framework examines privacy implications at various interaction stages. The aim
is to offer developers, policymakers, and researchers a tool for responsible
and secure implementation of conversational AI, filling the existing gap in
addressing privacy issues associated with text-based AI chatbots.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09717" title="Abstract">arXiv:2402.09717</a> [<a href="/pdf/2402.09717" title="Download PDF">pdf</a>, <a href="/format/2402.09717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visually Dehallucinative Instruction Generation: Know What You Don&#x27;t  Know
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+S">Sungguk Cha</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jusung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Younghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheoljong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">"When did the emperor Napoleon invented iPhone?" Such hallucination-inducing
question is well known challenge in generative language modeling. In this
study, we present an innovative concept of visual hallucination, referred to as
"I Know (IK)" hallucination, to address scenarios where "I Don't Know" is the
desired response. To effectively tackle this issue, we propose the VQAv2-IDK
benchmark, the subset of VQAv2 comprising unanswerable image-question pairs as
determined by human annotators. Stepping further, we present the visually
dehallucinative instruction generation method for IK hallucination and
introduce the IDK-Instructions visual instruction database. Our experiments
show that current methods struggle with IK hallucination. Yet, our approach
effectively reduces these hallucinations, proving its versatility across
different frameworks and datasets.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09720" title="Abstract">arXiv:2402.09720</a> [<a href="/pdf/2402.09720" title="Download PDF">pdf</a>, <a href="/format/2402.09720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpaceMeta: Global-Scale Massive Multi-User Virtual Interaction over LEO  Satellite Constellations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiahe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifei Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Satellite'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Low latency and high synchronization among users are critical for emerging
multi-user virtual interaction applications. However, the existing ground-based
cloud solutions are naturally limited by the complex ground topology and fiber
speeds, making it difficult to pace with the requirement of multi-user virtual
interaction. The growth of low earth orbit (LEO) satellite constellations
becomes a promising alternative to ground solutions. To fully exploit the
potential of the LEO satellite, in this paper, we study the satellite server
selection problem for global-scale multi-user interaction applications over LEO
constellations. We propose an effective server selection framework, called
SpaceMeta, that jointly selects the ingress satellite servers and relay servers
on the communication path to minimize latency and latency discrepancy among
users. Extensive experiments using real-world Starlink topology demonstrate
that SpaceMeta reduces the latency by 6.72% and the interquartile range (IQR)
of user latency by 39.50% compared with state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09721" title="Abstract">arXiv:2402.09721</a> [<a href="/pdf/2402.09721" title="Download PDF">pdf</a>, <a href="/ps/2402.09721" title="Download PostScript">ps</a>, <a href="/format/2402.09721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persuading a Learning Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
<p class="mathjax">We study a repeated Bayesian persuasion problem (and more generally, any
generalized principal-agent problem with complete information) where the
principal does not have commitment power and the agent uses algorithms to learn
to respond to the principal's signals. We reduce this problem to a one-shot
generalized principal-agent problem with an approximately-best-responding
agent. This reduction allows us to show that: if the agent uses contextual
no-regret learning algorithms, then the principal can guarantee a utility that
is arbitrarily close to the principal's optimal utility in the classic
non-learning model with commitment; if the agent uses contextual no-swap-regret
learning algorithms, then the principal cannot obtain any utility significantly
more than the optimal utility in the non-learning model with commitment. The
difference between the principal's obtainable utility in the learning model and
the non-learning model is bounded by the agent's regret (swap-regret). If the
agent uses mean-based learning algorithms (which can be no-regret but not
no-swap-regret), then the principal can do significantly better than the
non-learning model. These conclusions hold not only for Bayesian persuasion,
but also for any generalized principal-agent problem with complete information,
including Stackelberg games and contract design.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09722" title="Abstract">arXiv:2402.09722</a> [<a href="/pdf/2402.09722" title="Download PDF">pdf</a>, <a href="/format/2402.09722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hausler%2C+S">Stephen Hausler</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+D">David Hall</a>, 
<a href="/search/cs?searchtype=author&query=Mahendren%2C+S">Sutharsan Mahendren</a>, 
<a href="/search/cs?searchtype=author&query=Moghadam%2C+P">Peyman Moghadam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural fields, coordinate-based neural networks, have recently gained
popularity for implicitly representing a scene. In contrast to classical
methods that are based on explicit representations such as point clouds, neural
fields provide a continuous scene representation able to represent 3D geometry
and appearance in a way which is compact and ideal for robotics applications.
However, limited prior methods have investigated registering multiple neural
fields by directly utilising these continuous implicit representations. In this
paper, we present Reg-NF, a neural fields-based registration that optimises for
the relative 6-DoF transformation between two arbitrary neural fields, even if
those two fields have different scale factors. Key components of Reg-NF include
a bidirectional registration loss, multi-view surface sampling, and utilisation
of volumetric signed distance functions (SDFs). We showcase our approach on a
new neural field dataset for evaluating registration problems. We provide an
exhaustive set of experiments and ablation studies to identify the performance
of our approach, while also discussing limitations to provide future direction
to the research community on open challenges in utilizing neural fields in
unconstrained environments.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09724" title="Abstract">arXiv:2402.09724</a> [<a href="/pdf/2402.09724" title="Download PDF">pdf</a>, <a href="/format/2402.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Region Feature Descriptor Adapted to High Affine Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To address the issue of feature descriptors being ineffective in representing
grayscale feature information when images undergo high affine transformations,
leading to a rapid decline in feature matching accuracy, this paper proposes a
region feature descriptor based on simulating affine transformations using
classification. The proposed method initially categorizes images with different
affine degrees to simulate affine transformations and generate a new set of
images. Subsequently, it calculates neighborhood information for feature points
on this new image set. Finally, the descriptor is generated by combining the
grayscale histogram of the maximum stable extremal region to which the feature
point belongs and the normalized position relative to the grayscale centroid of
the feature point's region. Experimental results, comparing feature matching
metrics under affine transformation scenarios, demonstrate that the proposed
descriptor exhibits higher precision and robustness compared to existing
classical descriptors. Additionally, it shows robustness when integrated with
other descriptors.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09725" title="Abstract">arXiv:2402.09725</a> [<a href="/pdf/2402.09725" title="Download PDF">pdf</a>, <a href="/format/2402.09725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Non-autoregressive Machine Translation with Error Exposure and  Consistency Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Sufeng Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the
Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to
re-predict the masked low-confidence tokens. However, CMLM suffers from the
data distribution discrepancy between training and inference, where the
observed tokens are generated differently in the two cases. In this paper, we
address this problem with the training approaches of error exposure and
consistency regularization (EECR). We construct the mixed sequences based on
model prediction during training, and propose to optimize over the masked
tokens under imperfect observation conditions. We also design a consistency
learning method to constrain the data distribution for the masked tokens under
different observing situations to narrow down the gap between training and
inference. The experiments on five translation benchmarks obtains an average
improvement of 0.68 and 0.40 BLEU scores compared to the base models,
respectively, and our CMLMC-EECR achieves the best performance with a
comparable translation quality with the Transformer. The experiments results
demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09727" title="Abstract">arXiv:2402.09727</a> [<a href="/pdf/2402.09727" title="Download PDF">pdf</a>, <a href="/format/2402.09727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Canny%2C+J">John Canny</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+I">Ian Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://read-agent.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Current Large Language Models (LLMs) are not only limited to some maximum
context length, but also are not able to robustly consume long inputs. To
address these limitations, we propose ReadAgent, an LLM agent system that
increases effective context length up to 20x in our experiments. Inspired by
how humans interactively read long documents, we implement ReadAgent as a
simple prompting system that uses the advanced language capabilities of LLMs to
(1) decide what content to store together in a memory episode, (2) compress
those memory episodes into short episodic memories called gist memories, and
(3) take actions to look up passages in the original text if ReadAgent needs to
remind itself of relevant details to complete a task. We evaluate ReadAgent
against baselines using retrieval methods, using the original long contexts,
and using the gist memories. These evaluations are performed on three
long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.
ReadAgent outperforms the baselines on all three tasks while extending the
effective context window by 3-20x.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09728" title="Abstract">arXiv:2402.09728</a> [<a href="/pdf/2402.09728" title="Download PDF">pdf</a>, <a href="/format/2402.09728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shibli%2C+A+M">Ashfak Md Shibli</a>, 
<a href="/search/cs?searchtype=author&query=Pritom%2C+M+M+A">Mir Mehedi A. Pritom</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Maanak Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 12 figures, published in ISDFS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">SMS phishing, also known as "smishing", is a growing threat that tricks users
into disclosing private information or clicking into URLs with malicious
content through fraudulent mobile text messages. In recent past, we have also
observed a rapid advancement of conversational generative AI chatbot services
(e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large
language models (LLMs). These AI chatbots certainly have a lot of utilities but
it is not systematically understood how they can play a role in creating
threats and attacks. In this paper, we propose AbuseGPT method to show how the
existing generative AI-based chatbot services can be exploited by attackers in
real world to create smishing texts and eventually lead to craftier smishing
campaigns. To the best of our knowledge, there is no pre-existing work that
evidently shows the impacts of these generative text-based models on creating
SMS phishing. Thus, we believe this study is the first of its kind to shed
light on this emerging cybersecurity threat. We have found strong empirical
evidences to show that attackers can exploit ethical standards in the existing
generative AI-based chatbot services by crafting prompt injection attacks to
create newer smishing campaigns. We also discuss some future research
directions and guidelines to protect the abuse of generative AI-based services
and safeguard users from smishing attacks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09729" title="Abstract">arXiv:2402.09729</a> [<a href="/pdf/2402.09729" title="Download PDF">pdf</a>, <a href="/format/2402.09729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Prompt-based Decision Transformer for Customized VR Services  in Mobile Edge Computing System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tailin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiadong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+D+H+K">Danny H.K. Tsang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper investigates resource allocation to provide heterogeneous users
with customized virtual reality (VR) services in a mobile edge computing (MEC)
system. We first introduce a quality of experience (QoE) metric to measure user
experience, which considers the MEC system's latency, user attention levels,
and preferred resolutions. Then, a QoE maximization problem is formulated for
resource allocation to ensure the highest possible user experience,which is
cast as a reinforcement learning problem, aiming to learn a generalized policy
applicable across diverse user environments for all MEC servers. To learn the
generalized policy, we propose a framework that employs federated learning (FL)
and prompt-based sequence modeling to pre-train a common decision model across
MEC servers, which is named FedPromptDT. Using FL solves the problem of
insufficient local MEC data while protecting user privacy during offline
training. The design of prompts integrating user-environment cues and
user-preferred allocation improves the model's adaptability to various user
environments during online execution.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09730" title="Abstract">arXiv:2402.09730</a> [<a href="/pdf/2402.09730" title="Download PDF">pdf</a>, <a href="/ps/2402.09730" title="Download PostScript">ps</a>, <a href="/format/2402.09730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DOF: Accelerating High-order Differential Operators with Forward  Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chuwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Solving partial differential equations (PDEs) efficiently is essential for
analyzing complex physical systems. Recent advancements in leveraging deep
learning for solving PDE have shown significant promise. However, machine
learning methods, such as Physics-Informed Neural Networks (PINN), face
challenges in handling high-order derivatives of neural network-parameterized
functions. Inspired by Forward Laplacian, a recent method of accelerating
Laplacian computation, we propose an efficient computational framework,
Differential Operator with Forward-propagation (DOF), for calculating general
second-order differential operators without losing any precision. We provide
rigorous proof of the advantages of our method over existing methods,
demonstrating two times improvement in efficiency and reduced memory
consumption on any architectures. Empirical results illustrate that our method
surpasses traditional automatic differentiation (AutoDiff) techniques,
achieving 2x improvement on the MLP structure and nearly 20x improvement on the
MLP with Jacobian sparsity.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09731" title="Abstract">arXiv:2402.09731</a> [<a href="/pdf/2402.09731" title="Download PDF">pdf</a>, <a href="/format/2402.09731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POBEVM: Real-time Video Matting via Progressively Optimize the Target  Body and Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+J">Jianming Xian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Deep convolutional neural networks (CNNs) based approaches have achieved
great performance in video matting. Many of these methods can produce accurate
alpha estimation for the target body but typically yield fuzzy or incorrect
target edges. This is usually caused by the following reasons: 1) The current
methods always treat the target body and edge indiscriminately; 2) Target body
dominates the whole target with only a tiny proportion target edge. For the
first problem, we propose a CNN-based module that separately optimizes the
matting target body and edge (SOBE). And on this basis, we introduce a
real-time, trimap-free video matting method via progressively optimizing the
matting target body and edge (POBEVM) that is much lighter than previous
approaches and achieves significant improvements in the predicted target edge.
For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces
our network on the matting target edge. Experiments demonstrate our method
outperforms prior trimap-free matting methods on both Distinctions-646 (D646)
and VideoMatte240K(VM) dataset, especially in edge optimization.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09733" title="Abstract">arXiv:2402.09733</a> [<a href="/pdf/2402.09733" title="Download PDF">pdf</a>, <a href="/format/2402.09733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do LLMs Know about Hallucination? An Empirical Investigation of LLM&#x27;s  Hidden States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Hanyu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+K+Y">Kar Yan Tam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, 2 tables (13 pages, 12 figures, 13 tables including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can make up answers that are not real, and this
is known as hallucination. This research aims to see if, how, and to what
extent LLMs are aware of hallucination. More specifically, we check whether and
how an LLM reacts differently in its hidden states when it answers a question
right versus when it hallucinates. To do this, we introduce an experimental
framework which allows examining LLM's hidden states in different hallucination
situations. Building upon this framework, we conduct a series of experiments
with language models in the LLaMA family (Touvron et al., 2023). Our empirical
findings suggest that LLMs react differently when processing a genuine response
versus a fabricated one. We then apply various model interpretation techniques
to help understand and explain the findings better. Moreover, informed by the
empirical observations, we show great potential of using the guidance derived
from LLM's hidden representation space to mitigate hallucination. We believe
this work provides insights into how LLMs produce hallucinated answers and how
to make them occur less often.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09734" title="Abstract">arXiv:2402.09734</a> [<a href="/pdf/2402.09734" title="Download PDF">pdf</a>, <a href="/format/2402.09734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agents Need Not Know Their Purpose
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+P">Paulo Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Ensuring artificial intelligence behaves in such a way that is aligned with
human values is commonly referred to as the alignment challenge. Prior work has
shown that rational agents, behaving in such a way that maximizes a utility
function, will inevitably behave in such a way that is not aligned with human
values, especially as their level of intelligence goes up. Prior work has also
shown that there is no "one true utility function"; solutions must include a
more holistic approach to alignment. This paper describes oblivious agents:
agents that are architected in such a way that their effective utility function
is an aggregation of a known and hidden sub-functions. The hidden component, to
be maximized, is internally implemented as a black box, preventing the agent
from examining it. The known component, to be minimized, is knowledge of the
hidden sub-function. Architectural constraints further influence how agent
actions can evolve its internal environment model. We show that an oblivious
agent, behaving rationally, constructs an internal approximation of designers'
intentions (i.e., infers alignment), and, as a consequence of its architecture
and effective utility function, behaves in such a way that maximizes alignment;
i.e., maximizing the approximated intention function. We show that,
paradoxically, it does this for whatever utility function is used as the hidden
component and, in contrast with extant techniques, chances of alignment
actually improve as agent intelligence grows.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09735" title="Abstract">arXiv:2402.09735</a> [<a href="/pdf/2402.09735" title="Download PDF">pdf</a>, <a href="/format/2402.09735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DFORM: Diffeomorphic vector field alignment for assessing dynamics  across learned models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruiqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vedovati%2C+G">Giacomo Vedovati</a>, 
<a href="/search/cs?searchtype=author&query=Braver%2C+T">Todd Braver</a>, 
<a href="/search/cs?searchtype=author&query=Ching%2C+S">ShiNung Ching</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Dynamical system models such as Recurrent Neural Networks (RNNs) have become
increasingly popular as hypothesis-generating tools in scientific research.
Evaluating the dynamics in such networks is key to understanding their learned
generative mechanisms. However, comparison of learned dynamics across models is
challenging due to their inherent nonlinearity and because a priori there is no
enforced equivalence of their coordinate systems. Here, we propose the DFORM
(Diffeomorphic vector field alignment for comparing dynamics across learned
models) framework. DFORM learns a nonlinear coordinate transformation which
provides a continuous, maximally one-to-one mapping between the trajectories of
learned models, thus approximating a diffeomorphism between them. The mismatch
between DFORM-transformed vector fields defines the orbital similarity between
two models, thus providing a generalization of the concepts of smooth orbital
and topological equivalence. As an example, we apply DFORM to models trained on
a canonical neuroscience task, showing that learned dynamics may be
functionally similar, despite overt differences in attractor landscapes.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09736" title="Abstract">arXiv:2402.09736</a> [<a href="/pdf/2402.09736" title="Download PDF">pdf</a>, <a href="/format/2402.09736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Analytics-Empowered Frequent Pattern Mining for Decentralized  Web 3.0 Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Computer Communications (INFOCOM'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The emerging Web 3.0 paradigm aims to decentralize existing web services,
enabling desirable properties such as transparency, incentives, and privacy
preservation. However, current Web 3.0 applications supported by blockchain
infrastructure still cannot support complex data analytics tasks in a scalable
and privacy-preserving way. This paper introduces the emerging federated
analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to
stay local while still contributing to complex web analytics tasks in a
privacy-preserving way. We propose FedWeb, a tailored FA design for important
frequent pattern mining tasks in Web 3.0. FedWeb remarkably reduces the number
of required participating data owners to support privacy-preserving Web 3.0
data analytics based on a novel distributed differential privacy technique. The
correctness of mining results is guaranteed by a theoretically rigid candidate
filtering scheme based on Hoeffding's inequality and Chebychev's inequality.
Two response budget saving solutions are proposed to further reduce
participating data owners. Experiments on three representative Web 3.0
scenarios show that FedWeb can improve data utility by ~25.3% and reduce the
participating data owners by ~98.4%.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09738" title="Abstract">arXiv:2402.09738</a> [<a href="/pdf/2402.09738" title="Download PDF">pdf</a>, <a href="/format/2402.09738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Align before Attend: Aligning Visual and Textual Features for Multimodal  Hateful Content Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+E">Eftekhar Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Sharif%2C+O">Omar Sharif</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+M+M">Mohammed Moshiul Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Preum%2C+S+M">Sarah M. Preum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL-SRW, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal hateful content detection is a challenging task that requires
complex reasoning across visual and textual modalities. Therefore, creating a
meaningful multimodal representation that effectively captures the interplay
between visual and textual features through intermediate fusion is critical.
Conventional fusion techniques are unable to attend to the modality-specific
features effectively. Moreover, most studies exclusively concentrated on
English and overlooked other low-resource languages. This paper proposes a
context-aware attention framework for multimodal hateful content detection and
assesses it for both English and non-English languages. The proposed approach
incorporates an attention layer to meaningfully align the visual and textual
features. This alignment enables selective focus on modality-specific features
before fusing them. We evaluate the proposed approach on two benchmark hateful
meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English).
Evaluation results demonstrate our proposed approach's effectiveness with
F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores
show approximately $2.5$% and $3.2$% performance improvement over the
state-of-the-art systems on these datasets. Our implementation is available at
https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09739" title="Abstract">arXiv:2402.09739</a> [<a href="/pdf/2402.09739" title="Download PDF">pdf</a>, <a href="/format/2402.09739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuRating: Selecting High-Quality Data for Training Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wettig%2C+A">Alexander Wettig</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aatmik Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+S">Saumya Malik</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code, models and data are available at <a href="https://github.com/princeton-nlp/QuRating">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Selecting high-quality pre-training data is important for creating capable
language models, but existing methods rely on simple heuristics. We introduce
QuRating, a method for selecting pre-training data that captures the abstract
qualities of texts which humans intuitively perceive. In this paper, we
investigate four qualities - writing style, required expertise, facts &amp; trivia,
and educational value. We find that LLMs are able to discern these qualities
and observe that they are better at making pairwise judgments of texts than at
rating the quality of a text directly. We train a QuRater model to learn scalar
ratings from pairwise judgments, and use it to annotate a 260B training corpus
with quality ratings for each of the four criteria. In our experiments, we
select 30B tokens according to the different quality ratings and train
1.3B-parameter language models on the selected data. We find that it is
important to balance quality and diversity, as selecting only the highest-rated
documents leads to poor results. When we sample using quality ratings as logits
over documents, our models achieve lower perplexity and stronger in-context
learning performance than baselines. Beyond data selection, we use the quality
ratings to construct a training curriculum which improves performance without
changing the training dataset. We extensively analyze the quality ratings and
discuss their characteristics, biases, and wider implications.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09740" title="Abstract">arXiv:2402.09740</a> [<a href="/pdf/2402.09740" title="Download PDF">pdf</a>, <a href="/ps/2402.09740" title="Download PostScript">ps</a>, <a href="/format/2402.09740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inversion of limited-aperture Fresnel experimental data using  orthogonality sampling method with single and multiple sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+W">Won-Kwang Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this study, we consider the application of orthogonality sampling method
(OSM) with single and multiple sources for a fast identification of small
objects in limited-aperture inverse scattering problem. We first apply the OSM
with single source and show that the indicator function with single source can
be expressed by the Bessel function of order zero of the first kind, infinite
series of Bessel function of nonzero integer order of the first kind, range of
signal receiver, and the location of emitter. Based on this result, we explain
that the objects can be identified through the OSM with single source but the
identification is significantly influenced by the location of source and
applied frequency. For a successful improvement, we then consider the OSM with
multiple sources. Based on the identified structure of the OSM with single
source, we design an indicator function of the OSM with multiple sources and
show that it can be expressed by the square of the Bessel function of order
zero of the first kind an infinite series of the square of Bessel function of
nonzero integer order of the first kind. Based on the theoretical results, we
explain that the objects can be identified uniquely through the designed OSM.
Several numerical experiments with experimental data provided by the Institute
Fresnel demonstrate the pros and cons of the OSM with single source and how the
designed OSM with multiple sources behave.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09742" title="Abstract">arXiv:2402.09742</a> [<a href="/pdf/2402.09742" title="Download PDF">pdf</a>, <a href="/format/2402.09742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern  Doctors for Clinical Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhihao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jialong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+J">Jun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The incorporation of Large Language Models (LLMs) in healthcare marks a
significant advancement. However, the application has predominantly been
limited to discriminative and question-answering tasks, which does not fully
leverage their interactive potential. To address this limitation, our paper
presents AI Hospital, a framework designed to build a real-time interactive
diagnosis environment. To simulate the procedure, we collect high-quality
medical records to create patient, examiner, and medical director agents. AI
Hospital is then utilized for the interactive evaluation and collaboration of
LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark
where various LLMs serve as intern doctors for interactive diagnosis.
Subsequently, to improve diagnostic accuracy, we introduce a collaborative
mechanism that involves iterative discussions and a dispute resolution process
under the supervision of the medical director. In our experiments, we validate
the reliability of AI Hospital. The results not only explore the feasibility of
apply LLMs in clinical consultation but also confirm the effectiveness of the
dispute resolution focused collaboration method.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09743" title="Abstract">arXiv:2402.09743</a> [<a href="/pdf/2402.09743" title="Download PDF">pdf</a>, <a href="/format/2402.09743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quickest Detection of False Data Injection Attack in Distributed Process  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baba%2C+S+A">Saqib Abbas Baba</a>, 
<a href="/search/eess?searchtype=author&query=Chattopadhyay%2C+A">Arpan Chattopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper addresses the problem of detecting false data injection (FDI)
attacks in a distributed network without a fusion center, represented by a
connected graph among multiple agent nodes. Each agent node is equipped with a
sensor, and uses a Kalman consensus information filter (KCIF) to track a
discrete time global process with linear dynamics and additive Gaussian noise.
The state estimate of the global process at any sensor is computed from the
local observation history and the information received by that agent node from
its neighbors. At an unknown time, an attacker starts altering the local
observation of one agent node. In the Bayesian setting where there is a known
prior distribution of the attack beginning instant, we formulate a Bayesian
quickest change detection (QCD) problem for FDI detection in order to minimize
the mean detection delay subject to a false alarm probability constraint. While
it is well-known that the optimal Bayesian QCD rule involves checking the
Shriyaev's statistic against a threshold, we demonstrate how to compute the
Shriyaev's statistic at each node in a recursive fashion given our non-i.i.d.
observations. Next, we consider non-Bayesian QCD where the attack begins at an
arbitrary and unknown time, and the detector seeks to minimize the worst case
detection delay subject to a constraint on the mean time to false alarm and
probability of misidentification. We use the multiple hypothesis sequential
probability ratio test for attack detection and identification at each sensor.
For unknown attack strategy, we use the window-limited generalized likelihood
ratio (WL-GLR) algorithm to solve the QCD problem. Numerical results
demonstrate the performances and trade-offs of the proposed algorithms.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09745" title="Abstract">arXiv:2402.09745</a> [<a href="/pdf/2402.09745" title="Download PDF">pdf</a>, <a href="/format/2402.09745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient  Web End-to-End Flaky Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zihe Song</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Weike Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weihang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Accepted for publication in the proceedings of the ACM Web Conference 2024 (WWW 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Web end-to-end (e2e) testing evaluates the workflow of a web application. It
simulates real-world user scenarios to ensure the application flows behave as
expected. However, web e2e tests are notorious for being flaky, i.e., the tests
can produce inconsistent results despite no changes to the code. One common
type of flakiness is caused by nondeterministic execution orders between the
test code and the client-side code under test. In particular, UI-based
flakiness emerges as a notably prevalent and challenging issue to fix because
the test code has limited knowledge about the client-side code execution. In
this paper, we propose WEFix, a technique that can automatically generate fix
code for UI-based flakiness in web e2e testing. The core of our approach is to
leverage browser UI changes to predict the client-side code execution and
generate proper wait oracles. We evaluate the effectiveness and efficiency of
WEFix against 122 web e2e flaky tests from seven popular real-world projects.
Our results show that WEFix dramatically reduces the overhead (from 3.7$\times$
to 1.25$\times$) while achieving a high correctness (98%).
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09748" title="Abstract">arXiv:2402.09748</a> [<a href="/pdf/2402.09748" title="Download PDF">pdf</a>, <a href="/format/2402.09748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Compression and Efficient Inference for Large Language Models: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yicong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yongliu Long</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhengkai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liye Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binbin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofei He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, review 380 papers. The work is ongoing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Transformer based large language models have achieved tremendous success.
However, the significant memory and computational costs incurred during the
inference process make it challenging to deploy large models on
resource-constrained devices. In this paper, we investigate compression and
efficient inference methods for large language models from an algorithmic
perspective. Regarding taxonomy, similar to smaller models, compression and
acceleration algorithms for large language models can still be categorized into
quantization, pruning, distillation, compact architecture design, dynamic
networks. However, Large language models have two prominent characteristics
compared to smaller models: (1) Most of compression algorithms require
finetuning or even retraining the model after compression. The most notable
aspect of large models is the very high cost associated with model finetuning
or training. Therefore, many algorithms for large models, such as quantization
and pruning, start to explore tuning-free algorithms. (2) Large models
emphasize versatility and generalization rather than performance on a single
task. Hence, many algorithms, such as knowledge distillation, focus on how to
preserving their versatility and generalization after compression. Since these
two characteristics were not very pronounced in early large models, we further
distinguish large language models into medium models and ``real'' large models.
Additionally, we also provide an introduction to some mature frameworks for
efficient inference of large models, which can support basic compression or
acceleration algorithms, greatly facilitating model deployment for users.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09750" title="Abstract">arXiv:2402.09750</a> [<a href="/pdf/2402.09750" title="Download PDF">pdf</a>, <a href="/format/2402.09750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Large Language Models in Artistic Creation:  Collaboration and Reflection on Creative Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Anqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhizhuo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuanyuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, the potential of large language models (LLMs) has been widely used
in assisting programming. However, current research does not explore the artist
potential of LLMs in creative coding within artist and AI collaboration. Our
work probes the reflection type of artists in the creation process with such
collaboration. We compare two common collaboration approaches: invoking the
entire program and multiple subtasks. Our findings exhibit artists' different
stimulated reflections in two different methods. Our finding also shows the
correlation of reflection type with user performance, user satisfaction, and
subjective experience in two collaborations through conducting two methods,
including experimental data and qualitative interviews. In this sense, our work
reveals the artistic potential of LLM in creative coding. Meanwhile, we provide
a critical lens of human-AI collaboration from the artists' perspective and
expound design suggestions for future work of AI-assisted creative tasks.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09756" title="Abstract">arXiv:2402.09756</a> [<a href="/pdf/2402.09756" title="Download PDF">pdf</a>, <a href="/format/2402.09756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Experts for Network Optimization: A Large Language  Model-enabled Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yijing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+I">Dong In Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Optimizing various wireless user tasks poses a significant challenge for
networking systems because of the expanding range of user requirements. Despite
advancements in Deep Reinforcement Learning (DRL), the need for customized
optimization tasks for individual users complicates developing and applying
numerous DRL models, leading to substantial computation resource and energy
consumption and can lead to inconsistent outcomes. To address this issue, we
propose a novel approach utilizing a Mixture of Experts (MoE) framework,
augmented with Large Language Models (LLMs), to analyze user objectives and
constraints effectively, select specialized DRL experts, and weigh each
decision from the participating experts. Specifically, we develop a gate
network to oversee the expert models, allowing a collective of experts to
tackle a wide array of new tasks. Furthermore, we innovatively substitute the
traditional gate network with an LLM, leveraging its advanced reasoning
capabilities to manage expert model selection for joint decisions. Our proposed
method reduces the need to train new DRL models for each unique optimization
problem, decreasing energy consumption and AI model implementation costs. The
LLM-enabled MoE approach is validated through a general maze navigation task
and a specific network service provider utility maximization task,
demonstrating its effectiveness and practical applicability in optimizing
complex networking systems.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09757" title="Abstract">arXiv:2402.09757</a> [<a href="/pdf/2402.09757" title="Download PDF">pdf</a>, <a href="/ps/2402.09757" title="Download PostScript">ps</a>, <a href="/format/2402.09757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of CCC and ZCCS Through Additive Characters Over Galois  Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+G">Gobinda Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Majhi%2C+S">Sudhan Majhi</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Subhabrata Paul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The rapid progression in wireless communication technologies, especially in
multicarrier code-division multiple access (MC-CDMA), there is a need of
advanced code construction methods. Traditional approaches, mainly based on
generalized Boolean functions, have limitations in code length versatility.
This paper introduces a novel approach to constructing complete complementary
codes (CCC) and Z-complementary code sets (ZCCS), for reducing interference in
MC-CDMA systems. The proposed construction, distinct from Boolean
function-based approaches, employs additive characters over Galois fields
GF($p^{r}$), where $p$ is prime and $r$ is a positive integer. First, we
develop CCCs with lengths of $p^{r}$, which are then extended to construct ZCCS
with both unreported lengths and sizes of $np^{r}$, where $n$ are arbitrary
positive integers. The versatility of this method is further highlighted as it
includes the lengths of ZCCS reported in prior studies as special cases,
underscoring the method's comprehensive nature and superiority.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09759" title="Abstract">arXiv:2402.09759</a> [<a href="/pdf/2402.09759" title="Download PDF">pdf</a>, <a href="/format/2402.09759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Language Adaptive Pre-training: Extending State-of-the-Art  Large Language Models for Polish
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruci%C5%84ski%2C+S">Szymon Ruci&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the potential of fine-tuning foundational English Large
Language Models (LLMs) for generating Polish text. The first step involves
Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,
consisting of 276 million Polish tokens. The LAPT is followed by additional
fine-tuning aimed at solving nine KLEJ challenges. Our trained model
Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02
among decoder-based Polish models but also closely rivals the performance of
the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9
tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn
Polish. The LAPT was completed in less than five days using a consumer GPU,
highlighting the method's efficiency. The proficiency of the model in Polish
was significantly enhanced, demonstrating the viability of this approach for
adding new languages to existing LLMs by training just 1.2% of its parameters.
To contribute to the community's collaborative progress, the model has been
released as open-source.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09760" title="Abstract">arXiv:2402.09760</a> [<a href="/pdf/2402.09760" title="Download PDF">pdf</a>, <a href="/format/2402.09760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding Language Model with Chunking-Free In-Context Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hongjin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujia Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">This paper presents a novel Chunking-Free In-Context (CFIC) retrieval
approach, specifically tailored for Retrieval-Augmented Generation (RAG)
systems. Traditional RAG systems often struggle with grounding responses using
precise evidence text due to the challenges of processing lengthy documents and
filtering out irrelevant content. Commonly employed solutions, such as document
chunking and adapting language models to handle longer contexts, have their
limitations. These methods either disrupt the semantic coherence of the text or
fail to effectively address the issues of noise and inaccuracy in evidence
retrieval.
<br />CFIC addresses these challenges by circumventing the conventional chunking
process. It utilizes the encoded hidden states of documents for in-context
retrieval, employing auto-aggressive decoding to accurately identify the
specific evidence text required for user queries, eliminating the need for
chunking. CFIC is further enhanced by incorporating two decoding strategies,
namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies
not only improve the efficiency of the retrieval process but also ensure that
the fidelity of the generated grounding text evidence is maintained. Our
evaluations of CFIC on a range of open QA datasets demonstrate its superiority
in retrieving relevant and accurate evidence, offering a significant
improvement over traditional methods. By doing away with the need for document
chunking, CFIC presents a more streamlined, effective, and efficient retrieval
solution, making it a valuable advancement in the field of RAG systems.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09761" title="Abstract">arXiv:2402.09761</a> [<a href="/pdf/2402.09761" title="Download PDF">pdf</a>, <a href="/ps/2402.09761" title="Download PostScript">ps</a>, <a href="/format/2402.09761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework For Gait-Based User Demography Estimation Using Inertial  Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swami%2C+C+P">Chinmay Prakash Swami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Human gait has been shown to provide crucial motion cues for various
applications. Recognizing patterns in human gait has been widely adopted in
various application areas such as security, virtual reality gaming, medical
rehabilitation, and ailment identification. Furthermore, wearable inertial
sensors have been widely used for not only recording gait but also to predict
users' demography. Machine Learning techniques such as deep learning, combined
with inertial sensor signals, have shown promising results in recognizing
patterns in human gait and estimate users' demography. However, the black-box
nature of such deep learning models hinders the researchers from uncovering the
reasons behind the model's predictions. Therefore, we propose leveraging deep
learning and Layer-Wise Relevance Propagation (LRP) to identify the important
variables that play a vital role in identifying the users' demography such as
age and gender. To assess the efficacy of this approach we train a deep neural
network model on a large sensor-based gait dataset consisting of 745 subjects
to identify users' age and gender. Using LRP we identify the variables relevant
for characterizing the gait patterns. Thus, we enable interpretation of
non-linear ML models which are experts in identifying the users' demography
based on inertial signals. We believe this approach can not only provide
clinicians information about the gait parameters relevant to age and gender but
also can be expanded to analyze and diagnose gait disorders.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09764" title="Abstract">arXiv:2402.09764</a> [<a href="/pdf/2402.09764" title="Download PDF">pdf</a>, <a href="/format/2402.09764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Crowd Feedback via Distributional Preference Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dexun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kuicai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Deik%2C+D+G+X">Derrick Goh Xin Deik</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Deep Reinforcement Learning is widely used for aligning Large Language Models
(LLM) with human preference. However, the conventional reward modelling has
predominantly depended on human annotations provided by a select cohort of
individuals. Such dependence may unintentionally result in models that are
skewed to reflect the inclinations of these annotators, thereby failing to
represent the expectations of the wider population adequately. In this paper,
we introduce the Distributional Preference Reward Model (DPRM), a simple yet
effective framework to align large language models with a diverse set of human
preferences. To this end, we characterize the preferences by a beta
distribution, which can dynamically adapt to fluctuations in preference trends.
On top of that, we design an optimal-transportation-based loss to calibrate
DPRM to align with the preference distribution. Finally, the expected reward is
utilized to fine-tune an LLM policy to generate responses favoured by the
population. Our experiments show that DPRM significantly enhances the alignment
of LLMs with population preference, yielding more accurate, unbiased, and
contextually appropriate responses.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09765" title="Abstract">arXiv:2402.09765</a> [<a href="/pdf/2402.09765" title="Download PDF">pdf</a>, <a href="/format/2402.09765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Solving Stochastic Vehicle Routing Problem  with Time Windows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iklassov%2C+Z">Zangir Iklassov</a>, 
<a href="/search/cs?searchtype=author&query=Sobirov%2C+I">Ikboljon Sobirov</a>, 
<a href="/search/cs?searchtype=author&query=Solozabal%2C+R">Ruben Solozabal</a>, 
<a href="/search/cs?searchtype=author&query=Takac%2C+M">Martin Takac</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper introduces a reinforcement learning approach to optimize the
Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on
reducing travel costs in goods delivery. We develop a novel SVRP formulation
that accounts for uncertain travel costs and demands, alongside specific
customer time windows. An attention-based neural network trained through
reinforcement learning is employed to minimize routing costs. Our approach
addresses a gap in SVRP research, which traditionally relies on heuristic
methods, by leveraging machine learning. The model outperforms the Ant-Colony
Optimization algorithm, achieving a 1.73% reduction in travel costs. It
uniquely integrates external information, demonstrating robustness in diverse
environments, making it a valuable benchmark for future SVRP studies and
industry application.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09766" title="Abstract">arXiv:2402.09766</a> [<a href="/pdf/2402.09766" title="Download PDF">pdf</a>, <a href="/format/2402.09766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Variability to Stability: Advancing RecSys Benchmarking Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shevchenko%2C+V">Valeriy Shevchenko</a>, 
<a href="/search/cs?searchtype=author&query=Belousov%2C+N">Nikita Belousov</a>, 
<a href="/search/cs?searchtype=author&query=Vasilev%2C+A">Alexey Vasilev</a>, 
<a href="/search/cs?searchtype=author&query=Zholobov%2C+V">Vladimir Zholobov</a>, 
<a href="/search/cs?searchtype=author&query=Sosedka%2C+A">Artyom Sosedka</a>, 
<a href="/search/cs?searchtype=author&query=Semenova%2C+N">Natalia Semenova</a>, 
<a href="/search/cs?searchtype=author&query=Volodkevich%2C+A">Anna Volodkevich</a>, 
<a href="/search/cs?searchtype=author&query=Savchenko%2C+A">Andrey Savchenko</a>, 
<a href="/search/cs?searchtype=author&query=Zaytsev%2C+A">Alexey Zaytsev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages with 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the rapidly evolving domain of Recommender Systems (RecSys), new
algorithms frequently claim state-of-the-art performance based on evaluations
over a limited set of arbitrarily selected datasets. However, this approach may
fail to holistically reflect their effectiveness due to the significant impact
of dataset characteristics on algorithm performance. Addressing this
deficiency, this paper introduces a novel benchmarking methodology to
facilitate a fair and robust comparison of RecSys algorithms, thereby advancing
evaluation practices. By utilizing a diverse set of $30$ open datasets,
including two introduced in this work, and evaluating $11$ collaborative
filtering algorithms across $9$ metrics, we critically examine the influence of
dataset characteristics on algorithm performance. We further investigate the
feasibility of aggregating outcomes from multiple datasets into a unified
ranking. Through rigorous experimental analysis, we validate the reliability of
our methodology under the variability of datasets, offering a benchmarking
strategy that balances quality and computational demands. This methodology
enables a fair yet effective means of evaluating RecSys algorithms, providing
valuable guidance for future research endeavors.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09768" title="Abstract">arXiv:2402.09768</a> [<a href="/pdf/2402.09768" title="Download PDF">pdf</a>, <a href="/format/2402.09768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reeb Complements for Exploring Inclusions Between Isosurfaces From Two  Scalar Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujii%2C+A">Akito Fujii</a>, 
<a href="/search/cs?searchtype=author&query=Saeki%2C+O">Osamu Saeki</a>, 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+D">Daisuke Sakurai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">This article proposes to integrate two Reeb graphs with the information of
their isosurfaces' inclusion relation. As computing power evolves, there arise
numerical data that have small-scale physics inside larger ones -- for example,
small clouds in a simulation can be contained inside an atmospheric layer,
which is further contained in an enormous hurricane. Extracting such inclusions
between isosurfaces is a challenge for isosurfacing: the user would have to
explore the vast combinations of isosurfaces $(f_1^{-1}(l_1), f_2^{-1}(l_2))$
from scalar fields $f_i: M \to \mathbb{R}$, $i = 1, 2$, where $M$ is a domain
manifold and $f_i$ are physical quantities, to find inclusion of one isosurface
within another. For this, we propose the \textit{Reeb complement}, a
topological space that integrates two Reeb graphs with the inclusion relation.
The Reeb complement has a natural partition that classifies equivalent
containment of isosurfaces. This is a handy characteristic to let the Reeb
complement serve as an overview of the inclusion relationship in the data. We
also propose level-of-detail control of the inclusions through simplification
of the Reeb complement.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09769" title="Abstract">arXiv:2402.09769</a> [<a href="/pdf/2402.09769" title="Download PDF">pdf</a>, <a href="/format/2402.09769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning Using a Single Forward Pass
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+A">Aditya Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Pushkal Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Borthakur%2C+A">Ayon Borthakur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm
(SPELA). SPELA is a prime candidate for training and inference applications in
Edge AI devices. At the same time, SPELA can optimally cater to the need for a
framework to study perceptual representation learning and formation. SPELA has
distinctive features such as neural priors (in the form of embedded vectors),
no weight transport, no update locking of weights, complete local Hebbian
learning, single forward pass with no storage of activations, and single weight
update per sample. Juxtaposed with traditional approaches, SPELA operates
without the need for backpropagation. We show that our algorithm can perform
nonlinear classification on a noisy boolean operation dataset. Additionally, we
exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST.
Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on
MNIST, KMNIST, and Fashion MNIST, where it consistently outperforms
backpropagation.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09773" title="Abstract">arXiv:2402.09773</a> [<a href="/pdf/2402.09773" title="Download PDF">pdf</a>, <a href="/format/2402.09773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NutePrune: Efficient Progressive Pruning with Numerous Teachers for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xueting Han</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jing Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The considerable size of Large Language Models (LLMs) presents notable
deployment challenges, particularly on resource-constrained hardware.
Structured pruning, offers an effective means to compress LLMs, thereby
reducing storage costs and enhancing inference speed for more efficient
utilization. In this work, we study data-efficient and resource-efficient
structure pruning methods to obtain smaller yet still powerful models.
Knowledge Distillation is well-suited for pruning, as the intact model can
serve as an excellent teacher for pruned students. However, it becomes
challenging in the context of LLMs due to memory constraints. To address this,
we propose an efficient progressive Numerous-teacher pruning method
(NutePrune). NutePrune mitigates excessive memory costs by loading only one
intact model and integrating it with various masks and LoRA modules, enabling
it to seamlessly switch between teacher and student roles. This approach allows
us to leverage numerous teachers with varying capacities to progressively guide
the pruned model, enhancing overall performance. Extensive experiments across
various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot
experiments, NutePrune retains 97.17% of the performance of the original model
at 20% sparsity and 95.07% at 25% sparsity.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09776" title="Abstract">arXiv:2402.09776</a> [<a href="/pdf/2402.09776" title="Download PDF">pdf</a>, <a href="/format/2402.09776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Vote Timing in Online Elections With Public Tallies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaish%2C+A">Aviv Yaish</a>, 
<a href="/search/cs?searchtype=author&query=Abramova%2C+S">Svetlana Abramova</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+R">Rainer B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We study the effect of public tallies on online elections, in a setting where
voting is costly and voters are allowed to strategically time their votes. The
strategic importance of choosing \emph{when} to vote arises when votes are
public, such as in online event scheduling polls (e.\,g., Doodle), or in
blockchain governance mechanisms. In particular, there is a tension between
voting early to influence future votes and waiting to observe interim results
and avoid voting costs if the outcome has already been decided.
<br />Our study draws on empirical findings showing that ``temporal'' bandwagon
effects occur when interim results are revealed to the electorate: late voters
are more likely to vote for leading candidates. To capture this phenomenon, we
analyze a novel model where the electorate consists of informed voters who have
a preferred candidate, and uninformed swing voters who can be swayed according
to the interim outcome at the time of voting. In our main results, we prove the
existence of equilibria where both early and late voting occur with a positive
probability, and we characterize conditions that lead to the appearance of
``last minute'' voting behavior, where all informed voters vote late.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09780" title="Abstract">arXiv:2402.09780</a> [<a href="/pdf/2402.09780" title="Download PDF">pdf</a>, <a href="/format/2402.09780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TinyCL: An Efficient Hardware Architecture for Continual Learning on  Autonomous Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ressa%2C+E">Eugenio Ressa</a>, 
<a href="/search/cs?searchtype=author&query=Marchisio%2C+A">Alberto Marchisio</a>, 
<a href="/search/cs?searchtype=author&query=Martina%2C+M">Maurizio Martina</a>, 
<a href="/search/cs?searchtype=author&query=Masera%2C+G">Guido Masera</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09781" title="Abstract">arXiv:2402.09781</a> [<a href="/pdf/2402.09781" title="Download PDF">pdf</a>, <a href="/ps/2402.09781" title="Download PostScript">ps</a>, <a href="/format/2402.09781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review on Computer Vision Analysis of Aerial Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tetarwal%2C+V">Vivek Tetarwal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sandeep Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 112 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">With the emergence of new technologies in the field of airborne platforms and
imaging sensors, aerial data analysis is becoming very popular, capitalizing on
its advantages over land data. This paper presents a comprehensive review of
the computer vision tasks within the domain of aerial data analysis. While
addressing fundamental aspects such as object detection and tracking, the
primary focus is on pivotal tasks like change detection, object segmentation,
and scene-level analysis. The paper provides the comparison of various hyper
parameters employed across diverse architectures and tasks. A substantial
section is dedicated to an in-depth discussion on libraries, their
categorization, and their relevance to different domain expertise. The paper
encompasses aerial datasets, the architectural nuances adopted, and the
evaluation metrics associated with all the tasks in aerial data analysis.
Applications of computer vision tasks in aerial data across different domains
are explored, with case studies providing further insights. The paper
thoroughly examines the challenges inherent in aerial data analysis, offering
practical solutions. Additionally, unresolved issues of significance are
identified, paving the way for future research directions in the field of
aerial data analysis.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09782" title="Abstract">arXiv:2402.09782</a> [<a href="/pdf/2402.09782" title="Download PDF">pdf</a>, <a href="/format/2402.09782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MC-DBN: A Deep Belief Network-Based Model for Modality Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zihong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Haochen Xue</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengzhi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zile Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuliang Zhao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Computer Supported Cooperative Work in
  Design 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in multi-modal artificial intelligence (AI) have
revolutionized the fields of stock market forecasting and heart rate
monitoring. Utilizing diverse data sources can substantially improve prediction
accuracy. Nonetheless, additional data may not always align with the original
dataset. Interpolation methods are commonly utilized for handling missing
values in modal data, though they may exhibit limitations in the context of
sparse information. Addressing this challenge, we propose a Modality Completion
Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit
features of complete data to compensate for gaps between itself and additional
incomplete data. It ensures that the enhanced multi-modal data closely aligns
with the dynamic nature of the real world to enhance the effectiveness of the
model. We conduct evaluations of the MC-DBN model in two datasets from the
stock market forecasting and heart rate monitoring domains. Comprehensive
experiments showcase the model's capacity to bridge the semantic divide present
in multi-modal data, subsequently enhancing its performance. The source code is
available at: https://github.com/logan-0623/DBN-generate
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09784" title="Abstract">arXiv:2402.09784</a> [<a href="/pdf/2402.09784" title="Download PDF">pdf</a>, <a href="/format/2402.09784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Recommendation on Temporal Proximities with Contrastive  Learning and Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hansol Jung</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">Hyunwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+C">ChieHyeon Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sequential recommender systems identify user preferences from their past
interactions to predict subsequent items optimally. Although traditional
deep-learning-based models and modern transformer-based models in previous
studies capture unidirectional and bidirectional patterns within user-item
interactions, the importance of temporal contexts, such as individual
behavioral and societal trend patterns, remains underexplored. Notably, recent
models often neglect similarities in users' actions that occur implicitly among
users during analogous timeframes-a concept we term vertical temporal
proximity. These models primarily adapt the self-attention mechanisms of the
transformer to consider the temporal context in individual user actions.
Meanwhile, this adaptation still remains limited in considering the horizontal
temporal proximity within item interactions, like distinguishing between
subsequent item purchases within a week versus a month. To address these gaps,
we propose a sequential recommendation model called TemProxRec, which includes
contrastive learning and self-attention methods to consider temporal
proximities both across and within user-item interactions. The proposed
contrastive learning method learns representations of items selected in close
temporal periods across different users to be close. Simultaneously, the
proposed self-attention mechanism encodes temporal and positional contexts in a
user sequence using both absolute and relative embeddings. This way, our
TemProxRec accurately predicts the relevant items based on the user-item
interactions within a specific timeframe. We validate this work through
comprehensive experiments on TemProxRec, consistently outperforming existing
models on benchmark datasets as well as showing the significance of considering
the vertical and horizontal temporal proximities into sequential
recommendation.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09786" title="Abstract">arXiv:2402.09786</a> [<a href="/pdf/2402.09786" title="Download PDF">pdf</a>, <a href="/format/2402.09786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grissom%2C+A">Alvin Grissom II</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+R+F">Ryan F. Lei</a>, 
<a href="/search/cs?searchtype=author&query=Neto%2C+J+F+S+R">Jeova Farias Sales Rocha Neto</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bailey Lin</a>, 
<a href="/search/cs?searchtype=author&query=Trotter%2C+R">Ryan Trotter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative adversarial networks generate photorealistic faces that are often
indistinguishable by humans from real faces. We find that the discriminator in
the pre-trained StyleGAN3 model, a popular GAN network, systematically
stratifies scores by both image- and face-level qualities and that this
disproportionately affects images across gender, race, and other categories. We
examine the discriminator's bias for color and luminance across axes perceived
race and gender; we then examine axes common in research on stereotyping in
social psychology.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09790" title="Abstract">arXiv:2402.09790</a> [<a href="/pdf/2402.09790" title="Download PDF">pdf</a>, <a href="/ps/2402.09790" title="Download PostScript">ps</a>, <a href="/format/2402.09790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-vertebral CT-based FE models implementing linear isotropic  population-based material properties for the intervertebral discs cannot  accurately predict strains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garavelli%2C+C">Chiara Garavelli</a>, 
<a href="/search/cs?searchtype=author&query=Aldieri%2C+A">Alessandra Aldieri</a>, 
<a href="/search/cs?searchtype=author&query=Palanca%2C+M">Marco Palanca</a>, 
<a href="/search/cs?searchtype=author&query=Patruno%2C+L">Luca Patruno</a>, 
<a href="/search/cs?searchtype=author&query=Viceconti%2C+M">Marco Viceconti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures submitted to Biomechanics and Modeling in Mechanobiology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Vertebral fractures prediction in clinics lacks of accuracy. The most used
scores have limitations in distinguishing between subjects at risk or not.
Finite element (FE) models generated from computed tomography (CT) of these
patients may improve the predictive capability. Many models have already been
proposed but the most of them considered the single vertebral body, excluding
from the analysis the role of the inter-vertebral discs in the distribution of
the load through the spine. Multi-vertebral models instead allow to examine
more complex boundary condition. However, CT scans do not provide
subject-specif information about the material properties of the disc.
Consequently, the goal of the study was to validate a multi-vertebral FE model
with subject specific modelling of the vertebral bone and population-based
properties assigned to the disc, idealizing them with a linear isotropic
material. Boundary condition were assigned in order to reproduce an
experimental test performed on the same specimen and recorded using digital
image correlation technique (DIC). FE and DIC strains on the vertebral surfaces
are compared point-wise. Young's modulus values in the range 25-30 MPa allowed
to achieve a comparable order of magnitude between experimental and
computational data. However, the two distribution remained strongly different.
To conclude, subject-specific material properties need to be assigned also to
the discs as well as to the vertebrae to achieve acceptable accuracy in the
assessment of the fracture risk.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09792" title="Abstract">arXiv:2402.09792</a> [<a href="/pdf/2402.09792" title="Download PDF">pdf</a>, <a href="/ps/2402.09792" title="Download PostScript">ps</a>, <a href="/format/2402.09792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF)  on Deep Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+S">S. Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+A">A. Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+S">S. Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Dash%2C+G">G. Dash</a>, 
<a href="/search/cs?searchtype=author&query=Saraswat%2C+V">V. Saraswat</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+U">U. Ganguly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU)
architecture is energy-efficient as it utilizes dedicated neuromorphic hardware
and stochastic computation of weight updates for in-memory computing. Charge
Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs.
However, prior work has shown that the weight updates (V_T) in CTF-based RPU
are impacted by the non-ideal program time of CTF. The non-ideal program time
is affected by two factors of CTF. Firstly, the effects of the number of input
pulses (N) or pulse width (pw), and secondly, the gap between successive update
pulses (t_gap) used for the stochastic computation of weight updates.
Therefore, the impact of this non-ideal program time must be studied for neural
network training simulations. In this study, Firstly, we propose a pulse-train
design compensation technique to reduce the total error caused by non-ideal
program time of CTF and stochastic variance of a network. Secondly, we simulate
RPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST
datasets. We find that for larger N (~1000), learning performance approaches
the ideal (software-level) training level and, therefore, is not much impacted
by the choice of t_gap used to implement RPU-based weight updates. However, for
lower N (&lt;500), learning performance depends on T_gap of the pulses. Finally,
we also performed an ablation study to isolate the causal factor of the
improved learning performance. We conclude that the lower noise level in the
weight updates is the most likely significant factor to improve the learning
performance of DNN. Thus, our study attempts to compensate for the error caused
by non-ideal program time and standardize the pulse length (N) and pulse gap
(t_gap) specifications for CTF-based RPUs for accurate system-level on-chip
training.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09795" title="Abstract">arXiv:2402.09795</a> [<a href="/pdf/2402.09795" title="Download PDF">pdf</a>, <a href="/format/2402.09795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An advanced data fabric architecture leveraging homomorphic encryption  and federated learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rieyan%2C+S+A">Sakib Anwar Rieyan</a>, 
<a href="/search/cs?searchtype=author&query=News%2C+M+R+K">Md. Raisul Kabir News</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A+B+M+M">A.B.M. Muntasir Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Sadia Afrin Khan</a>, 
<a href="/search/cs?searchtype=author&query=Zaarif%2C+S+T+J">Sultan Tasneem Jawad Zaarif</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+G+R">Md. Golam Rabiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+M+M">Mohammad Mehedi Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Ianni%2C+M">Michele Ianni</a>, 
<a href="/search/cs?searchtype=author&query=Fortino%2C+G">Giancarlo Fortino</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Fusion, 102, 102004 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">Data fabric is an automated and AI-driven data fusion approach to accomplish
data management unification without moving data to a centralized location for
solving complex data problems. In a Federated learning architecture, the global
model is trained based on the learned parameters of several local models that
eliminate the necessity of moving data to a centralized repository for machine
learning. This paper introduces a secure approach for medical image analysis
using federated learning and partially homomorphic encryption within a
distributed data fabric architecture. With this method, multiple parties can
collaborate in training a machine-learning model without exchanging raw data
but using the learned or fused features. The approach complies with laws and
regulations such as HIPAA and GDPR, ensuring the privacy and security of the
data. The study demonstrates the method's effectiveness through a case study on
pituitary tumor classification, achieving a significant level of accuracy.
However, the primary focus of the study is on the development and evaluation of
federated learning and partially homomorphic encryption as tools for secure
medical image analysis. The results highlight the potential of these techniques
to be applied to other privacy-sensitive domains and contribute to the growing
body of research on secure and privacy-preserving machine learning.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09797" title="Abstract">arXiv:2402.09797</a> [<a href="/pdf/2402.09797" title="Download PDF">pdf</a>, <a href="/format/2402.09797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cross-talk robust multichannel VAD model for multiparty agent  interactions trained using synthetic re-recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Hyewon Han</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Naveen Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the Hands-free Speech Communication and Microphone Arrays (HSCMA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Human-Computer Interaction (cs.HC); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this work, we propose a novel cross-talk rejection framework for a
multi-channel multi-talker setup for a live multiparty interactive show. Our
far-field audio setup is required to be hands-free during live interaction and
comprises four adjacent talkers with directional microphones in the same space.
Such setups often introduce heavy cross-talk between channels, resulting in
reduced automatic speech recognition (ASR) and natural language understanding
(NLU) performance. To address this problem, we propose voice activity detection
(VAD) model for all talkers using multichannel information, which is then used
to filter audio for downstream tasks. We adopt a synthetic training data
generation approach through playback and re-recording for such scenarios,
simulating challenging speech overlap conditions. We train our models on this
synthetic data and demonstrate that our approach outperforms single-channel VAD
models and energy-based multi-channel VAD algorithm in various acoustic
environments. In addition to VAD results, we also present multiparty ASR
evaluation results to highlight the impact of using our VAD model for filtering
audio in downstream tasks by significantly reducing the insertion error.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09799" title="Abstract">arXiv:2402.09799</a> [<a href="/pdf/2402.09799" title="Download PDF">pdf</a>, <a href="/ps/2402.09799" title="Download PostScript">ps</a>, <a href="/format/2402.09799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Designing a wiki-based community knowledge management system for  personal science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kloppenborg%2C+K">Katharina Kloppenborg</a>, 
<a href="/search/cs?searchtype=author&query=Ball%2C+M+P">Mad Price Ball</a>, 
<a href="/search/cs?searchtype=author&query=Jonas%2C+S">Steven Jonas</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G+I">Gary Isaac Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Tzovaras%2C+B+G">Bastian Greshake Tzovaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> supplementary files are on Zenodo at <a href="https://zenodo.org/records/10659150">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Personal science is the practice of addressing personally relevant health
questions through self-research. Implementing personal science can be
challenging, due to the need to develop and adopt research protocols, tools,
and methods. While online communities can provide valuable peer support, tools
for systematically accessing community knowledge are lacking. The objective of
this study is to apply a participatory design process involving a community of
personal science practitioners to develop a peer-produced knowledge base that
supports the needs of practitioners as consumers and contributors of knowledge.
The process led to the development of the Personal Science Wiki, an open
repository for documenting and accessing individual self-tracking projects
while facilitating the establishment of consensus knowledge. After initial
design iterations and a field testing phase, we performed a user study with 21
participants to test and improve the platform, and to explore suitable
information architectures. The study deepened our understanding of barriers to
scaling the personal science community, established an infrastructure for
knowledge management actively used by the community, and provided lessons on
challenges, information needs, representations, and architectures to support
individuals with their personal health inquiries
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09800" title="Abstract">arXiv:2402.09800</a> [<a href="/pdf/2402.09800" title="Download PDF">pdf</a>, <a href="/format/2402.09800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale Benchmarking of Metaphor-based Optimization Heuristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vermetten%2C+D">Diederick Vermetten</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+C">Carola Doerr</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kononova%2C+A+V">Anna V. Kononova</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4ck%2C+T">Thomas B&#xe4;ck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The number of proposed iterative optimization heuristics is growing steadily,
and with this growth, there have been many points of discussion within the
wider community. One particular criticism that is raised towards many new
algorithms is their focus on metaphors used to present the method, rather than
emphasizing their potential algorithmic contributions. Several studies into
popular metaphor-based algorithms have highlighted these problems, even
showcasing algorithms that are functionally equivalent to older existing
methods. Unfortunately, this detailed approach is not scalable to the whole set
of metaphor-based algorithms. Because of this, we investigate ways in which
benchmarking can shed light on these algorithms. To this end, we run a set of
294 algorithm implementations on the BBOB function suite. We investigate how
the choice of the budget, the performance measure, or other aspects of
experimental design impact the comparison of these algorithms. Our results
emphasize why benchmarking is a key step in expanding our understanding of the
algorithm space, and what challenges still need to be overcome to fully gauge
the potential improvements to the state-of-the-art hiding behind the metaphors.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09801" title="Abstract">arXiv:2402.09801</a> [<a href="/pdf/2402.09801" title="Download PDF">pdf</a>, <a href="/format/2402.09801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EFUF: Efficient Fine-grained Unlearning Framework for Mitigating  Hallucinations in Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+S">Shangyu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+T">Tuo An</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyu Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multimodal large language models (MLLMs) have attracted increasing attention
in the past few years, but they may still generate descriptions that include
objects not present in the corresponding images, a phenomenon known as object
hallucination. To eliminate hallucinations, existing methods manually annotate
paired responses with and without hallucinations, and then employ various
alignment algorithms to improve the alignment capability between images and
text. However, they not only demand considerable computation resources during
the finetuning stage but also require expensive human annotation to construct
paired data needed by the alignment algorithms. To address these issues, we
borrow the idea of unlearning and propose an efficient fine-grained unlearning
framework (EFUF), which can eliminate hallucinations without the need for
paired data. Extensive experiments show that our method consistently reduces
hallucinations while preserving the generation quality with modest
computational overhead. Our code and datasets will be publicly available.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09803" title="Abstract">arXiv:2402.09803</a> [<a href="/pdf/2402.09803" title="Download PDF">pdf</a>, <a href="/format/2402.09803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Inverse Problems Approach to Pulse Wave Analysis in the Human Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Weissinger%2C+L">Lukas Weissinger</a>, 
<a href="/search/math?searchtype=author&query=Hubmer%2C+S">Simon Hubmer</a>, 
<a href="/search/math?searchtype=author&query=Ramlau%2C+R">Ronny Ramlau</a>, 
<a href="/search/math?searchtype=author&query=Voss%2C+H+U">Henning Uwe Voss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Cardiac pulsations in the human brain have received recent interest due to
their possible role in the pathogenesis of neurodegenerative diseases. Further
interest stems from their possible application as an endogenous signal source
that can be utilized for brain imaging in general. The (pulse-)wave describing
the blood flow velocity along an intracranial artery consists of a forward
(anterograde) and a backward (retrograde, reflected) part, but measurements of
this wave usually consist of a superposition of these components. In this
paper, we provide a mathematical framework for the inverse problem of
estimating the pulse wave velocity, as well as the forward and backward
component of the pulse wave separately from MRI measurements on the middle
cerebral artery. After a mathematical analysis of this problem, we consider
possible reconstruction approaches, and derive an alternate direction approach
for its solution. The resulting methods provide estimates for
anterograde/retrograde wave forms and the pulse wave velocity under specified
assumptions on a cerebrovascular model system.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09805" title="Abstract">arXiv:2402.09805</a> [<a href="/pdf/2402.09805" title="Download PDF">pdf</a>, <a href="/ps/2402.09805" title="Download PostScript">ps</a>, <a href="/format/2402.09805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Edge processing on LoRaWAN architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milani%2C+S">Stefano Milani</a>, 
<a href="/search/cs?searchtype=author&query=Chatzigiannakis%2C+I">Ioannis Chatzigiannakis</a>, 
<a href="/search/cs?searchtype=author&query=Garlisi%2C+D">Domenico Garlisi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Fraia%2C+M">Matteo Di Fraia</a>, 
<a href="/search/cs?searchtype=author&query=Pisani%2C+P">Patrizio Pisani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">LoRaWAN is a wireless technology that enables high-density deployments of IoT
devices. Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs
large cells to service a potentially extremely high number of devices. The
technology enforces a centralized architecture, directing all data generated by
the devices to a single network server for data processing. End-to-end
encryption is used to guarantee the confidentiality and security of data. In
this demo, we present \edgelora, a system architecture designed to incorporate
edge processing in LoRaWAN without compromising security and confidentiality of
data. \edgelora maintains backward compatibility and addresses scalability
issues arising from handling large amounts of data sourced from a diverse range
of devices. The demo provides evidence on the advantages in terms of reduced
latency, lower network bandwidth requirements, higher scalability, and improved
security and privacy resulting from the application of the Edge processing
paradigm to LoRaWAN.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09808" title="Abstract">arXiv:2402.09808</a> [<a href="/pdf/2402.09808" title="Download PDF">pdf</a>, <a href="/format/2402.09808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge of Pretrained Language Models on Surface Information of Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiraoka%2C+T">Tatsuya Hiraoka</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Do pretrained language models have knowledge regarding the surface
information of tokens? We examined the surface information stored in word or
subword embeddings acquired by pretrained language models from the perspectives
of token length, substrings, and token constitution. Additionally, we evaluated
the ability of models to generate knowledge regarding token surfaces. We
focused on 12 pretrained language models that were mainly trained on English
and Japanese corpora. Experimental results demonstrate that pretrained language
models have knowledge regarding token length and substrings but not token
constitution. Additionally, the results imply that there is a bottleneck on the
decoder side in terms of effectively utilizing acquired knowledge.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09809" title="Abstract">arXiv:2402.09809</a> [<a href="/pdf/2402.09809" title="Download PDF">pdf</a>, <a href="/ps/2402.09809" title="Download PostScript">ps</a>, <a href="/format/2402.09809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Scalable Math Support: Evidence on the Impact of an AI-  Tutor on Math Achievement in Ghana
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henkel%2C+O">Owen Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Horne-Robinson%2C+H">Hannah Horne-Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Kozhakhmetova%2C+N">Nessie Kozhakhmetova</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+A">Amanda Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This study evaluates the impact of Rori, an AI powered conversational math
tutor accessible via WhatsApp, on the math performance of approximately 1,000
students in grades 3-9 across 11 schools in Ghana. Each school was assigned to
a treatment group or control group; the students in the control group continued
their regular math instruction, while students in the treatment group engaged
with Rori, for two 30-minute sessions per week over 8 months in addition to
regular math instruction. We find that the math growth scores were
substantially higher for the treatment group with an effect size of 0.37, and
that the results were statistically significant (p &lt; 0.001). The fact that Rori
works with basic mobile devices on low-bandwidth data networks gives the
intervention strong potential to support personalized learning on other
low-and-middle-income countries (LMICs), where laptop ownership and high-speed
internet - prerequisite for many video-centered learning platforms - remain
extremely limited. While the results should be interpreted judiciously, as they
only report on year 1 of the intervention, and future research is necessary to
better understand which conditions are necessary for successful implementation,
they do suggest that chat-based tutoring solutions leveraging artificial
intelligence could offer a costeffective approach to enhancing learning
outcomes for millions of students globally.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09811" title="Abstract">arXiv:2402.09811</a> [<a href="/pdf/2402.09811" title="Download PDF">pdf</a>, <a href="/format/2402.09811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEXTRON: Weakly Supervised Multilingual Text Detection through Data  Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudale%2C+D">Dhruv Kudale</a>, 
<a href="/search/cs?searchtype=author&query=Kasuba%2C+B+V">Badri Vishal Kasuba</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+V">Venkatapathy Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+P">Parag Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the WACV 2024 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Several recent deep learning (DL) based techniques perform considerably well
on image-based multilingual text detection. However, their performance relies
heavily on the availability and quality of training data. There are numerous
types of page-level document images consisting of information in several
modalities, languages, fonts, and layouts. This makes text detection a
challenging problem in the field of computer vision (CV), especially for
low-resource or handwritten languages. Furthermore, there is a scarcity of
word-level labeled data for text detection, especially for multilingual
settings and Indian scripts that incorporate both printed and handwritten text.
Conventionally, Indian script text detection requires training a DL model on
plenty of labeled data, but to the best of our knowledge, no relevant datasets
are available. Manual annotation of such data requires a lot of time, effort,
and expertise. In order to solve this problem, we propose TEXTRON, a Data
Programming-based approach, where users can plug various text detection methods
into a weak supervision-based learning framework. One can view this approach to
multilingual text detection as an ensemble of different CV-based techniques and
DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on
a significant amount of language data in conjunction with CV-based methods to
improve text detection in other languages. We demonstrate that TEXTRON can
improve the detection performance for documents written in Indian languages,
despite the absence of corresponding labeled data. Further, through extensive
experimentation, we show improvement brought about by our approach over the
current State-of-the-art (SOTA) models, especially for handwritten Devanagari
text. Code and dataset has been made available at
https://github.com/IITB-LEAP-OCR/TEXTRON
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09812" title="Abstract">arXiv:2402.09812</a> [<a href="/pdf/2402.09812" title="Download PDF">pdf</a>, <a href="/format/2402.09812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamMatcher: Appearance Matching Self-Attention for  Semantically-Consistent Text-to-Image Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nam%2C+J">Jisu Nam</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heesu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">DongJae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Siyoon Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Seunggyu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page is available at <a href="https://ku-cvlab.github.io/DreamMatcher/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The objective of text-to-image (T2I) personalization is to customize a
diffusion model to a user-provided reference concept, generating diverse images
of the concept aligned with the target prompts. Conventional methods
representing the reference concepts using unique text embeddings often fail to
accurately mimic the appearance of the reference. To address this, one solution
may be explicitly conditioning the reference images into the target denoising
process, known as key-value replacement. However, prior works are constrained
to local editing since they disrupt the structure path of the pre-trained T2I
model. To overcome this, we propose a novel plug-in method, called
DreamMatcher, which reformulates T2I personalization as semantic matching.
Specifically, DreamMatcher replaces the target values with reference values
aligned by semantic matching, while leaving the structure path unchanged to
preserve the versatile capability of pre-trained T2I models for generating
diverse structures. We also introduce a semantic-consistent masking strategy to
isolate the personalized concept from irrelevant regions introduced by the
target prompts. Compatible with existing T2I models, DreamMatcher shows
significant improvements in complex scenarios. Intensive analyses demonstrate
the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09814" title="Abstract">arXiv:2402.09814</a> [<a href="/pdf/2402.09814" title="Download PDF">pdf</a>, <a href="/ps/2402.09814" title="Download PostScript">ps</a>, <a href="/format/2402.09814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A P&#xe9;clet-robust discontinuous Galerkin method for nonlinear diffusion  with advection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=da+Veiga%2C+L+B">Louren&#xe7;o Beir&#xe3;o da Veiga</a>, 
<a href="/search/math?searchtype=author&query=Di+Pietro%2C+D+A">Daniele A. Di Pietro</a>, 
<a href="/search/math?searchtype=author&query=Haile%2C+K+B">Kirubell B. Haile</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We analyze a Discontinuous Galerkin method for a problem with linear
advection-reaction and $p$-type diffusion, with Sobolev indices $p\in (1,
\infty)$. The discretization of the diffusion term is based on the full
gradient including jump liftings and interior-penalty stabilization while, for
the advective contribution, we consider a strengthened version of the classical
upwind scheme. The developed error estimates track the dependence of the local
contributions to the error on local P\'eclet numbers. A set of numerical tests
supports the theoretical derivations.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09816" title="Abstract">arXiv:2402.09816</a> [<a href="/pdf/2402.09816" title="Download PDF">pdf</a>, <a href="/format/2402.09816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model  via Cross-modal Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zavras%2C+A">Angelos Zavras</a>, 
<a href="/search/cs?searchtype=author&query=Michail%2C+D">Dimitrios Michail</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+B">Beg&#xfc;m Demir</a>, 
<a href="/search/cs?searchtype=author&query=Papoutsis%2C+I">Ioannis Papoutsis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep Learning (DL) is undergoing a paradigm shift with the emergence of
foundation models, aptly named by their crucial, yet incomplete nature. In this
work, we focus on Contrastive Language-Image Pre-training (CLIP), an
open-vocabulary foundation model, which achieves high accuracy across many
image classification tasks and is often competitive with a fully supervised
baseline without being explicitly trained. Nevertheless, there are still
domains where zero-shot CLIP performance is far from optimal, such as Remote
Sensing (RS) and medical imagery. These domains do not only exhibit
fundamentally different distributions compared to natural images, but also
commonly rely on complementary modalities, beyond RGB, to derive meaningful
insights. To this end, we propose a methodology for the purpose of aligning
distinct RS imagery modalities with the visual and textual modalities of CLIP.
Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal
with the distribution shift, accompanied by the cross-modal alignment of a RS
modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We
ultimately demonstrate our method on the tasks of RS imagery classification and
cross-modal retrieval. We empirically show that both robust fine-tuning and
cross-modal alignment translate to significant performance gains, across
several RS benchmark datasets. Notably, these enhancements are achieved without
the reliance on textual descriptions, without introducing any task-specific
parameters, without training from scratch and without catastrophic forgetting.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09820" title="Abstract">arXiv:2402.09820</a> [<a href="/pdf/2402.09820" title="Download PDF">pdf</a>, <a href="/ps/2402.09820" title="Download PostScript">ps</a>, <a href="/format/2402.09820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Cybersecurity Resilience in Finance with Deep Learning for  Advanced Threat Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); General Finance (q-fin.GN)

</div>
<p class="mathjax">In the age of the Internet, people's lives are increasingly dependent on
today's network technology. However, network technology is a double-edged
sword, bringing convenience to people but also posing many security challenges.
Maintaining network security and protecting the legitimate interests of users
is at the heart of network construction. Threat detection is an important part
of a complete and effective defense system. In the field of network information
security, the technical update of network attack and network protection is
spiraling. How to effectively detect unknown threats is one of the concerns of
network protection. Currently, network threat detection is usually based on
rules and traditional machine learning methods, which create artificial rules
or extract common spatiotemporal features, which cannot be applied to
large-scale data applications, and the emergence of unknown threats causes the
detection accuracy of the original model to decline. With this in mind, this
paper uses deep learning for advanced threat detection to improve cybersecurity
resilienc e in the financial industry. Many network security researchers have
shifted their focus to exceptio n-based intrusion detection techniques. The
detection technology mainly uses statistical machine learning methods -
collecting normal program and network behavior data, extracting
multidimensional features, and training decision machine learning models on
this basis (commonly used include naive Bayes, decision trees, support vector
machines, random forests, etc.). In the detection phase, program code or
network behavior that deviates from the normal value beyond the tolerance is
considered malicious code or network attack behavior.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09825" title="Abstract">arXiv:2402.09825</a> [<a href="/pdf/2402.09825" title="Download PDF">pdf</a>, <a href="/format/2402.09825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Lower Bounds for Approximating Parameterized Nearest Codeword  and Related Problems under ETH
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuangle Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bingkai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">In this paper we present a new gap-creating randomized self-reduction for
parameterized Maximum Likelihood Decoding problem over $\mathbb{F}_p$
($k$-MLD$_p$). The reduction takes a $k$-MLD$_p$ instance with $k\cdot n$
vectors as input, runs in time $f(k)n^{O(1)}$ for some computable function $f$,
outputs a $(3/2-\varepsilon)$-Gap-$k'$-MLD$_p$ instance for any
$\varepsilon&gt;0$, where $k'=O(k^2\log k)$. Using this reduction, we show that
assuming the randomized Exponential Time Hypothesis (ETH), no algorithms can
approximate $k$-MLD$_p$ (and therefore its dual problem $k$-NCP$_p$) within
factor $(3/2-\varepsilon)$ in $f(k)\cdot n^{o(\sqrt{k/\log k})}$ time for any
$\varepsilon&gt;0$.
<br />We then use reduction by Bhattacharyya, Ghoshal, Karthik and Manurangsi
(ICALP 2018) to amplify the $(3/2-\varepsilon)$-gap to any constant. As a
result, we show that assuming ETH, no algorithms can approximate $k$-NCP$_p$
and $k$-MDP$_p$ within $\gamma$-factor in $f(k)n^{o(k^{\varepsilon_\gamma})}$
time for some constant $\varepsilon_\gamma&gt;0$. Combining with the
gap-preserving reduction by Bennett, Cheraghchi, Guruswami and Ribeiro (STOC
2023), we also obtain similar lower bounds for $k$-MDP$_p$, $k$-CVP$_p$ and
$k$-SVP$_p$.
<br />These results improve upon the previous $f(k)n^{\Omega(\mathsf{poly} \log
k)}$ lower bounds for these problems under ETH using reductions by
Bhattacharyya et al. (J.ACM 2021) and Bennett et al. (STOC 2023).
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09828" title="Abstract">arXiv:2402.09828</a> [<a href="/pdf/2402.09828" title="Download PDF">pdf</a>, <a href="/ps/2402.09828" title="Download PostScript">ps</a>, <a href="/format/2402.09828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validation of homogenized finite element models of human metastatic  vertebrae using digital volume correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garavelli%2C+C">Chiara Garavelli</a>, 
<a href="/search/cs?searchtype=author&query=Aldieri%2C+A">Alessandra Aldieri</a>, 
<a href="/search/cs?searchtype=author&query=Palanca%2C+M">Marco Palanca</a>, 
<a href="/search/cs?searchtype=author&query=Dall%27Ara%2C+E">Enrico Dall&#x27;Ara</a>, 
<a href="/search/cs?searchtype=author&query=Viceconti%2C+M">Marco Viceconti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The incidence of vertebral fragility fracture is increased by the presence of
preexisting pathologies such as metastatic disease. Computational tools could
support the fracture prediction and consequently the decision of the best
medical treatment. Anyway, validation is required to use these tools in
clinical practice. To address this necessity, in this study subject-specific
homogenized finite element models of single vertebrae were generated from micro
CT images for both healthy and metastatic vertebrae and validated against
experimental data. More in detail, spine segments were tested under compression
and imaged with micro CT. The displacements field could be extracted for each
vertebra singularly using the digital volume correlation full-field technique.
Homogenized finite element models of each vertebra could hence be built from
the micro CT images, applying boundary conditions consistent with the
experimental displacements at the endplates. Numerical and experimental
displacements and strains fields were eventually compared. In addition, the
outcomes of a micro CT based homogenized model were compared to the ones of a
clinical-CT based model. Good agreement between experimental and computational
displacement fields, both for healthy and metastatic vertebrae, was found.
Comparison between micro CT based and clinical-CT based outcomes showed strong
correlations. Furthermore, models were able to qualitatively identify the
regions which experimentally showed the highest strain concentration. In
conclusion, the combination of experimental full-field technique and the
in-silico modelling allowed the development of a promising pipeline for
validation of fracture risk predictors, although further improvements in both
fields are needed to better analyse quantitatively the post-yield behaviour of
the vertebra.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09830" title="Abstract">arXiv:2402.09830</a> [<a href="/pdf/2402.09830" title="Download PDF">pdf</a>, <a href="/ps/2402.09830" title="Download PostScript">ps</a>, <a href="/format/2402.09830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing GANs for Fraud Detection: Model Training with Synthetic  Transaction Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Anomaly detection is a critical challenge across various research domains,
aiming to identify instances that deviate from normal data distributions. This
paper explores the application of Generative Adversarial Networks (GANs) in
fraud detection, comparing their advantages with traditional methods. GANs, a
type of Artificial Neural Network (ANN), have shown promise in modeling complex
data distributions, making them effective tools for anomaly detection. The
paper systematically describes the principles of GANs and their derivative
models, emphasizing their application in fraud detection across different
datasets. And by building a collection of adversarial verification graphs, we
will effectively prevent fraud caused by bots or automated systems and ensure
that the users in the transaction are real. The objective of the experiment is
to design and implement a fake face verification code and fraud detection
system based on Generative Adversarial network (GANs) algorithm to enhance the
security of the transaction process.The study demonstrates the potential of
GANs in enhancing transaction security through deep learning techniques.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09833" title="Abstract">arXiv:2402.09833</a> [<a href="/pdf/2402.09833" title="Download PDF">pdf</a>, <a href="/ps/2402.09833" title="Download PostScript">ps</a>, <a href="/format/2402.09833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R4: rapid reproducible robotics research open hardware control system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waltham%2C+C">Chris Waltham</a>, 
<a href="/search/cs?searchtype=author&query=Perrett%2C+A">Andy Perrett</a>, 
<a href="/search/cs?searchtype=author&query=Soni%2C+R">Rakshit Soni</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+C">Charles Fox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A key component of any robot is the interface between ROS2 software and
physical motors. New robots often use arbitrary, messy mixtures of closed and
open motor drivers and error-prone physical mountings, wiring, and connectors
to interface them. There is a need for a standardizing OSH component to
abstract this complexity, as Arduino did for interfacing to smaller components.
We present a OSH printed circuit board to solve this problem once and for all.
On the high-level side, it interfaces to Arduino Giga -- acting as an unusually
large and robust shield -- and thus to existing open source ROS software
stacks. On the lower-level side, it interfaces to existing emerging standard
open hardware including OSH motor drivers and relays, which can already be used
to drive fully open hardware wheeled and arm robots. This enables the creation
of a family of standardized, fully open hardware, fully reproducible, research
platforms.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09834" title="Abstract">arXiv:2402.09834</a> [<a href="/pdf/2402.09834" title="Download PDF">pdf</a>, <a href="/format/2402.09834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All in One and One for All: A Simple yet Effective Method towards  Cross-domain Graph Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haihong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aochuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiangguo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized the fields of computer
vision (CV) and natural language processing (NLP). One of the most notable
advancements of LLMs is that a single model is trained on vast and diverse
datasets spanning multiple domains -- a paradigm we term `All in One'. This
methodology empowers LLMs with super generalization capabilities, facilitating
an encompassing comprehension of varied data distributions. Leveraging these
capabilities, a single LLM demonstrates remarkable versatility across a variety
of domains -- a paradigm we term `One for All'. However, applying this idea to
the graph field remains a formidable challenge, with cross-domain pretraining
often resulting in negative transfer. This issue is particularly important in
few-shot learning scenarios, where the paucity of training data necessitates
the incorporation of external knowledge sources. In response to this challenge,
we propose a novel approach called Graph COordinators for PrEtraining (GCOPE),
that harnesses the underlying commonalities across diverse graph datasets to
enhance few-shot learning. Our novel methodology involves a unification
framework that amalgamates disparate graph datasets during the pretraining
phase to distill and transfer meaningful knowledge to target tasks. Extensive
experiments across multiple graph datasets demonstrate the superior efficacy of
our approach. By successfully leveraging the synergistic potential of multiple
graph datasets for pretraining, our work stands as a pioneering contribution to
the realm of graph foundational model.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09835" title="Abstract">arXiv:2402.09835</a> [<a href="/pdf/2402.09835" title="Download PDF">pdf</a>, <a href="/ps/2402.09835" title="Download PostScript">ps</a>, <a href="/format/2402.09835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Algorithms for Steiner Forest in Bounded Width Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldmann%2C+A+E">Andreas Emil Feldmann</a>, 
<a href="/search/cs?searchtype=author&query=Lampis%2C+M">Michael Lampis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this paper we reassess the parameterized complexity and approximability of
the well-studied Steiner Forest problem in several graph classes of bounded
width. The problem takes an edge-weighted graph and pairs of vertices as input,
and the aim is to find a minimum cost subgraph in which each given vertex pair
lies in the same connected component. It is known that this problem is APX-hard
in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback
vertex set size 2. However, Bateni, Hajiaghayi and Marx [JACM, 2011] gave an
approximation scheme with a runtime of $n^{O(\frac{k^2}{\varepsilon})}$ on
graphs of treewidth $k$. Our main result is a much faster efficient
parameterized approximation scheme (EPAS) with a runtime of
$2^{O(\frac{k^2}{\varepsilon} \log \frac{k^2}{\varepsilon})} \cdot n^{O(1)}$.
If $k$ instead is the vertex cover number of the input graph, we show how to
compute the optimum solution in $2^{O(k \log k)} \cdot n^{O(1)}$ time, and we
also prove that this runtime dependence on $k$ is asymptotically best possible,
under ETH. Furthermore, if $k$ is the size of a feedback edge set, then we
obtain a faster $2^{O(k)} \cdot n^{O(1)}$ time algorithm, which again cannot be
improved under ETH.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09836" title="Abstract">arXiv:2402.09836</a> [<a href="/pdf/2402.09836" title="Download PDF">pdf</a>, <a href="/format/2402.09836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Imitation: Generating Human Mobility from Context-aware Reasoning  with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenyang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fengli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+B">Bingbing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Human mobility behaviours are closely linked to various important societal
problems such as traffic congestion, and epidemic control. However, collecting
mobility data can be prohibitively expensive and involves serious privacy
issues, posing a pressing need for high-quality generative mobility models.
Previous efforts focus on learning the behaviour distribution from training
samples, and generate new mobility data by sampling the learned distributions.
They cannot effectively capture the coherent intentions that drive mobility
behavior, leading to low sample efficiency and semantic-awareness. Inspired by
the emergent reasoning ability in LLMs, we propose a radical perspective shift
that reformulates mobility generation as a commonsense reasoning problem. In
this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR)
framework that prompts LLM to recursively generate mobility behaviour.
Specifically, we design a context-aware chain-of-thoughts prompting technique
to align LLMs with context-aware mobility behaviour by few-shot in-context
learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit
the synergistic effect between LLM reasoning and mechanistic gravity model. It
leverages the step-by-step LLM reasoning to recursively generate a temporal
template of activity intentions, which are then mapped to physical locations
with a mechanistic gravity model. Experiments on two real-world datasets show
MobiGeaR achieves state-of-the-art performance across all metrics, and
substantially reduces the size of training samples at the same time. Besides,
MobiGeaR also significantly improves the semantic-awareness of mobility
generation by improving the intention accuracy by 62.23% and the generated
mobility data is proven effective in boosting the performance of downstream
applications. The implementation of our approach is available in the paper.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09838" title="Abstract">arXiv:2402.09838</a> [<a href="/pdf/2402.09838" title="Download PDF">pdf</a>, <a href="/format/2402.09838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performative Reinforcement Learning in Gradually Shifting Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rank%2C+B">Ben Rank</a>, 
<a href="/search/cs?searchtype=author&query=Triantafyllou%2C+S">Stelios Triantafyllou</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+D">Debmalya Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Radanovic%2C+G">Goran Radanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">When Reinforcement Learning (RL) agents are deployed in practice, they might
impact their environment and change its dynamics. Ongoing research attempts to
formally model this phenomenon and to analyze learning algorithms in these
models. To this end, we propose a framework where the current environment
depends on the deployed policy as well as its previous dynamics. This is a
generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our
framework allows to model scenarios where the environment gradually adjusts to
a deployed policy. We adapt two algorithms from the performative prediction
literature to our setting and propose a novel algorithm called Mixed Delayed
Repeated Retraining (MDRR). We provide conditions under which these algorithms
converge and compare them using three metrics: number of retrainings,
approximation guarantee, and number of samples per deployment. Unlike previous
approaches, MDRR combines samples from multiple deployments in its training.
This makes MDRR particularly suitable for scenarios where the environment's
response strongly depends on its previous dynamics, which are common in
practice. We experimentally compare the algorithms using a simulation-based
testbed and our results show that MDRR converges significantly faster than
previous approaches.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09841" title="Abstract">arXiv:2402.09841</a> [<a href="/pdf/2402.09841" title="Download PDF">pdf</a>, <a href="/format/2402.09841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAPDoc: Layout-Aware Prompting for Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamott%2C+M">Marcel Lamott</a>, 
<a href="/search/cs?searchtype=author&query=Weweler%2C+Y">Yves-Noel Weweler</a>, 
<a href="/search/cs?searchtype=author&query=Ulges%2C+A">Adrian Ulges</a>, 
<a href="/search/cs?searchtype=author&query=Shafait%2C+F">Faisal Shafait</a>, 
<a href="/search/cs?searchtype=author&query=Krechel%2C+D">Dirk Krechel</a>, 
<a href="/search/cs?searchtype=author&query=Obradovic%2C+D">Darko Obradovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at ICDAR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in training large language models (LLMs) using massive
amounts of solely textual data lead to strong generalization across many
domains and tasks, including document-specific tasks. Opposed to that there is
a trend to train multi-modal transformer architectures tailored for document
understanding that are designed specifically to fuse textual inputs with the
corresponding document layout. This involves a separate fine-tuning step for
which additional training data is required. At present, no document
transformers with comparable generalization to LLMs are available That raises
the question which type of model is to be preferred for document understanding
tasks. In this paper we investigate the possibility to use purely text-based
LLMs for document-specific tasks by using layout enrichment. We explore drop-in
modifications and rule-based methods to enrich purely textual LLM prompts with
layout information. In our experiments we investigate the effects on the
commercial ChatGPT model and the open-source LLM Solar. We demonstrate that
using our approach both LLMs show improved performance on various standard
document benchmarks. In addition, we study the impact of noisy OCR and layout
errors, as well as the limitations of LLMs when it comes to utilizing document
layout. Our results indicate that layout enrichment can improve the performance
of purely text-based LLMs for document understanding by up to 15% compared to
just using plain document text. In conclusion, this approach should be
considered for the best model choice between text-based LLM or multi-modal
document transformers.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09844" title="Abstract">arXiv:2402.09844</a> [<a href="/pdf/2402.09844" title="Download PDF">pdf</a>, <a href="/format/2402.09844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallou%C3%A9dec%2C+Q">Quentin Gallou&#xe9;dec</a>, 
<a href="/search/cs?searchtype=author&query=Beeching%2C+E">Edward Beeching</a>, 
<a href="/search/cs?searchtype=author&query=Romac%2C+C">Cl&#xe9;ment Romac</a>, 
<a href="/search/cs?searchtype=author&query=Dellandr%C3%A9a%2C+E">Emmanuel Dellandr&#xe9;a</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The search for a general model that can operate seamlessly across multiple
domains remains a key goal in machine learning research. The prevailing
methodology in Reinforcement Learning (RL) typically limits models to a single
task within a unimodal framework, a limitation that contrasts with the broader
vision of a versatile, multi-domain model. In this paper, we present Jack of
All Trades (JAT), a transformer-based model with a unique design optimized for
handling sequential decision-making tasks and multimodal data types. The JAT
model demonstrates its robust capabilities and versatility by achieving strong
performance on very different RL benchmarks, along with promising results on
Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a
single set of weights. The JAT model marks a significant step towards more
general, cross-domain AI model design, and notably, it is the first model of
its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat),
including a pioneering general-purpose dataset.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09845" title="Abstract">arXiv:2402.09845</a> [<a href="/pdf/2402.09845" title="Download PDF">pdf</a>, <a href="/format/2402.09845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JustSTART: How to Find an RSA Authentication Bypass on Xilinx  UltraScale(+) with Fuzzing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ender%2C+M">Maik Ender</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+F">Felix Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Fyrbiak%2C+M">Marc Fyrbiak</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+A">Amir Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Paar%2C+C">Christof Paar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Fuzzing is a well-established technique in the software domain to uncover
bugs and vulnerabilities. Yet, applications of fuzzing for security
vulnerabilities in hardware systems are scarce, as principal reasons are
requirements for design information access (HDL source code). Moreover,
observation of internal hardware state during runtime is typically an
ineffective information source, as its documentation is often not publicly
available. In addition, such observation during runtime is also inefficient due
to bandwidth-limited analysis interfaces (JTAG, and minimal introspection of
internal modules). In this work, we investigate fuzzing for 7-Series and
UltraScale(+) FPGA configuration engines, the control plane governing the
(secure) bitstream configuration within the FPGA. Our goal is to examine the
effectiveness of fuzzing to analyze and document the opaque inner workings of
FPGA configuration engines, with a primary emphasis on identifying security
vulnerabilities. Using only the publicly available chip and dispersed
documentation, we first design and implement ConFuzz, an advanced FPGA
configuration engine fuzzing and rapid prototyping framework. Based on our
detailed understanding of the bitstream file format, we then systematically
define 3 novel key fuzzing strategies for Xilinx configuration engines.
Moreover, our strategies are executed through mutational structure-aware
fuzzers and incorporate various novel custom-tailored, FPGA-specific
optimizations. Our evaluation reveals previously undocumented behavior within
the configuration engine, including critical findings such as system crashes
leading to unresponsive states of the FPGA. In addition, our investigations not
only lead to the rediscovery of the starbleed attack but also uncover JustSTART
(CVE-2023-20570), capable of circumventing RSA authentication for Xilinx
UltraScale(+). Note that we also discuss countermeasures.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09849" title="Abstract">arXiv:2402.09849</a> [<a href="/pdf/2402.09849" title="Download PDF">pdf</a>, <a href="/format/2402.09849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommendations for Baselines and Benchmarking Approximate Gaussian  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ober%2C+S+W">Sebastian W. Ober</a>, 
<a href="/search/cs?searchtype=author&query=Artemev%2C+A">Artem Artemev</a>, 
<a href="/search/cs?searchtype=author&query=Wagenl%C3%A4nder%2C+M">Marcel Wagenl&#xe4;nder</a>, 
<a href="/search/cs?searchtype=author&query=Grobins%2C+R">Rudolfs Grobins</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Wilk%2C+M">Mark van der Wilk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 25 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Gaussian processes (GPs) are a mature and widely-used component of the ML
toolbox. One of their desirable qualities is automatic hyperparameter
selection, which allows for training without user intervention. However, in
many realistic settings, approximations are typically needed, which typically
do require tuning. We argue that this requirement for tuning complicates
evaluation, which has led to a lack of a clear recommendations on which method
should be used in which situation. To address this, we make recommendations for
comparing GP approximations based on a specification of what a user should
expect from a method. In addition, we develop a training procedure for the
variational method of Titsias [2009] that leaves no choices to the user, and
show that this is a strong baseline that meets our specification. We conclude
that benchmarking according to our suggestions gives a clearer view of the
current state of the field, and uncovers problems that are still open that
future papers should address.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09850" title="Abstract">arXiv:2402.09850</a> [<a href="/pdf/2402.09850" title="Download PDF">pdf</a>, <a href="/format/2402.09850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of a metric for complex polynomials to bounded modification  of planar Pythagorean-hodograph curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Farouki%2C+R+T">Rida T. Farouki</a>, 
<a href="/search/math?searchtype=author&query=Knez%2C+M">Marjeta Knez</a>, 
<a href="/search/math?searchtype=author&query=Vitrih%2C+V">Vito Vitrih</a>, 
<a href="/search/math?searchtype=author&query=%C5%BDagar%2C+E">Emil &#x17d;agar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">By interpreting planar polynomial curves as complex-valued functions of a
real parameter, an inner product, norm, metric function, and the notion of
orthogonality may be defined for such curves. This approach is applied to the
complex pre-image polynomials that generate planar Pythagorean-hodograph (PH)
curves, to facilitate the implementation of bounded modifications of them that
preserve their PH nature. The problems of bounded modifications under the
constraint of fixed curve end points and end tangent directions, and of
increasing the arc length of a PH curve by a prescribed amount, are also
addressed.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09854" title="Abstract">arXiv:2402.09854</a> [<a href="/pdf/2402.09854" title="Download PDF">pdf</a>, <a href="/format/2402.09854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the efficiency of GP-GOMEA for higher-arity operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlender%2C+T">Thalea Schlender</a>, 
<a href="/search/cs?searchtype=author&query=Malafaia%2C+M">Mafalda Malafaia</a>, 
<a href="/search/cs?searchtype=author&query=Alderliesten%2C+T">Tanja Alderliesten</a>, 
<a href="/search/cs?searchtype=author&query=Bosman%2C+P+A+N">Peter A.N. Bosman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Deploying machine learning models into sensitive domains in our society
requires these models to be explainable. Genetic Programming (GP) can offer a
way to evolve inherently interpretable expressions. GP-GOMEA is a form of GP
that has been found particularly effective at evolving expressions that are
accurate yet of limited size and, thus, promote interpretability. Despite this
strength, a limitation of GP-GOMEA is template-based. This negatively affects
its scalability regarding the arity of operators that can be used, since with
increasing operator arity, an increasingly large part of the template tends to
go unused. In this paper, we therefore propose two enhancements to GP-GOMEA:
(i) semantic subtree inheritance, which performs additional variation steps
that consider the semantic context of a subtree, and (ii) greedy child
selection, which explicitly considers parts of the template that in standard
GP-GOMEA remain unused. We compare different versions of GP-GOMEA regarding
search enhancements on a set of continuous and discontinuous regression
problems, with varying tree depths and operator sets. Experimental results show
that both proposed search enhancements have a generally positive impact on the
performance of GP-GOMEA, especially when the set of operators to choose from is
large and contains higher-arity operators.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09860" title="Abstract">arXiv:2402.09860</a> [<a href="/pdf/2402.09860" title="Download PDF">pdf</a>, <a href="/format/2402.09860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Exploration to End of Life: Unpacking Sustainability in  Physicalization Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morais%2C+L">Luiz Morais</a>, 
<a href="/search/cs?searchtype=author&query=Panagiotidou%2C+G">Georgia Panagiotidou</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+S">Sarah Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Losev%2C+T">Tatiana Losev</a>, 
<a href="/search/cs?searchtype=author&query=Noonan%2C+R">Rebecca Noonan</a>, 
<a href="/search/cs?searchtype=author&query=Hinrichs%2C+U">Uta Hinrichs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data physicalizations have gained prominence across domains, but their
environmental impact has been largely overlooked. This work addresses this gap
by investigating the interplay between sustainability and physicalization
practices. We conducted interviews with experts from diverse backgrounds,
followed by a survey to gather insights into how they approach physicalization
projects and reflect on sustainability. Our thematic analysis revealed
sustainability considerations throughout the entire physicalization life cycle
-- a framework that encompasses various stages in a physicalization's
existence. Notably, we found no single agreed-upon definition for sustainable
physicalizations, highlighting the complexity of integrating sustainability
into physicalization practices. We outline sustainability challenges and
strategies based on participants' experiences and propose the Sustainable
Physicalization Practices (SuPPra) Matrix, providing a structured approach for
designers to reflect on and enhance the environmental impact of their future
physicalizations.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09865" title="Abstract">arXiv:2402.09865</a> [<a href="/pdf/2402.09865" title="Download PDF">pdf</a>, <a href="/format/2402.09865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ad%C5%BEemovi%C4%87%2C+M">Momir Ad&#x17e;emovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Tadi%C4%87%2C+P">Predrag Tadi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Petrovi%C4%87%2C+A">Andrija Petrovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Nikoli%C4%87%2C+M">Mladen Nikoli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Traditional tracking-by-detection systems typically employ Kalman filters
(KF) for state estimation. However, the KF requires domain-specific design
choices and it is ill-suited to handling non-linear motion patterns. To address
these limitations, we propose two innovative data-driven filtering methods. Our
first method employs a Bayesian filter with a trainable motion model to predict
an object's future location and combines its predictions with observations
gained from an object detector to enhance bounding box prediction accuracy.
Moreover, it dispenses with most domain-specific design choices characteristic
of the KF. The second method, an end-to-end trainable filter, goes a step
further by learning to correct detector errors, further minimizing the need for
domain expertise. Additionally, we introduce a range of motion model
architectures based on Recurrent Neural Networks, Neural Ordinary Differential
Equations, and Conditional Neural Processes, that are combined with the
proposed filtering methods. Our extensive evaluation across multiple datasets
demonstrates that our proposed filters outperform the traditional KF in object
tracking, especially in the case of non-linear motion patterns -- the use case
our filters are best suited to. We also conduct noise robustness analysis of
our filters with convincing positive results. We further propose a new cost
function for associating observations with tracks. Our tracker, which
incorporates this new association cost with our proposed filters, outperforms
the conventional SORT method and other motion-based trackers in multi-object
tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT
datasets.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09870" title="Abstract">arXiv:2402.09870</a> [<a href="/pdf/2402.09870" title="Download PDF">pdf</a>, <a href="/format/2402.09870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex Equilibrium-Free Stability and Performance Analysis of  Discrete-Time Nonlinear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Koelewijn%2C+P+J+W">Patrick J. W. Koelewijn</a>, 
<a href="/search/eess?searchtype=author&query=Weiland%2C+S">Siep Weiland</a>, 
<a href="/search/eess?searchtype=author&query=T%C3%B3th%2C+R">Roland T&#xf3;th</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IET Control Theory &amp; Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers the equilibrium-free stability and performance analysis
of discrete-time nonlinear systems. We consider two types of equilibrium-free
notions. Namely, the universal shifted concept, which considers stability and
performance w.r.t. all equilibrium points of the system, and the incremental
concept, which considers stability and performance between trajectories of the
system. In this paper, we show how universal shifted stability and performance
of discrete-time systems can be analyzed by making use of the time-difference
dynamics. Moreover, we extend the existing results for incremental
dissipativity for discrete-time systems based on dissipativity analysis of the
differential dynamics to more general state-dependent storage functions for
less conservative results. Finally, we show how both these equilibrium-free
notions can be cast as a convex analysis problem by making use of the linear
parameter-varying framework, which is also demonstrated by means of an example.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09871" title="Abstract">arXiv:2402.09871</a> [<a href="/pdf/2402.09871" title="Download PDF">pdf</a>, <a href="/format/2402.09871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuChin: A Chinese Colloquial Description Benchmark for Evaluating  Language Models in the Field of Music
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pengfei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jinyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+M">Ming Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kejun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The rapidly evolving multimodal Large Language Models (LLMs) urgently require
new benchmarks to uniformly evaluate their performance on understanding and
textually describing music. However, due to semantic gaps between Music
Information Retrieval (MIR) algorithms and human understanding, discrepancies
between professionals and the public, and low precision of annotations,
existing music description datasets cannot serve as benchmarks. To this end, we
present MuChin, the first open-source music description benchmark in Chinese
colloquial language, designed to evaluate the performance of multimodal LLMs in
understanding and describing music. We established the Caichong Music
Annotation Platform (CaiMAP) that employs an innovative multi-person,
multi-stage assurance method, and recruited both amateurs and professionals to
ensure the precision of annotations and alignment with popular semantics.
Utilizing this method, we built a dataset with multi-dimensional,
high-precision music annotations, the Caichong Music Dataset (CaiMD), and
carefully selected 1,000 high-quality entries to serve as the test set for
MuChin. Based on MuChin, we analyzed the discrepancies between professionals
and amateurs in terms of music description, and empirically demonstrated the
effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed
MuChin to evaluate existing music understanding models on their ability to
provide colloquial descriptions of music. All data related to the benchmark and
the code for scoring have been open-sourced.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09872" title="Abstract">arXiv:2402.09872</a> [<a href="/pdf/2402.09872" title="Download PDF">pdf</a>, <a href="/format/2402.09872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Reward: Evaluating and Enhancing Generative AI through  Million-User Feedback from an Online Creative Community
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isajanyan%2C+A">Arman Isajanyan</a>, 
<a href="/search/cs?searchtype=author&query=Shatveryan%2C+A">Artur Shatveryan</a>, 
<a href="/search/cs?searchtype=author&query=Kocharyan%2C+D">David Kocharyan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Humphrey Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages with 10 figures, accepted at ICLR 2024 as a spotlight, codes can be accessed at <a href="https://github.com/Picsart-AI-Research/Social-Reward">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Social reward as a form of community recognition provides a strong source of
motivation for users of online platforms to engage and contribute with content.
The recent progress of text-conditioned image synthesis has ushered in a
collaborative era where AI empowers users to craft original visual artworks
seeking community validation. Nevertheless, assessing these models in the
context of collective community preference introduces distinct challenges.
Existing evaluation methods predominantly center on limited size user studies
guided by image quality and prompt alignment. This work pioneers a paradigm
shift, unveiling Social Reward - an innovative reward modeling framework that
leverages implicit feedback from social network users engaged in creative
editing of generated images. We embark on an extensive journey of dataset
curation and refinement, drawing from Picsart: an online visual creation and
editing platform, yielding a first million-user-scale dataset of implicit human
preferences for user-generated visual art named Picsart Image-Social. Our
analysis exposes the shortcomings of current metrics in modeling community
creative preference of text-to-image models' outputs, compelling us to
introduce a novel predictive model explicitly tailored to address these
limitations. Rigorous quantitative experiments and user study show that our
Social Reward model aligns better with social popularity than existing metrics.
Furthermore, we utilize Social Reward to fine-tune text-to-image models,
yielding images that are more favored by not only Social Reward, but also other
established metrics. These findings highlight the relevance and effectiveness
of Social Reward in assessing community appreciation for AI-generated artworks,
establishing a closer alignment with users' creative goals: creating popular
visual art. Codes can be accessed at
https://github.com/Picsart-AI-Research/Social-Reward
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09874" title="Abstract">arXiv:2402.09874</a> [<a href="/pdf/2402.09874" title="Download PDF">pdf</a>, <a href="/format/2402.09874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huertas-Garc%C3%ADa%2C+%C3%81">&#xc1;lvaro Huertas-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn%2C+A">Alejandro Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Huertas-Tato%2C+J">Javier Huertas-Tato</a>, 
<a href="/search/cs?searchtype=author&query=Camacho%2C+D">David Camacho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adversarial attacks represent a substantial challenge in Natural Language
Processing (NLP). This study undertakes a systematic exploration of this
challenge in two distinct phases: vulnerability evaluation and resilience
enhancement of Transformer-based models under adversarial attacks.
<br />In the evaluation phase, we assess the susceptibility of three Transformer
configurations, encoder-decoder, encoder-only, and decoder-only setups, to
adversarial attacks of escalating complexity across datasets containing
offensive language and misinformation. Encoder-only models manifest a 14% and
21% performance drop in offensive language detection and misinformation
detection tasks, respectively. Decoder-only models register a 16% decrease in
both tasks, while encoder-decoder models exhibit a maximum performance drop of
14% and 26% in the respective tasks.
<br />The resilience-enhancement phase employs adversarial training, integrating
pre-camouflaged and dynamically altered data. This approach effectively reduces
the performance drop in encoder-only models to an average of 5% in offensive
language detection and 2% in misinformation detection tasks. Decoder-only
models, occasionally exceeding original performance, limit the performance drop
to 7% and 2% in the respective tasks. Although not surpassing the original
performance, Encoder-decoder models can reduce the drop to an average of 6% and
2% respectively.
<br />Results suggest a trade-off between performance and robustness, with some
models maintaining similar performance while gaining robustness. Our study and
adversarial training techniques have been incorporated into an open-source tool
for generating camouflaged datasets. However, methodology effectiveness depends
on the specific camouflage technique and data encountered, emphasizing the need
for continued exploration.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09877" title="Abstract">arXiv:2402.09877</a> [<a href="/pdf/2402.09877" title="Download PDF">pdf</a>, <a href="/format/2402.09877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computing Plans with Uniform Action Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pozanco%2C+A">Alberto Pozanco</a>, 
<a href="/search/cs?searchtype=author&query=Borrajo%2C+D">Daniel Borrajo</a>, 
<a href="/search/cs?searchtype=author&query=Veloso%2C+M">Manuela Veloso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In many real-world planning applications, agents might be interested in
finding plans whose actions have costs that are as uniform as possible. Such
plans provide agents with a sense of stability and predictability, which are
key features when humans are the agents executing plans suggested by planning
tools. This paper adapts three uniformity metrics to automated planning, and
introduce planning-based compilations that allow to lexicographically optimize
sum of action costs and action costs uniformity. Experimental results both in
well-known and novel planning benchmarks show that the reformulated tasks can
be effectively solved in practice to generate uniform plans.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09880" title="Abstract">arXiv:2402.09880</a> [<a href="/pdf/2402.09880" title="Download PDF">pdf</a>, <a href="/ps/2402.09880" title="Download PostScript">ps</a>, <a href="/format/2402.09880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inadequacies of Large Language Model Benchmarks in the Era of Generative  Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McIntosh%2C+T+R">Timothy R. McIntosh</a>, 
<a href="/search/cs?searchtype=author&query=Susnjak%2C+T">Teo Susnjak</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Watters%2C+P">Paul Watters</a>, 
<a href="/search/cs?searchtype=author&query=Halgamuge%2C+M+N">Malka N. Halgamuge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The rapid rise in popularity of Large Language Models (LLMs) with emerging
capabilities has spurred public curiosity to evaluate and compare different
LLMs, leading many researchers to propose their LLM benchmarks. Noticing
preliminary inadequacies in those benchmarks, we embarked on a study to
critically assess 23 state-of-the-art LLM benchmarks, using our novel unified
evaluation framework through the lenses of people, process, and technology,
under the pillars of functionality and security. Our research uncovered
significant limitations, including biases, difficulties in measuring genuine
reasoning, adaptability, implementation inconsistencies, prompt engineering
complexity, evaluator diversity, and the overlooking of cultural and
ideological norms in one comprehensive assessment. Our discussions emphasized
the urgent need for standardized methodologies, regulatory certainties, and
ethical guidelines in light of Artificial Intelligence (AI) advancements,
including advocating for an evolution from static benchmarks to dynamic
behavioral profiling to accurately capture LLMs' complex behaviors and
potential risks. Our study highlighted the necessity for a paradigm shift in
LLM evaluation methodologies, underlining the importance of collaborative
efforts for the development of universally accepted benchmarks and the
enhancement of AI systems' integration into society.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09881" title="Abstract">arXiv:2402.09881</a> [<a href="/pdf/2402.09881" title="Download PDF">pdf</a>, <a href="/format/2402.09881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Kernel Clustering via Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fleissner%2C+M">Maximilian Fleissner</a>, 
<a href="/search/cs?searchtype=author&query=Vankadara%2C+L+C">Leena Chennuru Vankadara</a>, 
<a href="/search/cs?searchtype=author&query=Ghoshdastidar%2C+D">Debarghya Ghoshdastidar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite the growing popularity of explainable and interpretable machine
learning, there is still surprisingly limited work on inherently interpretable
clustering methods. Recently, there has been a surge of interest in explaining
the classic k-means algorithm, leading to efficient algorithms that approximate
k-means clusters using axis-aligned decision trees. However, interpretable
variants of k-means have limited applicability in practice, where more flexible
clustering methods are often needed to obtain useful partitions of the data. In
this work, we investigate interpretable kernel clustering, and propose
algorithms that construct decision trees to approximate the partitions induced
by kernel k-means, a nonlinear extension of k-means. We further build on
previous work on explainable k-means and demonstrate how a suitable choice of
features allows preserving interpretability without sacrificing approximation
guarantees on the interpretable model.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09882" title="Abstract">arXiv:2402.09882</a> [<a href="/pdf/2402.09882" title="Download PDF">pdf</a>, <a href="/format/2402.09882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variability Modeling of Products, Processes, and Resources in  Cyber-Physical Production Systems Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meixner%2C+K">Kristof Meixner</a>, 
<a href="/search/cs?searchtype=author&query=Feichtinger%2C+K">Kevin Feichtinger</a>, 
<a href="/search/cs?searchtype=author&query=Fadhlillah%2C+H+S">Hafiyyan Sayyid Fadhlillah</a>, 
<a href="/search/cs?searchtype=author&query=Greiner%2C+S">Sandra Greiner</a>, 
<a href="/search/cs?searchtype=author&query=Marcher%2C+H">Hannes Marcher</a>, 
<a href="/search/cs?searchtype=author&query=Rabiser%2C+R">Rick Rabiser</a>, 
<a href="/search/cs?searchtype=author&query=Biffl%2C+S">Stefan Biffl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Cyber-Physical Production Systems (CPPSs), such as automated car
manufacturing plants, execute a configurable sequence of production steps to
manufacture products from a product portfolio. In CPPS engineering, domain
experts start with manually determining feasible production step sequences and
resources based on implicit knowledge. This process is hard to reproduce and
highly inefficient. In this paper, we present the Extended Iterative Process
Sequence Exploration (eIPSE) approach to derive variability models for
products, processes, and resources from a domain-specific description. To
automate the integrated exploration and configuration process for a CPPS, we
provide a toolchain which automatically reduces the configuration space and
allows to generate CPPS artifacts, such as control code for resources. We
evaluate the approach with four real-world use cases, including the generation
of control code artifacts, and an observational user study to collect feedback
from engineers with different backgrounds. The results confirm the usefulness
of the eIPSE approach and accompanying prototype to straightforwardly configure
a desired CPPS.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09883" title="Abstract">arXiv:2402.09883</a> [<a href="/pdf/2402.09883" title="Download PDF">pdf</a>, <a href="/format/2402.09883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lester: rotoscope animation through video object segmentation and  tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tous%2C+R">Ruben Tous</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Multimedia (cs.MM)

</div>
<p class="mathjax">This article introduces Lester, a novel method to automatically synthetise
retro-style 2D animations from videos. The method approaches the challenge
mainly as an object segmentation and tracking problem. Video frames are
processed with the Segment Anything Model (SAM) and the resulting masks are
tracked through subsequent frames with DeAOT, a method of hierarchical
propagation for semi-supervised video object segmentation. The geometry of the
masks' contours is simplified with the Douglas-Peucker algorithm. Finally,
facial traits, pixelation and a basic shadow effect can be optionally added.
The results show that the method exhibits an excellent temporal consistency and
can correctly process videos with different poses and appearances, dynamic
shots, partial shots and diverse backgrounds. The proposed method provides a
more simple and deterministic approach than diffusion models based
video-to-video translation pipelines, which suffer from temporal consistency
problems and do not cope well with pixelated and schematic outputs. The method
is also much most practical than techniques based on 3D human pose estimation,
which require custom handcrafted 3D models and are very limited with respect to
the type of scenes they can process.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09886" title="Abstract">arXiv:2402.09886</a> [<a href="/pdf/2402.09886" title="Download PDF">pdf</a>, <a href="/format/2402.09886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Learning of Visual Robot Localization Using LED State  Prediction as a Pretext Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nava%2C+M">Mirko Nava</a>, 
<a href="/search/cs?searchtype=author&query=Carlotti%2C+N">Nicholas Carlotti</a>, 
<a href="/search/cs?searchtype=author&query=Crupi%2C+L">Luca Crupi</a>, 
<a href="/search/cs?searchtype=author&query=Palossi%2C+D">Daniele Palossi</a>, 
<a href="/search/cs?searchtype=author&query=Giusti%2C+A">Alessandro Giusti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a novel self-supervised approach for learning to visually localize
robots equipped with controllable LEDs. We rely on a few training samples
labeled with position ground truth and many training samples in which only the
LED state is known, whose collection is cheap. We show that using LED state
prediction as a pretext task significantly helps to learn the visual
localization end task. The resulting model does not require knowledge of LED
states during inference. We instantiate the approach to visual relative
localization of nano-quadrotors: experimental results show that using our
pretext task significantly improves localization accuracy (from 68.3% to 76.2%)
and outperforms alternative strategies, such as a supervised baseline, model
pre-training, and an autoencoding pretext task. We deploy our model aboard a
27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a
peer nano-drone. Our approach, relying on position labels for only 300 images,
yields a mean tracking error of 4.2 cm versus 11.9 cm of a supervised baseline
model trained without our pretext task. Videos and code of the proposed
approach are available at https://github.com/idsia-robotics/leds-as-pretext
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09891" title="Abstract">arXiv:2402.09891</a> [<a href="/pdf/2402.09891" title="Download PDF">pdf</a>, <a href="/format/2402.09891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictors from causal features do not generalize better to new domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nastl%2C+V+Y">Vivian Y. Nastl</a>, 
<a href="/search/cs?searchtype=author&query=Hardt%2C+M">Moritz Hardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study how well machine learning models trained on causal features
generalize across domains. We consider 16 prediction tasks on tabular datasets
covering applications in health, employment, education, social benefits, and
politics. Each dataset comes with multiple domains, allowing us to test how
well a model trained in one domain performs in another. For each prediction
task, we select features that have a causal influence on the target of
prediction. Our goal is to test the hypothesis that models trained on causal
features generalize better across domains. Without exception, we find that
predictors using all available features, regardless of causality, have better
in-domain and out-of-domain accuracy than predictors using causal features.
Moreover, even the absolute drop in accuracy from one domain to the other is no
better for causal predictors than for models that use all features. If the goal
is to generalize to new domains, practitioners might as well train the best
possible model on all available features.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09894" title="Abstract">arXiv:2402.09894</a> [<a href="/pdf/2402.09894" title="Download PDF">pdf</a>, <a href="/format/2402.09894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not Just Novelty: A Longitudinal Study on Utility and Customization of  AI Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Tao Long</a>, 
<a href="/search/cs?searchtype=author&query=Gero%2C+K+I">Katy Ilonka Gero</a>, 
<a href="/search/cs?searchtype=author&query=Chilton%2C+L+B">Lydia B. Chilton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Generative AI brings novel and impressive abilities to help people in
everyday tasks. There are many AI workflows that solve real and complex
problems by chaining AI outputs together with human interaction. Although there
is an undeniable lure of AI, it's uncertain how useful generative AI workflows
are after the novelty wears off. Additionally, tools built with generative AI
have the potential to be personalized and adapted quickly and easily, but do
users take advantage of the potential to customize? We conducted a three-week
longitudinal study with 12 users to understand the familiarization and
customization of generative AI tools for science communication. Our study
revealed that the familiarization phase lasts for 4.3 sessions, where users
explore the capabilities of the workflow and which aspects they find useful.
After familiarization, the perceived utility of the system is rated higher than
before, indicating that the perceived utility of AI is not just a novelty
effect. The increase in benefits mainly comes from end-users' ability to
customize prompts, and thus appropriate the system to their own needs. This
points to a future where generative AI systems can allow us to design for
appropriation.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09896" title="Abstract">arXiv:2402.09896</a> [<a href="/pdf/2402.09896" title="Download PDF">pdf</a>, <a href="/format/2402.09896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papazafeiropoulos%2C+A">Anastasios Papazafeiropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+H">Hanxiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Kourtessis%2C+P">Pandelis Kourtessis</a>, 
<a href="/search/cs?searchtype=author&query=Ratnarajah%2C+T">Tharmalingam Ratnarajah</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>, 
<a href="/search/cs?searchtype=author&query=Papavassiliou%2C+S">Symeon Papavassiliou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, accepted in IEEE TVT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Simultaneously transmitting and reflecting \textcolor{black}{reconfigurable
intelligent surface} (STAR-RIS) is a promising implementation of RIS-assisted
systems that enables full-space coverage. However, STAR-RIS as well as
conventional RIS suffer from the double-fading effect. Thus, in this paper, we
propose the marriage of active RIS and STAR-RIS, denoted as ASTARS for massive
multiple-input multiple-output (mMIMO) systems, and we focus on the energy
splitting (ES) and mode switching (MS) protocols. Compared to prior literature,
we consider the impact of correlated fading, and we rely our analysis on the
two timescale protocol, being dependent on statistical channel state
information (CSI). On this ground, we propose a channel estimation method for
ASTARS with reduced overhead that accounts for its architecture. Next, we
derive a \textcolor{black}{closed-form expression} for the achievable sum-rate
for both types of users in the transmission and reflection regions in a unified
approach with significant practical advantages such as reduced complexity and
overhead, which result in a lower number of required iterations for convergence
compared to an alternating optimization (AO) approach. Notably, we maximize
simultaneously the amplitudes, the phase shifts, and the active amplifying
coefficients of the ASTARS by applying the projected gradient ascent method
(PGAM). Remarkably, the proposed optimization can be executed at every several
coherence intervals that reduces the processing burden considerably.
Simulations corroborate the analytical results, provide insight into the
effects of fundamental variables on the sum achievable SE, and present the
superiority of 16 ASTARS compared to passive STAR-RIS for a practical number of
surface elements.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09897" title="Abstract">arXiv:2402.09897</a> [<a href="/pdf/2402.09897" title="Download PDF">pdf</a>, <a href="/format/2402.09897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web  Application for Classifying COVID-19 Discussions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bishal%2C+M+M">Mahathir Mohammad Bishal</a>, 
<a href="/search/cs?searchtype=author&query=Chowdory%2C+M+R+H">Md. Rakibul Hassan Chowdory</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Anik Das</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+A">Muhammad Ashad Kabir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The COVID-19 pandemic has had adverse effects on both physical and mental
health. During this pandemic, numerous studies have focused on gaining insights
into health-related perspectives from social media. In this study, our primary
objective is to develop a machine learning-based web application for
automatically classifying COVID-19-related discussions on social media. To
achieve this, we label COVID-19-related Twitter data, provide benchmark
classification results, and develop a web application. We collected data using
the Twitter API and labeled a total of 6,667 tweets into five different
classes: health risks, prevention, symptoms, transmission, and treatment. We
extracted features using various feature extraction methods and applied them to
seven different traditional machine learning algorithms, including Decision
Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest
Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep
learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we
achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.
The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing
other traditional machine learning approaches. Our study not only contributes
to the field of health-related data analysis but also provides a valuable
resource in the form of a web-based tool for efficient data classification,
which can aid in addressing public health challenges and increasing awareness
during pandemics. We made the dataset and application publicly available, which
can be downloaded from this link
https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09898" title="Abstract">arXiv:2402.09898</a> [<a href="/pdf/2402.09898" title="Download PDF">pdf</a>, <a href="/ps/2402.09898" title="Download PostScript">ps</a>, <a href="/format/2402.09898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic construction of locally repairable codes with multiple  recovering sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Singsong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chaoping Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Locally repairable codes have been extensively investigated due to practical
applications in distributed and cloud storage systems in recent years. However,
not much work on asymptotic behavior of locally repairable codes has been done.
In particular, there is few result on constructive lower bound of asymptotic
behavior of locally repairable codes with multiple recovering sets. In this
paper, we construct some families of asymptotically good locally repairable
codes with multiple recovering sets via automorphism groups of function fields
of the Garcia-Stichtenoth towers. The main advantage of our construction is to
allow more flexibility of localities.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09900" title="Abstract">arXiv:2402.09900</a> [<a href="/pdf/2402.09900" title="Download PDF">pdf</a>, <a href="/format/2402.09900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Recurrent Reinforcement Learning with Memory Monoids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morad%2C+S">Steven Morad</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kortvelesy%2C+R">Ryan Kortvelesy</a>, 
<a href="/search/cs?searchtype=author&query=Liwicki%2C+S">Stephan Liwicki</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Prorok%2C+A">Amanda Prorok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In RL, memory models such as RNNs and transformers address Partially
Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent
Markov states. Neither model scales particularly well to long sequences,
especially compared to an emerging class of memory models sometimes called
linear recurrent models. We discover that the recurrent update of these models
is a monoid, leading us to formally define a novel memory monoid framework. We
revisit the traditional approach to batching in recurrent RL, highlighting both
theoretical and empirical deficiencies. Leveraging the properties of memory
monoids, we propose a new batching method that improves sample efficiency,
increases the return, and simplifies the implementation of recurrent loss
functions in RL.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09906" title="Abstract">arXiv:2402.09906</a> [<a href="/pdf/2402.09906" title="Download PDF">pdf</a>, <a href="/format/2402.09906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Representational Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongjin Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amanpreet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Kiela%2C+D">Douwe Kiela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages (15 main), 25 figures, 33 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09910" title="Abstract">arXiv:2402.09910</a> [<a href="/pdf/2402.09910" title="Download PDF">pdf</a>, <a href="/format/2402.09910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DE-COP: Detecting Copyrighted Content in Language Models Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duarte%2C+A+V">Andr&#xe9; V. Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+A+L">Arlindo L. Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">How can we detect if copyrighted content was used in the training process of
a language model, considering that the training data is typically undisclosed?
We are motivated by the premise that a language model is likely to identify
verbatim excerpts from its training text. We propose DE-COP, a method to
determine whether a piece of copyrighted content was included in training.
DE-COP's core approach is to probe an LLM with multiple-choice questions, whose
options include both verbatim text and their paraphrases. We construct
BookTection, a benchmark with excerpts from 165 books published prior and
subsequent to a model's training cutoff, along with their paraphrases. Our
experiments show that DE-COP surpasses the prior best method by 9.6% in
detection performance (AUC) on models with logits available. Moreover, DE-COP
also achieves an average accuracy of 72% for detecting suspect books on fully
black-box models where prior methods give $\approx$ 4% accuracy. Our code and
datasets are available at https://github.com/avduarte333/DE-COP_Method
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09911" title="Abstract">arXiv:2402.09911</a> [<a href="/pdf/2402.09911" title="Download PDF">pdf</a>, <a href="/format/2402.09911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Models with Pseudo- and Multisource- Knowledge  Graphs for Open-ended Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mitigating the hallucinations of Large Language Models (LLMs) and enhancing
them is a crucial task. Although some existing methods employ model
self-enhancement techniques, they fall short of effectively addressing unknown
factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails
to address the generalization across different KG sources and the enhancement
of open-ended answer questions simultaneously. To tackle these limitations,
there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge
Verification proposed. The enhancement of LLM using KG in an open-ended
question-answering setting is implemented by leveraging the Pseudo-Graph
Generation. Atomic Knowledge Verification utilizes atomic-level knowledge
querying and verification to achieve generalizability under different KG
sources. Compared to the baseline, this approach yields a minimum improvement
of 11.5 in the ROUGE-L score for open-ended questions. For precise questions,
we observe a minimum accuracy improvement of 7.5. Moreover, there is also
demonstration that this framework exhibits generalizability across different KG
sources. In summary, our results pave the way for enhancing LLMs by
incorporating Pseudo- and Multisource-KGs, particularly in the context of
open-ended questions.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09912" title="Abstract">arXiv:2402.09912</a> [<a href="/pdf/2402.09912" title="Download PDF">pdf</a>, <a href="/ps/2402.09912" title="Download PostScript">ps</a>, <a href="/format/2402.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient implementation of MPC for tracking using ADMM by decoupling  its semi-banded structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gracia%2C+V">Victor Gracia</a>, 
<a href="/search/eess?searchtype=author&query=Krupa%2C+P">Pablo Krupa</a>, 
<a href="/search/eess?searchtype=author&query=Limon%2C+D">Daniel Limon</a>, 
<a href="/search/eess?searchtype=author&query=Alamo%2C+T">Teodoro Alamo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Model Predictive Control (MPC) for tracking formulation presents numerous
advantages compared to standard MPC, such as a larger domain of attraction and
recursive feasibility even when abrupt changes in the reference are produced.
As a drawback, it includes some extra decision variables in its related
optimization problem, leading to a semi-banded structure that differs from the
banded structure encountered in standard MPC. This semi-banded structure
prevents the direct use of the efficient algorithms available for banded
problems. To address this issue, we present an algorithm based on the
alternating direction method of multipliers that explicitly takes advantage of
the underlying semi-banded structure of the MPC for tracking.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09916" title="Abstract">arXiv:2402.09916</a> [<a href="/pdf/2402.09916" title="Download PDF">pdf</a>, <a href="/format/2402.09916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BUSTER: a &quot;BUSiness Transaction Entity Recognition&quot; dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zugarini%2C+A">Andrea Zugarini</a>, 
<a href="/search/cs?searchtype=author&query=Zamai%2C+A">Andrew Zamai</a>, 
<a href="/search/cs?searchtype=author&query=Ernandes%2C+M">Marco Ernandes</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023), Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Albeit Natural Language Processing has seen major breakthroughs in the last
few years, transferring such advances into real-world business cases can be
challenging. One of the reasons resides in the displacement between popular
benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data
and long documents often affect real problems in vertical domains such as
finance, law and health. To support industry-oriented research, we present
BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists
of 3779 manually annotated documents on financial transactions. We establish
several baselines exploiting both general-purpose and domain-specific language
models. The best performing model is also used to automatically annotate 6196
documents, which we release as an additional silver corpus to BUSTER.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09919" title="Abstract">arXiv:2402.09919</a> [<a href="/pdf/2402.09919" title="Download PDF">pdf</a>, <a href="/format/2402.09919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Road Graph Generator: Mapping roads at construction sites from GPS data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Micha%C5%82owska%2C+K">Katarzyna Micha&#x142;owska</a>, 
<a href="/search/cs?searchtype=author&query=Holmestad%2C+H+M+B">Helga Margrete Bodahl Holmestad</a>, 
<a href="/search/cs?searchtype=author&query=Riemer-S%C3%B8rensen%2C+S">Signe Riemer-S&#xf8;rensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We present a method for road inference from GPS trajectories to map
construction sites. This task introduces a unique challenge due to the erratic
and non-standard movement patterns of construction machinery, which diverge
significantly from typical vehicular traffic on established roads. Our method
first identifies intersections in the road network that serve as critical
decision points, and later connects them with edges, producing a graph, which
subsequently can be used for planning and task-allocation. We demonstrate the
effectiveness of our approach by mapping roads at a real-life construction site
in Norway.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09921" title="Abstract">arXiv:2402.09921</a> [<a href="/pdf/2402.09921" title="Download PDF">pdf</a>, <a href="/format/2402.09921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and modelling cognitive biases in mobility choices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conrad%2C+C">Chloe Conrad</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+C">Carole Adam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> M1 internship report from Univ. Lyon 1 Claude Bernard. Internship was from October 2022 to June 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This report presents results from an M1 internship dedicated to agent-based
modelling and simulation of daily mobility choices. This simulation is intended
to be realistic enough to serve as a basis for a serious game about the
mobility transition. In order to ensure this level of realism, we conducted a
survey to measure if real mobility choices are made rationally, or how biased
they are. Results analysed here show that various biases could play a role in
decisions. We then propose an implementation in a GAMA agent-based simulation.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09923" title="Abstract">arXiv:2402.09923</a> [<a href="/pdf/2402.09923" title="Download PDF">pdf</a>, <a href="/format/2402.09923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dataset of Open-Domain Question Answering with Multiple-Span Answers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhiyi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shuyun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Ying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+W">Wentao Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-span answer extraction, also known as the task of multi-span question
answering (MSQA), is critical for real-world applications, as it requires
extracting multiple pieces of information from a text to answer complex
questions. Despite the active studies and rapid progress in English MSQA
research, there is a notable lack of publicly available MSQA benchmark in
Chinese. Previous efforts for constructing MSQA datasets predominantly
emphasized entity-centric contextualization, resulting in a bias towards
collecting factoid questions and potentially overlooking questions requiring
more detailed descriptive responses. To overcome these limitations, we present
CLEAN, a comprehensive Chinese multi-span question answering dataset that
involves a wide range of open-domain subjects with a substantial number of
instances requiring descriptive answers. Additionally, we provide established
models from relevant literature as baselines for CLEAN. Experimental results
and analysis show the characteristics and challenge of the newly proposed CLEAN
dataset for the community. Our dataset, CLEAN, will be publicly released at
zhiyiluo.site/misc/clean_v1.0_ sample.json.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09925" title="Abstract">arXiv:2402.09925</a> [<a href="/pdf/2402.09925" title="Download PDF">pdf</a>, <a href="/ps/2402.09925" title="Download PostScript">ps</a>, <a href="/format/2402.09925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Role Models in Software Practitioners&#x27; Career: An  Interview Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Gord%C3%B3n%2C+M">Mary S&#xe1;nchez-Gord&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Colomo-Palacios%2C+R">Ricardo Colomo-Palacios</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+A+S">Alex Sanchez Gordon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 Tables. To appear in CHASE 2024: Proceedings of the 17th International Conference on Cooperative and Human Aspects of Software Engineering, April 14-15, 2024, Lisbon, Portugal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 317th International Conference on
  Cooperative and Human Aspects of Software Engineering (CHASE 2024).
  Association for Computing Machinery, New York, NY, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">A role model is a person who serves as an example for others to follow,
especially in terms of values, behavior, achievements, and personal
characteristics. In this paper, authors study how role models influence
software practitioners careers, an aspect not studied in the literature before.
By means of this study, authors aim to understand if there are any salient role
model archetypes and what characteristics are valued by participants in their
role models. To do so, authors use a thematic coding approach to analyze the
data collected from interviewing ten Latin American software practitioners.
Findings reveal that role models were perceived as sources of knowledge, yet
the majority of participants, regardless of their career stage, displayed a
stronger interest in the human side and the moral values that their role models
embodied. This study also shows that any practitioner can be viewed as a role
model.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09934" title="Abstract">arXiv:2402.09934</a> [<a href="/pdf/2402.09934" title="Download PDF">pdf</a>, <a href="/format/2402.09934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paying Attention to Deflections: Mining Pragmatic Nuances for  Whataboutism Detection in Online Discourse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phi%2C+K">Khiem Phi</a>, 
<a href="/search/cs?searchtype=author&query=Faramarzi%2C+N+S">Noushin Salek Faramarzi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenlu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+R">Ritwik Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Whataboutism, a potent tool for disrupting narratives and sowing distrust,
remains under-explored in quantitative NLP research. Moreover, past work has
not distinguished its use as a strategy for misinformation and propaganda from
its use as a tool for pragmatic and semantic framing. We introduce new datasets
from Twitter and YouTube, revealing overlaps as well as distinctions between
whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on
recent work in linguistic semantics, we differentiate the `what about' lexical
construct from whataboutism. Our experiments bring to light unique challenges
in its accurate detection, prompting the introduction of a novel method using
attention weights for negative sample mining. We report significant
improvements of 4% and 10% over previous state-of-the-art methods in our
Twitter and YouTube collections, respectively.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09937" title="Abstract">arXiv:2402.09937</a> [<a href="/pdf/2402.09937" title="Download PDF">pdf</a>, <a href="/format/2402.09937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Evaluation of Evolving Highly Nonlinear Boolean Functions  in Odd Sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carlet%2C+C">Claude Carlet</a>, 
<a href="/search/cs?searchtype=author&query=%C3%90urasevic%2C+M">Marko &#xd0;urasevic</a>, 
<a href="/search/cs?searchtype=author&query=Jakobovic%2C+D">Domagoj Jakobovic</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>, 
<a href="/search/cs?searchtype=author&query=Mariot%2C+L">Luca Mariot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2311.11881">arXiv:2311.11881</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Boolean functions are mathematical objects used in diverse applications.
Different applications also have different requirements, making the research on
Boolean functions very active. In the last 30 years, evolutionary algorithms
have been shown to be a strong option for evolving Boolean functions in
different sizes and with different properties. Still, most of those works
consider similar settings and provide results that are mostly interesting from
the evolutionary algorithm's perspective. This work considers the problem of
evolving highly nonlinear Boolean functions in odd sizes. While the problem
formulation sounds simple, the problem is remarkably difficult, and the related
work is extremely scarce. We consider three solutions encodings and four
Boolean function sizes and run a detailed experimental analysis. Our results
show that the problem is challenging, and finding optimal solutions is
impossible except for the smallest tested size. However, once we added local
search to the evolutionary algorithm, we managed to find a Boolean function in
nine inputs with nonlinearity 241, which, to our knowledge, had never been
accomplished before with evolutionary algorithms.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09939" title="Abstract">arXiv:2402.09939</a> [<a href="/pdf/2402.09939" title="Download PDF">pdf</a>, <a href="/ps/2402.09939" title="Download PostScript">ps</a>, <a href="/format/2402.09939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI in the Construction Industry: A State-of-the-art Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taiwo%2C+R">Ridwan Taiwo</a>, 
<a href="/search/cs?searchtype=author&query=Bello%2C+I+T">Idris Temitope Bello</a>, 
<a href="/search/cs?searchtype=author&query=Abdulai%2C+S+F">Sulemana Fatoama Abdulai</a>, 
<a href="/search/cs?searchtype=author&query=Yussif%2C+A">Abdul-Mugis Yussif</a>, 
<a href="/search/cs?searchtype=author&query=Salami%2C+B+A">Babatunde Abiodun Salami</a>, 
<a href="/search/cs?searchtype=author&query=Saka%2C+A">Abdullahi Saka</a>, 
<a href="/search/cs?searchtype=author&query=Zayed%2C+T">Tarek Zayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, 11 figures, 20 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09941" title="Abstract">arXiv:2402.09941</a> [<a href="/pdf/2402.09941" title="Download PDF">pdf</a>, <a href="/format/2402.09941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedLion: Faster Adaptive Federated Optimization with Fewer Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhiwei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+T">Tsung-Hui Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In Federated Learning (FL), a framework to train machine learning models
across distributed data, well-known algorithms like FedAvg tend to have slow
convergence rates, resulting in high communication costs during training. To
address this challenge, we introduce FedLion, an adaptive federated
optimization algorithm that seamlessly incorporates key elements from the
recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into
the FL framework. Through comprehensive evaluations on two widely adopted FL
benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art
adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover,
thanks to the use of signed gradients in local training, FedLion substantially
reduces data transmission requirements during uplink communication when
compared to existing adaptive algorithms, further reducing communication costs.
Last but not least, this work also includes a novel theoretical analysis,
showcasing that FedLion attains faster convergence rate than established FL
algorithms like FedAvg.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09944" title="Abstract">arXiv:2402.09944</a> [<a href="/pdf/2402.09944" title="Download PDF">pdf</a>, <a href="/format/2402.09944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loopy-SLAM: Dense Neural SLAM with Loop Closures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liso%2C+L">Lorenzo Liso</a>, 
<a href="/search/cs?searchtype=author&query=Sandstr%C3%B6m%2C+E">Erik Sandstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Yugay%2C+V">Vladimir Yugay</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Oswald%2C+M+R">Martin R. Oswald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural RGBD SLAM techniques have shown promise in dense Simultaneous
Localization And Mapping (SLAM), yet face challenges such as error accumulation
during camera tracking resulting in distorted maps. In response, we introduce
Loopy-SLAM that globally optimizes poses and the dense 3D model. We use
frame-to-model tracking using a data-driven point-based submap generation
method and trigger loop closures online by performing global place recognition.
Robust pose graph optimization is used to rigidly align the local submaps. As
our representation is point based, map corrections can be performed efficiently
without the need to store the entire history of input frames used for mapping
as typically required by methods employing a grid based mapping structure.
Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet
datasets demonstrate competitive or superior performance in tracking, mapping,
and rendering accuracy when compared to existing dense neural RGBD SLAM
methods. Project page: notchla.github.io/Loopy-SLAM.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09947" title="Abstract">arXiv:2402.09947</a> [<a href="/pdf/2402.09947" title="Download PDF">pdf</a>, <a href="/format/2402.09947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Probabilistic Models with Distributional Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franceschi%2C+L">Luca Franceschi</a>, 
<a href="/search/cs?searchtype=author&query=Donini%2C+M">Michele Donini</a>, 
<a href="/search/cs?searchtype=author&query=Archambeau%2C+C">C&#xe9;dric Archambeau</a>, 
<a href="/search/cs?searchtype=author&query=Seeger%2C+M">Matthias Seeger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code coming soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A large branch of explainable machine learning is grounded in cooperative
game theory. However, research indicates that game-theoretic explanations may
mislead or be hard to interpret. We argue that often there is a critical
mismatch between what one wishes to explain (e.g. the output of a classifier)
and what current methods such as SHAP explain (e.g. the scalar probability of a
class). This paper addresses such gap for probabilistic models by generalising
cooperative games and value operators. We introduce the distributional values,
random variables that track changes in the model output (e.g. flipping of the
predicted class) and derive their analytic expressions for games with Gaussian,
Bernoulli and Categorical payoffs. We further establish several characterising
properties, and show that our framework provides fine-grained and insightful
explanations with case studies on vision and language models.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09949" title="Abstract">arXiv:2402.09949</a> [<a href="/pdf/2402.09949" title="Download PDF">pdf</a>, <a href="/format/2402.09949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Word Tokenization for Sequence Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gee%2C+L">Leonidas Gee</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>, 
<a href="/search/cs?searchtype=author&query=Ernandes%2C+M">Marco Ernandes</a>, 
<a href="/search/cs?searchtype=author&query=Zugarini%2C+A">Andrea Zugarini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing: Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models have proven highly successful at modelling a variety of
tasks. However, this comes at a steep computational cost that hinders wider
industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer
that goes beyond word boundaries by representing frequent multi-word
expressions as single tokens. MWTs produce a more compact and efficient
tokenization that yields two benefits: (1) Increase in performance due to a
greater coverage of input data given a fixed sequence length and budget; (2)
Faster and lighter inference due to the ability to reduce the sequence length
with negligible drops in performance. Our results show that MWT is more robust
across shorter sequence lengths, thus allowing for major speedups via early
sequence truncation.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09951" title="Abstract">arXiv:2402.09951</a> [<a href="/pdf/2402.09951" title="Download PDF">pdf</a>, <a href="/ps/2402.09951" title="Download PostScript">ps</a>, <a href="/format/2402.09951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strict width for Constraint Satisfaction Problems over homogeneous  strucures of finite duality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagy%2C+T">Tom&#xe1;&#x161; Nagy</a>, 
<a href="/search/cs?searchtype=author&query=Pinsker%2C+M">Michael Pinsker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We investigate the `local consistency implies global consistency' principle
of strict width among structures within the scope of the Bodirsky-Pinsker
dichotomy conjecture for infinite-domain Constraint Satisfaction Problems
(CSPs). Our main result implies that for certain CSP templates within the scope
of that conjecture, having bounded strict width has a concrete consequence on
the expressive power of the template called implicational simplicity. This in
turn yields an explicit bound on the relational width of the CSP, i.e., the
amount of local consistency needed to ensure the satisfiability of any
instance. Our result applies to first-order expansions of any homogeneous
$k$-uniform hypergraph, but more generally to any CSP template under the
assumption of finite duality and general abstract conditions mainly on its
automorphism group. In particular, it overcomes the restriction to binary
signatures in the pioneering work of Wrona.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09954" title="Abstract">arXiv:2402.09954</a> [<a href="/pdf/2402.09954" title="Download PDF">pdf</a>, <a href="/format/2402.09954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+J">Jiashu Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yajing Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Ling Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Q">Qian Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yongzhu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tangjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rongsheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Previous in-context learning (ICL) research has focused on tasks such as
classification, machine translation, text2table, etc., while studies on whether
ICL can improve human-like dialogue generation are scarce. Our work fills this
gap by systematically investigating the ICL capabilities of large language
models (LLMs) in persona-based dialogue generation, conducting extensive
experiments on high-quality real human Chinese dialogue datasets. From
experimental results, we draw three conclusions: 1) adjusting prompt
instructions is the most direct, effective, and economical way to improve
generation quality; 2) randomly retrieving demonstrations (demos) achieves the
best results, possibly due to the greater diversity and the amount of effective
information; counter-intuitively, retrieving demos with a context identical to
the query performs the worst; 3) even when we destroy the multi-turn
associations and single-turn semantics in the demos, increasing the number of
demos still improves dialogue performance, proving that LLMs can learn from
corrupted dialogue demos. Previous explanations of the ICL mechanism, such as
$n$-gram induction head, cannot fully account for this phenomenon.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09957" title="Abstract">arXiv:2402.09957</a> [<a href="/pdf/2402.09957" title="Download PDF">pdf</a>, <a href="/format/2402.09957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Designing Features for Condition Monitoring of Rotating Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maurya%2C+S">Seetaram Maurya</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+N+K">Nishchal K. Verma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Various methods for designing input features have been proposed for fault
recognition in rotating machines using one-dimensional raw sensor data. The
available methods are complex, rely on empirical approaches, and may differ
depending on the condition monitoring data used. Therefore, this article
proposes a novel algorithm to design input features that unifies the feature
extraction process for different time-series sensor data. This new insight for
designing/extracting input features is obtained through the lens of histogram
theory. The proposed algorithm extracts discriminative input features, which
are suitable for a simple classifier to deep neural network-based classifiers.
The designed input features are given as input to the classifier with
end-to-end training in a single framework for machine conditions recognition.
The proposed scheme has been validated through three real-time datasets: a)
acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The
real-time results and comparative study show the effectiveness of the proposed
scheme for the prediction of the machine's health states.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09959" title="Abstract">arXiv:2402.09959</a> [<a href="/pdf/2402.09959" title="Download PDF">pdf</a>, <a href="/format/2402.09959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based Federated Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jujia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), with their advanced contextual understanding
abilities, have demonstrated considerable potential in enhancing recommendation
systems via fine-tuning methods. However, fine-tuning requires users' behavior
data, which poses considerable privacy risks due to the incorporation of
sensitive user information. The unintended disclosure of such data could
infringe upon data protection laws and give rise to ethical issues. To mitigate
these privacy issues, Federated Learning for Recommendation (Fed4Rec) has
emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based
recommendation presents two main challenges: first, an increase in the
imbalance of performance across clients, affecting the system's efficiency over
time, and second, a high demand on clients' computational and storage resources
for local training and inference of LLMs.
<br />To address these challenges, we introduce a Privacy-Preserving LLM-based
Recommendation (PPLR) framework. The PPLR framework employs two primary
strategies. First, it implements a dynamic balance strategy, which involves the
design of dynamic parameter aggregation and adjustment of learning speed for
different clients during the training phase, to ensure relatively balanced
performance across all clients. Second, PPLR adopts a flexible storage
strategy, selectively retaining certain sensitive layers of the language model
on the client side while offloading non-sensitive layers to the server. This
approach aims to preserve user privacy while efficiently saving computational
and storage resources. Experimental results demonstrate that PPLR not only
achieves a balanced performance among clients but also enhances overall system
performance in a manner that is both computationally and storage-efficient,
while effectively protecting user privacy.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09961" title="Abstract">arXiv:2402.09961</a> [<a href="/pdf/2402.09961" title="Download PDF">pdf</a>, <a href="/ps/2402.09961" title="Download PostScript">ps</a>, <a href="/format/2402.09961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through  Dynamic Shift Extensions: A Deep Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saleh%2C+Z">Zead Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Hanbali%2C+A+A">Ahmad Al Hanbali</a>, 
<a href="/search/cs?searchtype=author&query=Baubaid%2C+A">Ahmad Baubaid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Crowdsourced delivery platforms face complex scheduling challenges to match
couriers and customer orders. We consider two types of crowdsourced couriers,
namely, committed and occasional couriers, each with different compensation
schemes. Crowdsourced delivery platforms usually schedule committed courier
shifts based on predicted demand. Therefore, platforms may devise an offline
schedule for committed couriers before the planning period. However, due to the
unpredictability of demand, there are instances where it becomes necessary to
make online adjustments to the offline schedule. In this study, we focus on the
problem of dynamically adjusting the offline schedule through shift extensions
for committed couriers. This problem is modeled as a sequential decision
process. The objective is to maximize platform profit by determining the shift
extensions of couriers and the assignments of requests to couriers. To solve
the model, a Deep Q-Network (DQN) learning approach is developed. Comparing
this model with the baseline policy where no extensions are allowed
demonstrates the benefits that platforms can gain from allowing shift
extensions in terms of reward, reduced lost order costs, and lost requests.
Additionally, sensitivity analysis showed that the total extension compensation
increases in a nonlinear manner with the arrival rate of requests, and in a
linear manner with the arrival rate of occasional couriers. On the compensation
sensitivity, the results showed that the normal scenario exhibited the highest
average number of shift extensions and, consequently, the fewest average number
of lost requests. These findings serve as evidence of the successful learning
of such dynamics by the DQN algorithm.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09962" title="Abstract">arXiv:2402.09962</a> [<a href="/pdf/2402.09962" title="Download PDF">pdf</a>, <a href="/format/2402.09962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViGEO: an Assessment of Vision GNNs in Earth Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colomba%2C+L">Luca Colomba</a>, 
<a href="/search/cs?searchtype=author&query=Garza%2C+P">Paolo Garza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SSTDM 2023 workshop, held in conjunction with ICDM 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Satellite missions and Earth Observation (EO) systems represent fundamental
assets for environmental monitoring and the timely identification of
catastrophic events, long-term monitoring of both natural resources and
human-made assets, such as vegetation, water bodies, forests as well as
buildings. Different EO missions enables the collection of information on
several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus,
given the recent advances of machine learning, computer vision and the
availability of labeled data, researchers demonstrated the feasibility and the
precision of land-use monitoring systems and remote sensing image
classification through the use of deep neural networks. Such systems may help
domain experts and governments in constant environmental monitoring, enabling
timely intervention in case of catastrophic events (e.g., forest wildfire in a
remote area). Despite the recent advances in the field of computer vision, many
works limit their analysis on Convolutional Neural Networks (CNNs) and, more
recently, to vision transformers (ViTs). Given the recent successes of Graph
Neural Networks (GNNs) on non-graph data, such as time-series and images, we
investigate the performances of a recent Vision GNN architecture (ViG) applied
to the task of land cover classification. The experimental results show that
ViG achieves state-of-the-art performances in multiclass and multilabel
classification contexts, surpassing both ViT and ResNet on large-scale
benchmarks.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09963" title="Abstract">arXiv:2402.09963</a> [<a href="/pdf/2402.09963" title="Download PDF">pdf</a>, <a href="/format/2402.09963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why are Sensitive Functions Hard for Transformers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+M">Michael Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Rofin%2C+M">Mark Rofin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Empirical studies have identified a range of learnability biases and
limitations of transformers, such as a persistent difficulty in learning to
compute simple formal languages such as PARITY, and a bias towards low-degree
functions. However, theoretical understanding remains limited, with existing
expressiveness theory either overpredicting or underpredicting realistic
learning abilities. We prove that, under the transformer architecture, the loss
landscape is constrained by the input-space sensitivity: Transformers whose
output is sensitive to many parts of the input string inhabit isolated points
in parameter space, leading to a low-sensitivity bias in generalization. We
show theoretically and empirically that this theory unifies a broad array of
empirical observations about the learning abilities and biases of transformers,
such as their generalization bias towards low sensitivity and low degree, and
difficulty in length generalization for PARITY. This shows that understanding
transformers' inductive biases requires studying not just their in-principle
expressivity, but also their loss landscape.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09965" title="Abstract">arXiv:2402.09965</a> [<a href="/pdf/2402.09965" title="Download PDF">pdf</a>, <a href="/ps/2402.09965" title="Download PostScript">ps</a>, <a href="/format/2402.09965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchy Representation of Data in Machine Learnings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yegang%2C+H">Han Yegang</a>, 
<a href="/search/cs?searchtype=author&query=Minjun%2C+P">Park Minjun</a>, 
<a href="/search/cs?searchtype=author&query=Duwon%2C+B">Byun Duwon</a>, 
<a href="/search/cs?searchtype=author&query=Inkyu%2C+P">Park Inkyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">When there are models with clear-cut judgment results for several data
points, it is possible that most models exhibit a relationship where if they
correctly judge one target, they also correctly judge another target.
Conversely, if most models incorrectly judge one target, they may also
incorrectly judge another target. We propose a method for visualizing this
hierarchy among targets. This information is expected to be beneficial for
model improvement.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09966" title="Abstract">arXiv:2402.09966</a> [<a href="/pdf/2402.09966" title="Download PDF">pdf</a>, <a href="/format/2402.09966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Textual Localization: Decomposing Multi-concept Images for  Subject-Driven Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shentu%2C+J">Junjie Shentu</a>, 
<a href="/search/cs?searchtype=author&query=Watson%2C+M">Matthew Watson</a>, 
<a href="/search/cs?searchtype=author&query=Moubayed%2C+N+A">Noura Al Moubayed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Subject-driven text-to-image diffusion models empower users to tailor the
model to new concepts absent in the pre-training dataset using a few sample
images. However, prevalent subject-driven models primarily rely on
single-concept input images, facing challenges in specifying the target concept
when dealing with multi-concept input images. To this end, we introduce a
textual localized text-to-image model (Texual Localization) to handle
multi-concept input images. During fine-tuning, our method incorporates a novel
cross-attention guidance to decompose multiple concepts, establishing distinct
connections between the visual representation of the target concept and the
identifier token in the text prompt. Experimental results reveal that our
method outperforms or performs comparably to the baseline models in terms of
image fidelity and image-text alignment on multi-concept input images. In
comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I
scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85%
higher in single-concept and multi-concept generation, respectively. Notably,
our method generates cross-attention maps consistent with the target concept in
the generated images, a capability absent in existing models.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09967" title="Abstract">arXiv:2402.09967</a> [<a href="/pdf/2402.09967" title="Download PDF">pdf</a>, <a href="/ps/2402.09967" title="Download PostScript">ps</a>, <a href="/format/2402.09967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Case Study: Testing Model Capabilities in Some Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Takumi%2C+S">Sato Takumi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jack Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) excel in generating personalized content and
facilitating interactive dialogues, showcasing their remarkable aptitude for a
myriad of applications. However, their capabilities in reasoning and providing
explainable outputs, especially within the context of reasoning abilities,
remain areas for improvement. In this study, we delve into the reasoning
abilities of LLMs, highlighting the current challenges and limitations that
hinder their effectiveness in complex reasoning scenarios.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09970" title="Abstract">arXiv:2402.09970</a> [<a href="/pdf/2402.09970" title="Download PDF">pdf</a>, <a href="/format/2402.09970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Parallel Sampling of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhiwei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiasheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+T">Tsung-Hui Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models have emerged as state-of-the-art generative models for image
generation. However, sampling from diffusion models is usually time-consuming
due to the inherent autoregressive nature of their sampling process. In this
work, we propose a novel approach that accelerates the sampling of diffusion
models by parallelizing the autoregressive process. Specifically, we
reformulate the sampling process as solving a system of triangular nonlinear
equations through fixed-point iteration. With this innovative formulation, we
explore several systematic techniques to further reduce the iteration steps
required by the solving process. Applying these techniques, we introduce
ParaTAA, a universal and training-free parallel sampling algorithm that can
leverage extra computational and memory resources to increase the sampling
speed. Our experiments demonstrate that ParaTAA can decrease the inference
steps required by common sequential sampling algorithms such as DDIM and DDPM
by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM
for Stable Diffusion, a widely-used text-to-image diffusion model, it can
produce the same images as the sequential sampling in only 7 inference steps.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09971" title="Abstract">arXiv:2402.09971</a> [<a href="/pdf/2402.09971" title="Download PDF">pdf</a>, <a href="/format/2402.09971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Vertex Integrity Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanaka%2C+T">Tesshu Hanaka</a>, 
<a href="/search/cs?searchtype=author&query=Lampis%2C+M">Michael Lampis</a>, 
<a href="/search/cs?searchtype=author&query=Vasilakis%2C+M">Manolis Vasilakis</a>, 
<a href="/search/cs?searchtype=author&query=Yoshiwatari%2C+K">Kanae Yoshiwatari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Vertex integrity is a graph parameter that measures the connectivity of a
graph. Informally, its meaning is that a graph has small vertex integrity if it
has a small separator whose removal disconnects the graph into connected
components which are themselves also small. Graphs with low vertex integrity
are extremely structured; this renders many hard problems tractable and has
recently attracted interest in this notion from the parameterized complexity
community. In this paper we revisit the NP-complete problem of computing the
vertex integrity of a given graph from the point of view of structural
parameterizations. We present a number of new results, which also answer some
recently posed open questions from the literature. Specifically: We show that
unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show
that the problem remains W[1]-hard if we parameterize by feedback edge set size
(via a reduction from a Bin Packing variant which may be of independent
interest); and complementing this we show that the problem is FPT by max-leaf
number. Furthermore, for weighted vertex integrity, we show that the problem
admits a single-exponential FPT algorithm parameterized by vertex cover or by
modular width, the latter result improving upon a previous algorithm which
required weights to be polynomially bounded.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09973" title="Abstract">arXiv:2402.09973</a> [<a href="/pdf/2402.09973" title="Download PDF">pdf</a>, <a href="/format/2402.09973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSTEM: A Cognitive Platform for Collecting Cyber Threat Intelligence in  the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+P">Prasasthy Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Nazari%2C+S">Sadaf Nazari</a>, 
<a href="/search/cs?searchtype=author&query=Kholgh%2C+D+K">Danial Khosh Kholgh</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoodi%2C+A">Alireza Mahmoodi</a>, 
<a href="/search/cs?searchtype=author&query=Seby%2C+J">Justin Seby</a>, 
<a href="/search/cs?searchtype=author&query=Kostakos%2C+P">Panos Kostakos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The extraction of cyber threat intelligence (CTI) from open sources is a
rapidly expanding defensive strategy that enhances the resilience of both
Information Technology (IT) and Operational Technology (OT) environments
against large-scale cyber-attacks. While previous research has focused on
improving individual components of the extraction process, the community lacks
open-source platforms for deploying streaming CTI data pipelines in the wild.
To address this gap, the study describes the implementation of an efficient and
well-performing platform capable of processing compute-intensive data pipelines
based on the cloud computing paradigm for real-time detection, collecting, and
sharing CTI from different online sources. We developed a prototype platform
(TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy,
Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index
IOCs in the wild. Moreover, the provisioning, monitoring, and management of the
TSTEM platform are achieved through infrastructure as a code (IaC). Custom
focus crawlers collect web content, which is then processed by a first-level
classifier to identify potential indicators of compromise (IOCs). If deemed
relevant, the content advances to a second level of extraction for further
examination. Throughout this process, state-of-the-art NLP models are utilized
for classification and entity extraction, enhancing the overall IOC extraction
methodology. Our experimental results indicate that these models exhibit high
accuracy (exceeding 98%) in the classification and extraction tasks, achieving
this performance within a time frame of less than a minute. The effectiveness
of our system can be attributed to a finely-tuned IOC extraction method that
operates at multiple stages, ensuring precise identification of relevant
information with low false positives.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09974" title="Abstract">arXiv:2402.09974</a> [<a href="/pdf/2402.09974" title="Download PDF">pdf</a>, <a href="/ps/2402.09974" title="Download PostScript">ps</a>, <a href="/format/2402.09974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference Mitigation for Network-Level ISAC: An Optimization  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongfang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xianghao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, and the relevant simulation code can be found at <a href="https://dongfang-xu.github.io/homepage/code/Two_cases.zip">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Future wireless networks are envisioned to simultaneously provide high
data-rate communication and ubiquitous environment-aware services for numerous
users. One promising approach to meet this demand is to employ network-level
integrated sensing and communications (ISAC) by jointly designing the signal
processing and resource allocation over the entire network. However, to unleash
the full potential of network-level ISAC, some critical challenges must be
tackled. Among them, interference management is one of the most significant
ones. In this article, we build up a bridge between interference mitigation
techniques and the corresponding optimization methods, which facilitates
efficient interference mitigation in network-level ISAC systems. In particular,
we first identify several types of interference in network-level ISAC systems,
including self-interference, mutual interference, crosstalk, clutter, and
multiuser interference. Then, we present several promising techniques that can
be utilized to suppress specific types of interference. For each type of
interference, we discuss the corresponding problem formulation and identify the
associated optimization methods. Moreover, to illustrate the effectiveness of
the proposed interference mitigation techniques, two concrete network-level
ISAC systems, namely coordinated cellular network-based and distributed
antenna-based ISAC systems, are investigated from interference management
perspective. Experiment results indicate that it is beneficial to
collaboratively employ different interference mitigation techniques and
leverage the network structure to achieve the full potential of network-level
ISAC. Finally, we highlight several promising future research directions for
the design of ISAC systems.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09977" title="Abstract">arXiv:2402.09977</a> [<a href="/pdf/2402.09977" title="Download PDF">pdf</a>, <a href="/format/2402.09977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Vocabulary Transfer for Language Model Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gee%2C+L">Leonidas Gee</a>, 
<a href="/search/cs?searchtype=author&query=Zugarini%2C+A">Andrea Zugarini</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>, 
<a href="/search/cs?searchtype=author&query=Torroni%2C+P">Paolo Torroni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2022): Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-world business applications require a trade-off between language model
performance and size. We propose a new method for model compression that relies
on vocabulary transfer. We evaluate the method on various vertical domains and
downstream tasks. Our results indicate that vocabulary transfer can be
effectively used in combination with other compression techniques, yielding a
significant reduction in model size and inference time while marginally
compromising on performance.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09982" title="Abstract">arXiv:2402.09982</a> [<a href="/pdf/2402.09982" title="Download PDF">pdf</a>, <a href="/ps/2402.09982" title="Download PostScript">ps</a>, <a href="/format/2402.09982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation and Transfer Learning Approaches Applied to Facial  Expressions Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Randellini%2C+E">Enrico Randellini</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>, 
<a href="/search/cs?searchtype=author&query=Sacca%27%2C+C">Claudio Sacca&#x27;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 11th International Conference on Artificial Intelligence, Soft Computing and Applications (AIAA 2021)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceeding of the 11th International Conference on Artificial
  Intelligence, Soft Computing and Applications (AIAA 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The face expression is the first thing we pay attention to when we want to
understand a person's state of mind. Thus, the ability to recognize facial
expressions in an automatic way is a very interesting research field. In this
paper, because the small size of available training datasets, we propose a
novel data augmentation technique that improves the performances in the
recognition task. We apply geometrical transformations and build from scratch
GAN models able to generate new synthetic images for each emotion type. Thus,
on the augmented datasets we fine tune pretrained convolutional neural networks
with different architectures. To measure the generalization ability of the
models, we apply extra-database protocol approach, namely we train models on
the augmented versions of training dataset and test them on two different
databases. The combination of these techniques allows to reach average accuracy
values of the order of 85\% for the InceptionResNetV2 model.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09984" title="Abstract">arXiv:2402.09984</a> [<a href="/pdf/2402.09984" title="Download PDF">pdf</a>, <a href="/format/2402.09984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetry-Breaking Augmentations for Ad Hoc Teamwork
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammond%2C+R">Ravi Hammond</a>, 
<a href="/search/cs?searchtype=author&query=Craggs%2C+D">Dustin Craggs</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mingyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Reid%2C+I">Ian Reid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently in review for ICML 2024. 16 pages (including references and appendix), 9 Figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In many collaborative settings, artificial intelligence (AI) agents must be
able to adapt to new teammates that use unknown or previously unobserved
strategies. While often simple for humans, this can be challenging for AI
agents. For example, if an AI agent learns to drive alongside others (a
training set) that only drive on one side of the road, it may struggle to adapt
this experience to coordinate with drivers on the opposite side, even if their
behaviours are simply flipped along the left-right symmetry. To address this we
introduce symmetry-breaking augmentations (SBA), which increases diversity in
the behaviour of training teammates by applying a symmetry-flipping operation.
By learning a best-response to the augmented set of teammates, our agent is
exposed to a wider range of behavioural conventions, improving performance when
deployed with novel teammates. We demonstrate this experimentally in two
settings, and show that our approach improves upon previous ad hoc teamwork
results in the challenging card game Hanabi. We also propose a general metric
for estimating symmetry-dependency amongst a given set of policies.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09989" title="Abstract">arXiv:2402.09989</a> [<a href="/pdf/2402.09989" title="Download PDF">pdf</a>, <a href="/format/2402.09989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Bridges: Reformulating Grounded Multimodal Named Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Di Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenkun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal
task that aims to identify named entities, entity types and their corresponding
visual regions. GMNER task exhibits two challenging properties: 1) The weak
correlation between image-text pairs in social media results in a significant
portion of named entities being ungroundable. 2) There exists a distinction
between coarse-grained referring expressions commonly used in similar tasks
(e.g., phrase localization, referring expression comprehension) and
fine-grained named entities. In this paper, we propose RiVEG, a unified
framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging
large language models (LLMs) as a connecting bridge. This reformulation brings
two benefits: 1) It maintains the optimal MNER performance and eliminates the
need for employing object detection methods to pre-extract regional features,
thereby naturally addressing two major limitations of existing GMNER methods.
2) The introduction of entity expansion expression and Visual Entailment (VE)
Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables
RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding
capabilities of any current or prospective multimodal pretraining models.
Extensive experiments demonstrate that RiVEG outperforms state-of-the-art
methods on the existing GMNER dataset and achieves absolute leads of 10.65%,
6.21%, and 8.83% in all three subtasks.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09990" title="Abstract">arXiv:2402.09990</a> [<a href="/pdf/2402.09990" title="Download PDF">pdf</a>, <a href="/format/2402.09990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIAViz: A Browser-based Visualization Tool for Computational Pathology  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Eastwood%2C+M">Mark Eastwood</a>, 
<a href="/search/eess?searchtype=author&query=Pocock%2C+J">John Pocock</a>, 
<a href="/search/eess?searchtype=author&query=Jahanifar%2C+M">Mostafa Jahanifar</a>, 
<a href="/search/eess?searchtype=author&query=Shephard%2C+A">Adam Shephard</a>, 
<a href="/search/eess?searchtype=author&query=Habib%2C+S">Skiros Habib</a>, 
<a href="/search/eess?searchtype=author&query=Alzaid%2C+E">Ethar Alzaid</a>, 
<a href="/search/eess?searchtype=author&query=Alsalemi%2C+A">Abdullah Alsalemi</a>, 
<a href="/search/eess?searchtype=author&query=Robertus%2C+J+L">Jan Lukas Robertus</a>, 
<a href="/search/eess?searchtype=author&query=Rajpoot%2C+N">Nasir Rajpoot</a>, 
<a href="/search/eess?searchtype=author&query=Raza%2C+S">Shan Raza</a>, 
<a href="/search/eess?searchtype=author&query=Minhas%2C+F">Fayyaz Minhas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Application note to be submitted to bioinformatics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Digital pathology has gained significant traction in modern healthcare
systems. This shift from optical microscopes to digital imagery brings with it
the potential for improved diagnosis, efficiency, and the integration of AI
tools into the pathologists workflow. A critical aspect of this is
visualization. Throughout the development of a machine learning (ML) model in
digital pathology, it is crucial to have flexible, openly available tools to
visualize models, from their outputs and predictions to the underlying
annotations and images used to train or test a model. We introduce TIAViz, a
Python-based visualization tool built into TIAToolbox which allows flexible,
interactive, fully zoomable overlay of a wide variety of information onto whole
slide images, including graphs, heatmaps, segmentations, annotations and other
WSIs. The UI is browser-based, allowing use either locally, on a remote
machine, or on a server to provide publicly available demos. This tool is open
source and is made available at:
https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation
(pip install tiatoolbox) and conda as part of TIAToolbox.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09992" title="Abstract">arXiv:2402.09992</a> [<a href="/pdf/2402.09992" title="Download PDF">pdf</a>, <a href="/format/2402.09992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning  under Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enders%2C+T">Tobias Enders</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+J">James Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+M">Maximilian Schiffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We study the robustness of deep reinforcement learning algorithms against
distribution shifts within contextual multi-stage stochastic combinatorial
optimization problems from the operations research domain. In this context,
risk-sensitive algorithms promise to learn robust policies. While this field is
of general interest to the reinforcement learning community, most studies
up-to-date focus on theoretical results rather than real-world performance.
With this work, we aim to bridge this gap by formally deriving a novel
risk-sensitive deep reinforcement learning algorithm while providing numerical
evidence for its efficacy. Specifically, we introduce discrete Soft
Actor-Critic for the entropic risk measure by deriving a version of the Bellman
equation for the respective Q-values. We establish a corresponding policy
improvement result and infer a practical algorithm. We introduce an environment
that represents typical contextual multi-stage stochastic combinatorial
optimization problems and perform numerical experiments to empirically validate
our algorithm's robustness against realistic distribution shifts, without
compromising performance on the training distribution. We show that our
algorithm is superior to risk-neutral Soft Actor-Critic as well as to two
benchmark approaches for robust deep reinforcement learning. Thereby, we
provide the first structured analysis on the robustness of reinforcement
learning under distribution shifts in the realm of contextual multi-stage
stochastic combinatorial optimization problems.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09993" title="Abstract">arXiv:2402.09993</a> [<a href="/pdf/2402.09993" title="Download PDF">pdf</a>, <a href="/format/2402.09993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalability limitations of Kademlia DHTs when enabling Data Availability  Sampling in Ethereum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cortes-Goicoechea%2C+M">Mikel Cortes-Goicoechea</a>, 
<a href="/search/cs?searchtype=author&query=Kiraly%2C+C">Csaba Kiraly</a>, 
<a href="/search/cs?searchtype=author&query=Ryajov%2C+D">Dmitriy Ryajov</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz-Tapia%2C+J+L">Jose Luis Mu&#xf1;oz-Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Bautista-Gomez%2C+L">Leonardo Bautista-Gomez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Scalability in blockchain remains a significant challenge, especially when
prioritizing decentralization and security. The Ethereum community has proposed
comprehensive data-sharding techniques to overcome storage, computational, and
network processing limitations. In this context, the propagation and
availability of large blocks become the subject of research to achieve scalable
data-sharding. This paper provides insights after exploring the usage of a
Kademlia-based DHT to enable Data Availability Sampling (DAS) in Ethereum. It
presents a DAS-DHT simulator to study this problem and validates the results of
the simulator with experiments in a real DHT network, IPFS. Our results help us
understand what parts of DAS can be achieved based on existing Kademlia DHT
solutions and which ones cannot. We discuss the limitations of DHT solutions
and discuss other alternatives.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09994" title="Abstract">arXiv:2402.09994</a> [<a href="/pdf/2402.09994" title="Download PDF">pdf</a>, <a href="/ps/2402.09994" title="Download PostScript">ps</a>, <a href="/format/2402.09994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Competitive Equilibrium by Nash Welfare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+J">Jugal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yixin Tao</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A9gh%2C+L+A">L&#xe1;szl&#xf3; A. V&#xe9;gh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We explore the relationship between two popular concepts on allocating
divisible items: competitive equilibrium (CE) and allocations with maximum Nash
welfare, i.e., allocations where the weighted geometric mean of the utilities
is maximal. When agents have homogeneous concave utility functions, these two
concepts coincide: the classical Eisenberg-Gale convex program that maximizes
Nash welfare over feasible allocations yields a competitive equilibrium.
However, these two concepts diverge for non-homogeneous utilities. From a
computational perspective, maximizing Nash welfare amounts to solving a convex
program for any concave utility functions, computing CE becomes PPAD-hard
already for separable piecewise linear concave (SPLC) utilities.
<br />We introduce the concept of Gale-substitute utility functions, an analogue of
the weak gross substitutes (WGS) property for the so-called Gale demand system.
For Gale-substitutes utilities, we show that any allocation maximizing Nash
welfare provides an approximate-CE with surprisingly strong guarantees, where
every agent gets at least half the maximum utility they can get at any CE, and
is approximately envy-free. Gale-substitutes include examples of utilities
where computing CE is PPAD hard: in particular, all separable concave
utilities, and the previously studied non-separable class of Leontief-free
utilities. We introduce a new, general class of utility functions called
generalized network utilities based on the generalized flow model; this class
includes SPLC and Leontief-free utilities. We show that all such utilities are
Gale-substitutes.
<br />Conversely, although some agents may get much higher utility at a Nash
welfare maximizing allocation than at a CE, we show a price of anarchy type
result: for general concave utilities, every CE achieves at least $(1/e)^{1/e}
&gt; 0.69$ fraction of the maximum Nash welfare, and this factor is tight.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09995" title="Abstract">arXiv:2402.09995</a> [<a href="/pdf/2402.09995" title="Download PDF">pdf</a>, <a href="/format/2402.09995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iJTyper: An Iterative Type Inference Framework for Java by Integrating  Constraint- and Statistically-based Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anji Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Neng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianguo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Inferring the types of API elements in incomplete code snippets (e.g., those
on Q&amp;A forums) is a prepositive step required to work with the code snippets.
Existing type inference methods can be mainly categorized as constraint-based
or statistically-based. The former imposes higher requirements on code syntax
and often suffers from low recall due to the syntactic limitation of code
snippets. The latter relies on the statistical regularities learned from a
training corpus and does not take full advantage of the type constraints in
code snippets, which may lead to low precision. In this paper, we propose an
iterative type inference framework for Java, called iJTyper, by integrating the
strengths of both constraint- and statistically-based methods. For a code
snippet, iJTyper first applies a constraint-based method and augments the code
context with the inferred types of API elements. iJTyper then applies a
statistically-based method to the augmented code snippet. The predicted
candidate types of API elements are further used to improve the
constraint-based method by reducing its pre-built knowledge base. iJTyper
iteratively executes both methods and performs code context augmentation and
knowledge base reduction until a termination condition is satisfied. Finally,
the final inference results are obtained by combining the results of both
methods. We evaluated iJTyper on two open-source datasets. Results show that 1)
iJTyper achieves high average precision/recall of 97.31% and 92.52% on both
datasets; 2) iJTyper significantly improves the recall of two state-of-the-art
baselines, SnR and MLMTyper, by at least 7.31% and 27.44%, respectively; and 3)
iJTyper improves the average precision/recall of the popular language model,
ChatGPT, by 3.25% and 0.51% on both datasets.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09997" title="Abstract">arXiv:2402.09997</a> [<a href="/pdf/2402.09997" title="Download PDF">pdf</a>, <a href="/format/2402.09997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed  Tasks in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Leilei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for
fine-tuning large language models (LLM). The modular and plug-and-play nature
of LoRA enables the integration of diverse domain-specific LoRAs to enhance the
capabilities of LLMs. Previous research on exploiting multiple LoRAs either
focuses on specific isolated downstream tasks or fixes the selection of LoRAs
during training. However, in real-world scenarios, LLMs receive diverse prompts
covering different tasks, and the pool of candidate LoRAs is often dynamically
updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose
framework that adaptively retrieves and composes multiple LoRAs according to
the input prompts. LoraRetriever contains three main components: firstly,
identifying and retrieving LoRAs relevant to the given input; secondly,
formulating strategies for effectively integrating the retrieved LoRAs; and
thirdly, developing efficient batch inference to accommodate heterogeneous
requests. Experimental results indicate that LoraRetriever consistently
outperforms the baselines, highlighting its practical effectiveness and
versatility.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10001" title="Abstract">arXiv:2402.10001</a> [<a href="/pdf/2402.10001" title="Download PDF">pdf</a>, <a href="/format/2402.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Attacks in Decentralized Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mrini%2C+A+E">Abdellah El Mrini</a>, 
<a href="/search/cs?searchtype=author&query=Cyffers%2C+E">Edwige Cyffers</a>, 
<a href="/search/cs?searchtype=author&query=Bellet%2C+A">Aur&#xe9;lien Bellet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Decentralized Gradient Descent (D-GD) allows a set of users to perform
collaborative learning without sharing their data by iteratively averaging
local model updates with their neighbors in a network graph. The absence of
direct communication between non-neighbor nodes might lead to the belief that
users cannot infer precise information about the data of others. In this work,
we demonstrate the opposite, by proposing the first attack against D-GD that
enables a user (or set of users) to reconstruct the private data of other users
outside their immediate neighborhood. Our approach is based on a reconstruction
attack against the gossip averaging protocol, which we then extend to handle
the additional challenges raised by D-GD. We validate the effectiveness of our
attack on real graphs and datasets, showing that the number of users
compromised by a single or a handful of attackers is often surprisingly large.
We empirically investigate some of the factors that affect the performance of
the attack, namely the graph topology, the number of attackers, and their
position in the graph.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10002" title="Abstract">arXiv:2402.10002</a> [<a href="/pdf/2402.10002" title="Download PDF">pdf</a>, <a href="/format/2402.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D  Point Cloud Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai-Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mofei Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">In perception, multiple sensory information is integrated to map visual
information from 2D views onto 3D objects, which is beneficial for
understanding in 3D environments. But in terms of a single 2D view rendered
from different angles, only limited partial information can be provided.The
richness and value of Multi-view 2D information can provide superior
self-supervised signals for 3D objects. In this paper, we propose a novel
self-supervised point cloud representation learning method, MM-Point, which is
driven by intra-modal and inter-modal similarity objectives. The core of
MM-Point lies in the Multi-modal interaction and transmission between 3D
objects and multiple 2D views at the same time. In order to more effectively
simultaneously perform the consistent cross-modal objective of 2D multi-view
information based on contrastive learning, we further propose Multi-MLP and
Multi-level Augmentation strategies. Through carefully designed transformation
strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point
demonstrates state-of-the-art (SOTA) performance in various downstream tasks.
For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset
ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,
comparable to fully supervised methods. Additionally, we demonstrate its
effectiveness in tasks such as few-shot classification, 3D part segmentation
and 3D semantic segmentation.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10005" title="Abstract">arXiv:2402.10005</a> [<a href="/pdf/2402.10005" title="Download PDF">pdf</a>, <a href="/format/2402.10005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal  Processing Analysis for Sounds, &amp; Strains Emerging Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+R">Ratul Ali</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+A">Aktarul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+M+S">Md. Shohel Rana</a>, 
<a href="/search/cs?searchtype=author&query=Nasrin%2C+S">Saila Nasrin</a>, 
<a href="/search/cs?searchtype=author&query=Shajol%2C+S+A">Sohel Afzal Shajol</a>, 
<a href="/search/cs?searchtype=author&query=Sadi%2C+P+D+A+H+M+S">Professor Dr. A.H.M. Saifullah Sadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, Article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Acoustic data serves as a fundamental cornerstone in advancing scientific and
engineering understanding across diverse disciplines, spanning biology,
communications, and ocean and Earth science. This inquiry meticulously explores
recent advancements and transformative potential within the domain of
acoustics, specifically focusing on machine learning (ML) and deep learning.
ML, comprising an extensive array of statistical techniques, proves
indispensable for autonomously discerning and leveraging patterns within data.
In contrast to traditional acoustics and signal processing, ML adopts a
data-driven approach, unveiling intricate relationships between features and
desired labels or actions, as well as among features themselves, given ample
training data. The application of ML to expansive sets of training data
facilitates the discovery of models elucidating complex acoustic phenomena such
as human speech and reverberation. The dynamic evolution of ML in acoustics
yields compelling results and holds substantial promise for the future. The
advent of electronic stethoscopes and analogous recording and data logging
devices has expanded the application of acoustic signal processing concepts to
the analysis of bowel sounds. This paper critically reviews existing literature
on acoustic signal processing for bowel sound analysis, outlining fundamental
approaches and applicable machine learning principles. It chronicles historical
progress in signal processing techniques that have facilitated the extraction
of valuable information from bowel sounds, emphasizing advancements in noise
reduction, segmentation, signal enhancement, feature extraction, sound
localization, and machine learning techniques...
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10009" title="Abstract">arXiv:2402.10009</a> [<a href="/pdf/2402.10009" title="Download PDF">pdf</a>, <a href="/format/2402.10009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manor%2C+H">Hila Manor</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Examples and code available in <a href="https://hilamanor.github.io/AudioEditing/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Editing signals using large pre-trained models, in a zero-shot manner, has
recently seen rapid advancements in the image domain. However, this wave has
yet to reach the audio domain. In this paper, we explore two zero-shot editing
techniques for audio signals, which use DDPM inversion on pre-trained diffusion
models. The first, adopted from the image domain, allows text-based editing.
The second, is a novel approach for discovering semantically meaningful editing
directions without supervision. When applied to music signals, this method
exposes a range of musically interesting modifications, from controlling the
participation of specific instruments to improvisations on the melody. Samples
can be found on our examples page in https://hilamanor.github.io/AudioEditing/
and code can be found in https://github.com/hilamanor/AudioEditing/ .
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10011" title="Abstract">arXiv:2402.10011</a> [<a href="/pdf/2402.10011" title="Download PDF">pdf</a>, <a href="/format/2402.10011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Group Equivariant Simplicial Message Passing Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Eijkelboom%2C+F">Floor Eijkelboom</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We introduce Clifford Group Equivariant Simplicial Message Passing Networks,
a method for steerable E(n)-equivariant message passing on simplicial
complexes. Our method integrates the expressivity of Clifford group-equivariant
layers with simplicial message passing, which is topologically more intricate
than regular graph message passing. Clifford algebras include higher-order
objects such as bivectors and trivectors, which express geometric features
(e.g., areas, volumes) derived from vectors. Using this knowledge, we represent
simplex features through geometric products of their vertices. To achieve
efficient simplicial message passing, we share the parameters of the message
network across different dimensions. Additionally, we restrict the final
message to an aggregation of the incoming messages from different dimensions,
leading to what we term shared simplicial message passing. Experimental results
show that our method is able to outperform both equivariant and simplicial
graph neural networks on a variety of geometric tasks.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10013" title="Abstract">arXiv:2402.10013</a> [<a href="/pdf/2402.10013" title="Download PDF">pdf</a>, <a href="/format/2402.10013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Empirical-Theoretical Gap in Neural Network Formal Language  Learning Using Minimum Description Length
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+N">Nur Lan</a>, 
<a href="/search/cs?searchtype=author&query=Chemla%2C+E">Emmanuel Chemla</a>, 
<a href="/search/cs?searchtype=author&query=Katzir%2C+R">Roni Katzir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 3 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Neural networks offer good approximation to many tasks but consistently fail
to reach perfect generalization, even when theoretical work shows that such
perfect solutions can be expressed by certain architectures. Using the task of
formal language learning, we focus on one simple formal language and show that
the theoretically correct solution is in fact not an optimum of commonly used
objectives -- even with regularization techniques that according to common
wisdom should lead to simple weights and good generalization (L1, L2) or other
meta-heuristics (early-stopping, dropout). However, replacing standard targets
with the Minimum Description Length objective (MDL) results in the correct
solution being an optimum.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10014" title="Abstract">arXiv:2402.10014</a> [<a href="/pdf/2402.10014" title="Download PDF">pdf</a>, <a href="/format/2402.10014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory Guidance: Enhanced Remote Driving of highly-automated  Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majstorovic%2C+D">Domagoj Majstorovic</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+S">Simon Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Diermeyer%2C+F">Frank Diermeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE Intelligent Vehicles Symposium (IV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Despite the rapid technological progress, autonomous vehicles still face a
wide range of complex driving situations that require human intervention.
Teleoperation technology offers a versatile and effective way to address these
challenges. The following work puts existing ideas into a modern context and
introduces a novel technical implementation of the trajectory guidance
teleoperation concept. The presented system was developed within a
high-fidelity simulation environment and experimentally validated,
demonstrating a realistic ride-hailing mission with prototype autonomous
vehicles and onboard passengers. The results indicate that the proposed concept
can be a viable alternative to the existing remote driving options, offering a
promising way to enhance teleoperation technology and improve overall operation
safety.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10015" title="Abstract">arXiv:2402.10015</a> [<a href="/pdf/2402.10015" title="Download PDF">pdf</a>, <a href="/ps/2402.10015" title="Download PostScript">ps</a>, <a href="/format/2402.10015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Piecewise Approach for the Analysis of Exact Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clinch%2C+K">Katie Clinch</a>, 
<a href="/search/cs?searchtype=author&query=Gaspers%2C+S">Serge Gaspers</a>, 
<a href="/search/cs?searchtype=author&query=Saffidine%2C+A">Abdallah Saffidine</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tiankuang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">To analyze the worst-case running time of branching algorithms, the majority
of work in exponential time algorithms focuses on designing complicated
branching rules over developing better analysis methods for simple algorithms.
In the mid-$2000$s, Fomin et al. [2005] introduced measure &amp; conquer, an
advanced general analysis method, sparking widespread adoption for obtaining
tighter worst-case running time upper bounds for many fundamental NP-complete
problems. Yet, much potential in this direction remains untapped, as most
subsequent work applied it without further advancement. Motivated by this, we
present piecewise analysis, a new general method that analyzes the running time
of branching algorithms. Our approach is to define a similarity ratio that
divides instances into groups and then analyze the running time within each
group separately. The similarity ratio is a scale between two parameters of an
instance I. Instead of relying on a single measure and a single analysis for
the whole instance space, our method allows to take advantage of different
intrinsic properties of instances with different similarity ratios. To showcase
its potential, we reanalyze two $17$-year-old algorithms from Fomin et al.
[2007] that solve $4$-Coloring and #$3$-Coloring respectively. The original
analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$
respectively for these algorithms, our analysis improves these running times to
$O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running
time $O(1.7215^n)$ is the first improvement in the best known running time for
the 4-Coloring problem since 2007.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10018" title="Abstract">arXiv:2402.10018</a> [<a href="/pdf/2402.10018" title="Download PDF">pdf</a>, <a href="/format/2402.10018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Stage Algorithm for Group Testing with Prior Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Portnoy%2C+A+C">Ayelet C. Portnoy</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Alejandro Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Quantitative Methods (q-bio.QM); Applications (stat.AP)

</div>
<p class="mathjax">In this paper, we propose an efficient multi-stage algorithm for non-adaptive
Group Testing (GT) with general correlated prior statistics. The proposed
solution can be applied to any correlated statistical prior represented in
trellis, e.g., finite state machines and Markov processes. We introduce a
variation of List Viterbi Algorithm (LVA) to enable accurate recovery using
much fewer tests than objectives, which efficiently gains from the correlated
prior statistics structure. Our numerical results demonstrate that the proposed
Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori
(MAP) performance with feasible complexity in practical regimes, such as with
COVID-19 and sparse signal recovery applications, and reduce in the scenarios
tested the number of pooled tests by at least $25\%$ compared to existing
classical low complexity GT algorithms. Moreover, we analytically characterize
the complexity of the proposed MSGT algorithm that guarantees its efficiency.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10021" title="Abstract">arXiv:2402.10021</a> [<a href="/pdf/2402.10021" title="Download PDF">pdf</a>, <a href="/format/2402.10021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAWEC: Sensing-Assisted Wireless Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haque%2C+K+F">Khandaker Foysal Haque</a>, 
<a href="/search/cs?searchtype=author&query=Meneghello%2C+F">Francesca Meneghello</a>, 
<a href="/search/cs?searchtype=author&query=Karim%2C+M+E">Md. Ebtidaul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at IEEE PerCom, Biarritz, France, March 11-15, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Emerging mobile virtual reality (VR) systems will require to continuously
perform complex computer vision tasks on ultra-high-resolution video frames
through the execution of deep neural networks (DNNs)-based algorithms. Since
state-of-the-art DNNs require computational power that is excessive for mobile
devices, techniques based on wireless edge computing (WEC) have been recently
proposed. However, existing WEC methods require the transmission and processing
of a high amount of video data which may ultimately saturate the wireless link.
In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing
(SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the
physical environment to reduce the end-to-end latency and overall computational
burden by transmitting to the edge server only the relevant data for the
delivery of the service. Our intuition is that the transmission of the portion
of the video frames where there are no changes with respect to previous frames
can be avoided. Specifically, we leverage wireless sensing techniques to
estimate the location of objects in the environment and obtain insights about
the environment dynamics. Hence, only the part of the frames where any
environmental change is detected is transmitted and processed. We evaluated
SAWEC by using a 10K 360$^{\circ}$ camera with a Wi-Fi 6 sensing system
operating at 160 MHz and performing localization and tracking. We perform
experiments in an anechoic chamber and a hall room with two human subjects in
six different setups. Experimental results show that SAWEC reduces the channel
occupation, and end-to-end latency by 93.81%, and 96.19% respectively while
improving the instance segmentation performance by 46.98% with respect to
state-of-the-art WEC approaches. For reproducibility purposes, we pledge to
share our whole dataset and code repository.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10022" title="Abstract">arXiv:2402.10022</a> [<a href="/pdf/2402.10022" title="Download PDF">pdf</a>, <a href="/ps/2402.10022" title="Download PostScript">ps</a>, <a href="/format/2402.10022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducing, Extending, and Analyzing Naming Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alpern%2C+R">Rachel Alpern</a>, 
<a href="/search/cs?searchtype=author&query=Lazer%2C+I">Ido Lazer</a>, 
<a href="/search/cs?searchtype=author&query=Tzachor%2C+I">Issar Tzachor</a>, 
<a href="/search/cs?searchtype=author&query=Hakim%2C+H">Hanit Hakim</a>, 
<a href="/search/cs?searchtype=author&query=Weissbuch%2C+S">Sapir Weissbuch</a>, 
<a href="/search/cs?searchtype=author&query=Feitelson%2C+D+G">Dror G. Feitelson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages with 10 figures and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Naming is very important in software development, as names are often the only
vehicle of meaning about what the code is intended to do. A recent study on how
developers choose names collected the names given by different developers for
the same objects. This enabled a study of these names' diversity and structure,
and the construction of a model of how names are created. We reproduce
different parts of this study in three independent experiments. Importantly, we
employ methodological variations rather than striving of an exact replication.
When the same results are obtained this then boosts our confidence in their
validity by demonstrating that they do not depend on the methodology.
<br />Our results indeed corroborate those of the original study in terms of the
diversity of names, the low probability of two developers choosing the same
name, and the finding that experienced developers tend to use slightly longer
names than inexperienced students. We explain name diversity by performing a
new analysis of the names, classifying the concepts represented in them as
universal (agreed upon), alternative (reflecting divergent views on a topic),
or optional (reflecting divergent opinions on whether to include this concept
at all). This classification enables new research directions concerning the
considerations involved in naming decisions. We also show that explicitly using
the model proposed in the original study to guide naming leads to the creation
of better names, whereas the simpler approach of just asking participants to
use longer and more detailed names does not.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10024" title="Abstract">arXiv:2402.10024</a> [<a href="/pdf/2402.10024" title="Download PDF">pdf</a>, <a href="/format/2402.10024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Augmented In-Context Learning for Unsupervised Word Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaoyiran Li</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 Pages, 3 Figures, 7 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown that, while large language models (LLMs) demonstrate
strong word translation or bilingual lexicon induction (BLI) capabilities in
few-shot setups, they still cannot match the performance of 'traditional'
mapping-based approaches in the unsupervised scenario where no seed translation
pairs are available, especially for lower-resource languages. To address this
challenge with LLMs, we propose self-augmented in-context learning (SAIL) for
unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a
set of high-confidence word translation pairs for in-context learning (ICL)
from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our
method shows substantial gains over zero-shot prompting of LLMs on two
established BLI benchmarks spanning a wide range of language pairs, also
outperforming mapping-based baselines across the board. In addition to
achieving state-of-the-art unsupervised BLI performance, we also conduct
comprehensive analyses on SAIL and discuss its limitations.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10028" title="Abstract">arXiv:2402.10028</a> [<a href="/pdf/2402.10028" title="Download PDF">pdf</a>, <a href="/format/2402.10028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models Meet Contextual Bandits with Large Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aouali%2C+I">Imad Aouali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Efficient exploration is a key challenge in contextual bandits due to the
large size of their action space, where uninformed exploration can result in
computational and statistical inefficiencies. Fortunately, the rewards of
actions are often correlated and this can be leveraged to explore them
efficiently. In this work, we capture such correlations using pre-trained
diffusion models; upon which we design diffusion Thompson sampling (dTS). Both
theoretical and algorithmic foundations are developed for dTS, and empirical
evaluation also shows its favorable performance.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10030" title="Abstract">arXiv:2402.10030</a> [<a href="/pdf/2402.10030" title="Download PDF">pdf</a>, <a href="/format/2402.10030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Literature Review of EM-SCA Attacks on Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zunaidi%2C+M+R">Muhammad Rusyaidi Zunaidi</a>, 
<a href="/search/cs?searchtype=author&query=Sayakkara%2C+A">Asanka Sayakkara</a>, 
<a href="/search/cs?searchtype=author&query=Scanlon%2C+M">Mark Scanlon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Cryptography is vital for data security, but cryptographic algorithms can
still be vulnerable to side-channel attacks (SCAs), physical assaults
exploiting power consumption and EM radiation. SCAs pose a significant threat
to cryptographic integrity, compromising device keys. While literature on SCAs
focuses on real-world devices, the rise of sophisticated devices necessitates
fresh approaches. Electromagnetic side-channel analysis (EM-SCA) gathers
information by monitoring EM radiation, capable of retrieving encryption keys
and detecting malicious activity. This study evaluates EM-SCA's impact on
encryption across scenarios and explores its role in digital forensics and law
enforcement. Addressing encryption susceptibility to EM-SCA can empower
forensic investigators in overcoming encryption challenges, maintaining their
crucial role in law enforcement. Additionally, the paper defines EM-SCA's
current state in attacking encryption, highlighting vulnerable and resistant
encryption algorithms and devices, and promising EM-SCA approaches. This study
offers a comprehensive analysis of EM-SCA in law enforcement and digital
forensics, suggesting avenues for further research.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10035" title="Abstract">arXiv:2402.10035</a> [<a href="/pdf/2402.10035" title="Download PDF">pdf</a>, <a href="/format/2402.10035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigation of Federated Learning Algorithms for Retinal Optical  Coherence Tomography Image Classification with Statistical Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amgain%2C+S">Sanskar Amgain</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+P">Prashant Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Bano%2C+S">Sophia Bano</a>, 
<a href="/search/cs?searchtype=author&query=del+Valle+Torres%2C+I">Ignacio del Valle Torres</a>, 
<a href="/search/cs?searchtype=author&query=Cunniffe%2C+M">Michael Cunniffe</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+V">Victor Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Beales%2C+P">Phil Beales</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Purpose: We apply federated learning to train an OCT image classifier
simulating a realistic scenario with multiple clients and statistical
heterogeneous data distribution where data in the clients lack samples of some
categories entirely.
<br />Methods: We investigate the effectiveness of FedAvg and FedProx to train an
OCT image classification model in a decentralized fashion, addressing privacy
concerns associated with centralizing data. We partitioned a publicly available
OCT dataset across multiple clients under IID and Non-IID settings and
conducted local training on the subsets for each client. We evaluated two
federated learning methods, FedAvg and FedProx for these settings.
<br />Results: Our experiments on the dataset suggest that under IID settings, both
methods perform on par with training on a central data pool. However, the
performance of both algorithms declines as we increase the statistical
heterogeneity across the client data, while FedProx consistently performs
better than FedAvg in the increased heterogeneity settings.
<br />Conclusion: Despite the effectiveness of federated learning in the
utilization of private data across multiple medical institutions, the large
number of clients and heterogeneous distribution of labels deteriorate the
performance of both algorithms. Notably, FedProx appears to be more robust to
the increased heterogeneity.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10036" title="Abstract">arXiv:2402.10036</a> [<a href="/pdf/2402.10036" title="Download PDF">pdf</a>, <a href="/format/2402.10036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Linear Online Tracking for Unknown Targets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tsiamis%2C+A">Anastasios Tsiamis</a>, 
<a href="/search/eess?searchtype=author&query=Karapetyan%2C+A">Aren Karapetyan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yueshan Li</a>, 
<a href="/search/eess?searchtype=author&query=Balta%2C+E+C">Efe C. Balta</a>, 
<a href="/search/eess?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we study the problem of online tracking in linear control
systems, where the objective is to follow a moving target. Unlike classical
tracking control, the target is unknown, non-stationary, and its state is
revealed sequentially, thus, fitting the framework of online non-stochastic
control. We consider the case of quadratic costs and propose a new algorithm,
called predictive linear online tracking (PLOT). The algorithm uses recursive
least squares with exponential forgetting to learn a time-varying dynamic model
of the target. The learned model is used in the optimal policy under the
framework of receding horizon control. We show the dynamic regret of PLOT
scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of
the target dynamics and $T$ is the time horizon. Unlike prior work, our
theoretical results hold for non-stationary targets. We implement PLOT on a
real quadrotor and provide open-source software, thus, showcasing one of the
first successful applications of online control methods on real hardware.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10038" title="Abstract">arXiv:2402.10038</a> [<a href="/pdf/2402.10038" title="Download PDF">pdf</a>, <a href="/format/2402.10038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization  Method for Alignment of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaki%2C+S">Saeed Khaki</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">JinJin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandra%2C+P">Prathap Ramachandra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) has been extensively
employed to align large language models with user intent. However, proximal
policy optimization (PPO) based RLHF is occasionally unstable requiring
significant hyperparameter finetuning, and computationally expensive to
maximize the estimated reward during alignment. Recently, direct preference
optimization (DPO) is proposed to address those challenges. However, DPO relies
on contrastive responses generated from human annotator and alternative LLM,
instead of the policy model, limiting the effectiveness of the RLHF. In this
paper, we addresses both challenges by systematically combining rejection
sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the
development of a supervised fine-tuned policy model (SFT). A varied set of k
responses per prompt are sampled directly from the SFT model. RS-DPO identifies
pairs of contrastive samples based on their reward distribution. Finally, we
apply DPO with the contrastive samples to align the model to human preference.
Our experiments indicate that our proposed method effectively fine-tunes LLMs
with limited resource environments, leading to improved alignment with user
intent. Furthermore, it outperforms existing methods, including RS, PPO, and
DPO.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10039" title="Abstract">arXiv:2402.10039</a> [<a href="/pdf/2402.10039" title="Download PDF">pdf</a>, <a href="/format/2402.10039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Accentuation: Revealing &#x27;What&#x27; Features Respond to in Natural  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamblin%2C+C">Chris Hamblin</a>, 
<a href="/search/cs?searchtype=author&query=Fel%2C+T">Thomas Fel</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Srijani Saha</a>, 
<a href="/search/cs?searchtype=author&query=Konkle%2C+T">Talia Konkle</a>, 
<a href="/search/cs?searchtype=author&query=Alvarez%2C+G">George Alvarez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Efforts to decode neural network vision models necessitate a comprehensive
grasp of both the spatial and semantic facets governing feature responses
within images. Most research has primarily centered around attribution methods,
which provide explanations in the form of heatmaps, showing where the model
directs its attention for a given feature. However, grasping 'where' alone
falls short, as numerous studies have highlighted the limitations of those
methods and the necessity to understand 'what' the model has recognized at the
focal point of its attention. In parallel, 'Feature visualization' offers
another avenue for interpreting neural network features. This approach
synthesizes an optimal image through gradient ascent, providing clearer
insights into 'what' features respond to. However, feature visualizations only
provide one global explanation per feature; they do not explain why features
activate for particular images. In this work, we introduce a new method to the
interpretability tool-kit, 'feature accentuation', which is capable of
conveying both where and what in arbitrary input images induces a feature's
response. At its core, feature accentuation is image-seeded (rather than
noise-seeded) feature visualization. We find a particular combination of
parameterization, augmentation, and regularization yields naturalistic
visualizations that resemble the seed image and target feature simultaneously.
Furthermore, we validate these accentuations are processed along a natural
circuit by the model. We make our precise implementation of feature
accentuation available to the community as the Faccent library, an extension of
Lucent.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10044" title="Abstract">arXiv:2402.10044</a> [<a href="/pdf/2402.10044" title="Download PDF">pdf</a>, <a href="/ps/2402.10044" title="Download PostScript">ps</a>, <a href="/format/2402.10044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Domain Generalizability of RF Fingerprints Through Multifractal  Dimension Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+B">Benjamin Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Hamdaoui%2C+B">Bechir Hamdaoui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE Conference on Communications and Network Security (CNS). arXiv admin note: substantial text overlap with <a href="/abs/2308.07925">arXiv:2308.07925</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">RF data-driven device fingerprinting through the use of deep learning has
recently surfaced as a possible method for enabling secure device
identification and authentication. Traditional approaches are commonly
susceptible to the domain adaptation problem where a model trained on data
collected under one domain performs badly when tested on data collected under a
different domain. Some examples of a domain change include varying the location
or environment of the device and varying the time or day of the data
collection. In this work, we propose using multifractal analysis and the
variance fractal dimension trajectory (VFDT) as a data representation input to
the deep neural network to extract device fingerprints that are domain
generalizable. We analyze the effectiveness of the proposed VFDT representation
in detecting device-specific signatures from hardware-impaired IQ (in-phase and
quadrature) signals, and we evaluate its robustness in real-world settings,
using an experimental testbed of 30 WiFi-enabled Pycom devices. Our
experimental results show that the proposed VFDT representation improves the
scalability, robustness and generalizability of the deep learning models
significantly compared to when using IQ data samples.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10045" title="Abstract">arXiv:2402.10045</a> [<a href="/pdf/2402.10045" title="Download PDF">pdf</a>, <a href="/ps/2402.10045" title="Download PostScript">ps</a>, <a href="/format/2402.10045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal  Neural Topic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiaheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Ruicheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yidong Chai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While short-form videos head to reshape the entire social media landscape,
experts are exceedingly worried about their depressive impacts on viewers, as
evidenced by medical studies. To prevent widespread consequences, platforms are
eager to predict these videos' impact on viewers' mental health. Subsequently,
they can take intervention measures, such as revising recommendation algorithms
and displaying viewer discretion. Nevertheless, applicable predictive methods
lack relevance to well-established medical knowledge, which outlines clinically
proven external and environmental factors of depression. To account for such
medical knowledge, we resort to an emergent methodological discipline, seeded
Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the
limitations of single-origin topics, unknown topic sources, unclear seed
supervision, and suboptimal convergence. To address those challenges, we
develop a novel Knowledge-guided Multimodal NTM to predict a short-form video's
depressive impact on viewers. Extensive empirical analyses using TikTok and
Douyin datasets prove that our method outperforms state-of-the-art benchmarks.
Our method also discovers medically relevant topics from videos that are linked
to depressive impact. We contribute to IS with a novel video analytics method
that is generalizable to other video classification problems. Practically, our
method can help platforms understand videos' mental impacts, thus adjusting
recommendations and video topic disclosure.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10046" title="Abstract">arXiv:2402.10046</a> [<a href="/pdf/2402.10046" title="Download PDF">pdf</a>, <a href="/format/2402.10046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Flawed is ECE? An Analysis via Logit Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chidambaram%2C+M">Muthu Chidambaram</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Holden Lee</a>, 
<a href="/search/cs?searchtype=author&query=McSwiggen%2C+C">Colin McSwiggen</a>, 
<a href="/search/cs?searchtype=author&query=Rezchikov%2C+S">Semon Rezchikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
<p class="mathjax">Informally, a model is calibrated if its predictions are correct with a
probability that matches the confidence of the prediction. By far the most
common method in the literature for measuring calibration is the expected
calibration error (ECE). Recent work, however, has pointed out drawbacks of
ECE, such as the fact that it is discontinuous in the space of predictors. In
this work, we ask: how fundamental are these issues, and what are their impacts
on existing results? Towards this end, we completely characterize the
discontinuities of ECE with respect to general probability measures on Polish
spaces. We then use the nature of these discontinuities to motivate a novel
continuous, easily estimated miscalibration metric, which we term
Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained
image classification models, we show in initial experiments that binned ECE
closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may
be avoidable in practice.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10050" title="Abstract">arXiv:2402.10050</a> [<a href="/pdf/2402.10050" title="Download PDF">pdf</a>, <a href="/format/2402.10050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Demand Myoelectric Control Using Wake Gestures to Eliminate False  Activations During Activities of Daily Living
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eddy%2C+E">Ethan Eddy</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+E">Evan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Bateman%2C+S">Scott Bateman</a>, 
<a href="/search/cs?searchtype=author&query=Scheme%2C+E">Erik Scheme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While myoelectric control has recently become a focus of increased research
as a possible flexible hands-free input modality, current control approaches
are prone to inadvertent false activations in real-world conditions. In this
work, a novel myoelectric control paradigm -- on-demand myoelectric control --
is proposed, designed, and evaluated, to reduce the number of unrelated muscle
movements that are incorrectly interpreted as input gestures . By leveraging
the concept of wake gestures, users were able to switch between a dedicated
control mode and a sleep mode, effectively eliminating inadvertent activations
during activities of daily living (ADLs). The feasibility of wake gestures was
demonstrated in this work through two online ubiquitous EMG control tasks with
varying difficulty levels; dismissing an alarm and controlling a robot. The
proposed control scheme was able to appropriately ignore almost all
non-targeted muscular inputs during ADLs (&gt;99.9%) while maintaining sufficient
sensitivity for reliable mode switching during intentional wake gesture
elicitation. These results highlight the potential of wake gestures as a
critical step towards enabling ubiquitous myoelectric control-based on-demand
input for a wide range of applications.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10051" title="Abstract">arXiv:2402.10051</a> [<a href="/pdf/2402.10051" title="Download PDF">pdf</a>, <a href="/format/2402.10051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SwissNYF: Tool Grounded LLM Agents for Black Box Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S+S">Somnath Sendhil Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+D">Dhruv Jain</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+E">Eshaan Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+R">Raunak Pandey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">While Large Language Models (LLMs) have demonstrated enhanced capabilities in
function-calling, these advancements primarily rely on accessing the functions'
responses. This methodology is practical for simpler APIs but faces scalability
issues with irreversible APIs that significantly impact the system, such as a
database deletion API. Similarly, processes requiring extensive time for each
API call and those necessitating forward planning, like automated action
pipelines, present complex challenges. Furthermore, scenarios often arise where
a generalized approach is needed because algorithms lack direct access to the
specific implementations of these functions or secrets to use them. Traditional
tool planning methods are inadequate in these cases, compelling the need to
operate within black-box environments. Unlike their performance in tool
manipulation, LLMs excel in black-box tasks, such as program synthesis.
Therefore, we harness the program synthesis capabilities of LLMs to strategize
tool usage in black-box settings, ensuring solutions are verified prior to
implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging
program synthesis for black box tool planning. Accompanied by SwissNYF, a
comprehensive suite that integrates black-box algorithms for planning and
verification tasks, addressing the aforementioned challenges and enhancing the
versatility and effectiveness of LLMs in complex API interactions. The public
code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10052" title="Abstract">arXiv:2402.10052</a> [<a href="/pdf/2402.10052" title="Download PDF">pdf</a>, <a href="/format/2402.10052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmemorization in Large Language Models via Self-Distillation and  Deliberate Imagination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y+R">Yijiang River Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongzhou Lin</a>, 
<a href="/search/cs?searchtype=author&query=Belkin%2C+M">Mikhail Belkin</a>, 
<a href="/search/cs?searchtype=author&query=Huerta%2C+R">Ramon Huerta</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While displaying impressive generation capabilities across many tasks, Large
Language Models (LLMs) still struggle with crucial issues of privacy violation
and unwanted exposure of sensitive data. This raises an essential question: how
should we prevent such undesired behavior of LLMs while maintaining their
strong generation and natural language understanding (NLU) capabilities? In
this work, we introduce a novel approach termed deliberate imagination in the
context of LLM unlearning. Instead of trying to forget memorized data, we
employ a self-distillation framework, guiding LLMs to deliberately imagine
alternative scenarios. As demonstrated in a wide range of experiments, the
proposed method not only effectively unlearns targeted text but also preserves
the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks.
Our results demonstrate the usefulness of this approach across different models
and sizes, and also with parameter-efficient fine-tuning, offering a novel
pathway to addressing the challenges with private and sensitive data in LLM
applications.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10053" title="Abstract">arXiv:2402.10053</a> [<a href="/pdf/2402.10053" title="Download PDF">pdf</a>, <a href="/format/2402.10053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using  Low-rank Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+S">Stefan Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+K">Kiran Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Gionis%2C+A">Aristides Gionis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at The WebConf 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Timeline algorithms are key parts of online social networks, but during
recent years they have been blamed for increasing polarization and disagreement
in our society. Opinion-dynamics models have been used to study a variety of
phenomena in online social networks, but an open question remains on how these
models can be augmented to take into account the fine-grained impact of
user-level timeline algorithms. We make progress on this question by providing
a way to model the impact of timeline algorithms on opinion dynamics.
Specifically, we show how the popular Friedkin--Johnsen opinion-formation model
can be augmented based on aggregate information, extracted from timeline data.
We use our model to study the problem of minimizing the polarization and
disagreement; we assume that we are allowed to make small changes to the users'
timeline compositions by strengthening some topics of discussion and penalizing
some others. We present a gradient descent-based algorithm for this problem,
and show that under realistic parameter settings, our algorithm computes a
$(1+\varepsilon)$-approximate solution in time $\tilde{O}(m\sqrt{n}
\lg(1/\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is
the number of vertices. We also present an algorithm that provably computes an
$\varepsilon$-approximation of our model in near-linear time. We evaluate our
method on real-world data and show that it effectively reduces the polarization
and disagreement in the network. Finally, we release an anonymized graph
dataset with ground-truth opinions and more than 27\,000 nodes (the previously
largest publicly available dataset contains less than 550 nodes).
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10058" title="Abstract">arXiv:2402.10058</a> [<a href="/pdf/2402.10058" title="Download PDF">pdf</a>, <a href="/format/2402.10058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Safer Large Language Models through Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+G">Guangyao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages in total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rapid advancement of Large Language Models (LLMs) has demonstrated their
vast potential across various domains, attributed to their extensive
pretraining knowledge and exceptional generalizability. However, LLMs often
encounter challenges in generating harmful content when faced with problematic
prompts. To address this problem, existing work attempted to implement a
gradient ascent based approach to prevent LLMs from producing harmful output.
While these methods can be effective, they frequently impact the model utility
in responding to normal prompts. To address this gap, we introduce Selective
Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,
designed to eliminate harmful knowledge while preserving utility on normal
prompts. Specifically, SKU is consisted of two stages: harmful knowledge
acquisition stage and knowledge negation stage. The first stage aims to
identify and acquire harmful knowledge within the model, whereas the second is
dedicated to remove this knowledge. SKU selectively isolates and removes
harmful knowledge in model parameters, ensuring the model's performance remains
robust on normal prompts. Our experiments conducted across various LLM
architectures demonstrate that SKU identifies a good balance point between
removing harmful information and preserving utility.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10059" title="Abstract">arXiv:2402.10059</a> [<a href="/pdf/2402.10059" title="Download PDF">pdf</a>, <a href="/format/2402.10059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial synchrony for free? New bounds for Byzantine agreement via a  generic transformation across network models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Civit%2C+P">Pierre Civit</a>, 
<a href="/search/cs?searchtype=author&query=Dzulfikar%2C+M+A">Muhammad Ayaz Dzulfikar</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S">Seth Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Komatovic%2C+J">Jovan Komatovic</a>, 
<a href="/search/cs?searchtype=author&query=Vidigueira%2C+M">Manuel Vidigueira</a>, 
<a href="/search/cs?searchtype=author&query=Zablotchi%2C+I">Igor Zablotchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Byzantine consensus allows n processes to decide on a common value, in spite
of arbitrary failures. The seminal Dolev-Reischuk bound states that any
deterministic solution to Byzantine consensus exchanges Omega(n^2) bits. In
recent years, great advances have been made in deterministic Byzantine
agreement for partially synchronous networks, with state-of-the-art
cryptographic solutions achieving O(n^2 \kappa) bits (where $\kappa$ is the
security parameter) and nearly matching the lower bound. In contrast, for
synchronous networks, optimal solutions with O(n^2) bits, with no cryptography
and the same failure tolerance, have been known for more than three decades.
Can this gap in network models be closed?
<br />In this paper, we present Repeater, the first generic transformation of
Byzantine agreement algorithms from synchrony to partial synchrony. Repeater is
modular, relying on existing and novel algorithms for its sub-modules. With the
right choice of modules, Repeater requires no additional cryptography, is
optimally resilient (n = 3t+1, where t is the maximum number of failures) and,
for constant-size inputs, preserves the worst-case per-process bit complexity
of the transformed synchronous algorithm. Leveraging Repeater, we present the
first partially synchronous algorithm that (1) achieves optimal bit complexity
(O(n^2) bits), (2) resists a computationally unbounded adversary (no
cryptography), and (3) is optimally-resilient (n = 3t+1), thus showing that the
Dolev-Reischuk bound is tight in partial synchrony. Moreover, we adapt Repeater
for long inputs, introducing several new algorithms with improved complexity
and weaker (or completely absent) cryptographic assumptions.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10061" title="Abstract">arXiv:2402.10061</a> [<a href="/pdf/2402.10061" title="Download PDF">pdf</a>, <a href="/format/2402.10061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-maps: Direct Depth Lookup for Event-based Structured Light Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morgenstern%2C+W">Wieland Morgenstern</a>, 
<a href="/search/cs?searchtype=author&query=Gard%2C+N">Niklas Gard</a>, 
<a href="/search/cs?searchtype=author&query=Baumann%2C+S">Simon Baumann</a>, 
<a href="/search/cs?searchtype=author&query=Hilsmann%2C+A">Anna Hilsmann</a>, 
<a href="/search/cs?searchtype=author&query=Eisert%2C+P">Peter Eisert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the CVPR 2023 Workshop on Event-based Vision: <a href="https://tub-rip.github.io/eventvision2023/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 4007-4015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a new approach to direct depth estimation for Spatial Augmented
Reality (SAR) applications using event cameras. These dynamic vision sensors
are a great fit to be paired with laser projectors for depth estimation in a
structured light approach. Our key contributions involve a conversion of the
projector time map into a rectified X-map, capturing x-axis correspondences for
incoming events and enabling direct disparity lookup without any additional
search. Compared to previous implementations, this significantly simplifies
depth estimation, making it more efficient, while the accuracy is similar to
the time map-based process. Moreover, we compensate non-linear temporal
behavior of cheap laser projectors by a simple time map calibration, resulting
in improved performance and increased depth estimation accuracy. Since depth
estimation is executed by two lookups only, it can be executed almost instantly
(less than 3 ms per frame with a Python implementation) for incoming events.
This allows for real-time interactivity and responsiveness, which makes our
approach especially suitable for SAR experiences where low latency, high frame
rates and direct feedback are crucial. We present valuable insights gained into
data transformed into X-maps and evaluate our depth from disparity estimation
against the state of the art time map-based results. Additional results and
code are available on our project page: https://fraunhoferhhi.github.io/X-maps/
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10062" title="Abstract">arXiv:2402.10062</a> [<a href="/pdf/2402.10062" title="Download PDF">pdf</a>, <a href="/format/2402.10062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zhihang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingyuan Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jieping Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. 19 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">For a machine learning model deployed in real world scenarios, the ability of
detecting out-of-distribution (OOD) samples is indispensable and challenging.
Most existing OOD detection methods focused on exploring advanced training
skills or training-free tricks to prevent the model from yielding overconfident
confidence score for unknown samples. The training-based methods require
expensive training cost and rely on OOD samples which are not always available,
while most training-free methods can not efficiently utilize the prior
information from the training data. In this work, we propose an
\textbf{O}ptimal \textbf{P}arameter and \textbf{N}euron \textbf{P}runing
(\textbf{OPNP}) approach, which aims to identify and remove those parameters
and neurons that lead to over-fitting. The main method is divided into two
steps. In the first step, we evaluate the sensitivity of the model parameters
and neurons by averaging gradients over all training samples. In the second
step, the parameters and neurons with exceptionally large or close to zero
sensitivities are removed for prediction. Our proposal is training-free,
compatible with other post-hoc methods, and exploring the information from all
training data. Extensive experiments are performed on multiple OOD detection
tasks and model architectures, showing that our proposed OPNP consistently
outperforms the existing methods by a large margin.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10063" title="Abstract">arXiv:2402.10063</a> [<a href="/pdf/2402.10063" title="Download PDF">pdf</a>, <a href="/format/2402.10063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing the Causal Effects in Class-Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Junhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huawen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Class-Incremental Learning (CIL) is a practical and challenging problem for
achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs)
have led to breakthroughs in both visual and natural language processing tasks.
Despite recent studies showing PTMs' potential ability to learn sequentially, a
plethora of work indicates the necessity of alleviating the catastrophic
forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we
reveal that the crux lies in the imbalanced causal effects between new and old
data. Specifically, the new data encourage models to adapt to new classes while
hindering the adaptation of old classes. Similarly, the old data encourages
models to adapt to old classes while hindering the adaptation of new classes.
In other words, the adaptation process between new and old classes conflicts
from the causal perspective. To alleviate this problem, we propose Balancing
the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for
building causal paths from both new and old data to the prediction of new and
classes, respectively. In this way, the model is encouraged to adapt to all
classes with causal effects from both new and old data and thus alleviates the
causal imbalance problem. We conduct extensive experiments on continual image
classification, continual text classification, and continual named entity
recognition. Empirical results show that BaCE outperforms a series of CIL
methods on different tasks and settings.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10064" title="Abstract">arXiv:2402.10064</a> [<a href="/pdf/2402.10064" title="Download PDF">pdf</a>, <a href="/ps/2402.10064" title="Download PostScript">ps</a>, <a href="/format/2402.10064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Maize: Cyclic and conditional computational graphs for  molecular simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6hr%2C+T">Thomas L&#xf6;hr</a>, 
<a href="/search/cs?searchtype=author&query=Dodds%2C+M">Michael Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lili Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kabeshov%2C+M">Mikhail Kabeshov</a>, 
<a href="/search/cs?searchtype=author&query=Assante%2C+M">Michele Assante</a>, 
<a href="/search/cs?searchtype=author&query=Janet%2C+J">Jon-Paul Janet</a>, 
<a href="/search/cs?searchtype=author&query=Kl%C3%A4hn%2C+M">Marco Kl&#xe4;hn</a>, 
<a href="/search/cs?searchtype=author&query=Engkvist%2C+O">Ola Engkvist</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many computational chemistry and molecular simulation workflows can be
expressed as graphs. This abstraction is useful to modularize and potentially
reuse existing components, as well as provide parallelization and ease
reproducibility. Existing tools represent the computation as a directed acyclic
graph (DAG), thus allowing efficient execution by parallelization of concurrent
branches. These systems can, however, generally not express cyclic and
conditional workflows. We therefore developed Maize, a workflow manager for
cyclic and conditional graphs based on the principles of flow-based
programming. By running each node of the graph concurrently in separate
processes and allowing communication at any time through dedicated inter-node
channels, arbitrary graph structures can be executed. We demonstrate the
effectiveness of the tool on a dynamic active learning task in computational
drug design, involving the use of a small molecule generative model and an
associated scoring system.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10065" title="Abstract">arXiv:2402.10065</a> [<a href="/pdf/2402.10065" title="Download PDF">pdf</a>, <a href="/format/2402.10065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Much Does Each Datapoint Leak Your Privacy? Quantifying the  Per-datum Membership Leakage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azize%2C+A">Achraf Azize</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+D">Debabrota Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the per-datum Membership Inference Attacks (MIAs), where an attacker
aims to infer whether a fixed target datum has been included in the input
dataset of an algorithm and thus, violates privacy. First, we define the
membership leakage of a datum as the advantage of the optimal adversary
targeting to identify it. Then, we quantify the per-datum membership leakage
for the empirical mean, and show that it depends on the Mahalanobis distance
between the target datum and the data-generating distribution. We further
assess the effect of two privacy defences, i.e. adding Gaussian noise and
sub-sampling. We quantify exactly how both of them decrease the per-datum
membership leakage. Our analysis builds on a novel proof technique that
combines an Edgeworth expansion of the likelihood ratio test and a
Lindeberg-Feller central limit theorem. Our analysis connects the existing
likelihood ratio and scalar product attacks, and also justifies different
canary selection strategies used in the privacy auditing literature. Finally,
our experiments demonstrate the impacts of the leakage score, the sub-sampling
ratio and the noise scale on the per-datum membership leakage as indicated by
the theory.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10066" title="Abstract">arXiv:2402.10066</a> [<a href="/pdf/2402.10066" title="Download PDF">pdf</a>, <a href="/format/2402.10066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung  Nodule Invasiveness Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khademi%2C+S">Sadaf Khademi</a>, 
<a href="/search/cs?searchtype=author&query=Oikonomou%2C+A">Anastasia Oikonomou</a>, 
<a href="/search/cs?searchtype=author&query=Plataniotis%2C+K+N">Konstantinos N. Plataniotis</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A">Arash Mohammadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Drawing inspiration from the primate brain's intriguing evidence accumulation
process, and guided by models from cognitive psychology and neuroscience, the
paper introduces the NYCTALE framework, a neuro-inspired and evidence
accumulation-based Transformer architecture. The proposed neuro-inspired
NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for
lung cancer diagnosis. In nature, Nyctales are small owls known for their
nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE
operates in a similarly vigilant manner, i.e., processing data in an
evidence-based fashion and making predictions dynamically/adaptively. Distinct
from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the
NYCTALE performs predictions only when sufficient amount of evidence is
accumulated. In other words, instead of processing all or a pre-defined subset
of CT slices, for each person, slices are provided one at a time. The NYCTALE
framework then computes an evidence vector associated with contribution of each
new CT image. A decision is made once the total accumulated evidence surpasses
a specific threshold. Preliminary experimental analyses conducted using a
challenging in-house dataset comprising 114 subjects. The results are
noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even
with approximately 60% less training data on this demanding and small dataset.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10067" title="Abstract">arXiv:2402.10067</a> [<a href="/pdf/2402.10067" title="Download PDF">pdf</a>, <a href="/format/2402.10067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based policy generation for intent-based management of applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dzeparoska%2C+K">Kristina Dzeparoska</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jieyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tizghadam%2C+A">Ali Tizghadam</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted for publication in 2023 19th International Conference on Network and Service Management (CNSM), 3rd International Workshop on Analytics for Service and Application Management (AnServApp 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 19th International Conference on Network and Service
  Management (CNSM), 2023, pp. 1-7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automated management requires decomposing high-level user requests, such as
intents, to an abstraction that the system can understand and execute. This is
challenging because even a simple intent requires performing a number of
ordered steps. And the task of identifying and adapting these steps (as
conditions change) requires a decomposition approach that cannot be exactly
pre-defined beforehand. To tackle these challenges and support automated intent
decomposition and execution, we explore the few-shot capability of Large
Language Models (LLMs). We propose a pipeline that progressively decomposes
intents by generating the required actions using a policy-based abstraction.
This allows us to automate the policy execution by creating a closed control
loop for the intent deployment. To do so, we generate and map the policies to
APIs and form application management loops that perform the necessary
monitoring, analysis, planning and execution. We evaluate our proposal with a
use-case to fulfill and assure an application service chain of virtual network
functions. Using our approach, we can generalize and generate the necessary
steps to realize intents, thereby enabling intent automation for application
management.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10069" title="Abstract">arXiv:2402.10069</a> [<a href="/pdf/2402.10069" title="Download PDF">pdf</a>, <a href="/format/2402.10069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning fast changing slow in spiking neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Capone%2C+C">Cristiano Capone</a>, 
<a href="/search/cs?searchtype=author&query=Muratore%2C+P">Paolo Muratore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning (RL) faces substantial challenges when applied to
real-life problems, primarily stemming from the scarcity of available data due
to limited interactions with the environment. This limitation is exacerbated by
the fact that RL often demands a considerable volume of data for effective
learning. The complexity escalates further when implementing RL in recurrent
spiking networks, where inherent noise introduced by spikes adds a layer of
difficulty. Life-long learning machines must inherently resolve the
plasticity-stability paradox. Striking a balance between acquiring new
knowledge and maintaining stability is crucial for artificial agents. In this
context, we take inspiration from machine learning technology and introduce a
biologically plausible implementation of proximal policy optimization, arguing
that it significantly alleviates this challenge. Our approach yields two
notable advancements: first, the ability to assimilate new information without
necessitating alterations to the current policy, and second, the capability to
replay experiences without succumbing to policy divergence. Furthermore, when
contrasted with other experience replay (ER) techniques, our method
demonstrates the added advantage of being computationally efficient in an
online setting. We demonstrate that the proposed methodology enhances the
efficiency of learning, showcasing its potential impact on neuromorphic and
real-world applications.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10072" title="Abstract">arXiv:2402.10072</a> [<a href="/pdf/2402.10072" title="Download PDF">pdf</a>, <a href="/format/2402.10072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Joint Source-Channel Coding for Efficient and Reliable  Cross-Technology Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shumin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaodong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yaping Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qinglin Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Cross-technology communication (CTC) is a promising technique that enables
direct communications among incompatible wireless technologies without needing
hardware modification. However, it has not been widely adopted in real-world
applications due to its inefficiency and unreliability. To address this issue,
this paper proposes a deep joint source-channel coding (DJSCC) scheme to enable
efficient and reliable CTC. The proposed scheme builds a neural-network-based
encoder and decoder at the sender side and the receiver side, respectively, to
achieve two critical tasks simultaneously: 1) compressing the messages to the
point where only their essential semantic meanings are preserved; 2) ensuring
the robustness of the semantic meanings when they are transmitted across
incompatible technologies. The scheme incorporates existing CTC coding
algorithms as domain knowledge to guide the encoder-decoder pair to learn the
characteristics of CTC links better. Moreover, the scheme constructs shared
semantic knowledge for the encoder and decoder, allowing semantic meanings to
be converted into very few bits for cross-technology transmissions, thus
further improving the efficiency of CTC. Extensive simulations verify that the
proposed scheme can reduce the transmission overhead by up to 97.63\% and
increase the structural similarity index measure by up to 734.78%, compared
with the state-of-the-art CTC scheme.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10073" title="Abstract">arXiv:2402.10073</a> [<a href="/pdf/2402.10073" title="Download PDF">pdf</a>, <a href="/format/2402.10073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Both Matter: Enhancing the Emotional Intelligence of Large Language  Models without Compromising the General Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weixiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuojun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotional Intelligence (EI), consisting of emotion perception, emotion
cognition and emotion expression, plays the critical roles in improving user
interaction experience for the current large language model (LLM) based
conversational general AI assistants. Previous works mainly focus on raising
the emotion perception ability of them via naive fine-tuning on EI-related
classification or regression tasks. However, this leads to the incomplete
enhancement of EI and catastrophic forgetting of the general intelligence (GI).
To this end, we first introduce \textsc{EiBench}, a large-scale collection of
EI-related tasks in the text-to-text formation with task instructions that
covers all three aspects of EI, which lays a solid foundation for the
comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular
\underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement
method (\textbf{MoEI}), consisting of Modular Parameter Expansion and
intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs
without compromise their GI. Extensive experiments on two representative
LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness
of MoEI to improving EI while maintain GI.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10074" title="Abstract">arXiv:2402.10074</a> [<a href="/pdf/2402.10074" title="Download PDF">pdf</a>, <a href="/format/2402.10074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengcheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiapeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph neural networks (GNNs) have recently demonstrated significant success.
Active learning for GNNs aims to query the valuable samples from the unlabeled
data for annotation to maximize the GNNs' performance at a low cost. However,
most existing methods for reinforced active learning in GNNs may lead to a
highly imbalanced class distribution, especially in highly skewed class
scenarios. This further adversely affects the classification performance. To
tackle this issue, in this paper, we propose a novel reinforced class-balanced
active learning framework for GNNs, namely, GraphCBAL. It learns an optimal
policy to acquire class-balanced and informative nodes for annotation,
maximizing the performance of GNNs trained with selected labeled nodes.
GraphCBAL designs class-balance-aware states, as well as a reward function that
achieves trade-off between model performance and class balance. We further
upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to
obtain a more class-balanced labeled set. Extensive experiments on multiple
datasets demonstrate the effectiveness of the proposed approaches, achieving
superior performance over state-of-the-art baselines. In particular, our
methods can strike the balance between classification results and class
balance.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10076" title="Abstract">arXiv:2402.10076</a> [<a href="/pdf/2402.10076" title="Download PDF">pdf</a>, <a href="/format/2402.10076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QUICK: Quantization-aware Interleaving and Conflict-free Kernel for  efficient LLM inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+D">Daehyun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sarang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jiwoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyungjun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce QUICK, a group of novel optimized CUDA kernels for the efficient
inference of quantized Large Language Models (LLMs). QUICK addresses the shared
memory bank-conflict problem of state-of-the-art mixed precision matrix
multiplication kernels. Our method interleaves the quantized weight matrices of
LLMs offline to skip the shared memory write-back after the dequantization. We
demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger
batches and up to 1.94x throughput gain on representative LLM models on various
NVIDIA GPU devices.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10077" title="Abstract">arXiv:2402.10077</a> [<a href="/pdf/2402.10077" title="Download PDF">pdf</a>, <a href="/ps/2402.10077" title="Download PostScript">ps</a>, <a href="/format/2402.10077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a large-scale fused and labeled dataset of human pose while  interacting with robots in shared urban areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sherafat%2C+E">E. Sherafat</a>, 
<a href="/search/cs?searchtype=author&query=Farooq%2C+B">B. Farooq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Sherafat E., and Farooq B., (2024). Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas. In the proceedings of the 103rd Annual Meeting of Transportation Research Board. Washington DC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the last decade, Autonomous Delivery Robots (ADRs) have transformed
conventional delivery methods, responding to the growing e-commerce demand.
However, the readiness of ADRs to navigate safely among pedestrians in shared
urban areas remains an open question. We contend that there are crucial
research gaps in understanding their interactions with pedestrians in such
environments. Human Pose Estimation is a vital stepping stone for various
downstream applications, including pose prediction and socially aware robot
path-planning. Yet, the absence of an enriched and pose-labeled dataset
capturing human-robot interactions in shared urban areas hinders this
objective. In this paper, we bridge this gap by repurposing, fusing, and
labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and
Simultaneous Localization and Mapping (SLAM), respectively. The resulting
unique dataset represents thousands of real-world indoor and outdoor
human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose
visual and numeric outputs and provided ground truth poses using manual
annotation. To overcome the distance bias present in the traditional MPJPE
metric, this study introduces a novel human pose estimation error metric called
Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into
it. Findings demonstrate that YOLOv7 effectively estimates human pose in both
datasets. However, it exhibits weaker performance in specific scenarios, like
indoor, crowded scenes with a focused light source, where both MPJPE and MSJE
are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs
better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17
seq1), achieving MSJE values of 5.29 and 3.38, respectively.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10078" title="Abstract">arXiv:2402.10078</a> [<a href="/pdf/2402.10078" title="Download PDF">pdf</a>, <a href="/format/2402.10078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EventF2S: Asynchronous and Sparse Spiking AER Framework using  Neuromorphic-Friendly Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annamalai%2C+L">Lakshmi Annamalai</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+C+S">Chetan Singh Thakur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Bio-inspired Address Event Representation (AER) sensors have attracted
significant popularity owing to their low power consumption, high sparsity, and
high temporal resolution. Spiking Neural Network (SNN) has become the inherent
choice for AER data processing. However, the integration of the AER-SNN
paradigm has not adequately explored asynchronous processing, neuromorphic
compatibility, and sparse spiking, which are the key requirements of
resource-constrained applications. To address this gap, we introduce a
brain-inspired AER-SNN object recognition solution, which includes a data
encoder integrated with a First-To-Spike recognition network. Being fascinated
by the functionality of neurons in the visual cortex, we designed the solution
to be asynchronous and compatible with neuromorphic hardware. Furthermore, we
have adapted the principle of denoising and First-To-Spike coding to achieve
optimal spike signaling, significantly reducing computation costs. Experimental
evaluation has demonstrated that the proposed method incurs significantly less
computation cost to achieve state-of-the-art competitive accuracy. Overall, the
proposed solution offers an asynchronous and cost-effective AER recognition
system that harnesses the full potential of AER sensors.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10079" title="Abstract">arXiv:2402.10079</a> [<a href="/pdf/2402.10079" title="Download PDF">pdf</a>, <a href="/format/2402.10079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of the Learning-based Camera and Lidar Simulation Methods for  Autonomous Driving Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghighi%2C+H">Hamed Haghighi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaomeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+H">Hao Jing</a>, 
<a href="/search/cs?searchtype=author&query=Dianati%2C+M">Mehrdad Dianati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Perception sensors, particularly camera and Lidar, are key elements of
Autonomous Driving Systems (ADS) that enable them to comprehend their
surroundings for informed driving and control decisions. Therefore, developing
realistic camera and Lidar simulation methods, also known as camera and Lidar
models, is of paramount importance to effectively conduct simulation-based
testing for ADS. Moreover, the rise of deep learning-based perception models
has propelled the prevalence of perception sensor models as valuable tools for
synthesising diverse training datasets. The traditional sensor simulation
methods rely on computationally expensive physics-based algorithms,
specifically in complex systems such as ADS. Hence, the current potential
resides in learning-based models, driven by the success of deep generative
models in synthesising high-dimensional data. This paper reviews the current
state-of-the-art in learning-based sensor simulation methods and validation
approaches, focusing on two main types of perception sensors: cameras and
Lidars. This review covers two categories of learning-based approaches, namely
raw-data-based and object-based models. Raw-data-based methods are explained
concerning the employed learning strategy, while object-based models are
categorised based on the type of error considered. Finally, the paper
illustrates commonly used validation techniques for evaluating perception
sensor models and highlights the existing research gaps in the area.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10080" title="Abstract">arXiv:2402.10080</a> [<a href="/pdf/2402.10080" title="Download PDF">pdf</a>, <a href="/format/2402.10080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal hierarchies of regular languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Place%2C+T">Thomas Place</a>, 
<a href="/search/cs?searchtype=author&query=Zeitoun%2C+M">Marc Zeitoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">We classify the regular languages using an operator $\mathcal{C}\mapsto
TL(\mathcal{C})$. For each input class of languages $\mathcal{C}$, it builds a
larger class $TL(\mathcal{C})$ consisting of all languages definable in a
variant of unary temporal logic whose future/past modalities depend on
$\mathcal{C}$. This defines the temporal hierarchy of basis $\mathcal{C}$:
level $n$ is built by applying this operator $n$ times to $\mathcal{C}$. This
hierarchy is closely related to another one, the concatenation hierarchy of
basis $\mathcal{C}$. In particular, the union of all levels in both hierarchies
is the same.
<br />We focus on bases $\mathcal{G}$ of group languages and natural extensions
thereof, denoted $\mathcal{G}^+$. We prove that the temporal hierarchies of
bases $\mathcal{G}$ and $\mathcal{G}^+$ are strictly intertwined, and we
compare them to the corresponding concatenation hierarchies. Furthermore, we
look at two standard problems on classes of languages: membership (decide if an
input language is in the class) and separation (decide, for two input regular
languages $L_1,L_2$, if there is a language $K$ in the class with $L_1
\subseteq K$ and $L_2 \cap K = \emptyset$). We prove that if separation is
decidable for $\mathcal{G}$, then so is membership for level two in the
temporal hierarchies of bases $\mathcal{G}$ and $\mathcal{G}^+$. Moreover, we
take a closer look at the case where $\mathcal{G}$ is the trivial class
$ST=\{\emptyset,A^*\}$. The levels one in the hierarchies of bases $ST$ and
$ST^+$ are the standard variants of unary temporal logic while the levels two
were considered recently using alternate definitions. We prove that for these
two bases, level two has decidable separation. Combined with earlier results
about the operator $\mathcal{G}\mapsto TL(\mathcal{G})$, this implies that the
levels three have decidable membership.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10082" title="Abstract">arXiv:2402.10082</a> [<a href="/pdf/2402.10082" title="Download PDF">pdf</a>, <a href="/format/2402.10082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedRDF: A Robust and Dynamic Aggregation Function against Poisoning  Attacks in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campos%2C+E+M">Enrique M&#xe1;rmol Campos</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+A+G">Aurora Gonz&#xe1;lez Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+J+L+H">Jos&#xe9; Luis Hern&#xe1;ndez Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Skarmeta%2C+A">Antonio Skarmeta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures, and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Federated Learning (FL) represents a promising approach to typical privacy
concerns associated with centralized Machine Learning (ML) deployments. Despite
its well-known advantages, FL is vulnerable to security attacks such as
Byzantine behaviors and poisoning attacks, which can significantly degrade
model performance and hinder convergence. The effectiveness of existing
approaches to mitigate complex attacks, such as median, trimmed mean, or Krum
aggregation functions, has been only partially demonstrated in the case of
specific attacks. Our study introduces a novel robust aggregation mechanism
utilizing the Fourier Transform (FT), which is able to effectively handling
sophisticated attacks without prior knowledge of the number of attackers.
Employing this data technique, weights generated by FL clients are projected
into the frequency domain to ascertain their density function, selecting the
one exhibiting the highest frequency. Consequently, malicious clients' weights
are excluded. Our proposed approach was tested against various model poisoning
attacks, demonstrating superior performance over state-of-the-art aggregation
methods.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10083" title="Abstract">arXiv:2402.10083</a> [<a href="/pdf/2402.10083" title="Download PDF">pdf</a>, <a href="/ps/2402.10083" title="Download PostScript">ps</a>, <a href="/format/2402.10083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots  in Ophthalmology and LLM-based evaluation using GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+T+F">Ting Fang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Elangovan%2C+K">Kabilan Elangovan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y">Yao Jie</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+L">Li Yong</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J">Joshua Lim</a>, 
<a href="/search/cs?searchtype=author&query=Poh%2C+S">Stanley Poh</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+W+Y">Wei Yan Ng</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Daniel Lim</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yuhe Ke</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+D+S+W">Daniel Shu Wei Ting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 Pages, 1 Figure, 8 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Purpose: To assess the alignment of GPT-4-based evaluation to human clinician
experts, for the evaluation of responses to ophthalmology-related patient
queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology
questions and paired answers were created by ophthalmologists to represent
commonly asked patient questions, divided into fine-tuning (368; 92%), and
testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,
LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,
additional 8 glaucoma QnA pairs were included. 200 responses to the testing
dataset were generated by 5 fine-tuned LLMs for evaluation. A customized
clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on
clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4
evaluation was then compared against ranking by 5 clinicians for clinical
alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest
(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),
LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4
evaluation demonstrated significant agreement with human clinician rankings,
with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80
respectively; while correlation based on Cohen Kappa was more modest at 0.50.
Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical
inaccuracies in the LLM-generated responses, which were appropriately
identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment
of GPT-4 evaluation highlighted its potential to streamline the clinical
evaluation of LLM chatbot responses to healthcare-related queries. By
complementing the existing clinician-dependent manual grading, this efficient
and automated evaluation could assist the validation of future developments in
LLM applications for healthcare.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10085" title="Abstract">arXiv:2402.10085</a> [<a href="/pdf/2402.10085" title="Download PDF">pdf</a>, <a href="/format/2402.10085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Develop End-to-End Anomaly Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mengoli%2C+E">Emanuele Mengoli</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhiyuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wutao Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Anomaly detection plays a crucial role in ensuring network robustness.
However, implementing intelligent alerting systems becomes a challenge when
considering scenarios in which anomalies can be caused by both malicious and
non-malicious events, leading to the difficulty of determining anomaly
patterns. The lack of labeled data in the computer networking domain further
exacerbates this issue, impeding the development of robust models capable of
handling real-world scenarios. To address this challenge, in this paper, we
propose an end-to-end anomaly detection model development pipeline. This
framework makes it possible to consume user feedback and enable continuous
user-centric model performance evaluation and optimization. We demonstrate the
efficacy of the framework by way of introducing and bench-marking a new
forecasting model -- named \emph{Lachesis} -- on a real-world networking
problem. Experiments have demonstrated the robustness and effectiveness of the
two proposed versions of \emph{Lachesis} compared with other models proposed in
the literature. Our findings underscore the potential for improving the
performance of data-driven products over their life cycles through a harmonized
integration of user feedback and iterative development.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10086" title="Abstract">arXiv:2402.10086</a> [<a href="/pdf/2402.10086" title="Download PDF">pdf</a>, <a href="/format/2402.10086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznietsov%2C+A">Anton Kuznietsov</a>, 
<a href="/search/cs?searchtype=author&query=Gyevnar%2C+B">Balint Gyevnar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+S">Steven Peters</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificial Intelligence (AI) shows promising applications for the perception
and planning tasks in autonomous driving (AD) due to its superior performance
compared to conventional methods. However, inscrutable AI systems exacerbate
the existing challenge of safety assurance of AD. One way to mitigate this
challenge is to utilize explainable AI (XAI) techniques. To this end, we
present the first comprehensive systematic literature review of explainable
methods for safe and trustworthy AD. We begin by analyzing the requirements for
AI in the context of AD, focusing on three key aspects: data, model, and
agency. We find that XAI is fundamental to meeting these requirements. Based on
this, we explain the sources of explanations in AI and describe a taxonomy of
XAI. We then identify five key contributions of XAI for safe and trustworthy AI
in AD, which are interpretable design, interpretable surrogate models,
interpretable monitoring, auxiliary explanations, and interpretable validation.
Finally, we propose a modular framework called SafeX to integrate these
contributions, enabling explanation delivery to users while simultaneously
ensuring the safety of AI models.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10087" title="Abstract">arXiv:2402.10087</a> [<a href="/pdf/2402.10087" title="Download PDF">pdf</a>, <a href="/ps/2402.10087" title="Download PostScript">ps</a>, <a href="/format/2402.10087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Covert Routing in Heterogeneous Networks Using  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+J">Justin Kong</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+T+J">Terrence J. Moore</a>, 
<a href="/search/cs?searchtype=author&query=Dagefu%2C+F+T">Fikadu T. Dagefu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This letter investigates covert routing communications in a heterogeneous
network where a source transmits confidential data to a destination with the
aid of relaying nodes where each transmitter judiciously chooses one modality
among multiple communication modalities. We develop a novel reinforcement
learning-based covert routing algorithm that finds a route from the source to
the destination where each node identifies its next hop and modality only based
on the local feedback information received from its neighboring nodes. We show
based on numerical simulations that the proposed covert routing strategy has
only negligible performance loss compared to the optimal centralized routing
scheme.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10088" title="Abstract">arXiv:2402.10088</a> [<a href="/pdf/2402.10088" title="Download PDF">pdf</a>, <a href="/format/2402.10088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical hybrid modeling for flexible tool use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Priorelli%2C+M">Matteo Priorelli</a>, 
<a href="/search/cs?searchtype=author&query=Stoianov%2C+I+P">Ivilin Peev Stoianov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In a recent computational framework called active inference, discrete models
can be linked to their continuous counterparts to perform decision-making in
changing environments. From another perspective, simple agents can be combined
to better capture the causal relationships of the world. How can we use these
two features together to achieve efficient goal-directed behavior? We present
an architecture composed of several hybrid -- continuous and discrete -- units
replicating the agent's configuration, controlled by a high-level discrete
model that achieves dynamic planning and synchronized behavior. Additional
factorizations within each level allow to represent hierarchically other agents
and objects in relation to the self. We evaluate this hierarchical hybrid model
on a non-trivial task: reaching a moving object after having picked a moving
tool. This study extends past work on control as inference and proposes an
alternative direction to deep reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10090" title="Abstract">arXiv:2402.10090</a> [<a href="/pdf/2402.10090" title="Download PDF">pdf</a>, <a href="/ps/2402.10090" title="Download PostScript">ps</a>, <a href="/format/2402.10090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PICS: Pipeline for Image Captioning and Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosario%2C+G">Grant Rosario</a>, 
<a href="/search/cs?searchtype=author&query=Noever%2C+D">David Noever</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The growing volume of digital images necessitates advanced systems for
efficient categorization and retrieval, presenting a significant challenge in
database management and information retrieval. This paper introduces PICS
(Pipeline for Image Captioning and Search), a novel approach designed to
address the complexities inherent in organizing large-scale image repositories.
PICS leverages the advancements in Large Language Models (LLMs) to automate the
process of image captioning, offering a solution that transcends traditional
manual annotation methods. The approach is rooted in the understanding that
meaningful, AI-generated captions can significantly enhance the searchability
and accessibility of images in large databases. By integrating sentiment
analysis into the pipeline, PICS further enriches the metadata, enabling
nuanced searches that extend beyond basic descriptors. This methodology not
only simplifies the task of managing vast image collections but also sets a new
precedent for accuracy and efficiency in image retrieval. The significance of
PICS lies in its potential to transform image database systems, harnessing the
power of machine learning and natural language processing to meet the demands
of modern digital asset management.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10091" title="Abstract">arXiv:2402.10091</a> [<a href="/pdf/2402.10091" title="Download PDF">pdf</a>, <a href="/format/2402.10091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Based Product Matching -- Semi-Supervised Clustering Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinek%2C+A">Alicja Martinek</a>, 
<a href="/search/cs?searchtype=author&query=%C5%81ukasik%2C+S">Szymon &#x141;ukasik</a>, 
<a href="/search/cs?searchtype=author&query=Gandomi%2C+A+H">Amir H. Gandomi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Matching identical products present in multiple product feeds constitutes a
crucial element of many tasks of e-commerce, such as comparing product
offerings, dynamic price optimization, and selecting the assortment
personalized for the client. It corresponds to the well-known machine learning
task of entity matching, with its own specificity, like omnipresent
unstructured data or inaccurate and inconsistent product descriptions. This
paper aims to present a new philosophy to product matching utilizing a
semi-supervised clustering approach. We study the properties of this method by
experimenting with the IDEC algorithm on the real-world dataset using
predominantly textual features and fuzzy string matching, with more standard
approaches as a point of reference. Encouraging results show that unsupervised
matching, enriched with a small annotated sample of product links, could be a
possible alternative to the dominant supervised strategy, requiring extensive
manual data labeling.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10092" title="Abstract">arXiv:2402.10092</a> [<a href="/pdf/2402.10092" title="Download PDF">pdf</a>, <a href="/format/2402.10092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Workflow Optimization for Parallel Split Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tirana%2C+J">Joana Tirana</a>, 
<a href="/search/cs?searchtype=author&query=Tsigkari%2C+D">Dimitra Tsigkari</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+G">George Iosifidis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzopoulos%2C+D">Dimitris Chatzopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE INFOCOM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Split learning (SL) has been recently proposed as a way to enable
resource-constrained devices to train multi-parameter neural networks (NNs) and
participate in federated learning (FL). In a nutshell, SL splits the NN model
into parts, and allows clients (devices) to offload the largest part as a
processing task to a computationally powerful helper. In parallel SL, multiple
helpers can process model parts of one or more clients, thus, considerably
reducing the maximum training time over all clients (makespan). In this paper,
we focus on orchestrating the workflow of this operation, which is critical in
highly heterogeneous systems, as our experiments show. In particular, we
formulate the joint problem of client-helper assignments and scheduling
decisions with the goal of minimizing the training makespan, and we prove that
it is NP-hard. We propose a solution method based on the decomposition of the
problem by leveraging its inherent symmetry, and a second one that is fully
scalable. A wealth of numerical evaluations using our testbed's measurements
allow us to build a solution strategy comprising these methods. Moreover, we
show that this strategy finds a near-optimal solution, and achieves a shorter
makespan than the baseline scheme by up to 52.3%.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10093" title="Abstract">arXiv:2402.10093</a> [<a href="/pdf/2402.10093" title="Download PDF">pdf</a>, <a href="/format/2402.10093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alkin%2C+B">Benedikt Alkin</a>, 
<a href="/search/cs?searchtype=author&query=Miklautz%2C+L">Lukas Miklautz</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning
boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted
in the insight that optimal representations within MIM models generally reside
in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive
heads that are connected to diverse intermediate layers. In each head, a
modified nearest neighbor objective helps to construct respective semantic
clusters.
<br />The refinement process is short but effective. Within a few epochs, we refine
the features of MIM models from subpar to state-of-the-art, off-the-shelf
features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K,
achieves new state-of-the-art results in linear probing (84.7%) and low-shot
classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K
1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%,
outperforming larger models that were trained on up to 2000x more data such as
DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page:
https://ml-jku.github.io/MIM-Refiner
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10095" title="Abstract">arXiv:2402.10095</a> [<a href="/pdf/2402.10095" title="Download PDF">pdf</a>, <a href="/format/2402.10095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadin%2C+S">Shahar Yadin</a>, 
<a href="/search/cs?searchtype=author&query=Elata%2C+N">Noam Elata</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A prominent family of methods for learning data distributions relies on
density ratio estimation (DRE), where a model is trained to $\textit{classify}$
between data samples and samples from some reference distribution. These
techniques are successful in simple low-dimensional settings but fail to
achieve good results on complex high-dimensional data, like images. A different
family of methods for learning distributions is that of denoising diffusion
models (DDMs), in which a model is trained to $\textit{denoise}$ data samples.
These approaches achieve state-of-the-art results in image, video, and audio
generation. In this work, we present $\textit{Classification Diffusion Models}$
(CDMs), a generative technique that adopts the denoising-based formalism of
DDMs while making use of a classifier that predicts the amount of noise added
to a clean signal, similarly to DRE methods. Our approach is based on the
observation that an MSE-optimal denoiser for white Gaussian noise can be
expressed in terms of the gradient of a cross-entropy-optimal classifier for
predicting the noise level. As we illustrate, CDM achieves better denoising
results compared to DDM, and leads to at least comparable FID in image
generation. CDM is also capable of highly efficient one-step exact likelihood
estimation, achieving state-of-the-art results among methods that use a single
step. Code is available on the project's webpage in
https://shaharYadin.github.io/CDM/ .
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10097" title="Abstract">arXiv:2402.10097</a> [<a href="/pdf/2402.10097" title="Download PDF">pdf</a>, <a href="/format/2402.10097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Federated Learning in Heterogeneous Wireless Networks with  Independent Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jiaxiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yanzhao Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaofeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Juncheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, published to IEEE International Conference on Communications (ICC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Federated Learning (FL) algorithms commonly sample a random subset of clients
to address the straggler issue and improve communication efficiency. While
recent works have proposed various client sampling methods, they have
limitations in joint system and data heterogeneity design, which may not align
with practical heterogeneous wireless networks. In this work, we advocate a new
independent client sampling strategy to minimize the wall-clock training time
of FL, while considering data heterogeneity and system heterogeneity in both
communication and computation. We first derive a new convergence bound for
non-convex loss functions with independent client sampling and then propose an
adaptive bandwidth allocation scheme. Furthermore, we propose an efficient
independent client sampling algorithm based on the upper bounds on the
convergence rounds and the expected per-round training time, to minimize the
wall-clock time of FL, while considering both the data and system
heterogeneity. Experimental results under practical wireless network settings
with real-world prototype demonstrate that the proposed independent sampling
scheme substantially outperforms the current best sampling schemes under
various training models and datasets.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10098" title="Abstract">arXiv:2402.10098</a> [<a href="/pdf/2402.10098" title="Download PDF">pdf</a>, <a href="/format/2402.10098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-tuning-free data entry error unlearning with adaptive  selective synaptic dampening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoepf%2C+S">Stefan Schoepf</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+J">Jack Foster</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data entry constitutes a fundamental component of the machine learning
pipeline, yet it frequently results in the introduction of labelling errors.
When a model has been trained on a dataset containing such errors its
performance is reduced. This leads to the challenge of efficiently unlearning
the influence of the erroneous data to improve the model performance without
needing to completely retrain the model. While model editing methods exist for
cases in which the correct label for a wrong entry is known, we focus on the
case of data entry errors where we do not know the correct labels for the
erroneous data. Our contribution is twofold. First, we introduce an extension
to the selective synaptic dampening unlearning method that removes the need for
parameter tuning, making unlearning accessible to practitioners. We demonstrate
the performance of this extension, adaptive selective synaptic dampening
(ASSD), on various ResNet18 and Vision Transformer unlearning tasks. Second, we
demonstrate the performance of ASSD in a supply chain delay prediction problem
with labelling errors using real-world data where we randomly introduce various
levels of labelling errors. The application of this approach is particularly
compelling in industrial settings, such as supply chain management, where a
significant portion of data entry occurs manually through Excel sheets,
rendering it error-prone. ASSD shows strong performance on general unlearning
benchmarks and on the error correction problem where it outperforms fine-tuning
for error correction.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10099" title="Abstract">arXiv:2402.10099</a> [<a href="/pdf/2402.10099" title="Download PDF">pdf</a>, <a href="/format/2402.10099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Any-Shift Prompting for Generalization over Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zehao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiayi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Derakhshani%2C+M+M">Mohammad Mahdi Derakhshani</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+S">Shengcai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image-language models with prompt learning have shown remarkable advances in
numerous downstream vision tasks. Nevertheless, conventional prompt learning
methods overfit their training distribution and lose the generalization ability
on test distributions. To improve generalization across various distribution
shifts, we propose any-shift prompting: a general probabilistic inference
framework that considers the relationship between training and test
distributions during prompt learning. We explicitly connect training and test
distributions in the latent space by constructing training and test prompts in
a hierarchical architecture. Within this framework, the test prompt exploits
the distribution relationships to guide the generalization of the CLIP
image-language model from training to any test distribution. To effectively
encode the distribution information and their relationships, we further
introduce a transformer inference network with a pseudo-shift training
mechanism. The network generates the tailored test prompt with both training
and test information in a feedforward pass, avoiding extra training costs at
test time. Extensive experiments on twenty-three datasets demonstrate the
effectiveness of any-shift prompting on the generalization over various
distribution shifts.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10100" title="Abstract">arXiv:2402.10100</a> [<a href="/pdf/2402.10100" title="Download PDF">pdf</a>, <a href="/format/2402.10100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning In: Analysis of Audio Classifier Performance in Clinical Settings  with Limited Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+H">Hamza Mahdi</a>, 
<a href="/search/cs?searchtype=author&query=Nashnoush%2C+E">Eptehal Nashnoush</a>, 
<a href="/search/cs?searchtype=author&query=Saab%2C+R">Rami Saab</a>, 
<a href="/search/cs?searchtype=author&query=Balachandar%2C+A">Arjun Balachandar</a>, 
<a href="/search/cs?searchtype=author&query=Dagli%2C+R">Rishit Dagli</a>, 
<a href="/search/cs?searchtype=author&query=Perri%2C+L+X">Lucas X. Perri</a>, 
<a href="/search/cs?searchtype=author&query=Khosravani%2C+H">Houman Khosravani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This study assesses deep learning models for audio classification in a
clinical setting with the constraint of small datasets reflecting real-world
prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt,
alongside transformer models like ViT, SWIN, and AST, and compare them against
pre-trained audio models such as YAMNet and VGGish. Our method highlights the
benefits of pre-training on large datasets before fine-tuning on specific
clinical data. We prospectively collected two first-of-their-kind patient audio
datasets from stroke patients. We investigated various preprocessing
techniques, finding that RGB and grayscale spectrogram transformations affect
model performance differently based on the priors they learn from pre-training.
Our findings indicate CNNs can match or exceed transformer models in small
dataset contexts, with DenseNet-Contrastive and AST models showing notable
performance. This study highlights the significance of incremental marginal
gains through model selection, pre-training, and preprocessing in sound
classification; this offers valuable insights for clinical diagnostics that
rely on audio classification.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10101" title="Abstract">arXiv:2402.10101</a> [<a href="/pdf/2402.10101" title="Download PDF">pdf</a>, <a href="/format/2402.10101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Based Situation Awareness for Multiple Missiles Evasion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scukins%2C+E">Edvards Scukins</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+M">Markus Klein</a>, 
<a href="/search/cs?searchtype=author&query=Kroon%2C+L">Lars Kroon</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96gren%2C+P">Petter &#xd6;gren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the effective range of air-to-air missiles increases, it becomes harder
for human operators to maintain the situational awareness needed to keep a UAV
safe. In this work, we propose a decision support tool to help UAV operators in
Beyond Visual Range (BVR) air combat scenarios assess the risks of different
options and make decisions based on those. Earlier work focused on the threat
posed by a single missile, and in this work, we extend the ideas to several
missile threats. The proposed method uses Deep Neural Networks (DNN) to learn
from high-fidelity simulations to provide the operator with an outcome estimate
for a set of different strategies. Our results demonstrate that the proposed
system can manage multiple incoming missiles, evaluate a family of options, and
recommend the least risky course of action.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10102" title="Abstract">arXiv:2402.10102</a> [<a href="/pdf/2402.10102" title="Download PDF">pdf</a>, <a href="/format/2402.10102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A privacy-preserving, distributed and cooperative FCM-based learning  approach for Cancer Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salmeron%2C+J+L">Jose L. Salmeron</a>, 
<a href="/search/cs?searchtype=author&query=Ar%C3%A9valo%2C+I">Irina Ar&#xe9;valo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Rough Sets: International Joint Conference, IJCRS 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Distributed Artificial Intelligence is attracting interest day by day. In
this paper, the authors introduce an innovative methodology for distributed
learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a
privacy-preserving way. The authors design a training scheme for collaborative
FCM learning that offers data privacy compliant with the current regulation.
This method is applied to a cancer detection problem, proving that the
performance of the model is improved by the Federated Learning process, and
obtaining similar results to the ones that can be found in the literature.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10104" title="Abstract">arXiv:2402.10104</a> [<a href="/pdf/2402.10104" title="Download PDF">pdf</a>, <a href="/format/2402.10104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on  Geometry Problem-Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Moshfeghi%2C+Y">Yashar Moshfeghi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) and Multi-Modal Models
(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,
their proficiency in tackling geometry math problems, which necessitates an
integrated understanding of both textual and visual information, has not been
thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,
a comprehensive collection that includes a main subset of 2000 problems, a 750
problem subset focusing on backward reasoning, an augmented subset of 2000
problems, and a hard subset of 300 problems. This benchmark facilitates a
deeper investigation into the performance of LLMs and MMs on solving geometry
math problems. Our evaluation of ten LLMs and MMs across these varied subsets
reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on
the main subset but only a 6.00\% accuracy on the challenging subset. This
highlights the critical need for testing models against datasets on which they
have not been pre-trained. Additionally, our findings indicate that GPT-series
models perform more effectively on problems they have rephrased, suggesting a
promising method for enhancing model capabilities.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10107" title="Abstract">arXiv:2402.10107</a> [<a href="/pdf/2402.10107" title="Download PDF">pdf</a>, <a href="/format/2402.10107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantized Embedding Vectors for Controllable Diffusion Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+C">Cheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinye Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Novak%2C+D">Daniel Novak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Improving the controllability, portability, and inference speed of diffusion
language models (DLMs) is a key challenge in natural language generation. While
recent research has shown significant success in complex text generation with
language models, the memory and computational power are still very demanding
and fall short of expectations, which naturally results in low portability and
instability for the models. To mitigate these issues, numerous well-established
methods were proposed for neural network quantization. To further enhance their
portability of independent deployment as well as improve their stability
evaluated by language perplexity, we propose a novel approach called the
Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM
builds upon the recent successful controllable DLMs by remodeling the
task-specific embedding space via quantization. This leads to a gradient-based
controller for the generation tasks, and more stable intermediate latent
variables are obtained, which naturally brings in an accelerated convergence as
well as better controllability. Additionally, the adaption fine-tuning method
is employed to reduce tunable weights. Experimental results on five challenging
fine-grained control tasks demonstrate that QE-CDLM compares favorably to
existing methods in terms of quality and feasibility, achieving better
perplexity and lightweight fine-tuning.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10109" title="Abstract">arXiv:2402.10109</a> [<a href="/pdf/2402.10109" title="Download PDF">pdf</a>, <a href="/format/2402.10109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McInerney%2C+D+J">Denis Jered McInerney</a>, 
<a href="/search/cs?searchtype=author&query=Dickinson%2C+W">William Dickinson</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+L">Lucy Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+A">Andrea Young</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+G">Geoffrey Young</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Meent%2C+J">Jan-Willem van de Meent</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10110" title="Abstract">arXiv:2402.10110</a> [<a href="/pdf/2402.10110" title="Download PDF">pdf</a>, <a href="/format/2402.10110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Reflection-Tuning: Student-Selected Data Recycling for LLM  Instruction-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shwai He</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning is critical to large language models (LLMs) for achieving
better instruction following and task adaptation capabilities but its success
heavily relies on the training data quality. Many recent methods focus on
improving the data quality but often overlook the compatibility of the data
with the student model being finetuned. This paper introduces Selective
Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection
and introspection for improving existing data quality with the data selection
capability of the student LLM, to automatically refine existing
instruction-tuning data. This teacher-student collaboration produces
high-quality and student-compatible instruction-response pairs, resulting in
sample-efficient instruction tuning and LLMs of superior performance. Selective
Reflection-Tuning is a data augmentation and synthesis that generally improves
LLM finetuning and self-improvement without collecting brand-new data. We apply
our method to Alpaca and WizardLM data and achieve much stronger and top-tier
7B and 13B LLMs. Our codes, models, and data will be released at
https://github.com/tianyi-lab/Reflection_Tuning.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10115" title="Abstract">arXiv:2402.10115</a> [<a href="/pdf/2402.10115" title="Download PDF">pdf</a>, <a href="/format/2402.10115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Visual Stimuli from EEG Recordings using Transformer-encoder  based EEG encoder and GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Rahul Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Bhavsar%2C+A">Arnav Bhavsar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In this study, we tackle a modern research challenge within the field of
perceptual brain decoding, which revolves around synthesizing images from EEG
signals using an adversarial deep learning framework. The specific objective is
to recreate images belonging to various object categories by leveraging EEG
recordings obtained while subjects view those images. To achieve this, we
employ a Transformer-encoder based EEG encoder to produce EEG encodings, which
serve as inputs to the generator component of the GAN network. Alongside the
adversarial loss, we also incorporate perceptual loss to enhance the quality of
the generated images.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10118" title="Abstract">arXiv:2402.10118</a> [<a href="/pdf/2402.10118" title="Download PDF">pdf</a>, <a href="/format/2402.10118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reusing Softmax Hardware Unit for GELU Computation in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peltekis%2C+C">Christodoulos Peltekis</a>, 
<a href="/search/cs?searchtype=author&query=Alexandridi%2C+K">Kosmas Alexandridi</a>, 
<a href="/search/cs?searchtype=author&query=Dimitrakopoulos%2C+G">Giorgos Dimitrakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AICAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformers have improved drastically the performance of natural language
processing (NLP) and computer vision applications. The computation of
transformers involves matrix multiplications and non-linear activation
functions such as softmax and GELU (Gaussion Error Linear Unit) that are
accelerated directly in hardware. Currently, function evaluation is done
separately for each function and rarely allows for hardware reuse. To mitigate
this problem, in this work, we map the computation of GELU to a softmax
operator. In this way, the efficient hardware units designed already for
softmax can be reused for computing GELU as well. Computation of GELU can enjoy
the inherent vectorized nature of softmax and produce in parallel multiple GELU
outcomes. Experimental results show that computing GELU via a pre-existing and
incrementally modified softmax hardware unit (a) does not reduce the accuracy
of representative NLP applications and (b) allows the reduction of the overall
hardware area and power by 6.1% and 11.9%, respectively, on average.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10119" title="Abstract">arXiv:2402.10119</a> [<a href="/pdf/2402.10119" title="Download PDF">pdf</a>, <a href="/format/2402.10119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Neural Network Policy Iteration: Algorithms,  Convergence, and Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Meng%2C+Y">Yiming Meng</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+R">Ruikun Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Mukherjee%2C+A">Amartya Mukherjee</a>, 
<a href="/search/eess?searchtype=author&query=Fitzsimmons%2C+M">Maxwell Fitzsimmons</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+C">Christopher Song</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Solving nonlinear optimal control problems is a challenging task,
particularly for high-dimensional problems. We propose algorithms for
model-based policy iterations to solve nonlinear optimal control problems with
convergence guarantees. The main component of our approach is an iterative
procedure that utilizes neural approximations to solve linear partial
differential equations (PDEs), ensuring convergence. We present two variants of
the algorithms. The first variant formulates the optimization problem as a
linear least square problem, drawing inspiration from extreme learning machine
(ELM) for solving PDEs. This variant efficiently handles low-dimensional
problems with high accuracy. The second variant is based on a physics-informed
neural network (PINN) for solving PDEs and has the potential to address
high-dimensional problems. We demonstrate that both algorithms outperform
traditional approaches, such as Galerkin methods, by a significant margin. We
provide a theoretical analysis of both algorithms in terms of convergence of
neural approximations towards the true optimal solutions in a general setting.
Furthermore, we employ formal verification techniques to demonstrate the
verifiable stability of the resulting controllers.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10122" title="Abstract">arXiv:2402.10122</a> [<a href="/pdf/2402.10122" title="Download PDF">pdf</a>, <a href="/format/2402.10122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating subjectivity and bias in AI development indices: A robust  approach to redefining country rankings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campello%2C+B+S+C">Betania Silva C Campello</a>, 
<a href="/search/cs?searchtype=author&query=Pelegrina%2C+G+D">Guilherme Dean Pelegrina</a>, 
<a href="/search/cs?searchtype=author&query=Pelissari%2C+R">Renata Pelissari</a>, 
<a href="/search/cs?searchtype=author&query=Suyama%2C+R">Ricardo Suyama</a>, 
<a href="/search/cs?searchtype=author&query=Duarte%2C+L+T">Leonardo Tomazeli Duarte</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Countries worldwide have been implementing different actions national
strategies for Artificial Intelligence (AI) to shape policy priorities and
guide their development concerning AI. Several AI indices have emerged to
assess countries' progress in AI development, aiding decision-making on
investments and policy choices. Typically, these indices combine multiple
indicators using linear additive methods such as weighted sums, although they
are limited in their ability to account for interactions among indicators.
Another limitation concerns the use of deterministic weights, which can be
perceived as subjective and vulnerable to debate and scrutiny, especially by
nations that feel disadvantaged. Aiming at mitigating these problems, we
conduct a methodological analysis to derive AI indices based on multiple
criteria decision analysis. Initially, we assess correlations between different
AI dimensions and employ the Choquet integral to model them. Thus, we apply the
Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity
analysis using both weighted sum and Choquet integral in order to evaluate the
stability of the indices with regard the weights. Finally, we introduce a novel
ranking methodology based on SMAA, which considers several sets of weights to
derive the ranking of countries. As a result, instead of using predefined
weights, in the proposed approach, the ranking is achieved based on the
probabilities of countries in occupying a specific position. In the
computational analysis, we utilize the data employed in The Global AI Index
proposed by Tortoise. Results reveal correlations in the data, and our approach
effectively mitigates bias. In the sensitivity analysis, we scrutinize changes
in the ranking resulting from weight adjustments. We demonstrate that our
proposal rankings closely align with those derived from weight variations,
proving to be more robust.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10128" title="Abstract">arXiv:2402.10128</a> [<a href="/pdf/2402.10128" title="Download PDF">pdf</a>, <a href="/format/2402.10128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GES: Generalized Exponential Splatting for Efficient Radiance Field  Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamdi%2C+A">Abdullah Hamdi</a>, 
<a href="/search/cs?searchtype=author&query=Melas-Kyriazi%2C+L">Luke Melas-Kyriazi</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+G">Guocheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+J">Jinjie Mai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruoshi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vondrick%2C+C">Carl Vondrick</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Vedaldi%2C+A">Andrea Vedaldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
<br />It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10130" title="Abstract">arXiv:2402.10130</a> [<a href="/pdf/2402.10130" title="Download PDF">pdf</a>, <a href="/format/2402.10130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Continual Learning Ready for Real-world Challenges?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kontogianni%2C+T">Theodora Kontogianni</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuanwen Yue</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Despite continual learning's long and well-established academic history, its
application in real-world scenarios remains rather limited. This paper contends
that this gap is attributable to a misalignment between the actual challenges
of continual learning and the evaluation protocols in use, rendering proposed
solutions ineffective for addressing the complexities of real-world setups. We
validate our hypothesis and assess progress to date, using a new 3D semantic
segmentation benchmark, OCL-3DSS. We investigate various continual learning
schemes from the literature by utilizing more realistic protocols that
necessitate online and continual learning for dynamic, real-world scenarios
(eg., in robotics and 3D vision applications). The outcomes are sobering: all
considered methods perform poorly, significantly deviating from the upper bound
of joint offline training. This raises questions about the applicability of
existing methods in realistic settings. Our paper aims to initiate a paradigm
shift, advocating for the adoption of continual learning methods through new
experimental protocols that better emulate real-world conditions to facilitate
breakthroughs in the field.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10133" title="Abstract">arXiv:2402.10133</a> [<a href="/pdf/2402.10133" title="Download PDF">pdf</a>, <a href="/format/2402.10133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Reasoning: Personalized Content Generation Without the Cold  Start Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hafnar%2C+D">Davor Hafnar</a> (1), 
<a href="/search/cs?searchtype=author&query=Dem%C5%A1ar%2C+J">Jure Dem&#x161;ar</a> (1 and 2) ((1) Faculty of Computer and Information Science, University of Ljubljana (2) Department of Psychology, Faculty of Arts, University of Ljubljana)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Procedural content generation uses algorithmic techniques to create large
amounts of new content for games at much lower production costs. In newer
approaches, procedural content generation utilizes machine learning. However,
these methods usually require expensive collection of large amounts of data, as
well as the development and training of fairly complex learning models, which
can be both extremely time-consuming and expensive. The core of our research is
to explore whether we can lower the barrier to the use of personalized
procedural content generation through a more practical and generalizable
approach with large language models. Matching game content with player
preferences benefits both players, who enjoy the game more, and developers, who
increasingly depend on players enjoying the game before being able to monetize
it. Therefore, this paper presents a novel approach to achieving
personalization by using large language models to propose levels based on the
gameplay data continuously collected from individual players. We compared the
levels generated using our approach with levels generated with more traditional
procedural generation techniques. Our easily reproducible method has proven
viable in a production setting and outperformed levels generated by traditional
methods in the probability that a player will not quit the game mid-level.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10135" title="Abstract">arXiv:2402.10135</a> [<a href="/pdf/2402.10135" title="Download PDF">pdf</a>, <a href="/format/2402.10135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking federated strategies in Peer-to-Peer Federated learning for  biomedical data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salmeron%2C+J+L">Jose L. Salmeron</a>, 
<a href="/search/cs?searchtype=author&query=Ar%C3%A9valo%2C+I">Irina Ar&#xe9;valo</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Celma%2C+A">Antonio Ruiz-Celma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Heliyon 9 (2023) e16925
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The increasing requirements for data protection and privacy has attracted a
huge research interest on distributed artificial intelligence and specifically
on federated learning, an emerging machine learning approach that allows the
construction of a model between several participants who hold their own private
data. In the initial proposal of federated learning the architecture was
centralised and the aggregation was done with federated averaging, meaning that
a central server will orchestrate the federation using the most straightforward
averaging strategy. This research is focused on testing different federated
strategies in a peer-to-peer environment. The authors propose various
aggregation strategies for federated learning, including weighted averaging
aggregation, using different factors and strategies based on participant
contribution. The strategies are tested with varying data sizes to identify the
most robust ones. This research tests the strategies with several biomedical
datasets and the results of the experiments show that the accuracy-based
weighted average outperforms the classical federated averaging method.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10137" title="Abstract">arXiv:2402.10137</a> [<a href="/pdf/2402.10137" title="Download PDF">pdf</a>, <a href="/format/2402.10137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yimai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Vandyke%2C+D">David Vandyke</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In light of recent advances in large language models~(LLMs), the expectations
for the next generation of virtual assistants include enhanced naturalness and
adaptability across diverse usage scenarios. However, the creation of
high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be
slow and costly. To address these challenges, we introduce Task-Oriented
Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its
automatic generation pipeline. The TOAD dataset simulates realistic app context
interaction and provide a variety of system response style options. Two aspects
of system response styles are considered, verbosity level and users' expression
mirroring. We benchmark TOAD on two response generation tasks and the results
show that modeling more verbose or responses without user expression mirroring
is more challenging.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10138" title="Abstract">arXiv:2402.10138</a> [<a href="/pdf/2402.10138" title="Download PDF">pdf</a>, <a href="/ps/2402.10138" title="Download PostScript">ps</a>, <a href="/format/2402.10138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transaction Capacity, Security and Latency in Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doger%2C+M">Mustafa Doger</a>, 
<a href="/search/cs?searchtype=author&query=Ulukus%2C+S">Sennur Ulukus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Discrete Mathematics (cs.DM); Information Theory (cs.IT)

</div>
<p class="mathjax">We analyze how secure a block is after the block becomes k-deep, i.e.,
security-latency, for Nakamoto consensus under an exponential network delay
model. We give parameter regimes for which transactions are safe when
sufficiently deep in the chain. We compare our results for Nakamoto consensus
under bounded network delay models and obtain analogous bounds for safety
violation threshold. Next, modeling the blockchain system as a batch service
queue with exponential network delay, we connect the security-latency analysis
to sustainable transaction rate of the queue system. As our model assumes
exponential network delay, batch service queue models give a meaningful
trade-off between transaction capacity, security and latency. As adversary can
attack the queue service to hamper the service process, we consider two
different attacks for adversary. In an extreme scenario, we modify the
selfish-mining attack for this purpose and consider its effect on the
sustainable transaction rate of the queue.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10139" title="Abstract">arXiv:2402.10139</a> [<a href="/pdf/2402.10139" title="Download PDF">pdf</a>, <a href="/ps/2402.10139" title="Download PostScript">ps</a>, <a href="/format/2402.10139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast interpolation and multiplication of unbalanced polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giorgi%2C+P">Pascal Giorgi</a>, 
<a href="/search/cs?searchtype=author&query=Grenet%2C+B">Bruno Grenet</a>, 
<a href="/search/cs?searchtype=author&query=Cray%2C+A+P+d">Armelle Perret du Cray</a>, 
<a href="/search/cs?searchtype=author&query=Roche%2C+D+S">Daniel S. Roche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We consider the classical problems of interpolating a polynomial given a
black box for evaluation, and of multiplying two polynomials, in the setting
where the bit-lengths of the coefficients may vary widely, so-called unbalanced
polynomials. Writing s for the total bit-length and D for the degree, our new
algorithms have expected running time $\tilde{O}(s \log D)$, whereas previous
methods for (resp.) dense or sparse arithmetic have at least $\tilde{O}(sD)$ or
$\tilde{O}(s^2)$ bit complexity.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10142" title="Abstract">arXiv:2402.10142</a> [<a href="/pdf/2402.10142" title="Download PDF">pdf</a>, <a href="/format/2402.10142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Changing Probabilities via Dynamic Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madani%2C+O">Omid Madani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages, 24 figures, 17 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Consider a predictor, a learner, whose input is a stream of discrete items.
The predictor's task, at every time point, is probabilistic multiclass
prediction, i.e., to predict which item may occur next by outputting zero or
more candidate items, each with a probability, after which the actual item is
revealed and the predictor learns from this observation. To output
probabilities, the predictor keeps track of the proportions of the items it has
seen. The predictor has constant (limited) space and we seek efficient
prediction and update techniques: The stream is unbounded, the set of items is
unknown to the predictor and their totality can also grow unbounded. Moreover,
there is non-stationarity: the underlying frequencies of items may change,
substantially, from time to time. For instance, new items may start appearing
and a few currently frequent items may cease to occur again. The predictor,
being space-bounded, need only provide probabilities for those items with
(currently) sufficiently high frequency, i.e., the salient items. This problem
is motivated in the setting of prediction games, a self-supervised learning
regime where concepts serve as both the predictors and the predictands, and the
set of concepts grows over time, resulting in non-stationarities as new
concepts are generated and used. We develop moving average techniques designed
to respond to such non-stationarities in a timely manner, and explore their
properties. One is a simple technique based on queuing of count snapshots, and
another is a combination of queuing together with an extended version of sparse
EMA. The latter combination supports predictand-specific dynamic learning
rates. We find that this flexibility allows for a more accurate and timely
convergence.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10145" title="Abstract">arXiv:2402.10145</a> [<a href="/pdf/2402.10145" title="Download PDF">pdf</a>, <a href="/format/2402.10145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A chaotic maps-based privacy-preserving distributed deep learning for  incomplete and Non-IID datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ar%C3%A9valo%2C+I">Irina Ar&#xe9;valo</a>, 
<a href="/search/cs?searchtype=author&query=Salmeron%2C+J+L">Jose L. Salmeron</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Emerging Topics in Computing, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning is a machine learning approach that enables the training
of a deep learning model among several participants with sensitive data that
wish to share their own knowledge without compromising the privacy of their
data. In this research, the authors employ a secured Federated Learning method
with an additional layer of privacy and proposes a method for addressing the
non-IID challenge. Moreover, differential privacy is compared with
chaotic-based encryption as layer of privacy. The experimental approach
assesses the performance of the federated deep learning model with differential
privacy using both IID and non-IID data. In each experiment, the Federated
Learning process improves the average performance metrics of the deep neural
network, even in the case of non-IID data.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10150" title="Abstract">arXiv:2402.10150</a> [<a href="/pdf/2402.10150" title="Download PDF">pdf</a>, <a href="/format/2402.10150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yiwei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaoliang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TMLR in 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In self-supervised contrastive learning, a widely-adopted objective function
is InfoNCE, which uses the heuristic cosine similarity for the representation
comparison, and is closely related to maximizing the Kullback-Leibler
(KL)-based mutual information. In this paper, we aim at answering two
intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides
the popular cosine similarity, can we design a better similarity function? We
provide answers to both questions by generalizing the KL-based mutual
information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)
using the $f$-divergences. To answer the first question, we provide a wide
range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,
alignment and uniformity), and meanwhile result in similar or even superior
performance. For the second question, assuming that the joint feature
distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian
similarity with better interpretability and empirical performance. Finally, we
identify close relationships between the $f$-MICL objective and several popular
InfoNCE-based objectives. Using benchmark tasks from both vision and natural
language, we empirically evaluate $f$-MICL with different $f$-divergences on
various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that
$f$-MICL generally outperforms the benchmarks and the best-performing
$f$-divergence is task and dataset dependent.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10151" title="Abstract">arXiv:2402.10151</a> [<a href="/pdf/2402.10151" title="Download PDF">pdf</a>, <a href="/format/2402.10151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ControlLM: Crafting Diverse Personalities for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Y">Yixuan Weng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As language models continue to scale in size and capability, they display an
array of emerging behaviors, both beneficial and concerning. This heightens the
need to control model behaviors. We hope to be able to control the personality
traits of language models at the inference-time so as to have various character
features, on top of which the requirements of different types of tasks can be
met. Personality is a higher-level and more abstract behavioral representation
for language models. We introduce ControlLM, which leverages differential
activation patterns, derived from contrasting behavioral prompts in the model's
latent space, to influence the model's personality traits at inference. This
approach allows for the precise, real-time adjustment of model behavior. First,
we demonstrate ControlLM's capacity to elicit diverse persona behaviors without
any training, while precision control allows personality traits to closely
match average human values. Subsequently, we showcase improved reasoning and
question answering through selective amplification of beneficial attributes
like conscientiousness and friendliness. We hope that this work will inspire
research on controlling human-like behaviors of language models and provide
insights for future research. Our code is publicly available at:
https://github.com/wengsyx/ControlLM.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10152" title="Abstract">arXiv:2402.10152</a> [<a href="/pdf/2402.10152" title="Download PDF">pdf</a>, <a href="/format/2402.10152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new type of simplified inverse Lax-Wendroff boundary treatment I:  hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+S">Shihao Liu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+T">Tingting Li</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+Z">Ziqiang Cheng</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+Y">Yan Jiang</a>, 
<a href="/search/math?searchtype=author&query=Shu%2C+C">Chi-Wang Shu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+M">Mengping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we design a new kind of high order inverse Lax-Wendroff (ILW)
boundary treatment for solving hyperbolic conservation laws with finite
difference method on a Cartesian mesh. This new ILW method decomposes the
construction of ghost point values near inflow boundary into two steps:
interpolation and extrapolation. At first, we impose values of some artificial
auxiliary points through a polynomial interpolating the interior points near
the boundary. Then, we will construct a Hermite extrapolation based on those
auxiliary point values and the spatial derivatives at boundary obtained via the
ILW procedure. This polynomial will give us the approximation to the ghost
point value. By an appropriate selection of those artificial auxiliary points,
high-order accuracy and stable results can be achieved. Moreover, theoretical
analysis indicates that comparing with the original ILW method, especially for
higher order accuracy, the new proposed one would require fewer terms using the
relatively complicated ILW procedure and thus improve computational efficiency
on the premise of maintaining accuracy and stability. We perform numerical
experiments on several benchmarks, including one- and two-dimensional scalar
equations and systems. The robustness and efficiency of the proposed scheme is
numerically verified.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10153" title="Abstract">arXiv:2402.10153</a> [<a href="/pdf/2402.10153" title="Download PDF">pdf</a>, <a href="/format/2402.10153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study  for Diabetes Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbasian%2C+M">Mahyar Abbasian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhongqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Khatibi%2C+E">Elahe Khatibi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengfei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nagesh%2C+N">Nitish Nagesh</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, and 2 tables, conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Effective diabetes management is crucial for maintaining health in diabetic
patients. Large Language Models (LLMs) have opened new avenues for diabetes
management, facilitating their efficacy. However, current LLM-based approaches
are limited by their dependence on general sources and lack of integration with
domain-specific knowledge, leading to inaccurate responses. In this paper, we
propose a knowledge-infused LLM-powered conversational health agent (CHA) for
diabetic patients. We customize and leverage the open-source openCHA framework,
enhancing our CHA with external knowledge and analytical capabilities. This
integration involves two key components: 1) incorporating the American Diabetes
Association dietary guidelines and the Nutritionix information and 2) deploying
analytical tools that enable nutritional intake calculation and comparison with
the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes
100 diabetes-related questions on daily meal choices and assessing the
potential risks associated with the suggested diet. Our findings show that the
proposed agent demonstrates superior performance in generating responses to
manage essential nutrients.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10158" title="Abstract">arXiv:2402.10158</a> [<a href="/pdf/2402.10158" title="Download PDF">pdf</a>, <a href="/format/2402.10158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoNet: Neural Estimation of Mutual Information without Test-Time  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Song Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qunsong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yanchao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Estimating mutual correlations between random variables or data streams is
essential for intelligent behavior and decision-making. As a fundamental
quantity for measuring statistical relationships, mutual information has been
extensively studied and utilized for its generality and equitability. However,
existing methods often lack the efficiency needed for real-time applications,
such as test-time optimization of a neural network, or the differentiability
required for end-to-end learning, like histograms. We introduce a neural
network called InfoNet, which directly outputs mutual information estimations
of data streams by leveraging the attention mechanism and the computational
efficiency of deep learning infrastructures. By maximizing a dual formulation
of mutual information through large-scale simulated training, our approach
circumvents time-consuming test-time optimization and offers generalization
ability. We evaluate the effectiveness and generalization of our proposed
mutual information estimation scheme on various families of distributions and
applications. Our results demonstrate that InfoNet and its training process
provide a graceful efficiency-accuracy trade-off and order-preserving
properties. We will make the code and models available as a comprehensive
toolbox to facilitate studies in different fields requiring real-time mutual
information estimation.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10161" title="Abstract">arXiv:2402.10161</a> [<a href="/pdf/2402.10161" title="Download PDF">pdf</a>, <a href="/format/2402.10161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robotic Exploration using Generalized Behavioral Entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suresh%2C+A">Aamodh Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Nieto-Granda%2C+C">Carlos Nieto-Granda</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+S">Sonia Martinez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">This work presents and evaluates a novel strategy for robotic exploration
that leverages human models of uncertainty perception. To do this, we introduce
a measure of uncertainty that we term ``Behavioral entropy'', which builds on
Prelec's probability weighting from Behavioral Economics. We show that the new
operator is an admissible generalized entropy, analyze its theoretical
properties and compare it with other common formulations such as Shannon's and
Renyi's. In particular, we discuss how the new formulation is more expressive
in the sense of measures of sensitivity and perceptiveness to uncertainty
introduced here. Then we use Behavioral entropy to define a new type of utility
function that can guide a frontier-based environment exploration process. The
approach's benefits are illustrated and compared in a Proof-of-Concept and
ROS-unity simulation environment with a Clearpath Warthog robot. We show that
the robot equipped with Behavioral entropy explores faster than Shannon and
Renyi entropies.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10163" title="Abstract">arXiv:2402.10163</a> [<a href="/pdf/2402.10163" title="Download PDF">pdf</a>, <a href="/format/2402.10163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karuvally%2C+A">Arjun Karuvally</a>, 
<a href="/search/cs?searchtype=author&query=Sejnowski%2C+T+J">Terrence J. Sejnowski</a>, 
<a href="/search/cs?searchtype=author&query=Siegelmann%2C+H+T">Hava T. Siegelmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.02430">arXiv:2310.02430</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Traveling waves are a fundamental phenomenon in the brain, playing a crucial
role in short-term information storage. In this study, we leverage the concept
of traveling wave dynamics within a neural lattice to formulate a theoretical
model of neural working memory, study its properties, and its real world
implications in AI. The proposed model diverges from traditional approaches,
which assume information storage in static, register-like locations updated by
interference. Instead, the model stores data as waves that is updated by the
wave's boundary conditions. We rigorously examine the model's capabilities in
representing and learning state histories, which are vital for learning
history-dependent dynamical systems. The findings reveal that the model
reliably stores external information and enhances the learning process by
addressing the diminishing gradient problem. To understand the model's
real-world applicability, we explore two cases: linear boundary condition and
non-linear, self-attention-driven boundary condition. The experiments reveal
that the linear scenario is effectively learned by Recurrent Neural Networks
(RNNs) through backpropagation when modeling history-dependent dynamical
systems. Conversely, the non-linear scenario parallels the autoregressive loop
of an attention-only transformer. Collectively, our findings suggest the
broader relevance of traveling waves in AI and its potential in advancing
neural network architectures.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10168" title="Abstract">arXiv:2402.10168</a> [<a href="/pdf/2402.10168" title="Download PDF">pdf</a>, <a href="/format/2402.10168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSRGM -- Sequence Classification and Ranking in Indian Classical  Music with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhusudhan%2C+S+T">Sathwik Tejaswi Madhusudhan</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhary%2C+G">Girish Chowdhary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a
melodic framework for compositions and improvisations alike. Raga Recognition
is an important music information retrieval task in ICM as it can aid numerous
downstream applications ranging from music recommendations to organizing huge
music collections. In this work, we propose a deep learning based approach to
Raga recognition. Our approach employs efficient pre possessing and learns
temporal sequences in music data using Long Short Term Memory based Recurrent
Neural Networks (LSTM-RNN). We train and test the network on smaller sequences
sampled from the original audio while the final inference is performed on the
audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during
inference on the Comp Music Carnatic dataset and its 10 Raga subset
respectively making it the state-of-the-art for the Raga recognition task. Our
approach also enables sequence ranking which aids us in retrieving melodic
patterns from a given music data base that are closely related to the presented
query sequence.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10171" title="Abstract">arXiv:2402.10171</a> [<a href="/pdf/2402.10171" title="Download PDF">pdf</a>, <a href="/format/2402.10171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Engineering for Scaling Language Models to 128K Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xinyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code at <a href="https://github.com/FranxYao/Long-Context-Data-Engineering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the continual pretraining recipe for scaling language models'
context lengths to 128K, with a focus on data engineering. We hypothesize that
long context modeling, in particular \textit{the ability to utilize information
at arbitrary input locations}, is a capability that is mostly already acquired
through large-scale pretraining, and that this capability can be readily
extended to contexts substantially longer than seen during training~(e.g., 4K
to 128K) through lightweight continual pretraining on appropriate data mixture.
We investigate the \textit{quantity} and \textit{quality} of the data for
continual pretraining: (1) for quantity, we show that 500 million to 5 billion
tokens are enough to enable the model to retrieve information anywhere within
the 128K context; (2) for quality, our results equally emphasize \textit{domain
balance} and \textit{length upsampling}. Concretely, we find that naively
upsampling longer data on certain domains like books, a common practice of
existing work, gives suboptimal performance, and that a balanced domain mixture
is important. We demonstrate that continual pretraining of the full model on
1B-5B tokens of such data is an effective and affordable strategy for scaling
the context length of language models to 128K. Our recipe outperforms strong
open-source long-context models and closes the gap to frontier models like
GPT-4 128K.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10172" title="Abstract">arXiv:2402.10172</a> [<a href="/pdf/2402.10172" title="Download PDF">pdf</a>, <a href="/format/2402.10172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AhmadiTeshnizi%2C+A">Ali AhmadiTeshnizi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Optimization problems are pervasive in sectors from manufacturing and
distribution to healthcare. However, most such problems are still solved
heuristically by hand rather than optimally by state-of-the-art solvers because
the expertise required to formulate and solve these problems limits the
widespread adoption of optimization tools and techniques. This paper introduces
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve (mixed integer) linear programming problems from their natural language
descriptions. OptiMUS can develop mathematical models, write and debug solver
code, evaluate the generated solutions, and improve its model and code based on
these evaluations. OptiMUS utilizes a modular structure to process problems,
allowing it to handle problems with long descriptions and complex data without
long prompts. Experiments demonstrate that OptiMUS outperforms existing
state-of-the-art methods on easy datasets by more than $20\%$ and on hard
datasets (including a new dataset, NLP4LP, released with this paper that
features long and complex problems) by more than $30\%$.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10174" title="Abstract">arXiv:2402.10174</a> [<a href="/pdf/2402.10174" title="Download PDF">pdf</a>, <a href="/format/2402.10174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overapproximation of Non-Linear Integer Arithmetic for Smart Contract  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hozzov%C3%A1%2C+P">Petra Hozzov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Bend%C3%ADk%2C+J">Jaroslav Bend&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Nutz%2C+A">Alexander Nutz</a>, 
<a href="/search/cs?searchtype=author&query=Rodeh%2C+Y">Yoav Rodeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures, presented at The International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The need to solve non-linear arithmetic constraints presents a major obstacle
to the automatic verification of smart contracts. In this case study we focus
on the two overapproximation techniques used by the industry verification tool
Certora Prover: overapproximation of non-linear integer arithmetic using linear
integer arithmetic and using non-linear real arithmetic. We compare the
performance of contemporary SMT solvers on verification conditions produced by
the Certora Prover using these two approximations against the natural
non-linear integer arithmetic encoding. Our evaluation shows that the use of
the overapproximation methods leads to solving a significant number of new
problems.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10175" title="Abstract">arXiv:2402.10175</a> [<a href="/pdf/2402.10175" title="Download PDF">pdf</a>, <a href="/format/2402.10175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for  Positional Discourse Coherence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yixuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent large language models (LLMs) have shown remarkable performance in
aligning generated text with user intentions across various tasks. When it
comes to long-form text generation, there has been a growing interest in
generation from a discourse coherence perspective. However, existing lexical or
semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the
discourse coherence. The development of discourse-specific automatic evaluation
methods for assessing the output of LLMs warrants greater focus and
exploration. In this paper, we present a novel automatic metric designed to
quantify the discourse divergence between two long-form articles. Extensive
experiments on three datasets from representative domains demonstrate that our
metric aligns more closely with human preferences and GPT-4 coherence
evaluation, outperforming existing evaluation methods.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10176" title="Abstract">arXiv:2402.10176</a> [<a href="/pdf/2402.10176" title="Download PDF">pdf</a>, <a href="/format/2402.10176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toshniwal%2C+S">Shubham Toshniwal</a>, 
<a href="/search/cs?searchtype=author&query=Moshkov%2C+I">Ivan Moshkov</a>, 
<a href="/search/cs?searchtype=author&query=Narenthiran%2C+S">Sean Narenthiran</a>, 
<a href="/search/cs?searchtype=author&query=Gitman%2C+D">Daria Gitman</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Fei Jia</a>, 
<a href="/search/cs?searchtype=author&query=Gitman%2C+I">Igor Gitman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Data and models are available at <a href="https://huggingface.co/collections/nvidia/openmath-65c5619de2ba059be0775014">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown the immense potential of synthetically generated
datasets for training large language models (LLMs), especially for acquiring
targeted skills. Current large-scale math instruction tuning datasets such as
MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed
using outputs from closed-source LLMs with commercially restrictive licenses. A
key reason limiting the use of open-source LLMs in these data generation
pipelines has been the wide gap between the mathematical skills of the best
closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on
the recent progress in open-source LLMs, our proposed prompting novelty, and
some brute-force scaling, we construct OpenMathInstruct-1, a math instruction
tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by
synthesizing code-interpreter solutions for GSM8K and MATH, two popular math
reasoning benchmarks, using the recently released and permissively licensed
Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of
OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which
is competitive with the best gpt-distilled models. We release our code, models,
and the OpenMathInstruct-1 dataset under a commercially permissive license.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10177" title="Abstract">arXiv:2402.10177</a> [<a href="/pdf/2402.10177" title="Download PDF">pdf</a>, <a href="/format/2402.10177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Scale Constrained Clustering With Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schesch%2C+B">Benedikt Schesch</a>, 
<a href="/search/cs?searchtype=author&query=Caserta%2C+M">Marco Caserta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LEANOPT-24 AAAI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given a network, allocating resources at clusters level, rather than at each
node, enhances efficiency in resource allocation and usage. In this paper, we
study the problem of finding fully connected disjoint clusters to minimize the
intra-cluster distances and maximize the number of nodes assigned to the
clusters, while also ensuring that no two nodes within a cluster exceed a
threshold distance. While the problem can easily be formulated using a binary
linear model, traditional combinatorial optimization solvers struggle when
dealing with large-scale instances. We propose an approach to solve this
constrained clustering problem via reinforcement learning. Our method involves
training an agent to generate both feasible and (near) optimal solutions. The
agent learns problem-specific heuristics, tailored to the instances encountered
in this task. In the results section, we show that our algorithm finds near
optimal solutions, even for large scale instances.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10178" title="Abstract">arXiv:2402.10178</a> [<a href="/pdf/2402.10178" title="Download PDF">pdf</a>, <a href="/format/2402.10178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and  Agent Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Junfeng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinsong Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The emergence of Large Language Models (LLMs) like ChatGPT has inspired the
development of LLM-based agents capable of addressing complex, real-world
tasks. However, these agents often struggle during task execution due to
methodological constraints, such as error propagation and limited adaptability.
To address this issue, we propose a multi-agent framework based on dynamic Task
Decomposition and Agent Generation (TDAG). This framework dynamically
decomposes complex tasks into smaller subtasks and assigns each to a
specifically generated subagent, thereby enhancing adaptability in diverse and
unpredictable real-world tasks. Simultaneously, existing benchmarks often lack
the granularity needed to evaluate incremental progress in complex, multi-step
tasks. In response, we introduce ItineraryBench in the context of travel
planning, featuring interconnected, progressively complex tasks with a
fine-grained evaluation system. ItineraryBench is designed to assess agents'
abilities in memory, planning, and tool usage across tasks of varying
complexity. Our experimental results reveal that TDAG significantly outperforms
established baselines, showcasing its superior adaptability and context
awareness in complex task scenarios.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10182" title="Abstract">arXiv:2402.10182</a> [<a href="/pdf/2402.10182" title="Download PDF">pdf</a>, <a href="/format/2402.10182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent Demonstration in General-Sum Dynamic Games via Iterative  Linear-Quadratic Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jingqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Siththaranjan%2C+A">Anand Siththaranjan</a>, 
<a href="/search/eess?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>, 
<a href="/search/eess?searchtype=author&query=Tomlin%2C+C">Claire Tomlin</a>, 
<a href="/search/eess?searchtype=author&query=Bajcsy%2C+A">Andrea Bajcsy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review by L4DC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Autonomous agents should be able to coordinate with other agents without
knowing their intents ahead of time. While prior work has studied how agents
can gather information about the intent of others, in this work we study the
inverse problem: how agents can demonstrate their intent to others, within the
framework of general-sum dynamic games. We first present a model of this intent
demonstration problem and then propose an algorithm that enables an agent to
trade off their task performance and intent demonstration to improve the
overall system's performance. To scale to continuous states and action spaces
as well as to nonlinear dynamics and costs, our algorithm leverages
linear-quadratic approximations with an efficient intent teaching guarantee.
Our empirical results show that intent demonstration accelerates other agents'
learning and enables the demonstrating agent to balance task performance with
intent expression.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10184" title="Abstract">arXiv:2402.10184</a> [<a href="/pdf/2402.10184" title="Download PDF">pdf</a>, <a href="/format/2402.10184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Josef Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">There is a trilemma in reinforcement learning from human feedback (RLHF): the
incompatibility between highly diverse contexts, low labeling cost, and
reliable alignment performance. Here we aim to mitigate such incompatibility
through the design of dataset information structures during reward modeling.
Specifically, we first reexamine the RLHF process and propose a theoretical
framework portraying it as an autoencoding process over text distributions. Our
framework formalizes the RLHF objective of ensuring distributional consistency
between human preference and large language model (LLM) behavior. Building on
this framework, we then systematically investigate the performance impact of
information structure in the reward modeling stage of RLHF. To further
understand reward generalization in the reward modeling stage, we introduce a
new method based on random graph theory that models generalization in the
semantic space. A key insight of our analysis is the superiority of the
tree-based information structure in reward modeling, compared to chain-based
baselines adopted by conventional RLHF methods. We derive that under highly
complex contexts with limited data, the tree-based reward model (RM) induces up
to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where
$n$ is the dataset size. To validate our theoretical contribution, we
demonstrate that on three different NLP tasks, the tree-based RM achieves 65%
win rate on average against chain-based baselines. Looking forward, we hope our
framework can serve as a step towards understanding goal misgeneralization.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10186" title="Abstract">arXiv:2402.10186</a> [<a href="/pdf/2402.10186" title="Download PDF">pdf</a>, <a href="/format/2402.10186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-consistent Validation for Machine Learning Electronic Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Gengyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gengchen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zekun Lou</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han-sen Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Machine learning has emerged as a significant approach to efficiently tackle
electronic structure problems. Despite its potential, there is less guarantee
for the model to generalize to unseen data that hinders its application in
real-world scenarios. To address this issue, a technique has been proposed to
estimate the accuracy of the predictions. This method integrates machine
learning with self-consistent field methods to achieve both low validation cost
and interpret-ability. This, in turn, enables exploration of the model's
ability with active learning and instills confidence in its integration into
real-world studies.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10189" title="Abstract">arXiv:2402.10189</a> [<a href="/pdf/2402.10189" title="Download PDF">pdf</a>, <a href="/format/2402.10189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Decomposition and Quantification for In-Context Learning of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xujiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiyou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+M">Mika Oishi</a>, 
<a href="/search/cs?searchtype=author&query=Osaki%2C+T">Takao Osaki</a>, 
<a href="/search/cs?searchtype=author&query=Matsuda%2C+K">Katsushi Matsuda</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jie Ji</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+G">Guangji Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In-context learning has emerged as a groundbreaking ability of Large Language
Models (LLMs) and revolutionized various fields by providing a few
task-relevant demonstrations in the prompt. However, trustworthy issues with
LLM's response, such as hallucination, have also been actively discussed.
Existing works have been devoted to quantifying the uncertainty in LLM's
response, but they often overlook the complex nature of LLMs and the uniqueness
of in-context learning. In this work, we delve into the predictive uncertainty
of LLMs associated with in-context learning, highlighting that such
uncertainties may stem from both the provided demonstrations (aleatoric
uncertainty) and ambiguities tied to the model's configurations (epistemic
uncertainty). We propose a novel formulation and corresponding estimation
method to quantify both types of uncertainties. The proposed method offers an
unsupervised way to understand the prediction of in-context learning in a
plug-and-play fashion. Extensive experiments are conducted to demonstrate the
effectiveness of the decomposition. The code and data are available at:
\url{https://github.com/lingchen0331/UQ_ICL}.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10191" title="Abstract">arXiv:2402.10191</a> [<a href="/pdf/2402.10191" title="Download PDF">pdf</a>, <a href="/format/2402.10191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedAnchor: Enhancing Federated Semi-Supervised Learning with Label  Contrastive Loss for Unlabeled Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xinchi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sani%2C+L">Lorenzo Sani</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Heng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wanru Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gusmao%2C+P+P+B">Pedro P. B. Gusmao</a>, 
<a href="/search/cs?searchtype=author&query=Alibeigi%2C+M">Mina Alibeigi</a>, 
<a href="/search/cs?searchtype=author&query=Iacob%2C+A">Alex Iacob</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) is a distributed learning paradigm that facilitates
collaborative training of a shared global model across devices while keeping
data localized. The deployment of FL in numerous real-world applications faces
delays, primarily due to the prevalent reliance on supervised tasks. Generating
detailed labels at edge devices, if feasible, is demanding, given resource
constraints and the imperative for continuous data updates. In addressing these
challenges, solutions such as federated semi-supervised learning (FSSL), which
relies on unlabeled clients' data and a limited amount of labeled data on the
server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL
method that introduces a unique double-head structure, called anchor head,
paired with the classification head trained exclusively on labeled anchor data
on the server. The anchor head is empowered with a newly designed label
contrastive loss based on the cosine similarity metric. Our approach mitigates
the confirmation bias and overfitting issues associated with pseudo-labeling
techniques based on high-confidence model prediction samples. Extensive
experiments on CIFAR10/100 and SVHN datasets demonstrate that our method
outperforms the state-of-the-art method by a significant margin in terms of
convergence rate and model accuracy.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10192" title="Abstract">arXiv:2402.10192</a> [<a href="/pdf/2402.10192" title="Download PDF">pdf</a>, <a href="/format/2402.10192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Excitation Projective Simulation with a Many-Body Physics Inspired  Inductive Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=LeMaitre%2C+P+A">Philip A. LeMaitre</a>, 
<a href="/search/cs?searchtype=author&query=Krumm%2C+M">Marius Krumm</a>, 
<a href="/search/cs?searchtype=author&query=Briegel%2C+H+J">Hans J. Briegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures. Code repository at <a href="https://github.com/MariusKrumm/ManyBodyMEPS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Quantum Physics (quant-ph)

</div>
<p class="mathjax">With the impressive progress of deep learning, applications relying on
machine learning are increasingly being integrated into daily life. However,
most deep learning models have an opaque, oracle-like nature making it
difficult to interpret and understand their decisions. This problem led to the
development of the field known as eXplainable Artificial Intelligence (XAI).
One method in this field known as Projective Simulation (PS) models a
chain-of-thought as a random walk of a particle on a graph with vertices that
have concepts attached to them. While this description has various benefits,
including the possibility of quantization, it cannot be naturally used to model
thoughts that combine several concepts simultaneously. To overcome this
limitation, we introduce Multi-Excitation Projective Simulation (mePS), a
generalization that considers a chain-of-thought to be a random walk of several
particles on a hypergraph. A definition for a dynamic hypergraph is put forward
to describe the agent's training history along with applications to AI and
hypergraph visualization. An inductive bias inspired by the remarkably
successful few-body interaction models used in quantum many-body physics is
formalized for our classical mePS framework and employed to tackle the
exponential complexity associated with naive implementations of hypergraphs. We
prove that our inductive bias reduces the complexity from exponential to
polynomial, with the exponent representing the cutoff on how many particles can
interact. We numerically apply our method to two toy environments and a more
complex scenario modelling the diagnosis of a broken computer. These
environments demonstrate the resource savings provided by an appropriate choice
of inductive bias, as well as showcasing aspects of interpretability. A quantum
model for mePS is also briefly outlined and some future directions for it are
discussed.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10193" title="Abstract">arXiv:2402.10193</a> [<a href="/pdf/2402.10193" title="Download PDF">pdf</a>, <a href="/format/2402.10193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BitDelta: Your Fine-Tune May Only Be Worth One Bit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">James Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Guangxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+T">Tri Dao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianle Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) are typically trained in two phases:
pre-training on large internet-scale datasets, and fine-tuning for downstream
tasks. Given the higher computational demand of pre-training, it's intuitive to
assume that fine-tuning adds less new information to the model, and is thus
more compressible. We explore this assumption by decomposing the weights of
fine-tuned models into their pre-trained components and an additional delta. We
introduce a simple method, BitDelta, which successfully quantizes this delta
down to 1 bit without compromising performance. This interesting finding not
only highlights the potential redundancy of information added during
fine-tuning, but also has significant implications for the multi-tenant serving
and multi-tenant storage of fine-tuned models. By enabling the use of a single
high-precision base model accompanied by multiple 1-bit deltas, BitDelta
dramatically reduces GPU memory requirements by more than 10x, which can also
be translated to enhanced generation latency in multi-tenant settings. We
validate BitDelta through experiments across Llama-2 and Mistral model
families, and on models up to 70B parameters, showcasing minimal performance
degradation over all tested settings.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10196" title="Abstract">arXiv:2402.10196</a> [<a href="/pdf/2402.10196" title="Download PDF">pdf</a>, <a href="/format/2402.10196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+L">Lingbo Mo</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zeyi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Boyuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10198" title="Abstract">arXiv:2402.10198</a> [<a href="/pdf/2402.10198" title="Download PDF">pdf</a>, <a href="/format/2402.10198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Transformers in Time Series Forecasting with  Sharpness-Aware Minimization and Channel-Wise Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ilbert%2C+R">Romain Ilbert</a>, 
<a href="/search/cs?searchtype=author&query=Odonnat%2C+A">Ambroise Odonnat</a>, 
<a href="/search/cs?searchtype=author&query=Feofanov%2C+V">Vasilii Feofanov</a>, 
<a href="/search/cs?searchtype=author&query=Virmaux%2C+A">Aladin Virmaux</a>, 
<a href="/search/cs?searchtype=author&query=Paolo%2C+G">Giuseppe Paolo</a>, 
<a href="/search/cs?searchtype=author&query=Palpanas%2C+T">Themis Palpanas</a>, 
<a href="/search/cs?searchtype=author&query=Redko%2C+I">Ievgen Redko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformer-based architectures achieved breakthrough performance in natural
language processing and computer vision, yet they remain inferior to simpler
linear baselines in multivariate long-term forecasting. To better understand
this phenomenon, we start by studying a toy linear forecasting problem for
which we show that transformers are incapable of converging to their true
solution despite their high expressive power. We further identify the attention
of transformers as being responsible for this low generalization capacity.
Building upon this insight, we propose a shallow lightweight transformer model
that successfully escapes bad local minima when optimized with sharpness-aware
optimization. We empirically demonstrate that this result extends to all
commonly used real-world multivariate time series datasets. In particular,
SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on
average, while having ~4 times fewer parameters. The code is available at
https://github.com/romilbert/samformer.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10200" title="Abstract">arXiv:2402.10200</a> [<a href="/pdf/2402.10200" title="Download PDF">pdf</a>, <a href="/format/2402.10200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Thought Reasoning Without Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In enhancing the reasoning capabilities of large language models (LLMs),
prior research primarily focuses on specific prompting techniques such as
few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while
effective, often involve manually intensive prompt engineering. Our study takes
a novel approach by asking: Can LLMs reason effectively without prompting? Our
findings reveal that, intriguingly, CoT reasoning paths can be elicited from
pre-trained LLMs by simply altering the \textit{decoding} process. Rather than
conventional greedy decoding, we investigate the top-$k$ alternative tokens,
uncovering that CoT paths are frequently inherent in these sequences. This
approach not only bypasses the confounders of prompting but also allows us to
assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe
that the presence of a CoT in the decoding path correlates with a higher
confidence in the model's decoded answer. This confidence metric effectively
differentiates between CoT and non-CoT paths. Extensive empirical studies on
various reasoning benchmarks show that the proposed CoT-decoding substantially
outperforms the standard greedy decoding.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10202" title="Abstract">arXiv:2402.10202</a> [<a href="/pdf/2402.10202" title="Download PDF">pdf</a>, <a href="/format/2402.10202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Associative Memory and Probabilistic Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaeffer%2C+R">Rylan Schaeffer</a>, 
<a href="/search/cs?searchtype=author&query=Zahedi%2C+N">Nika Zahedi</a>, 
<a href="/search/cs?searchtype=author&query=Khona%2C+M">Mikail Khona</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+D">Dhruv Pai</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+S">Sang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Ostrow%2C+M">Mitchell Ostrow</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+S">Sarthak Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Carranza%2C+A">Andres Carranza</a>, 
<a href="/search/cs?searchtype=author&query=Fiete%2C+I+R">Ila Rani Fiete</a>, 
<a href="/search/cs?searchtype=author&query=Gromov%2C+A">Andrey Gromov</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Associative memory and probabilistic modeling are two fundamental topics in
artificial intelligence. The first studies recurrent neural networks designed
to denoise, complete and retrieve data, whereas the second studies learning and
sampling from probability distributions. Based on the observation that
associative memory's energy functions can be seen as probabilistic modeling's
negative log likelihoods, we build a bridge between the two that enables useful
flow of ideas in both directions. We showcase four examples: First, we propose
new energy-based models that flexibly adapt their energy functions to new
in-context datasets, an approach we term \textit{in-context learning of energy
functions}. Second, we propose two new associative memory models: one that
dynamically creates new memories as necessitated by the training data using
Bayesian nonparametrics, and another that explicitly computes proportional
memory assignments using the evidence lower bound. Third, using tools from
associative memory, we analytically and numerically characterize the memory
capacity of Gaussian kernel density estimators, a widespread tool in
probababilistic modeling. Fourth, we study a widespread implementation choice
in transformers -- normalization followed by self attention -- to show it
performs clustering on the hypersphere. Altogether, this work urges further
exchange of useful ideas between these two continents of artificial
intelligence.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10206" title="Abstract">arXiv:2402.10206</a> [<a href="/pdf/2402.10206" title="Download PDF">pdf</a>, <a href="/format/2402.10206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%A5nkestad%2C+M">Maria B&#xe5;nkestad</a>, 
<a href="/search/cs?searchtype=author&query=Andersson%2C+J">Jennifer Andersson</a>, 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reducing a graph while preserving its overall structure is an important
problem with many applications. Typically, the reduction approaches either
remove edges (sparsification) or merge nodes (coarsening) in an unsupervised
way with no specific downstream task in mind. In this paper, we present an
approach for subsampling graph structures using an Ising model defined on
either the nodes or edges and learning the external magnetic field of the Ising
model using a graph neural network. Our approach is task-specific as it can
learn how to reduce a graph for a specific downstream task in an end-to-end
fashion. The utilized loss function of the task does not even have to be
differentiable. We showcase the versatility of our approach on three distinct
applications: image segmentation, 3D shape sparsification, and sparse
approximate matrix inverse determination.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10207" title="Abstract">arXiv:2402.10207</a> [<a href="/pdf/2402.10207" title="Download PDF">pdf</a>, <a href="/format/2402.10207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shuang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We consider the problem of multi-objective alignment of foundation models
with human preferences, which is a critical step towards helpful and harmless
AI systems. However, it is generally costly and unstable to fine-tune large
foundation models using reinforcement learning (RL), and the
multi-dimensionality, heterogeneity, and conflicting nature of human
preferences further complicate the alignment process. In this paper, we
introduce Rewards-in-Context (RiC), which conditions the response of a
foundation model on multiple rewards in its prompt context and applies
supervised fine-tuning for alignment. The salient features of RiC are
simplicity and adaptivity, as it only requires supervised fine-tuning of a
single foundation model and supports dynamic adjustment for user preferences
during inference time. Inspired by the analytical solution of an abstracted
convex optimization problem, our dynamic inference-time adjustment method
approaches the Pareto-optimal solution for multiple objectives. Empirical
evidence demonstrates the efficacy of our method in aligning both Large
Language Models (LLMs) and diffusion models to accommodate diverse rewards with
only around $10\%$ GPU hours compared with multi-objective RL baseline.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10208" title="Abstract">arXiv:2402.10208</a> [<a href="/pdf/2402.10208" title="Download PDF">pdf</a>, <a href="/format/2402.10208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering the Pre-Fine-Tuning Weights of Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horwitz%2C+E">Eliahu Horwitz</a>, 
<a href="/search/cs?searchtype=author&query=Kahana%2C+J">Jonathan Kahana</a>, 
<a href="/search/cs?searchtype=author&query=Hoshen%2C+Y">Yedid Hoshen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The dominant paradigm in generative modeling consists of two steps: i)
pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained
model with human values via fine-tuning. This practice is considered safe, as
no current method can recover the unsafe, pre-fine-tuning model weights. In
this paper, we demonstrate that this assumption is often false. Concretely, we
present Spectral DeTuning, a method that can recover the weights of the
pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In
contrast to previous attacks that attempt to recover pre-fine-tuning
capabilities, our method aims to recover the exact pre-fine-tuning weights. Our
approach exploits this new vulnerability against large-scale models such as a
personalized Stable Diffusion and an aligned Mistral.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10210" title="Abstract">arXiv:2402.10210</a> [<a href="/pdf/2402.10210" title="Download PDF">pdf</a>, <a href="/format/2402.10210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Huizhuo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Fine-tuning Diffusion Models remains an underexplored frontier in generative
artificial intelligence (GenAI), especially when compared with the remarkable
progress made in fine-tuning Large Language Models (LLMs). While cutting-edge
diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised
fine-tuning, their performance inevitably plateaus after seeing a certain
volume of data. Recently, reinforcement learning (RL) has been employed to
fine-tune diffusion models with human preference data, but it requires at least
two images ("winner" and "loser" images) for each text prompt. In this paper,
we introduce an innovative technique called self-play fine-tuning for diffusion
models (SPIN-Diffusion), where the diffusion model engages in competition with
its earlier versions, facilitating an iterative self-improvement process. Our
approach offers an alternative to conventional supervised fine-tuning and RL
strategies, significantly improving both model performance and alignment. Our
experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms
the existing supervised fine-tuning method in aspects of human preference
alignment and visual appeal right from its first iteration. By the second
iteration, it exceeds the performance of RLHF-based methods across all metrics,
achieving these results with less data.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10211" title="Abstract">arXiv:2402.10211</a> [<a href="/pdf/2402.10211" title="Download PDF">pdf</a>, <a href="/format/2402.10211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical State Space Models for Continuous Sequence-to-Sequence  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhirangi%2C+R">Raunaq Bhirangi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pattabiraman%2C+V">Venkatesh Pattabiraman</a>, 
<a href="/search/cs?searchtype=author&query=Majidi%2C+C">Carmel Majidi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhinav Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Hellebrekers%2C+T">Tess Hellebrekers</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+L">Lerrel Pinto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Signal Processing (eess.SP)

</div>
<p class="mathjax">Reasoning from sequences of raw sensory data is a ubiquitous problem across
fields ranging from medical devices to robotics. These problems often involve
using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to
predict sequences of desirable physical quantities (e.g. force, inertial
measurements). While classical approaches are powerful for locally-linear
prediction problems, they often fall short when using real-world sensors. These
sensors are typically non-linear, are affected by extraneous variables (e.g.
vibration), and exhibit data-dependent drift. For many problems, the prediction
task is exacerbated by small labeled datasets since obtaining ground-truth
labels requires expensive equipment. In this work, we present Hierarchical
State-Space Models (HiSS), a conceptually simple, new technique for continuous
sequential prediction. HiSS stacks structured state-space models on top of each
other to create a temporal hierarchy. Across six real-world sensor datasets,
from tactile-based state prediction to accelerometer-based inertial
measurement, HiSS outperforms state-of-the-art sequence models such as causal
Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments
further indicate that HiSS demonstrates efficient scaling to smaller datasets
and is compatible with existing data-filtering techniques. Code, datasets and
videos can be found on https://hiss-csp.github.io.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 16 Feb 24</h3>
<dl>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09319" title="Abstract">arXiv:2402.09319</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.09319" title="Download PDF">pdf</a>, <a href="/format/2402.09319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eulerian Formulation of the Tensor-Based Morphology Equations for  Strain-Based Blood Damage Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Dirkes%2C+N">Nico Dirkes</a>, 
<a href="/search/physics?searchtype=author&query=Key%2C+F">Fabian Key</a>, 
<a href="/search/physics?searchtype=author&query=Behr%2C+M">Marek Behr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The development of blood-handling medical devices, such as ventricular assist
devices, requires the analysis of their biocompatibility. Among other aspects,
this includes hemolysis, i.e., red blood cell damage. For this purpose,
computational fluid dynamics (CFD) methods are employed to predict blood flow
in prototypes. The most basic hemolysis models directly estimate red blood cell
damage from fluid stress in the resulting flow field. More advanced models
explicitly resolve cell deformation. On the downside, these models are
typically written in a Lagrangian formulation, i.e., they require pathline
tracking. We present a new Eulerian description of cell deformation, enabling
the evaluation of the solution across the whole domain. The resulting hemolysis
model can be applied to any converged CFD simulation due to one-way coupling
with the fluid velocity field. We discuss the efficient numerical treatment of
the model equations in a stabilized finite element context. We validate the
model by comparison to the original Lagrangian formulation in selected
benchmark flows. Two more complex test cases demonstrate the method's
capabilities in real-world applications. The results highlight the advantages
over previous hemolysis models. In conclusion, the model holds great potential
for the design process of future generations of medical devices.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09416" title="Abstract">arXiv:2402.09416</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.09416" title="Download PDF">pdf</a>, <a href="/format/2402.09416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Manifold Transformation for Protein Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+B">Bozhen Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zang%2C+Z">Zelin Zang</a>, 
<a href="/search/q-bio?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Protein representation learning is critical in various tasks in biology, such
as drug design and protein structure or function prediction, which has
primarily benefited from protein language models and graph neural networks.
These models can capture intrinsic patterns from protein sequences and
structures through masking and task-related losses. However, the learned
protein representations are usually not well optimized, leading to performance
degradation due to limited data, difficulty adapting to new tasks, etc. To
address this, we propose a new \underline{d}eep \underline{m}anifold
\underline{t}ransformation approach for universal \underline{p}rotein
\underline{r}epresentation \underline{l}earning (DMTPRL). It employs manifold
learning strategies to improve the quality and adaptability of the learned
embeddings. Specifically, we apply a novel manifold learning loss during
training based on the graph inter-node similarity. Our proposed DMTPRL method
outperforms state-of-the-art baselines on diverse downstream tasks across
popular datasets. This validates our approach for learning universal and robust
protein representations. We promise to release the code after acceptance.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09419" title="Abstract">arXiv:2402.09419</a> (cross-list from eess.SP) [<a href="/pdf/2402.09419" title="Download PDF">pdf</a>, <a href="/format/2402.09419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multidimensional Gabor-Like Filters Derived from Gaussian Functions on  Logarithmic Frequency Axes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Devakumar%2C+D">Dherik Devakumar</a>, 
<a href="/search/eess?searchtype=author&query=Eidheim%2C+O+C">Ole Christian Eidheim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">A novel wavelet-like function is presented that makes it convenient to create
filter banks given mainly two parameters that influence the focus area and the
filter count. This is accomplished by computing the inverse Fourier transform
of Gaussian functions on logarithmic frequency axes in the frequency domain.
The resulting filters are similar to Gabor filters and represent oriented brief
signal oscillations of different sizes. The wavelet-like function can be
thought of as a generalized Log-Gabor filter that is multidimensional, always
uses Gaussian functions on logarithmic frequency axes, and innately includes
low-pass filters from Gaussian functions located at the frequency domain
origin.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09421" title="Abstract">arXiv:2402.09421</a> (cross-list from eess.SP) [<a href="/pdf/2402.09421" title="Download PDF">pdf</a>, <a href="/format/2402.09421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EEG Based Generative Depression Discriminator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mao%2C+Z">Ziming Mao</a>, 
<a href="/search/eess?searchtype=author&query=wu%2C+H">Hao wu</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+Y">Yongxi Tan</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+Y">Yuhe Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Depression is a very common but serious mood disorder.In this paper, We built
a generative detection network(GDN) in accordance with three physiological
laws. Our aim is that we expect the neural network to learn the relevant brain
activity based on the EEG signal and, at the same time, to regenerate the
target electrode signal based on the brain activity. We trained two generators,
the first one learns the characteristics of depressed brain activity, and the
second one learns the characteristics of control group's brain activity. In the
test, a segment of EEG signal was put into the two generators separately, if
the relationship between the EEG signal and brain activity conforms to the
characteristics of a certain category, then the signal generated by the
generator of the corresponding category is more consistent with the original
signal. Thus it is possible to determine the category corresponding to a
certain segment of EEG signal. We obtained an accuracy of 92.30\% on the MODMA
dataset and 86.73\% on the HUSM dataset. Moreover, this model is able to output
explainable information, which can be used to help the user to discover
possible misjudgments of the network.Our code will be released.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09424" title="Abstract">arXiv:2402.09424</a> (cross-list from eess.SP) [<a href="/pdf/2402.09424" title="Download PDF">pdf</a>, <a href="/format/2402.09424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epilepsy Seizure Detection and Prediction using an Approximate Spiking  Convolutional Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Q">Qinyu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+C">Congyi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Shih-Chii Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at the 2024 IEEE International Symposium on Circuits and Systems (ISCAS), Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Epilepsy is a common disease of the nervous system. Timely prediction of
seizures and intervention treatment can significantly reduce the accidental
injury of patients and protect the life and health of patients. This paper
presents a neuromorphic Spiking Convolutional Transformer, named Spiking
Conformer, to detect and predict epileptic seizure segments from scalped
long-term electroencephalogram (EEG) recordings. We report evaluation results
from the Spiking Conformer model using the Boston Children's Hospital-MIT
(CHB-MIT) EEG dataset. By leveraging spike-based addition operations, the
Spiking Conformer significantly reduces the classification computational cost
compared to the non-spiking model. Additionally, we introduce an approximate
spiking neuron layer to further reduce spike-triggered neuron updates by nearly
38% without sacrificing accuracy. Using raw EEG data as input, the proposed
Spiking Conformer achieved an average sensitivity rate of 94.9% and a
specificity rate of 99.3% for the seizure detection task, and 96.8%, 89.5% for
the seizure prediction task, and needs &gt;10x fewer operations compared to the
non-spiking equivalent model.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09426" title="Abstract">arXiv:2402.09426</a> (cross-list from eess.SP) [<a href="/pdf/2402.09426" title="Download PDF">pdf</a>, <a href="/format/2402.09426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Koopman Autoencoder for Predictive Covert Communication Against  UAV Surveillance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Krishnan%2C+S">Sivaram Krishnan</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Jihong Park</a>, 
<a href="/search/eess?searchtype=author&query=Sherman%2C+G">Gregory Sherman</a>, 
<a href="/search/eess?searchtype=author&query=Campbell%2C+B">Benjamin Campbell</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+J">Jinho Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Low Probability of Detection (LPD) communication aims to obscure the very
presence of radio frequency (RF) signals, going beyond just hiding the content
of the communication. However, the use of Unmanned Aerial Vehicles (UAVs)
introduces a challenge, as UAVs can detect RF signals from the ground by
hovering over specific areas of interest. With the growing utilization of UAVs
in modern surveillance, there is a crucial need for a thorough understanding of
their unknown nonlinear dynamic trajectories to effectively implement LPD
communication. Unfortunately, this critical information is often not readily
available, posing a significant hurdle in LPD communication. To address this
issue, we consider a case-study for enabling terrestrial LPD communication in
the presence of multiple UAVs that are engaged in surveillance. We introduce a
novel framework that combines graph neural networks (GNN) with Koopman theory
to predict the trajectories of multiple fixed-wing UAVs over an extended
prediction horizon. Using the predicted UAV locations, we enable LPD
communication in a terrestrial ad-hoc network by controlling nodes' transmit
powers to keep the received power at UAVs' predicted locations minimized. Our
extensive simulations validate the efficacy of the proposed framework in
accurately predicting the trajectories of multiple UAVs, thereby effectively
establishing LPD communication.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09427" title="Abstract">arXiv:2402.09427</a> (cross-list from eess.SP) [<a href="/pdf/2402.09427" title="Download PDF">pdf</a>, <a href="/format/2402.09427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zakharchenko%2C+A">Aleksei Zakharchenko</a>, 
<a href="/search/eess?searchtype=author&query=Farber%2C+S">Sharon Farber</a>, 
<a href="/search/eess?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 14 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Many Internet of Things applications utilize low-cost, micro,
electro-mechanical inertial sensors. A common task is orientation estimation.
To tackle such a task, attitude and heading reference system algorithms are
applied. Relying on the gyroscope readings, the accelerometer readings are used
to update the attitude angles, and magnetometer measurements are utilized to
update the heading angle. In indoor environments, magnetometers suffer from
interference that degrades their performance. This mainly influences
applications focused on estimating the heading angle like finding the heading
angle of a closet or fridge door. To circumvent such situations, we propose
DoorINet, an end-to-end deep-learning framework to calculate the heading angle
from door-mounted, low-cost inertial sensors without using magnetometers. To
evaluate our approach, we record a unique dataset containing 391 minutes of
accelerometer and gyroscope measurements and corresponding ground-truth heading
angle. We show that our proposed approach outperforms commonly used, model
based approaches and data-driven methods.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09433" title="Abstract">arXiv:2402.09433</a> (cross-list from eess.SP) [<a href="/pdf/2402.09433" title="Download PDF">pdf</a>, <a href="/format/2402.09433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electrical Behavior Association Mining for Household ShortTerm Energy  Consumption Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+H">Heyang Yu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yuxi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yintao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Geng%2C+G">Guangchao Geng</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Q">Quanyuan Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 figures and 4 tables; This manuscript is submitted for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Accurate household short-term energy consumption forecasting (STECF) is
crucial for home energy management, but it is technically challenging, due to
highly random behaviors of individual residential users. To improve the
accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF
methodology that leverages association mining in electrical behaviors. First, a
probabilistic association quantifying and discovering method is proposed to
model the pairwise behaviors association and generate associated clusters.
Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based
forecasting is provided to explore the temporal correlation and enhance
accuracy. The testing results demonstrate that this methodology yields a
significant enhancement in the STECF.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09434" title="Abstract">arXiv:2402.09434</a> (cross-list from eess.SP) [<a href="/pdf/2402.09434" title="Download PDF">pdf</a>, <a href="/format/2402.09434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous  Network for Human Activity Recognition in Flawed Wearable Sensor Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+M">Mengna Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+D">Dong Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+X">Xu Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiufeng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Dalin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+S">Shengyong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+C+S">Christian S. Jensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The popularity and diffusion of wearable devices provides new opportunities
for sensor-based human activity recognition that leverages deep learning-based
algorithms. Although impressive advances have been made, two major challenges
remain. First, sensor data is often incomplete or noisy due to sensor placement
and other issues as well as data transmission failure, calling for imputation
of missing values, which also introduces noise. Second, human activity has
multi-scale characteristics. Thus, different groups of people and even the same
person may behave differently under different circumstances. To address these
challenges, we propose a multilevel heterogeneous neural network, called MHNN,
for sensor data analysis. We utilize multilevel discrete wavelet decomposition
to extract multi-resolution features from sensor data. This enables
distinguishing signals with different frequencies, thereby suppressing noise.
As the components resulting from the decomposition are heterogeneous, we equip
the proposed model with heterogeneous feature extractors that enable the
learning of multi-scale features. Due to the complementarity of these features,
we also include a cross aggregation module for enhancing their interactions. An
experimental study using seven publicly available datasets offers evidence that
MHNN can outperform other cutting-edge models and offers evidence of robustness
to missing values and noise. An ablation study confirms the importance of each
module.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09438" title="Abstract">arXiv:2402.09438</a> (cross-list from eess.SP) [<a href="/pdf/2402.09438" title="Download PDF">pdf</a>, <a href="/format/2402.09438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subject-Independent Deep Architecture for EEG-based Motor Imagery  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sartipi%2C+S">Shadi Sartipi</a>, 
<a href="/search/eess?searchtype=author&query=Cetin%2C+M">Mujdat Cetin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Motor imagery (MI) classification based on electroencephalogram (EEG) is a
widely-used technique in non-invasive brain-computer interface (BCI) systems.
Since EEG recordings suffer from heterogeneity across subjects and labeled data
insufficiency, designing a classifier that performs the MI independently from
the subject with limited labeled samples would be desirable. To overcome these
limitations, we propose a novel subject-independent semi-supervised deep
architecture (SSDA). The proposed SSDA consists of two parts: an unsupervised
and a supervised element. The training set contains both labeled and unlabeled
data samples from multiple subjects. First, the unsupervised part, known as the
columnar spatiotemporal auto-encoder (CST-AE), extracts latent features from
all the training samples by maximizing the similarity between the original and
reconstructed data. A dimensional scaling approach is employed to reduce the
dimensionality of the representations while preserving their discriminability.
Second, a supervised part learns a classifier based on the labeled training
samples using the latent features acquired in the unsupervised part. Moreover,
we employ center loss in the supervised part to minimize the embedding space
distance of each point in a class to its center. The model optimizes both parts
of the network in an end-to-end fashion. The performance of the proposed SSDA
is evaluated on test subjects who were not seen by the model during the
training phase. To assess the performance, we use two benchmark EEG-based MI
task datasets. The results demonstrate that SSDA outperforms state-of-the-art
methods and that a small number of labeled training samples can be sufficient
for strong classification performance.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09439" title="Abstract">arXiv:2402.09439</a> (cross-list from eess.SP) [<a href="/pdf/2402.09439" title="Download PDF">pdf</a>, <a href="/ps/2402.09439" title="Download PostScript">ps</a>, <a href="/format/2402.09439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Al-Nahhal%2C+I">Ibrahim Al-Nahhal</a>, 
<a href="/search/eess?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fanggang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Integrated sensing and communication (ISAC) and intelligent reflecting
surface (IRS) are viewed as promising technologies for future generations of
wireless networks. This paper investigates the channel estimation problem in an
IRS-assisted ISAC system. A deep-learning framework is proposed to estimate the
sensing and communication (S&amp;C) channels in such a system. Considering
different propagation environments of the S&amp;C channels, two deep neural network
(DNN) architectures are designed to realize this framework. The first DNN is
devised at the ISAC base station for estimating the sensing channel, while the
second DNN architecture is assigned to each downlink user equipment to estimate
its communication channel. Moreover, the input-output pairs to train the DNNs
are carefully designed. Simulation results show the superiority of the proposed
estimation approach compared to the benchmark scheme under various
signal-to-noise ratio conditions and system parameters.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09440" title="Abstract">arXiv:2402.09440</a> (cross-list from eess.SP) [<a href="/pdf/2402.09440" title="Download PDF">pdf</a>, <a href="/ps/2402.09440" title="Download PostScript">ps</a>, <a href="/format/2402.09440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Learning Machine-based Channel Estimation in IRS-Assisted  Multi-User ISAC System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Al-Nahhal%2C+I">Ibrahim Al-Nahhal</a>, 
<a href="/search/eess?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fanggang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Shin%2C+H">Hyundong Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-user integrated sensing and communication (ISAC) assisted by
intelligent reflecting surface (IRS) has been recently investigated to provide
a high spectral and energy efficiency transmission. This paper proposes a
practical channel estimation approach for the first time to an IRS-assisted
multiuser ISAC system. The estimation problem in such a system is challenging
since the sensing and communication (SAC) signals interfere with each other,
and the passive IRS lacks signal processing ability. A two-stage approach is
proposed to transfer the overall estimation problem into sub-ones, successively
including the direct and reflected channels estimation. Based on this scheme,
the ISAC base station (BS) estimates all the SAC channels associated with the
target and uplink users, while each downlink user estimates the downlink
communication channels individually. Considering a low-cost demand of the ISAC
BS and downlink users, the proposed two-stage approach is realized by an
efficient neural network (NN) framework that contains two different extreme
learning machine (ELM) structures to estimate the above SAC channels. Moreover,
two types of input-output pairs to train the ELMs are carefully devised, which
impact the estimation accuracy and computational complexity under different
system parameters. Simulation results reveal a substantial performance
improvement achieved by the proposed ELM-based approach over the least-squares
and NN-based benchmarks, with reduced training complexity and faster training
speed.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09441" title="Abstract">arXiv:2402.09441</a> (cross-list from eess.SP) [<a href="/pdf/2402.09441" title="Download PDF">pdf</a>, <a href="/ps/2402.09441" title="Download PostScript">ps</a>, <a href="/format/2402.09441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and  Communication System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Al-Nahhal%2C+I">Ibrahim Al-Nahhal</a>, 
<a href="/search/eess?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fanggang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Integrated sensing and communication (ISAC), and intelligent reflecting
surface (IRS) are envisioned as revolutionary technologies to enhance spectral
and energy efficiencies for next wireless system generations. For the first
time, this paper focuses on the channel estimation problem in an IRS-assisted
ISAC system. This problem is challenging due to the lack of signal processing
capacity in passive IRS, as well as the presence of mutual interference between
sensing and communication (SAC) signals in ISAC systems. A three-stage approach
is proposed to decouple the estimation problem into sub-ones, including the
estimation of the direct SAC channels in the first stage, reflected
communication channel in the second stage, and reflected sensing channel in the
third stage. The proposed three-stage approach is based on a deep-learning
framework, which involves two different convolutional neural network (CNN)
architectures to estimate the channels at the full-duplex ISAC base station.
Furthermore, two types of input-output pairs to train the CNNs are carefully
designed, which affect the estimation performance under various signal-to-noise
ratio conditions and system parameters. Simulation results validate the
superiority of the proposed estimation approach compared to the least-squares
baseline scheme, and its computational complexity is also analyzed.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09442" title="Abstract">arXiv:2402.09442</a> (cross-list from eess.SP) [<a href="/pdf/2402.09442" title="Download PDF">pdf</a>, <a href="/ps/2402.09442" title="Download PostScript">ps</a>, <a href="/format/2402.09442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progress in artificial intelligence applications based on the  combination of self-driven sensors and deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wan%2C+W">Weixiang Wan</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+W">Wenjian Sun</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+L">Linying Pan</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jingyu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This aticle was accepted by ieee conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the era of Internet of Things, how to develop a smart sensor system with
sustainable power supply, easy deployment and flexible use has become a
difficult problem to be solved. The traditional power supply has problems such
as frequent replacement or charging when in use, which limits the development
of wearable devices. The contact-to-separate friction nanogenerator (TENG) was
prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human
motion energy was collected by human body arrangement, and human motion posture
was monitored according to the changes of output electrical signals. In 2012,
Academician Wang Zhong lin and his team invented the triboelectric
nanogenerator (TENG), which uses Maxwell displacement current as a driving
force to directly convert mechanical stimuli into electrical signals, so it can
be used as a self-driven sensor. Teng-based sensors have the advantages of
simple structure and high instantaneous power density, which provides an
important means for building intelligent sensor systems. At the same time,
machine learning, as a technology with low cost, short development cycle,
strong data processing ability and prediction ability, has a significant effect
on the processing of a large number of electrical signals generated by TENG,
and the combination with TENG sensors will promote the rapid development of
intelligent sensor networks in the future. Therefore, this paper is based on
the intelligent sound monitoring and recognition system of TENG, which has good
sound recognition capability, and aims to evaluate the feasibility of the sound
perception module architecture in ubiquitous sensor networks.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09443" title="Abstract">arXiv:2402.09443</a> (cross-list from eess.SP) [<a href="/pdf/2402.09443" title="Download PDF">pdf</a>, <a href="/ps/2402.09443" title="Download PostScript">ps</a>, <a href="/format/2402.09443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of algorithms for predicting fatigue using EEG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rakhmatulin%2C+I">Ildar Rakhmatulin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2401.15766">arXiv:2401.15766</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Fatigue detection is of paramount importance in enhancing safety,
productivity, and well-being across diverse domains, including transportation,
healthcare, and industry. This scientific paper presents a comprehensive
investigation into the application of machine learning algorithms for the
detection of physiological fatigue using Electroencephalogram (EEG) signals.
The primary objective of this study was to assess the efficacy of various
algorithms in predicting an individual's level of fatigue based on EEG data.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09444" title="Abstract">arXiv:2402.09444</a> (cross-list from eess.SP) [<a href="/pdf/2402.09444" title="Download PDF">pdf</a>, <a href="/format/2402.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Action Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zeng%2C+L">Ling-An Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+W">Wei-Shi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Image Processing 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Action quality assessment (AQA) is to assess how well an action is performed.
Previous works perform modelling by only the use of visual information,
ignoring audio information. We argue that although AQA is highly dependent on
visual information, the audio is useful complementary information for improving
the score regression accuracy, especially for sports with background music,
such as figure skating and rhythmic gymnastics. To leverage multimodal
information for AQA, i.e., RGB, optical flow and audio information, we propose
a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models
modality-specific information and mixed-modality information. Our model
consists of with three modality-specific branches that independently explore
modality-specific information and a mixed-modality branch that progressively
aggregates the modality-specific information from the modality-specific
branches. To build the bridge between modality-specific branches and the
mixed-modality branch, three novel modules are proposed. First, a
Modality-specific Feature Decoder module is designed to selectively transfer
modality-specific information to the mixed-modality branch. Second, when
exploring the interaction between modality-specific information, we argue that
using an invariant multimodal fusion policy may lead to suboptimal results, so
as to take the potential diversity in different parts of an action into
consideration. Therefore, an Adaptive Fusion Module is proposed to learn
adaptive multimodal fusion policies in different parts of an action. This
module consists of several FusionNets for exploring different multimodal fusion
strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a
module called Cross-modal Feature Decoder is designed to transfer cross-modal
features generated by Adaptive Fusion Module to the mixed-modality branch.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09445" title="Abstract">arXiv:2402.09445</a> (cross-list from eess.SP) [<a href="/pdf/2402.09445" title="Download PDF">pdf</a>, <a href="/format/2402.09445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+M">Mengxi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Rey%2C+V+F">Vitor Fortes Rey</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ray%2C+L+S+S">Lala Shakti Swarup Ray</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Lukowicz%2C+P">Paul Lukowicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by percom2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Automatic and precise fitness activity recognition can be beneficial in
aspects from promoting a healthy lifestyle to personalized preventative
healthcare. While IMUs are currently the prominent fitness tracking modality,
through iMove, we show bio-impedence can help improve IMU-based fitness
tracking through sensor fusion and contrastive learning.To evaluate our
methods, we conducted an experiment including six upper body fitness activities
performed by ten subjects over five days to collect synchronized data from
bio-impedance across two wrists and IMU on the left wrist.The contrastive
learning framework uses the two modalities to train a better IMU-only
classification model, where bio-impedance is only required at the training
phase, by which the average Macro F1 score with the input of a single IMU was
improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU
baseline model. We have also shown how bio-impedance can improve human activity
recognition (HAR) directly through sensor fusion, reaching an average Macro F1
score of 89.57 \% (two modalities required for both training and inference)
even if Bio-impedance alone has an average macro F1 score of 75.36 \%, which is
outperformed by IMU alone. In addition, similar results were obtained in an
extended study on lower body fitness activity classification, demonstrating the
generalisability of our approach.Our findings underscore the potential of
sensor fusion and contrastive learning as valuable tools for advancing fitness
activity recognition, with bio-impedance playing a pivotal role in augmenting
the capabilities of IMU-based systems.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09447" title="Abstract">arXiv:2402.09447</a> (cross-list from eess.SP) [<a href="/pdf/2402.09447" title="Download PDF">pdf</a>, <a href="/format/2402.09447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and  Natural Grasp Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rabiee%2C+A">Ali Rabiee</a>, 
<a href="/search/eess?searchtype=author&query=Ghafoori%2C+S">Sima Ghafoori</a>, 
<a href="/search/eess?searchtype=author&query=Cetera%2C+A">Anna Cetera</a>, 
<a href="/search/eess?searchtype=author&query=Abiri%2C+R">Reza Abiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">This research aims to decode hand grasps from Electroencephalograms (EEGs)
for dexterous neuroprosthetic development and Brain-Computer Interface (BCI)
applications, especially for patients with motor disorders. Particularly, it
focuses on distinguishing two complex natural power and precision grasps in
addition to a neutral condition as a no-movement condition using a new
EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved
generating time-frequency and topographic maps from wavelet power coefficients.
Then, by using machine learning techniques with novel wavelet features, we
achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement
vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs
Precision, demonstrating the effectiveness of these features in EEG-based grasp
differentiation. In contrast to previous studies, a critical part of our study
was permutation feature importance analysis, which highlighted key features for
grasp classification. It revealed that the most crucial brain activities during
grasping occur in the motor cortex, within the alpha and beta frequency bands.
These insights demonstrate the potential of wavelet features in real-time
neuroprosthetic technology and BCI applications.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09448" title="Abstract">arXiv:2402.09448</a> (cross-list from eess.SP) [<a href="/pdf/2402.09448" title="Download PDF">pdf</a>, <a href="/format/2402.09448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Conventional and Tripolar EEG for  High-Performance Reach-to-Grasp BCI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rabiee%2C+A">Ali Rabiee</a>, 
<a href="/search/eess?searchtype=author&query=Ghafoori%2C+S">Sima Ghafoori</a>, 
<a href="/search/eess?searchtype=author&query=Cetera%2C+A">Anna Cetera</a>, 
<a href="/search/eess?searchtype=author&query=Besio%2C+W">Walter Besio</a>, 
<a href="/search/eess?searchtype=author&query=Abiri%2C+R">Reza Abiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study aims to enhance BCI applications for individuals with motor
impairments by comparing the effectiveness of tripolar EEG (tEEG) with
conventional EEG. The focus is on interpreting and decoding various grasping
movements, such as power grasp and precision grasp. The goal is to determine
which EEG technology is more effective in processing and translating grasp
related neural signals. The approach involved experimenting on ten healthy
participants who performed two distinct grasp movements: power grasp and
precision grasp, with a no movement condition serving as the baseline. Our
research presents a thorough comparison between EEG and tEEG in decoding
grasping movements. This comparison spans several key parameters, including
signal to noise ratio (SNR), spatial resolution via functional connectivity,
ERPs, and wavelet time frequency analysis. Additionally, our study involved
extracting and analyzing statistical features from the wavelet coefficients,
and both binary and multiclass classification methods were employed. Four
machine learning algorithms were used to evaluate the decoding accuracies. Our
results indicated that tEEG demonstrated superior performance over conventional
EEG in various aspects. This included a higher signal to noise ratio, enhanced
spatial resolution, and more informative data in ERPs and wavelet time
frequency analysis. The use of tEEG led to notable improvements in decoding
accuracy for differentiating movement types. Specifically, tEEG achieved around
90% accuracy in binary and 75.97% for multiclass classification. These results
are markedly better than those from standard EEG, which recorded a maximum of
77.85% and 61.27% in similar tasks, respectively. These findings highlight the
superior effectiveness of tEEG over EEG in decoding grasp types and its
competitive or superior performance in complex classifications compared with
existing research.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09450" title="Abstract">arXiv:2402.09450</a> (cross-list from eess.SP) [<a href="/pdf/2402.09450" title="Download PDF">pdf</a>, <a href="/format/2402.09450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Masked Representation Learning to Capture Spatio-Temporal  Relationship of Electrocardiogram
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Na%2C+Y">Yeongyeon Na</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+M">Minje Park</a>, 
<a href="/search/eess?searchtype=author&query=Tae%2C+Y">Yunwon Tae</a>, 
<a href="/search/eess?searchtype=author&query=Joo%2C+S">Sunghoon Joo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024.The first three authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Electrocardiograms (ECG) are widely employed as a diagnostic tool for
monitoring electrical signals originating from a heart. Recent machine learning
research efforts have focused on the application of screening various diseases
using ECG signals. However, adapting to the application of screening disease is
challenging in that labeled ECG data are limited. Achieving general
representation through self-supervised learning (SSL) is a well-known approach
to overcome the scarcity of labeled data; however, a naive application of SSL
to ECG data, without considering the spatial-temporal relationships inherent in
ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM
(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn
spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM
outperforms other SSL baseline methods in various experimental settings for
arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is
adaptable to various lead combinations. Through quantitative and qualitative
analysis, we show a spatio-temporal relationship within ECG data.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09452" title="Abstract">arXiv:2402.09452</a> (cross-list from eess.SP) [<a href="/pdf/2402.09452" title="Download PDF">pdf</a>, <a href="/format/2402.09452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Distribution Dynamics in Real-World WiFi-Based Patient Activity  Monitoring for Home Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Monjur%2C+M">Mahathir Monjur</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jingye Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuntong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiaomeng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chengdong Li</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+H">Hyejin Park</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Shieh%2C+K">Karl Shieh</a>, 
<a href="/search/eess?searchtype=author&query=Munir%2C+S">Sirajum Munir</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+L">Lixin Song</a>, 
<a href="/search/eess?searchtype=author&query=Nirjon%2C+S">Shahriar Nirjon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper examines the application of WiFi signals for real-world monitoring
of daily activities in home healthcare scenarios. While the state-of-the-art of
WiFi-based activity recognition is promising in lab environments, challenges
arise in real-world settings due to environmental, subject, and system
configuration variables, affecting accuracy and adaptability. The research
involved deploying systems in various settings and analyzing data shifts. It
aims to guide realistic development of robust, context-aware WiFi sensing
systems for elderly care. The findings suggest a shift in WiFi-based activity
sensing, bridging the gap between academic research and practical applications,
enhancing life quality through technology.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09453" title="Abstract">arXiv:2402.09453</a> (cross-list from eess.SP) [<a href="/pdf/2402.09453" title="Download PDF">pdf</a>, <a href="/ps/2402.09453" title="Download PostScript">ps</a>, <a href="/format/2402.09453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving EEG Signal Classification Accuracy Using Wasserstein  Generative Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Joshua Park</a>, 
<a href="/search/eess?searchtype=author&query=Mahey%2C+P">Priyanshu Mahey</a>, 
<a href="/search/eess?searchtype=author&query=Adeniyi%2C+O">Ore Adeniyi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Electroencephalography (EEG) plays a vital role in recording brain activities
and is integral to the development of brain-computer interface (BCI)
technologies. However, the limited availability and high variability of EEG
signals present substantial challenges in creating reliable BCIs. To address
this issue, we propose a practical solution drawing on the latest developments
in deep learning and Wasserstein Generative Adversarial Network (WGAN). The
WGAN was trained on the BCI2000 dataset, consisting of around 1500 EEG
recordings and 64 channels from 45 individuals. The generated EEG signals were
evaluated via three classifiers yielding improved average accuracies. The
quality of generated signals measured using Frechet Inception Distance (FID)
yielded scores of 1.345 and 11.565 for eyes-open and closed respectively. Even
without a spectral or spatial loss term, our WGAN model was able to emulate the
spectral and spatial properties of the EEG training data. The WGAN-generated
data mirrored the dominant alpha activity during closed-eye resting and high
delta waves in the training data in its topographic map and power spectral
density (PSD) plot. Our research testifies to the potential of WGANs in
addressing the limited EEG data issue for BCI development by enhancing a small
dataset to improve classifier generalizability.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09458" title="Abstract">arXiv:2402.09458</a> (cross-list from math.LO) [<a href="/pdf/2402.09458" title="Download PDF">pdf</a>, <a href="/ps/2402.09458" title="Download PostScript">ps</a>, <a href="/format/2402.09458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative consistency of Set Matrix Theory with ZF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cabbolet%2C+M+J+T+F">Marcoen J.T.F. Cabbolet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages; to appear in Logique et Analyse
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Set Matrix Theory (SMT) has been introduced in Log. Anal. 225: 59-82 (2014)
as a generalization of ZF, in which matrices constructed from sets are treated
as urelements, that is, as objects that are not sets but that can be elements
of sets. Here we prove that SMT is relatively consistent with ZF.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09459" title="Abstract">arXiv:2402.09459</a> (cross-list from eess.SP) [<a href="/pdf/2402.09459" title="Download PDF">pdf</a>, <a href="/ps/2402.09459" title="Download PostScript">ps</a>, <a href="/format/2402.09459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body  Parts Orientation Tracking and 3D Movement Visualization on an Avatar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gonz%C3%A1lez-Alonso%2C+J">Javier Gonz&#xe1;lez-Alonso</a>, 
<a href="/search/eess?searchtype=author&query=Oviedo-Pastor%2C+D">David Oviedo-Pastor</a>, 
<a href="/search/eess?searchtype=author&query=Aguado%2C+H+J">H&#xe9;ctor J. Aguado</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%ADaz-Pernas%2C+F+J">Francisco J. D&#xed;az-Pernas</a>, 
<a href="/search/eess?searchtype=author&query=Gonz%C3%A1lez-Ortega%2C+D">David Gonz&#xe1;lez-Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Mart%C3%ADnez-Zarzuela%2C+M">Mario Mart&#xed;nez-Zarzuela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2021, 21, 6642
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Recent studies confirm the applicability of Inertial Measurement Unit
(IMU)-based systems for human motion analysis. Notwithstanding, high-end
IMU-based commercial solutions are yet too expensive and complex to democratize
their use among a wide range of potential users. Less featured entry-level
commercial solutions are being introduced in the market, trying to fill this
gap, but still present some limitations that need to be overcome. At the same
time, there is a growing number of scientific papers using not commercial, but
custom do-it-yourself IMU-based systems in medical and sports applications.
Even though these solutions can help to popularize the use of this technology,
they have more limited features and the description on how to design and build
them from scratch is yet too scarce in the literature. The aim of this work is
two-fold: (1) Proving the feasibility of building an affordable custom solution
aimed at simultaneous multiple body parts orientation tracking; while providing
a detailed bottom-up description of the required hardware, tools, and
mathematical operations to estimate and represent 3D movement in real-time. (2)
Showing how the introduction of a custom 2.4 GHz communication protocol
including a channel hopping strategy can address some of the current
communication limitations of entry-level commercial solutions. The proposed
system can be used for wireless real-time human body parts orientation tracking
with up to 10 custom sensors, at least at 50 Hz. In addition, it provides a
more reliable motion data acquisition in Bluetooth and Wi-Fi crowded
environments, where the use of entry-level commercial solutions might be
unfeasible. This system can be used as a groundwork for developing affordable
human motion analysis solutions that do not require an accurate kinematic
analysis.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09460" title="Abstract">arXiv:2402.09460</a> (cross-list from eess.SP) [<a href="/pdf/2402.09460" title="Download PDF">pdf</a>, <a href="/format/2402.09460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised learning based end-to-end delayless generative fixed-filter  active noise control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Luo%2C+Z">Zhengding Luo</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+D">Dongyuan Shi</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+X">Xiaoyi Shen</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+W">Woon-Seng Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Delayless noise control is achieved by our earlier generative fixed-filter
active noise control (GFANC) framework through efficient coordination between
the co-processor and real-time controller. However, the one-dimensional
convolutional neural network (1D CNN) in the co-processor requires initial
training using labelled noise datasets. Labelling noise data can be
resource-intensive and may introduce some biases. In this paper, we propose an
unsupervised-GFANC approach to simplify the 1D CNN training process and enhance
its practicality. During training, the co-processor and real-time controller
are integrated into an end-to-end differentiable ANC system. This enables us to
use the accumulated squared error signal as the loss for training the 1D CNN.
With this unsupervised learning paradigm, the unsupervised-GFANC method not
only omits the labelling process but also exhibits better noise reduction
performance compared to the supervised GFANC method in real noise experiments.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09461" title="Abstract">arXiv:2402.09461</a> (cross-list from eess.SP) [<a href="/pdf/2402.09461" title="Download PDF">pdf</a>, <a href="/format/2402.09461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Approach to WaveNet Architecture for RF Signal Separation with  Learnable Dilation and Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/eess?searchtype=author&query=Alhammadi%2C+A">Ahmed Alhammadi</a>, 
<a href="/search/eess?searchtype=author&query=Quran%2C+A">Abdullah Quran</a>, 
<a href="/search/eess?searchtype=author&query=Ali%2C+A+S">Abubakar Sani Ali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we address the intricate issue of RF signal separation by
presenting a novel adaptation of the WaveNet architecture that introduces
learnable dilation parameters, significantly enhancing signal separation in
dense RF spectrums. Our focused architectural refinements and innovative data
augmentation strategies have markedly improved the model's ability to discern
complex signal sources. This paper details our comprehensive methodology,
including the refined model architecture, data preparation techniques, and the
strategic training strategy that have been pivotal to our success. The efficacy
of our approach is evidenced by the substantial improvements recorded: a
58.82\% increase in SINR at a BER of $10^{-3}$ for OFDM-QPSK with EMI Signal 1,
surpassing traditional benchmarks. Notably, our model achieved first place in
the challenge \cite{datadrivenrf2024}, demonstrating its superior performance
and establishing a new standard for machine learning applications within the RF
communications domain.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09462" title="Abstract">arXiv:2402.09462</a> (cross-list from eess.SP) [<a href="/pdf/2402.09462" title="Download PDF">pdf</a>, <a href="/format/2402.09462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic differential equations for performance analysis of wireless  communication systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Amar%2C+E+B">Eya Ben Amar</a>, 
<a href="/search/eess?searchtype=author&query=Rached%2C+N+B">Nadhir Ben Rached</a>, 
<a href="/search/eess?searchtype=author&query=Tempone%2C+R">Raul Tempone</a>, 
<a href="/search/eess?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper addresses the difficulty of characterizing the time-varying nature
of fading channels. The current time-invariant models often fall short of
capturing and tracking these dynamic characteristics. To overcome this
limitation, we explore using of stochastic differential equations (SDEs) and
Markovian projection to model signal envelope variations, considering scenarios
involving Rayleigh, Rice, and Hoyt distributions. Furthermore, it is of
practical interest to study the performance of channels modeled by SDEs. In
this work, we investigate the fade duration metric, representing the time
during which the signal remains below a specified threshold within a fixed time
interval. We estimate the complementary cumulative distribution function (CCDF)
of the fade duration using Monte Carlo simulations, and analyze the influence
of system parameters on its behavior. Finally, we leverage importance sampling,
a known variance-reduction technique, to estimate the tail of the CCDF
efficiently.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09464" title="Abstract">arXiv:2402.09464</a> (cross-list from eess.SP) [<a href="/pdf/2402.09464" title="Download PDF">pdf</a>, <a href="/ps/2402.09464" title="Download PostScript">ps</a>, <a href="/format/2402.09464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Different Algorithms (Might) Uncover Different Patterns: A Brain-Age  Prediction Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ettling%2C+T">Tobias Ettling</a>, 
<a href="/search/eess?searchtype=author&query=Saba-Sadiya%2C+S">Sari Saba-Sadiya</a>, 
<a href="/search/eess?searchtype=author&query=Roig%2C+G">Gemma Roig</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), pp. 4051-4058
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Machine learning is a rapidly evolving field with a wide range of
applications, including biological signal analysis, where novel algorithms
often improve the state-of-the-art. However, robustness to algorithmic
variability - measured by different algorithms, consistently uncovering similar
findings - is seldom explored. In this paper we investigate whether established
hypotheses in brain-age prediction from EEG research validate across
algorithms. First, we surveyed literature and identified various features known
to be informative for brain-age prediction. We employed diverse feature
extraction techniques, processing steps, and models, and utilized the
interpretative power of SHapley Additive exPlanations (SHAP) values to align
our findings with the existing research in the field. Few of our models
achieved state-of-the-art performance on the specific data-set we utilized.
Moreover, analysis demonstrated that while most models do uncover similar
patterns in the EEG signals, some variability could still be observed. Finally,
a few prominent findings could only be validated using specific models. We
conclude by suggesting remedies to the potential implications of this lack of
robustness to model variability.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09465" title="Abstract">arXiv:2402.09465</a> (cross-list from eess.SP) [<a href="/pdf/2402.09465" title="Download PDF">pdf</a>, <a href="/format/2402.09465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for  Intuitive Responsiveness and High-Accuracy Motor Imagery Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nallani%2C+S+V+C">Sriram V.C. Nallani</a>, 
<a href="/search/eess?searchtype=author&query=Ramachandran%2C+G">Gautham Ramachandran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current approaches to prosthetic control are limited by their reliance on
traditional methods, which lack real-time adaptability and intuitive
responsiveness. These limitations are particularly pronounced in assistive
technologies designed for individuals with diverse cognitive states and motor
intentions. In this paper, we introduce a framework that leverages
Reinforcement Learning (RL) with Deep Q-Networks (DQN) for classification
tasks. Additionally, we present a preprocessing technique using the Common
Spatial Pattern (CSP) for multiclass motor imagery (MI) classification in a
One-Versus-The-Rest (OVR) manner. The subsequent 'csp space' transformation
retains the temporal dimension of EEG signals, crucial for extracting
discriminative features. The integration of DQN with a 1D-CNN-LSTM architecture
optimizes the decision-making process in real-time, thereby enhancing the
system's adaptability to the user's evolving needs and intentions. We elaborate
on the data processing methods for two EEG motor imagery datasets. Our
innovative model, RLEEGNet, incorporates a 1D-CNN-LSTM architecture as the
Online Q-Network within the DQN, facilitating continuous adaptation and
optimization of control strategies through feedback. This mechanism allows the
system to learn optimal actions through trial and error, progressively
improving its performance. RLEEGNet demonstrates high accuracy in classifying
MI-EEG signals, achieving as high as 100% accuracy in MI tasks across both the
GigaScience (3-class) and BCI-IV-2a (4-class) datasets. These results highlight
the potential of combining DQN with a 1D-CNN-LSTM architecture to significantly
enhance the adaptability and responsiveness of BCI systems.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09466" title="Abstract">arXiv:2402.09466</a> (cross-list from eess.SP) [<a href="/pdf/2402.09466" title="Download PDF">pdf</a>, <a href="/format/2402.09466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Learning with Uncertainty-based Quadruplet Selection for  Interference Classification in GNSS Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ott%2C+F">Felix Ott</a>, 
<a href="/search/eess?searchtype=author&query=Heublein%2C+L">Lucas Heublein</a>, 
<a href="/search/eess?searchtype=author&query=Raichur%2C+N+L">Nisha Lakshmana Raichur</a>, 
<a href="/search/eess?searchtype=author&query=Feigl%2C+T">Tobias Feigl</a>, 
<a href="/search/eess?searchtype=author&query=Hansen%2C+J">Jonathan Hansen</a>, 
<a href="/search/eess?searchtype=author&query=R%C3%BCgamer%2C+A">Alexander R&#xfc;gamer</a>, 
<a href="/search/eess?searchtype=author&query=Mutschler%2C+C">Christopher Mutschler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Jamming devices pose a significant threat by disrupting signals from the
global navigation satellite system (GNSS), compromising the robustness of
accurate positioning. Detecting anomalies in frequency snapshots is crucial to
counteract these interferences effectively. The ability to adapt to diverse,
unseen interference characteristics is essential for ensuring the reliability
of GNSS in real-world applications. In this paper, we propose a few-shot
learning (FSL) approach to adapt to new interference classes. Our method
employs quadruplet selection for the model to learn representations using
various positive and negative interference classes. Furthermore, our quadruplet
variant selects pairs based on the aleatoric and epistemic uncertainty to
differentiate between similar classes. We recorded a dataset at a motorway with
eight interference classes on which our FSL method with quadruplet loss
outperforms other FSL techniques in jammer classification accuracy with 97.66%.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09467" title="Abstract">arXiv:2402.09467</a> (cross-list from stat.ML) [<a href="/pdf/2402.09467" title="Download PDF">pdf</a>, <a href="/format/2402.09467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Thresholding Linear Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rivera%2C+E+O">Eduardo Ochoa Rivera</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2006.16073">arXiv:2006.16073</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study a novel pure exploration problem: the $\epsilon$-Thresholding Bandit
Problem (TBP) with fixed confidence in stochastic linear bandits. We prove a
lower bound for the sample complexity and extend an algorithm designed for Best
Arm Identification in the linear case to TBP that is asymptotically optimal.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09474" title="Abstract">arXiv:2402.09474</a> (cross-list from eess.SP) [<a href="/pdf/2402.09474" title="Download PDF">pdf</a>, <a href="/format/2402.09474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering Heartbeat Signatures: A Vision Transformer Approach to  Explainable Atrial Fibrillation Detection from ECG Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mohan%2C+A">Aruna Mohan</a>, 
<a href="/search/eess?searchtype=author&query=Elbers%2C+D">Danne Elbers</a>, 
<a href="/search/eess?searchtype=author&query=Zilbershot%2C+O">Or Zilbershot</a>, 
<a href="/search/eess?searchtype=author&query=Afghah%2C+F">Fatemeh Afghah</a>, 
<a href="/search/eess?searchtype=author&query=Vorchheimer%2C+D">David Vorchheimer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Submitted to IEEE EMBC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09476" title="Abstract">arXiv:2402.09476</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.09476" title="Download PDF">pdf</a>, <a href="/format/2402.09476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Enabled Lung Cancer Prognosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Darvish%2C+M">Mahtab Darvish</a>, 
<a href="/search/q-bio?searchtype=author&query=Trask%2C+R">Ryan Trask</a>, 
<a href="/search/q-bio?searchtype=author&query=Tallon%2C+P">Patrick Tallon</a>, 
<a href="/search/q-bio?searchtype=author&query=Khansari%2C+M">M&#xe9;lina Khansari</a>, 
<a href="/search/q-bio?searchtype=author&query=Ren%2C+L">Lei Ren</a>, 
<a href="/search/q-bio?searchtype=author&query=Hershman%2C+M">Michelle Hershman</a>, 
<a href="/search/q-bio?searchtype=author&query=Yousefi%2C+B">Bardia Yousefi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the author's version of a book chapter entitled: "Cancer Research: An Interdisciplinary Approach", Springer
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Springer book chapter "Cancer Research: An Interdisciplinary
  Approach" 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Lung cancer is the primary cause of cancer-related mortality, claiming
approximately 1.79 million lives globally in 2020, with an estimated 2.21
million new cases diagnosed within the same period. Among these, Non-Small Cell
Lung Cancer (NSCLC) is the predominant subtype, characterized by a notably
bleak prognosis and low overall survival rate of approximately 25% over five
years across all disease stages. However, survival outcomes vary considerably
based on the stage at diagnosis and the therapeutic interventions administered.
Recent advancements in artificial intelligence (AI) have revolutionized the
landscape of lung cancer prognosis. AI-driven methodologies, including machine
learning and deep learning algorithms, have shown promise in enhancing survival
prediction accuracy by efficiently analyzing complex multi-omics data and
integrating diverse clinical variables. By leveraging AI techniques, clinicians
can harness comprehensive prognostic insights to tailor personalized treatment
strategies, ultimately improving patient outcomes in NSCLC. Overviewing
AI-driven data processing can significantly help bolster the understanding and
provide better directions for using such systems.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09479" title="Abstract">arXiv:2402.09479</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.09479" title="Download PDF">pdf</a>, <a href="/ps/2402.09479" title="Download PostScript">ps</a>, <a href="/format/2402.09479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Biomechanical comparison between manual and motorless device assisted  patient handling: sitting to and from standing position
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Riccoboni%2C+J">Jean-Baptiste Riccoboni</a> (PPRIME), 
<a href="/search/physics?searchtype=author&query=Monnet%2C+T">Tony Monnet</a> (PPRIME), 
<a href="/search/physics?searchtype=author&query=Eon%2C+A">Antoine Eon</a> (PPRIME), 
<a href="/search/physics?searchtype=author&query=Lacouture%2C+P">Patrick Lacouture</a> (PPRIME), 
<a href="/search/physics?searchtype=author&query=Gazeau%2C+J">Jean-Pierre Gazeau</a> (PPRIME), 
<a href="/search/physics?searchtype=author&query=Campone%2C+M">Mario Campone</a> (UNICANCER/ICO)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Ergonomics, 2021, 90, pp.103284
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Background: Occupational safety and health institutions report that
caregivers areparticularly at risk of developing work-related musculoskeletal
disorders (WRMSDs) andpatient handling is often pointed out as one of the main
causes. While lots of studiesaddressed the use of assistive devices in order to
protect caregivers, it seems that motorlessstand-up lifts have not been studied
yet.Objectives: The aim of this work is to provide quantitative data about the
loads in thelow back area, as well as qualitative data about subjects
perceptions, resulting from the useof a motorless stand-up lift and to compare
them to those resulting from manual patienthandling.Methods: Nine caregivers
participated to motion capture and ground reaction forcesmeasurement sessions.
These recordings were performed in three cases of handling:manual handling with
one caregiver, manual handling with two caregivers, motorless deviceassisted
handling. Forces and torques at the L5/S1 joint were then estimated
throughInverse Dynamics process. A questionnaire about manual and motorless
device assistedhandling was also submitted.Results: Motorless device assisted
handling involved the smallest loads whereasmanual handling with one caregiver
involved the biggest loads.Conclusions: Our findings suggest that, if the
situation allows it, caregivers should behelped by another caregiver or use a
motorless stand-up lift when handling a patient fromsitting to standing
position or from standing to sitting position considering the reduced
loadsthese aids involve.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09483" title="Abstract">arXiv:2402.09483</a> (cross-list from stat.ML) [<a href="/pdf/2402.09483" title="Download PDF">pdf</a>, <a href="/ps/2402.09483" title="Download PostScript">ps</a>, <a href="/format/2402.09483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oracle-Efficient Differentially Private Learning with Public Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Block%2C+A">Adam Block</a>, 
<a href="/search/stat?searchtype=author&query=Bun%2C+M">Mark Bun</a>, 
<a href="/search/stat?searchtype=author&query=Desai%2C+R">Rathin Desai</a>, 
<a href="/search/stat?searchtype=author&query=Shetty%2C+A">Abhishek Shetty</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+S">Steven Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to statistical lower bounds on the learnability of many function classes
under privacy constraints, there has been recent interest in leveraging public
data to improve the performance of private learning algorithms. In this model,
algorithms must always guarantee differential privacy with respect to the
private samples while also ensuring learning guarantees when the private data
distribution is sufficiently close to that of the public data. Previous work
has demonstrated that when sufficient public, unlabelled data is available,
private learning can be made statistically tractable, but the resulting
algorithms have all been computationally inefficient. In this work, we present
the first computationally efficient, algorithms to provably leverage public
data to learn privately whenever a function class is learnable non-privately,
where our notion of computational efficiency is with respect to the number of
calls to an optimization oracle for the function class. In addition to this
general result, we provide specialized algorithms with improved sample
complexities in the special cases when the function class is convex or when the
task is binary classification.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09495" title="Abstract">arXiv:2402.09495</a> (cross-list from q-fin.RM) [<a href="/pdf/2402.09495" title="Download PDF">pdf</a>, <a href="/format/2402.09495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Potential of Network-Based Features for Fraud Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Azarm%2C+C">Catayoun Azarm</a>, 
<a href="/search/q-fin?searchtype=author&query=Acar%2C+E">Erman Acar</a>, 
<a href="/search/q-fin?searchtype=author&query=van+Zeelt%2C+M">Mickey van Zeelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Online transaction fraud presents substantial challenges to businesses and
consumers, risking significant financial losses. Conventional rule-based
systems struggle to keep pace with evolving fraud tactics, leading to high
false positive rates and missed detections. Machine learning techniques offer a
promising solution by leveraging historical data to identify fraudulent
patterns. This article explores using the personalised PageRank (PPR) algorithm
to capture the social dynamics of fraud by analysing relationships between
financial accounts. The primary objective is to compare the performance of
traditional features with the addition of PPR in fraud detection models.
Results indicate that integrating PPR enhances the model's predictive power,
surpassing the baseline model. Additionally, the PPR feature provides unique
and valuable information, evidenced by its high feature importance score.
Feature stability analysis confirms consistent feature distributions across
training and test datasets.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09524" title="Abstract">arXiv:2402.09524</a> (cross-list from quant-ph) [<a href="/pdf/2402.09524" title="Download PDF">pdf</a>, <a href="/format/2402.09524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Quantum Compression for Higgs Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Belis%2C+V">Vasilis Belis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Odagiu%2C+P">Patrick Odagiu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Grossi%2C+M">Michele Grossi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Reiter%2C+F">Florentin Reiter</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dissertori%2C+G">G&#xfc;nther Dissertori</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vallecorsa%2C+S">Sofia Vallecorsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">Quantum machine learning provides a fundamentally novel and promising
approach to analyzing data. However, many data sets are too complex for
currently available quantum computers. Consequently, quantum machine learning
applications conventionally resort to dimensionality reduction algorithms,
e.g., auto-encoders, before passing data through the quantum models. We show
that using a classical auto-encoder as an independent preprocessing step can
significantly decrease the classification performance of a quantum machine
learning algorithm. To ameliorate this issue, we design an architecture that
unifies the preprocessing and quantum classification algorithms into a single
trainable model: the guided quantum compression model. The utility of this
model is demonstrated by using it to identify the Higgs boson in proton-proton
collisions at the LHC, where the conventional approach proves ineffective.
Conversely, the guided quantum compression model excels at solving this
classification problem, achieving a good accuracy. Additionally, the model
developed herein shows better performance compared to the classical benchmark
when using only low-level kinematic features.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09567" title="Abstract">arXiv:2402.09567</a> (cross-list from eess.IV) [<a href="/pdf/2402.09567" title="Download PDF">pdf</a>, <a href="/format/2402.09567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial  Network for early-to-late frame conversion in dynamic cardiac PET inter-frame  motion correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+X">Xueqi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+L">Luyao Shi</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiongchao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Qiong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+H">Huidong Xie</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yi-Hwa Liu</a>, 
<a href="/search/eess?searchtype=author&query=Palyo%2C+R">Richard Palyo</a>, 
<a href="/search/eess?searchtype=author&query=Miller%2C+E+J">Edward J. Miller</a>, 
<a href="/search/eess?searchtype=author&query=Sinusas%2C+A+J">Albert J. Sinusas</a>, 
<a href="/search/eess?searchtype=author&query=Staib%2C+L+H">Lawrence H. Staib</a>, 
<a href="/search/eess?searchtype=author&query=Spottiswoode%2C+B">Bruce Spottiswoode</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Dvornek%2C+N+C">Nicha C. Dvornek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under revision at Medical Image Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Inter-frame motion in dynamic cardiac positron emission tomography (PET)
using rubidium-82 (82-Rb) myocardial perfusion imaging impacts myocardial blood
flow (MBF) quantification and the diagnosis accuracy of coronary artery
diseases. However, the high cross-frame distribution variation due to rapid
tracer kinetics poses a considerable challenge for inter-frame motion
correction, especially for early frames where intensity-based image
registration techniques often fail. To address this issue, we propose a novel
method called Temporally and Anatomically Informed Generative Adversarial
Network (TAI-GAN) that utilizes an all-to-one mapping to convert early frames
into those with tracer distribution similar to the last reference frame. The
TAI-GAN consists of a feature-wise linear modulation layer that encodes
channel-wise parameters generated from temporal information and rough cardiac
segmentation masks with local shifts that serve as anatomical information. Our
proposed method was evaluated on a clinical 82-Rb PET dataset, and the results
show that our TAI-GAN can produce converted early frames with high image
quality, comparable to the real reference frames. After TAI-GAN conversion, the
motion estimation accuracy and subsequent myocardial blood flow (MBF)
quantification with both conventional and deep learning-based motion correction
methods were improved compared to using the original frames.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09575" title="Abstract">arXiv:2402.09575</a> (cross-list from math.OC) [<a href="/pdf/2402.09575" title="Download PDF">pdf</a>, <a href="/format/2402.09575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Impact of Computation in Adaptive Dynamic Programming for  Stochastic LQR Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cao%2C+W">Wenhan Cao</a>, 
<a href="/search/math?searchtype=author&query=Capone%2C+A">Alexandre Capone</a>, 
<a href="/search/math?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>, 
<a href="/search/math?searchtype=author&query=Pan%2C+W">Wei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Adaptive dynamic programming (ADP) for stochastic linear quadratic regulation
(LQR) demands the precise computation of stochastic integrals during policy
iteration (PI). In a fully model-free problem setting, this computation can
only be approximated by state samples collected at discrete time points using
computational methods such as the canonical Euler-Maruyama method. Our research
reveals a critical phenomenon: the sampling period can significantly impact
control performance. This impact is due to the fact that computational errors
introduced in each step of PI can significantly affect the algorithm's
convergence behavior, which in turn influences the resulting control policy. We
draw a parallel between PI and Newton's method applied to the Ricatti equation
to elucidate how the computation impacts control. In this light, the
computational error in each PI step manifests itself as an extra error term in
each step of Newton's method, with its upper bound proportional to the
computational error. Furthermore, we demonstrate that the convergence rate for
ADP in stochastic LQR problems using the Euler-Maruyama method is O(h), with h
being the sampling period. A sensorimotor control task finally validates these
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09597" title="Abstract">arXiv:2402.09597</a> (cross-list from math.CO) [<a href="/pdf/2402.09597" title="Download PDF">pdf</a>, <a href="/ps/2402.09597" title="Download PostScript">ps</a>, <a href="/format/2402.09597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consecutive Power Occurrences in Sturmian Words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bell%2C+J">Jason Bell</a>, 
<a href="/search/math?searchtype=author&query=Schulz%2C+C">Chris Schulz</a>, 
<a href="/search/math?searchtype=author&query=Shallit%2C+J">Jeffrey Shallit</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Formal Languages and Automata Theory (cs.FL); Number Theory (math.NT)

</div>
<p class="mathjax">We show that every Sturmian word has the property that the distance between
consecutive ending positions of cubes occurring in the word is always bounded
by $10$ and this bound is optimal, extending a result of Rampersad, who proved
that the bound $9$ holds for the Fibonacci word. We then give a general result
showing that for every $e \in [1,(5+\sqrt{5})/2)$ there is a natural number
$N$, depending only on $e$, such that every Sturmian word has the property that
the distance between consecutive ending positions of $e$-powers occurring in
the word is uniformly bounded by $N$.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09598" title="Abstract">arXiv:2402.09598</a> (cross-list from stat.ML) [<a href="/pdf/2402.09598" title="Download PDF">pdf</a>, <a href="/format/2402.09598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCMC-driven learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bouchard-C%C3%B4t%C3%A9%2C+A">Alexandre Bouchard-C&#xf4;t&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Campbell%2C+T">Trevor Campbell</a>, 
<a href="/search/stat?searchtype=author&query=Pleiss%2C+G">Geoff Pleiss</a>, 
<a href="/search/stat?searchtype=author&query=Surjanovic%2C+N">Nikola Surjanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Computation (stat.CO)

</div>
<p class="mathjax">This paper is intended to appear as a chapter for the Handbook of Markov
Chain Monte Carlo. The goal of this chapter is to unify various problems at the
intersection of Markov chain Monte Carlo (MCMC) and machine
learning$\unicode{x2014}$which includes black-box variational inference,
adaptive MCMC, normalizing flow construction and transport-assisted MCMC,
surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov
chain gradient descent, Markovian score climbing, and
more$\unicode{x2014}$within one common framework. By doing so, the theory and
methods developed for each may be translated and generalized.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09619" title="Abstract">arXiv:2402.09619</a> (cross-list from eess.SP) [<a href="/pdf/2402.09619" title="Download PDF">pdf</a>, <a href="/ps/2402.09619" title="Download PostScript">ps</a>, <a href="/format/2402.09619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A  Distributed Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhou Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Atapattu%2C+S">Saman Atapattu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yizhu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+S">Sumei Sun</a>, 
<a href="/search/eess?searchtype=author&query=Sithamparanathan%2C+K">Kandeepan Sithamparanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI); Statistics Theory (math.ST)

</div>
<p class="mathjax">This paper presents an optimization approach for cooperative Medium Access
Control (MAC) techniques in Vehicular Ad Hoc Networks (VANETs) equipped with
Roadside Unit (RSU) to enhance network throughput. Our method employs a
distributed cooperative MAC scheme based on Carrier Sense Multiple Access with
Collision Avoidance (CSMA/CA) protocol, featuring selective RSU probing and
adaptive transmission. It utilizes a dual timescale channel access framework,
with a ``large-scale'' phase accounting for gradual changes in vehicle
locations and a ``small-scale'' phase adapting to rapid channel fluctuations.
We propose the RSU Probing and Cooperative Access (RPCA) strategy, a two-stage
approach based on dynamic inter-vehicle distances from the RSU. Using optimal
sequential planned decision theory, we rigorously prove its optimality in
maximizing average system throughput per large-scale phase. For practical
implementation in VANETs, we develop a distributed MAC algorithm with periodic
location updates. It adjusts thresholds based on inter-vehicle and vehicle-RSU
distances during the large-scale phase and accesses channels following the RPCA
strategy with updated thresholds during the small-scale phase. Simulation
results confirm the effectiveness and efficiency of our algorithm.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09622" title="Abstract">arXiv:2402.09622</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2402.09622" title="Download PDF">pdf</a>, <a href="/format/2402.09622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advanced fuel fusion, phase space engineering, and structure-preserving  geometric algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Qin%2C+H">Hong Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Mathematical Physics (math-ph); Numerical Analysis (math.NA); Symplectic Geometry (math.SG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Non-thermal advanced fuel fusion trades the requirement of a large amount of
recirculating tritium in the system for that of large recirculating power.
Phase space engineering technologies utilizing externally injected
electromagnetic fields can be applied to meet the challenge of maintaining
non-thermal particle distributions at a reasonable cost. The physical processes
of the phase space engineering are studied from a theoretical and algorithmic
perspective. It is emphasized that the operational space of phase space
engineering is limited by the underpinning symplectic dynamics of charged
particles. The phase space incompressibility according to the Liouville theorem
is just one of many constraints, and Gromov's non-squeezing theorem determines
the minimum footprints of the charged particles on every conjugate phase space
plane. In this sense and level of sophistication, the mathematical abstraction
of phase space engineering is symplectic topology. To simulate the processes of
phase space engineering, such as the Maxwell demon and electromagnetic energy
extraction, and to accurately calculate the minimum footprints of charged
particles, recently developed structure-preserving geometric algorithms can be
used. The family of algorithms conserves exactly, on discretized spacetime,
symplecticity and thus incompressibility, non-squeezability, and symplectic
capacities. The algorithms apply to the dynamics of charged particles under the
influence of external electromagnetic fields as well as the charged
particle-electromagnetic field system governed by the Vlasov-Maxwell equations.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09623" title="Abstract">arXiv:2402.09623</a> (cross-list from stat.ML) [<a href="/pdf/2402.09623" title="Download PDF">pdf</a>, <a href="/format/2402.09623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformalized Adaptive Forecasting of Heterogeneous Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhou%2C+Y">Yanfei Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Lindemann%2C+L">Lars Lindemann</a>, 
<a href="/search/stat?searchtype=author&query=Sesia%2C+M">Matteo Sesia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a new conformal method for generating simultaneous
forecasting bands guaranteed to cover the entire path of a new random
trajectory with sufficiently high probability. Prompted by the need for
dependable uncertainty estimates in motion planning applications where the
behavior of diverse objects may be more or less unpredictable, we blend
different techniques from online conformal prediction of single and multiple
time series, as well as ideas for addressing heteroscedasticity in regression.
This solution is both principled, providing precise finite-sample guarantees,
and effective, often leading to more informative predictions than prior
methods.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09636" title="Abstract">arXiv:2402.09636</a> (cross-list from eess.IV) [<a href="/pdf/2402.09636" title="Download PDF">pdf</a>, <a href="/format/2402.09636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital  Subtraction Angiography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baur%2C+K">Kathleen Baur</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+X">Xin Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Torio%2C+E">Erickson Torio</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+R">Rose Du</a>, 
<a href="/search/eess?searchtype=author&query=Juvekar%2C+P">Parikshit Juvekar</a>, 
<a href="/search/eess?searchtype=author&query=Dorent%2C+R">Reuben Dorent</a>, 
<a href="/search/eess?searchtype=author&query=Golby%2C+A">Alexandra Golby</a>, 
<a href="/search/eess?searchtype=author&query=Frisken%2C+S">Sarah Frisken</a>, 
<a href="/search/eess?searchtype=author&query=Haouchine%2C+N">Nazim Haouchine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for publication at SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Although Digital Subtraction Angiography (DSA) is the most important imaging
for visualizing cerebrovascular anatomy, its interpretation by clinicians
remains difficult. This is particularly true when treating arteriovenous
malformations (AVMs), where entangled vasculature connecting arteries and veins
needs to be carefully identified.The presented method aims to enhance DSA image
series by highlighting critical information via automatic classification of
vessels using a combination of two learning models: An unsupervised machine
learning method based on Independent Component Analysis that decomposes the
phases of flow and a convolutional neural network that automatically delineates
the vessels in image space. The proposed method was tested on clinical DSA
images series and demonstrated efficient differentiation between arteries and
veins that provides a viable solution to enhance visualizations for clinical
use.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09658" title="Abstract">arXiv:2402.09658</a> (cross-list from eess.IV) [<a href="/pdf/2402.09658" title="Download PDF">pdf</a>, <a href="/ps/2402.09658" title="Download PostScript">ps</a>, <a href="/format/2402.09658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF  Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naderi%2C+A+M">Amir Mohammad Naderi</a>, 
<a href="/search/eess?searchtype=author&query=Casey%2C+J+G">Jennifer G. Casey</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+M">Mao-Hsiang Huang</a>, 
<a href="/search/eess?searchtype=author&query=Victorio%2C+R">Rachelle Victorio</a>, 
<a href="/search/eess?searchtype=author&query=Chiang%2C+D+Y">David Y. Chiang</a>, 
<a href="/search/eess?searchtype=author&query=MacRae%2C+C">Calum MacRae</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+H">Hung Cao</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+V+A">Vandana A. Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Quantifying cardiovascular parameters like ejection fraction in zebrafish as
a host of biological investigations has been extensively studied. Since current
manual monitoring techniques are time-consuming and fallible, several image
processing frameworks have been proposed to automate the process. Most of these
works rely on supervised deep-learning architectures. However, supervised
methods tend to be overfitted on their training dataset. This means that
applying the same framework to new data with different imaging setups and
mutant types can severely decrease performance. We have developed a Zebrafish
Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac
function in zebrafish. In this work, we further applied data augmentation,
Transfer Learning (TL), and Test Time Augmentation (TTA) to ZACAF to improve
the performance for the quantification of cardiovascular function
quantification in zebrafish. This strategy can be integrated with the available
frameworks to aid other researchers. We demonstrate that using TL, even with a
constrained dataset, the model can be refined to accommodate a novel microscope
setup, encompassing diverse mutant types and accommodating various video
recording protocols. Additionally, as users engage in successive rounds of TL,
the model is anticipated to undergo substantial enhancements in both
generalizability and accuracy. Finally, we applied this approach to assess the
cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09682" title="Abstract">arXiv:2402.09682</a> (cross-list from eess.SP) [<a href="/pdf/2402.09682" title="Download PDF">pdf</a>, <a href="/format/2402.09682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Range Backscatter Connectivity via Spaceborne Synthetic Aperture  Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ecola%2C+G">Geneva Ecola</a>, 
<a href="/search/eess?searchtype=author&query=Yen%2C+B">Bill Yen</a>, 
<a href="/search/eess?searchtype=author&query=Priyantha%2C+B">Bodhi Priyantha</a>, 
<a href="/search/eess?searchtype=author&query=Chandra%2C+R">Ranveer Chandra</a>, 
<a href="/search/eess?searchtype=author&query=Kapetanovic%2C+Z">Zerina Kapetanovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">SarComms is a new communication method that enables passive satellite
backscatter connectivity using existing spaceborne synthetic aperture radar
(SAR) signals. We demonstrate that SAR signals from the European Space Agency's
Sentinel-1 satellite, used for imaging the Earth, can also be leveraged to
enable ground-to-satellite connectivity. This paper presents the first
cooperative, on-the-ground target that modulates SAR backscatter to send
information bits and analyzes how to extract it from publicly available
Sentinel-1 datasets. To demonstrate the system's feasibility, we evaluate the
effectiveness of corner reflectors in the field, develop a deployment algorithm
to optimize reflector placement and prototype modulating corner reflectors
(both mechanically and electrically controlled) to change the amplitude of
backscattered SAR signals.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09697" title="Abstract">arXiv:2402.09697</a> (cross-list from econ.TH) [<a href="/pdf/2402.09697" title="Download PDF">pdf</a>, <a href="/format/2402.09697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Three-Layer Data Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Fallah%2C+A">Alireza Fallah</a>, 
<a href="/search/econ?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/econ?searchtype=author&query=Makhdoumi%2C+A">Ali Makhdoumi</a>, 
<a href="/search/econ?searchtype=author&query=Malekian%2C+A">Azarakhsh Malekian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study a three-layer data market comprising users (data owners), platforms,
and a data buyer. Each user benefits from platform services in exchange for
data, incurring privacy loss when their data, albeit noisily, is shared with
the buyer. The user chooses platforms to share data with, while platforms
decide on data noise levels and pricing before selling to the buyer. The buyer
selects platforms to purchase data from. We model these interactions via a
multi-stage game, focusing on the subgame Nash equilibrium. We find that when
the buyer places a high value on user data (and platforms can command high
prices), all platforms offer services to the user who joins and shares data
with every platform. Conversely, when the buyer's valuation of user data is
low, only large platforms with low service costs can afford to serve users. In
this scenario, users exclusively join and share data with these low-cost
platforms. Interestingly, increased competition benefits the buyer, not the
user: as the number of platforms increases, the user utility does not improve
while the buyer utility improves. However, increasing the competition improves
the overall utilitarian welfare. Building on our analysis, we then study
regulations to improve the user utility. We discover that banning data sharing
maximizes user utility only when all platforms are low-cost. In mixed markets
of high- and low-cost platforms, users prefer a minimum noise mandate over a
sharing ban. Imposing this mandate on high-cost platforms and banning data
sharing for low-cost ones further enhances user utility.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09698" title="Abstract">arXiv:2402.09698</a> (cross-list from stat.ME) [<a href="/pdf/2402.09698" title="Download PDF">pdf</a>, <a href="/format/2402.09698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Evidence Across Filtrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Choe%2C+Y+J">Yo Joong Choe</a>, 
<a href="/search/stat?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">In anytime-valid sequential inference, it is known that any admissible
inference procedure must be based on test martingales and their composite
generalization, called e-processes, which are nonnegative processes whose
expectation at any arbitrary stopping time is upper-bounded by one. An
e-process quantifies the accumulated evidence against a composite null
hypothesis over a sequence of outcomes. This paper studies methods for
combining e-processes that are computed using different information sets, i.e.,
filtrations, for a null hypothesis. Even though e-processes constructed on the
same filtration can be combined effortlessly (e.g., by averaging), e-processes
constructed on different filtrations cannot be combined as easily because their
validity in a coarser filtration does not translate to validity in a finer
filtration. We discuss three concrete examples of such e-processes in the
literature: exchangeability tests, independence tests, and tests for evaluating
and comparing forecasts with lags. Our main result establishes that these
e-processes can be lifted into any finer filtration using adjusters, which are
functions that allow betting on the running maximum of the accumulated wealth
(thereby insuring against the loss of evidence). We also develop randomized
adjusters that can improve the power of the resulting sequential inference
procedure.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09705" title="Abstract">arXiv:2402.09705</a> (cross-list from quant-ph) [<a href="/pdf/2402.09705" title="Download PDF">pdf</a>, <a href="/format/2402.09705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Depth QFT over IBM Heavy-hex Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gao%2C+X">Xiangyu Gao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jin%2C+Y">Yuwei Jin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guo%2C+M">Minghao Guo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+H">Henry Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+E+Z">Eddy Z. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Compiling a given quantum algorithm into a target hardware architecture is a
challenging optimization problem. The compiler must take into consideration the
coupling graph of physical qubits and the gate operation dependencies. The
existing noise in hardware architectures requires the compilation to use as few
running cycles as possible. Existing approaches include using SAT solver or
heuristics to complete the mapping but these may cause the issue of either long
compilation time (e.g., timeout after hours) or suboptimal compilation results
in terms of running cycles (e.g., exponentially increasing number of total
cycles).
<br />In this paper, we propose an efficient mapping approach for Quantum Fourier
Transformation (QFT) circuits over the existing IBM heavy-hex architecture.
Such proposal first of all turns the architecture into a structure consisting
of a straight line with dangling qubits, and then do the mapping over this
generated structure recursively. The calculation shows that there is a linear
depth upper bound for the time complexity of these structures and for a special
case where there is 1 dangling qubit in every 5 qubits, the time complexity is
5N+O(1). All these results are better than state of the art methods.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09714" title="Abstract">arXiv:2402.09714</a> (cross-list from math.OC) [<a href="/pdf/2402.09714" title="Download PDF">pdf</a>, <a href="/format/2402.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Accelerated Distributed Stochastic Gradient Method with Momentum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+K">Kun Huang</a>, 
<a href="/search/math?searchtype=author&query=Pu%2C+S">Shi Pu</a>, 
<a href="/search/math?searchtype=author&query=Nedi%C4%87%2C+A">Angelia Nedi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In this paper, we introduce an accelerated distributed stochastic gradient
method with momentum for solving the distributed optimization problem, where a
group of $n$ agents collaboratively minimize the average of the local objective
functions over a connected network. The method, termed ``Distributed Stochastic
Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the
momentum tracking technique as well as the Loopless Chebyshev Acceleration
(LCA) method. We show that DSMT can asymptotically achieve comparable
convergence rates as centralized stochastic gradient descent (SGD) method under
a general variance condition regarding the stochastic gradients. Moreover, the
number of iterations (transient times) required for DSMT to achieve such rates
behaves as $\mathcal{O}(n^{5/3}/(1-\lambda))$ for minimizing general smooth
objective functions, and $\mathcal{O}(\sqrt{n/(1-\lambda)})$ under the
Polyak-{\L}ojasiewicz (PL) condition. Here, the term $1-\lambda$ denotes the
spectral gap of the mixing matrix related to the underlying network topology.
Notably, the obtained results do not rely on multiple inter-node communications
or stochastic gradient accumulation per iteration, and the transient times are
the shortest under the setting to the best of our knowledge.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09723" title="Abstract">arXiv:2402.09723</a> (cross-list from stat.ML) [<a href="/pdf/2402.09723" title="Download PDF">pdf</a>, <a href="/format/2402.09723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Arm Identification for Prompt Learning under a Limited Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shi%2C+C">Chengshuai Shi</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+K">Kun Yang</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+C">Cong Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The remarkable instruction-following capability of large language models
(LLMs) has sparked a growing interest in automatically learning suitable
prompts. However, while many effective methods have been proposed, the cost
incurred during the learning process (e.g., accessing LLM and evaluating the
responses) has not been considered. To overcome this limitation, this work
explicitly incorporates a finite budget constraint into prompt learning.
Towards developing principled solutions, a novel connection is established
between prompt learning and fixed-budget best arm identification (BAI-FB) in
multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE
(besT aRm Identification for Prompt LEarning) is proposed to harness the power
of BAI-FB in prompt learning systematically. Unique characteristics of prompt
learning further lead to two embedding-based enhancements of TRIPLE by
exploiting the ideas of clustering and function approximation. Extensive
experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2
demonstrate the significant performance improvement of TRIPLE over the previous
baselines while satisfying the limited budget constraints.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09747" title="Abstract">arXiv:2402.09747</a> (cross-list from eess.IV) [<a href="/pdf/2402.09747" title="Download PDF">pdf</a>, <a href="/format/2402.09747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is more: Ensemble Learning for Retinal Disease Recognition Under  Limited Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+H">Hong Peng</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+S">Shengchao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+S">Sufen Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Retinal optical coherence tomography (OCT) images provide crucial insights
into the health of the posterior ocular segment. Therefore, the advancement of
automated image analysis methods is imperative to equip clinicians and
researchers with quantitative data, thereby facilitating informed
decision-making. The application of deep learning (DL)-based approaches has
gained extensive traction for executing these analysis tasks, demonstrating
remarkable performance compared to labor-intensive manual analyses. However,
the acquisition of Retinal OCT images often presents challenges stemming from
privacy concerns and the resource-intensive labeling procedures, which
contradicts the prevailing notion that DL models necessitate substantial data
volumes for achieving superior performance. Moreover, limitations in available
computational resources constrain the progress of high-performance medical
artificial intelligence, particularly in less developed regions and countries.
This paper introduces a novel ensemble learning mechanism designed for
recognizing retinal diseases under limited resources (e.g., data, computation).
The mechanism leverages insights from multiple pre-trained models, facilitating
the transfer and adaptation of their knowledge to Retinal OCT images. This
approach establishes a robust model even when confronted with limited labeled
data, eliminating the need for an extensive array of parameters, as required in
learning from scratch. Comprehensive experimentation on real-world datasets
demonstrates that the proposed approach can achieve superior performance in
recognizing Retinal OCT images, even when dealing with exceedingly restricted
labeled datasets. Furthermore, this method obviates the necessity of learning
extensive-scale parameters, making it well-suited for deployment in
low-resource scenarios.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09752" title="Abstract">arXiv:2402.09752</a> (cross-list from physics.optics) [<a href="/pdf/2402.09752" title="Download PDF">pdf</a>, <a href="/ps/2402.09752" title="Download PostScript">ps</a>, <a href="/format/2402.09752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector spectrometer with Hertz-level resolution and super-recognition  capability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Qing%2C+T">Ting Qing</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+S">Shupeng Li</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+H">Huashan Yang</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+L">Lihan Wang</a>, 
<a href="/search/physics?searchtype=author&query=Fang%2C+Y">Yijie Fang</a>, 
<a href="/search/physics?searchtype=author&query=Tang%2C+X">Xiaohu Tang</a>, 
<a href="/search/physics?searchtype=author&query=Cao%2C+M">Meihui Cao</a>, 
<a href="/search/physics?searchtype=author&query=Lu%2C+J">Jianming Lu</a>, 
<a href="/search/physics?searchtype=author&query=He%2C+J">Jijun He</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+J">Junqiu Liu</a>, 
<a href="/search/physics?searchtype=author&query=Lv%2C+Y">Yueguang Lv</a>, 
<a href="/search/physics?searchtype=author&query=Pan%2C+S">Shilong Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Systems and Control (eess.SY); Applied Physics (physics.app-ph); Quantum Physics (quant-ph)

</div>
<p class="mathjax">High-resolution optical spectrometers are crucial in revealing intricate
characteristics of signals, determining laser frequencies, measuring physical
constants, identifying substances, and advancing biosensing applications.
Conventional spectrometers, however, often grapple with inherent trade-offs
among spectral resolution, wavelength range, and accuracy. Furthermore, even at
high resolution, resolving overlapping spectral lines during spectroscopic
analyses remains a huge challenge. Here, we propose a vector spectrometer with
ultrahigh resolution, combining broadband optical frequency hopping, ultrafine
microwave-photonic scanning, and vector detection. A programmable
frequency-hopping laser was developed, facilitating a sub-Hz linewidth and
Hz-level frequency stability, an improvement of four and six orders of
magnitude, respectively, compared to those of state-of-the-art tunable lasers.
We also designed an asymmetric optical transmitter and receiver to eliminate
measurement errors arising from modulation nonlinearity and multi-channel
crosstalk. The resultant vector spectrometer exhibits an unprecedented
frequency resolution of 2 Hz, surpassing the state-of-the-art by four orders of
magnitude, over a 33-nm range. Through high-resolution vector analysis, we
observed that group delay information enhances the separation capability of
overlapping spectral lines by over 47%, significantly streamlining the
real-time identification of diverse substances. Our technique fills the gap in
optical spectrometers with resolutions below 10 kHz and enables vector
measurement to embrace revolution in functionality.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09754" title="Abstract">arXiv:2402.09754</a> (cross-list from stat.ML) [<a href="/pdf/2402.09754" title="Download PDF">pdf</a>, <a href="/format/2402.09754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust SVD Made Easy: A fast and reliable algorithm for large-scale data  analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Han%2C+S">Sangil Han</a>, 
<a href="/search/stat?searchtype=author&query=Kim%2C+K">Kyoowon Kim</a>, 
<a href="/search/stat?searchtype=author&query=Jung%2C+S">Sungkyu Jung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">The singular value decomposition (SVD) is a crucial tool in machine learning
and statistical data analysis. However, it is highly susceptible to outliers in
the data matrix. Existing robust SVD algorithms often sacrifice speed for
robustness or fail in the presence of only a few outliers. This study
introduces an efficient algorithm, called Spherically Normalized SVD, for
robust SVD approximation that is highly insensitive to outliers,
computationally scalable, and provides accurate approximations of singular
vectors. The proposed algorithm achieves remarkable speed by utilizing only two
applications of a standard reduced-rank SVD algorithm to appropriately scaled
data, significantly outperforming competing algorithms in computation times. To
assess the robustness of the approximated singular vectors and their subspaces
against data contamination, we introduce new notions of breakdown points for
matrix-valued input, including row-wise, column-wise, and block-wise breakdown
points. Theoretical and empirical analyses demonstrate that our algorithm
exhibits higher breakdown points compared to standard SVD and its
modifications. We empirically validate the effectiveness of our approach in
applications such as robust low-rank approximation and robust principal
component analysis of high-dimensional microarray datasets. Overall, our study
presents a highly efficient and robust solution for SVD approximation that
overcomes the limitations of existing algorithms in the presence of outliers.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09755" title="Abstract">arXiv:2402.09755</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.09755" title="Download PDF">pdf</a>, <a href="/ps/2402.09755" title="Download PostScript">ps</a>, <a href="/format/2402.09755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Jiang%2C+L">Linfeng Jiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+Y">Yuan Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Single-cell RNA sequencing (scRNA-seq) determines RNA expression at
single-cell resolution. It provides a powerful tool for studying immunity,
regulation, and other life activities of cells. However, due to the limitations
of the sequencing technique, the scRNA-seq data are represented with sparsity,
whichcontains missing gene values, i.e., zero values, called dropout.
Therefore, it is necessary to impute missing values before analyzing scRNA-seq
data. However, existing imputation computation methods often only focus on the
identification of technical zeros or imputing all zeros based on cell
similarity. This study proposes a new method (SFAG) to reconstruct the gene
expression relationship matrix by usinggraph regularization technology to
preserve the high-dimensional manifold information of the data, andto mine the
relationship between genes and cells in the data, and then uses a method of
averaging the clustering results to fill in the identified technical zeros.
Experimental results show that SFAGcan helpimprove downstream analysis and
reconstruct cell trajectory
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09772" title="Abstract">arXiv:2402.09772</a> (cross-list from math.ST) [<a href="/pdf/2402.09772" title="Download PDF">pdf</a>, <a href="/format/2402.09772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Experimental Design for Partially Observable Pure Birth  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eshragh%2C+A">Ali Eshragh</a>, 
<a href="/search/math?searchtype=author&query=Skerritt%2C+M+P">Matthew P. Skerritt</a>, 
<a href="/search/math?searchtype=author&query=Salvy%2C+B">Bruno Salvy</a>, 
<a href="/search/math?searchtype=author&query=McCallum%2C+T">Thomas McCallum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We develop an efficient algorithm to find optimal observation times by
maximizing the Fisher information for the birth rate of a partially observable
pure birth process involving $n$ observations. Partially observable implies
that at each of the $n$ observation time points for counting the number of
individuals present in the pure birth process, each individual is observed
independently with a fixed probability $p$, modeling detection difficulties or
constraints on resources. We apply concepts and techniques from generating
functions, using a combination of symbolic and numeric computation, to
establish a recursion for evaluating and optimizing the Fisher information. Our
numerical results reveal the efficacy of this new method. An implementation of
the algorithm is available publicly.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09796" title="Abstract">arXiv:2402.09796</a> (cross-list from stat.ML) [<a href="/pdf/2402.09796" title="Download PDF">pdf</a>, <a href="/ps/2402.09796" title="Download PostScript">ps</a>, <a href="/format/2402.09796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-form Filtering for Non-linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cantelobre%2C+T">Th&#xe9;ophile Cantelobre</a>, 
<a href="/search/stat?searchtype=author&query=Ciliberto%2C+C">Carlo Ciliberto</a>, 
<a href="/search/stat?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>, 
<a href="/search/stat?searchtype=author&query=Rudi%2C+A">Alessandro Rudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Sequential Bayesian Filtering aims to estimate the current state distribution
of a Hidden Markov Model, given the past observations. The problem is
well-known to be intractable for most application domains, except in notable
cases such as the tabular setting or for linear dynamical systems with gaussian
noise. In this work, we propose a new class of filters based on Gaussian PSD
Models, which offer several advantages in terms of density approximation and
computational efficiency. We show that filtering can be efficiently performed
in closed form when transitions and observations are Gaussian PSD Models. When
the transition and observations are approximated by Gaussian PSD Models, we
show that our proposed estimator enjoys strong theoretical guarantees, with
estimation error that depends on the quality of the approximation and is
adaptive to the regularity of the transition probabilities. In particular, we
identify regimes in which our proposed filter attains a TV $\epsilon$-error
with memory and computational complexity of $O(\epsilon^{-1})$ and
$O(\epsilon^{-3/2})$ respectively, including the offline learning step, in
contrast to the $O(\epsilon^{-2})$ complexity of sampling methods such as
particle filtering.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09802" title="Abstract">arXiv:2402.09802</a> (cross-list from stat.ML) [<a href="/pdf/2402.09802" title="Download PDF">pdf</a>, <a href="/format/2402.09802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Criterion collapse and loss distribution control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Holland%2C+M+J">Matthew J. Holland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we consider the notion of "criterion collapse," in which
optimization of one metric implies optimality in another, with a particular
focus on conditions for collapse into error probability minimizers under a wide
variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM)
to non-monotonic criteria underlying recent ascent-descent algorithms explored
in the literature (Flooding, SoftAD). We show how collapse in the context of
losses with a Bernoulli distribution goes far beyond existing results for CVaR
and DRO, then expand our scope to include surrogate losses, showing conditions
where monotonic criteria such as tilted ERM cannot avoid collapse, whereas
non-monotonic alternatives can.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09804" title="Abstract">arXiv:2402.09804</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.09804" title="Download PDF">pdf</a>, <a href="/format/2402.09804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coevolution of relationship and interaction in cooperative dynamical  multiplex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Xiong%2C+X">Xiaojin Xiong</a>, 
<a href="/search/physics?searchtype=author&query=Zeng%2C+Z">Ziyan Zeng</a>, 
<a href="/search/physics?searchtype=author&query=Feng%2C+M">Minyu Feng</a>, 
<a href="/search/physics?searchtype=author&query=Szolnoki%2C+A">Attila Szolnoki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 two-column pages, 6 figures, to be published in Chaos
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chaos 34(2) (2024) 023118
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Computer Science and Game Theory (cs.GT); Pattern Formation and Solitons (nlin.PS)

</div>
<p class="mathjax">While actors in a population can interact with anyone else freely, social
relations significantly influence our inclination towards particular
individuals. The consequence of such interactions, however, may also form the
intensity of our relations established earlier. These dynamical processes are
captured via a coevolutionary model staged in multiplex networks with two
distinct layers. In a so-called relationship layer the weights of edges among
players may change in time as a consequence of games played in the alternative
interaction layer. As an reasonable assumption, bilateral cooperation confirms
while mutual defection weakens these weight factors. Importantly, the fitness
of a player, which basically determines the success of a strategy imitation,
depends not only on the payoff collected from interactions, but also on the
individual relationship index calculated from the mentioned weight factors of
related edges. Within the framework of weak prisoner's dilemma situation we
explore the potential outcomes of the mentioned coevolutionary process where we
assume different topologies for relationship layer. We find that higher average
degree of the relationship graph is more beneficial to maintain cooperation in
regular graphs, but the randomness of links could be a decisive factor in harsh
situations. Surprisingly, a stronger coupling between relationship index and
fitness discourage the evolution of cooperation by weakening the direct
consequence of a strategy change. To complete our study we also monitor how the
distribution of relationship index vary and detect a strong relation between
its polarization and the general cooperation level.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09807" title="Abstract">arXiv:2402.09807</a> (cross-list from math.OC) [<a href="/pdf/2402.09807" title="Download PDF">pdf</a>, <a href="/format/2402.09807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two trust region type algorithms for solving nonconvex-strongly concave  minimax problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yao%2C+T">Tongliang Yao</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a
Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE)
algorithm for solving nonconvex-strongly concave minimax problems. Both
algorithms can find an $(\epsilon, \sqrt{\epsilon})$-second order stationary
point(SSP) within $\mathcal{O}(\epsilon^{-1.5})$ iterations, which matches the
best well known iteration complexity.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09821" title="Abstract">arXiv:2402.09821</a> (cross-list from eess.AS) [<a href="/pdf/2402.09821" title="Download PDF">pdf</a>, <a href="/format/2402.09821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models for Audio Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lemercier%2C+J">Jean-Marie Lemercier</a>, 
<a href="/search/eess?searchtype=author&query=Richter%2C+J">Julius Richter</a>, 
<a href="/search/eess?searchtype=author&query=Welker%2C+S">Simon Welker</a>, 
<a href="/search/eess?searchtype=author&query=Moliner%2C+E">Eloi Moliner</a>, 
<a href="/search/eess?searchtype=author&query=V%C3%A4lim%C3%A4ki%2C+V">Vesa V&#xe4;lim&#xe4;ki</a>, 
<a href="/search/eess?searchtype=author&query=Gerkmann%2C+T">Timo Gerkmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full paper invited to the IEEE Signal Processing Magazine Special Issue "Model-based and Data-Driven Audio Signal Processing"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">With the development of audio playback devices and fast data transmission,
the demand for high sound quality is rising, for both entertainment and
communications. In this quest for better sound quality, challenges emerge from
distortions and interferences originating at the recording side or caused by an
imperfect transmission pipeline. To address this problem, audio restoration
methods aim to recover clean sound signals from the corrupted input data. We
present here audio restoration algorithms based on diffusion models, with a
focus on speech enhancement and music restoration tasks. Traditional
approaches, often grounded in handcrafted rules and statistical heuristics,
have shaped our understanding of audio signals. In the past decades, there has
been a notable shift towards data-driven methods that exploit the modeling
capabilities of deep neural networks (DNNs). Deep generative models, and among
them diffusion models, have emerged as powerful techniques for learning complex
data distributions. However, relying solely on DNN-based learning approaches
carries the risk of reducing interpretability, particularly when employing
end-to-end models. Nonetheless, data-driven approaches allow more flexibility
in comparison to statistical model-based frameworks whose performance depends
on distributional and statistical assumptions that can be difficult to
guarantee. Here, we aim to show that diffusion models can combine the best of
both worlds and offer the opportunity to design audio restoration algorithms
with a good degree of interpretability and a remarkable performance in terms of
sound quality.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09824" title="Abstract">arXiv:2402.09824</a> (cross-list from math.DS) [<a href="/pdf/2402.09824" title="Download PDF">pdf</a>, <a href="/format/2402.09824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the discrete-time origins of replicator dynamics: from convergence to  instability and chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Falniowski%2C+F">Fryderyk Falniowski</a>, 
<a href="/search/math?searchtype=author&query=Mertikopoulos%2C+P">Panayotis Mertikopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We consider three distinct discrete-time models of learning and evolution in
games: a biological model based on intra-species selective pressure, the
dynamics induced by pairwise proportional imitation, and the exponential /
multiplicative weights algorithm for online learning. Even though these models
share the same continuous-time limit - the replicator dynamics - we show that
second-order effects play a crucial role and may lead to drastically different
behaviors in each model, even in very simple, symmetric two by two games.
Specifically, we study the resulting discrete-time dynamics in a class of
parametrized congestion games, and we show that (i) in the biological model of
intra-species competition, the dynamics remain convergent for any parameter
value; (ii) the dynamics of pairwise proportional imitation for different
equilibrium configurations exhibit an entire range of behaviors for large step
size (stability, instability, and even Li-Yorke chaos); while (iii) for the
exponential / multiplicative weights (EW) algorithm increasing step size will
(almost) inevitably lead to chaos (again, in the formal, Li-Yorke sense). This
divergence of behaviors comes in stark contrast to the globally convergent
behavior of the replicator dynamics, and serves to delineate the extent to
which the replicator dynamics provide a useful predictor for the long-run
behavior of their discrete-time origins.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09846" title="Abstract">arXiv:2402.09846</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.09846" title="Download PDF">pdf</a>, <a href="/ps/2402.09846" title="Download PostScript">ps</a>, <a href="/format/2402.09846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Approach to Radar-based QPE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yo%2C+T">Ting-Shuo Yo</a>, 
<a href="/search/physics?searchtype=author&query=Su%2C+S">Shih-Hao Su</a>, 
<a href="/search/physics?searchtype=author&query=Chu%2C+J">Jung-Lien Chu</a>, 
<a href="/search/physics?searchtype=author&query=Chang%2C+C">Chiao-Wei Chang</a>, 
<a href="/search/physics?searchtype=author&query=Kuo%2C+H">Hung-Chi Kuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures. Published in Earth and Space Science
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Earth Space Sci. 2021, 8, e2020EA001340
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">In this study, we propose a volume-to-point framework for quantitative
precipitation estimation (QPE) based on the Quantitative Precipitation
Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data
set. With a data volume consisting of the time series of gridded radar
reflectivities over the Taiwan area, we used machine learning algorithms to
establish a statistical model for QPE in weather stations. The model extracts
spatial and temporal features from the input data volume and then associates
these features with the location-specific precipitations. In contrast to QPE
methods based on the Z-R relation, we leverage the machine learning algorithms
to automatically detect the evolution and movement of weather systems and
associate these patterns to a location with specific topographic attributes.
Specifically, we evaluated this framework with the hourly precipitation data of
45 weather stations in Taipei during 2013-2016. In comparison to the
operational QPE scheme used by the Central Weather Bureau, the volume-to-point
framework performed comparably well in general cases and excelled in detecting
heavy-rainfall events. By using the current results as the reference benchmark,
the proposed method can integrate the heterogeneous data sources and
potentially improve the forecast in extreme precipitation scenarios.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09867" title="Abstract">arXiv:2402.09867</a> (cross-list from eess.SP) [<a href="/pdf/2402.09867" title="Download PDF">pdf</a>, <a href="/format/2402.09867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Taufique%2C+Z">Zain Taufique</a>, 
<a href="/search/eess?searchtype=author&query=Altaf%2C+M+A+B">Muhammad Awais Bin Altaf</a>, 
<a href="/search/eess?searchtype=author&query=Miele%2C+A">Antonio Miele</a>, 
<a href="/search/eess?searchtype=author&query=Liljeberg%2C+P">Pasi Liljeberg</a>, 
<a href="/search/eess?searchtype=author&query=Kanduri%2C+A">Anil Kanduri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Electroencephalography (EEG) recordings are analyzed using battery-powered
wearable devices to monitor brain activities and neurological disorders. These
applications require long and continuous processing to generate feasible
results. However, wearable devices are constrained with limited energy and
computation resources, owing to their small sizes for practical use cases.
Embedded heterogeneous multi-core platforms (HMPs) can provide better
performance within limited energy budgets for EEG applications. Error
resilience of the EEG application pipeline can be exploited further to maximize
the performance and energy gains with HMPs. However, disciplined tuning of
approximation on embedded HMPs requires a thorough exploration of the
accuracy-performance-power trade-off space. In this work, we characterize the
error resilience of three EEG applications, including Epileptic Seizure
Detection, Sleep Stage Classification, and Stress Detection on the real-world
embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial
evaluation of power-performance-accuracy trade-offs of EEG applications at
different approximation, power, and performance levels to provide insights into
the disciplined tuning of approximation in EEG applications on embedded
platforms.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09914" title="Abstract">arXiv:2402.09914</a> (cross-list from math.SG) [<a href="/pdf/2402.09914" title="Download PDF">pdf</a>, <a href="/ps/2402.09914" title="Download PostScript">ps</a>, <a href="/format/2402.09914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the EHZ capacity is NP-hard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leipold%2C+K">Karla Leipold</a>, 
<a href="/search/math?searchtype=author&query=Vallentin%2C+F">Frank Vallentin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symplectic Geometry (math.SG)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
<p class="mathjax">The Ekeland-Hofer-Zehnder capacity (EHZ capacity) is a fundamental symplectic
invariant of convex bodies. We show that computing the EHZ capacity of
polytopes is NP-hard. For this we reduce the feedback arc set problem in
bipartite tournaments to computing the EHZ capacity of simplices.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09948" title="Abstract">arXiv:2402.09948</a> (cross-list from eess.SP) [<a href="/pdf/2402.09948" title="Download PDF">pdf</a>, <a href="/format/2402.09948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural 5G Indoor Localization with IMU Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ermolov%2C+A">Aleksandr Ermolov</a>, 
<a href="/search/eess?searchtype=author&query=Kadambi%2C+S">Shreya Kadambi</a>, 
<a href="/search/eess?searchtype=author&query=Arnold%2C+M">Maximilian Arnold</a>, 
<a href="/search/eess?searchtype=author&query=Hirzallah%2C+M">Mohammed Hirzallah</a>, 
<a href="/search/eess?searchtype=author&query=Amiri%2C+R">Roohollah Amiri</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+D+S+M">Deepak Singh Mahendar Singh</a>, 
<a href="/search/eess?searchtype=author&query=Yerramalli%2C+S">Srinivas Yerramalli</a>, 
<a href="/search/eess?searchtype=author&query=Dijkman%2C+D">Daniel Dijkman</a>, 
<a href="/search/eess?searchtype=author&query=Porikli%2C+F">Fatih Porikli</a>, 
<a href="/search/eess?searchtype=author&query=Yoo%2C+T">Taesang Yoo</a>, 
<a href="/search/eess?searchtype=author&query=Major%2C+B">Bence Major</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Radio signals are well suited for user localization because they are
ubiquitous, can operate in the dark and maintain privacy. Many prior works
learn mappings between channel state information (CSI) and position
fully-supervised. However, that approach relies on position labels which are
very expensive to acquire. In this work, this requirement is relaxed by using
pseudo-labels during deployment, which are calculated from an inertial
measurement unit (IMU). We propose practical algorithms for IMU double
integration and training of the localization system. We show decimeter-level
accuracy on simulated and challenging real data of 5G measurements. Our
IMU-supervised method performs similarly to fully-supervised, but requires much
less effort to deploy.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09975" title="Abstract">arXiv:2402.09975</a> (cross-list from eess.IV) [<a href="/pdf/2402.09975" title="Download PDF">pdf</a>, <a href="/ps/2402.09975" title="Download PostScript">ps</a>, <a href="/format/2402.09975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Current and future roles of artificial intelligence in retinopathy of  prematurity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jafarizadeh%2C+A">Ali Jafarizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Maleki%2C+S+F">Shadi Farabi Maleki</a>, 
<a href="/search/eess?searchtype=author&query=Pouya%2C+P">Parnia Pouya</a>, 
<a href="/search/eess?searchtype=author&query=Sobhi%2C+N">Navid Sobhi</a>, 
<a href="/search/eess?searchtype=author&query=Abdollahi%2C+M">Mirsaeed Abdollahi</a>, 
<a href="/search/eess?searchtype=author&query=Pedrammehr%2C+S">Siamak Pedrammehr</a>, 
<a href="/search/eess?searchtype=author&query=Lim%2C+C+P">Chee Peng Lim</a>, 
<a href="/search/eess?searchtype=author&query=Asadi%2C+H">Houshyar Asadi</a>, 
<a href="/search/eess?searchtype=author&query=Alizadehsani%2C+R">Roohallah Alizadehsani</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+R">Ru-San Tan</a>, 
<a href="/search/eess?searchtype=author&query=Islam%2C+S+M+S">Sheikh Mohammad Shariful Islam</a>, 
<a href="/search/eess?searchtype=author&query=Acharya%2C+U+R">U. Rajendra Acharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures, 2 tables, 235 references, 1 supplementary table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Retinopathy of prematurity (ROP) is a severe condition affecting premature
infants, leading to abnormal retinal blood vessel growth, retinal detachment,
and potential blindness. While semi-automated systems have been used in the
past to diagnose ROP-related plus disease by quantifying retinal vessel
features, traditional machine learning (ML) models face challenges like
accuracy and overfitting. Recent advancements in deep learning (DL), especially
convolutional neural networks (CNNs), have significantly improved ROP detection
and classification. The i-ROP deep learning (i-ROP-DL) system also shows
promise in detecting plus disease, offering reliable ROP diagnosis potential.
This research comprehensively examines the contemporary progress and challenges
associated with using retinal imaging and artificial intelligence (AI) to
detect ROP, offering valuable insights that can guide further investigation in
this domain. Based on 89 original studies in this field (out of 1487 studies
that were comprehensively reviewed), we concluded that traditional methods for
ROP diagnosis suffer from subjectivity and manual analysis, leading to
inconsistent clinical decisions. AI holds great promise for improving ROP
management. This review explores AI's potential in ROP detection,
classification, diagnosis, and prognosis.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09978" title="Abstract">arXiv:2402.09978</a> (cross-list from physics.app-ph) [<a href="/pdf/2402.09978" title="Download PDF">pdf</a>, <a href="/ps/2402.09978" title="Download PostScript">ps</a>, <a href="/format/2402.09978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning for the design of non-Hermitian topolectrical circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/physics?searchtype=author&query=Sun%2C+J">Jinyang Sun</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+X">Xiumei Wang</a>, 
<a href="/search/physics?searchtype=author&query=Jiang%2C+H">Hengxuan Jiang</a>, 
<a href="/search/physics?searchtype=author&query=Zhu%2C+D">Dandan Zhu</a>, 
<a href="/search/physics?searchtype=author&query=Zhou%2C+X">Xingping Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Non-Hermitian topological phases can produce some remarkable properties,
compared with their Hermitian counterpart, such as the breakdown of
conventional bulk-boundary correspondence and the non-Hermitian topological
edge mode. Here, we introduce several algorithms with multi-layer perceptron
(MLP), and convolutional neural network (CNN) in the field of deep learning, to
predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we
use the smallest module of the periodic circuit as one unit to construct
high-dimensional circuit data features. Further, we use the Dense Convolutional
Network (DenseNet), a type of convolutional neural network that utilizes dense
connections between layers to design a non-Hermitian topolectrical Chern
circuit, as the DenseNet algorithm is more suitable for processing
high-dimensional data. Our results demonstrate the effectiveness of the deep
learning network in capturing the global topological characteristics of a
non-Hermitian system based on training data.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09983" title="Abstract">arXiv:2402.09983</a> (cross-list from math.OC) [<a href="/pdf/2402.09983" title="Download PDF">pdf</a>, <a href="/format/2402.09983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistix: modular optimisation in JAX and Equinox
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rader%2C+J">Jason Rader</a>, 
<a href="/search/math?searchtype=author&query=Lyons%2C+T">Terry Lyons</a>, 
<a href="/search/math?searchtype=author&query=Kidger%2C+P">Patrick Kidger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">We introduce Optimistix: a nonlinear optimisation library built in JAX and
Equinox. Optimistix introduces a novel, modular approach for its minimisers and
least-squares solvers. This modularity relies on new practical abstractions for
optimisation which we call search and descent, and which generalise classical
notions of line search, trust-region, and learning-rate algorithms. It provides
high-level APIs and solvers for minimisation, nonlinear least-squares,
root-finding, and fixed-point iteration. Optimistix is available at
https://github.com/patrick-kidger/optimistix.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10010" title="Abstract">arXiv:2402.10010</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.10010" title="Download PDF">pdf</a>, <a href="/format/2402.10010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing signal detectability in learning-based CT reconstruction with  a model observer inspired loss function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lantz%2C+M">Megan Lantz</a>, 
<a href="/search/physics?searchtype=author&query=Sidky%2C+E+Y">Emil Y. Sidky</a>, 
<a href="/search/physics?searchtype=author&query=Reiser%2C+I+S">Ingrid S. Reiser</a>, 
<a href="/search/physics?searchtype=author&query=Pan%2C+X">Xiaochuan Pan</a>, 
<a href="/search/physics?searchtype=author&query=Ongie%2C+G">Gregory Ongie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Deep neural networks used for reconstructing sparse-view CT data are
typically trained by minimizing a pixel-wise mean-squared error or similar loss
function over a set of training images. However, networks trained with such
pixel-wise losses are prone to wipe out small, low-contrast features that are
critical for screening and diagnosis. To remedy this issue, we introduce a
novel training loss inspired by the model observer framework to enhance the
detectability of weak signals in the reconstructions. We evaluate our approach
on the reconstruction of synthetic sparse-view breast CT data, and demonstrate
an improvement in signal detectability with the proposed loss.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10012" title="Abstract">arXiv:2402.10012</a> (cross-list from math.CO) [<a href="/pdf/2402.10012" title="Download PDF">pdf</a>, <a href="/format/2402.10012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Countably Colorful Hyperplane Transversal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakraborty%2C+S">Sutanoya Chakraborty</a>, 
<a href="/search/math?searchtype=author&query=Ghosh%2C+A">Arijit Ghosh</a>, 
<a href="/search/math?searchtype=author&query=Nandi%2C+S">Soumi Nandi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Let $\left\{ \mathcal{F}_{n}\right\}_{n \in \mathbb{N}}$ be an infinite
sequence of families of compact connected sets in $\mathbb{R}^{d}$. An infinite
sequence of compact connected sets $\left\{ B_{n} \right\}_{n\in \mathbb{N}}$
is called heterochromatic sequence from $\left\{ \mathcal{F}_{n}\right\}_{n \in
\mathbb{N}}$ if there exists an infinite sequence $\left\{ i_{n} \right\}_{n\in
\mathbb{N}}$ of natural numbers satisfying the following two properties: (a)
$\{i_{n}\}_{n\in \mathbb{N}}$ is a monotonically increasing sequence, and (b)
for all $n \in \mathbb{N}$, we have $B_{n} \in \mathcal{F}_{i_n}$. We show that
if every heterochromatic sequence from $\left\{ \mathcal{F}_{n}\right\}_{n \in
\mathbb{N}}$ contains $d+1$ sets that can be pierced by a single hyperplane
then there exists a finite collection $\mathcal{H}$ of hyperplanes from
$\mathbb{R}^{d}$ that pierces all but finitely many families from $\left\{
\mathcal{F}_{n}\right\}_{n \in \mathbb{N}}$. As a direct consequence of our
result, we get that if every countable subcollection from an infinite family
$\mathcal{F}$ of compact connected sets in $\mathbb{R}^{d}$ contains $d+1$ sets
that can be pierced by a single hyperplane then $\mathcal{F}$ can be pierced by
finitely many hyperplanes. To establish the optimality of our result we show
that, for all $d \in \mathbb{N}$, there exists an infinite sequence $\left\{
\mathcal{F}_{n}\right\}_{n \in \mathbb{N}}$ of families of compact connected
sets satisfying the following two conditions: (1) for all $n \in \mathbb{N}$,
$\mathcal{F}_{n}$ is not pierceable by finitely many hyperplanes, and (2) for
any $m \in \mathbb{N}$ and every sequence $\left\{B_n\right\}_{n=m}^{\infty}$
of compact connected sets in $\mathbb{R}^d$, where $B_i\in\mathcal{F}_i$ for
all $i \geq m$, there exists a hyperplane in $\mathbb{R}^d$ that pierces at
least $d+1$ sets in the sequence.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10025" title="Abstract">arXiv:2402.10025</a> (cross-list from math.CO) [<a href="/pdf/2402.10025" title="Download PDF">pdf</a>, <a href="/format/2402.10025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An improved lower bound on the Shannon capacities of complements of odd  cycles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhu%2C+D+G">Daniel G. Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Improving a 2003 result of Bohman and Holzman, we show that for $n \geq 1$,
the Shannon capacity of the complement of the $2n+1$-cycle is at least
$(2^{r_n} + 1)^{1/r_n} = 2 + \Omega(2^{-r_n}/r_n)$, where $r_n = \exp(O((\log
n)^2))$ is the number of partitions of $2(n-1)$ into powers of $2$.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10026" title="Abstract">arXiv:2402.10026</a> (cross-list from eess.IV) [<a href="/pdf/2402.10026" title="Download PDF">pdf</a>, <a href="/format/2402.10026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sahoo%2C+A+R">Alok Ranjan Sahoo</a>, 
<a href="/search/eess?searchtype=author&query=Chakraborty%2C+P">Pavan Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Hyper spectral images have drawn the attention of the researchers for its
complexity to classify. It has nonlinear relation between the materials and the
spectral information provided by the HSI image. Deep learning methods have
shown superiority in learning this nonlinearity in comparison to traditional
machine learning methods. Use of 3-D CNN along with 2-D CNN have shown great
success for learning spatial and spectral features. However, it uses
comparatively large number of parameters. Moreover, it is not effective to
learn inter layer information. Hence, this paper proposes a neural network
combining 3-D CNN, 2-D CNN and Bi-LSTM. The performance of this model has been
tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data
sets. The results are compared with the state of-the-art deep learning-based
models. This model performed better in all three datasets. It could achieve
99.83, 99.98 and 100 percent accuracy using only 30 percent trainable
parameters of the state-of-art model in IP, PU and SA datasets respectively.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10043" title="Abstract">arXiv:2402.10043</a> (cross-list from stat.ML) [<a href="/pdf/2402.10043" title="Download PDF">pdf</a>, <a href="/format/2402.10043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to validate average calibration for machine learning regression  tasks ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pernot%2C+P">Pascal Pernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Average calibration of the uncertainties of machine learning regression tasks
can be tested in two ways. One way is to estimate the calibration error (CE) as
the difference between the mean absolute error (MSE) and the mean variance (MV)
or mean squared uncertainty. The alternative is to compare the mean squared
z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different
conclusion, as illustrated on an ensemble of datasets from the recent machine
learning uncertainty quantification literature. It is shown here that the CE is
very sensitive to the distribution of uncertainties, and notably to the
presence of outlying uncertainties, and that it cannot be used reliably for
calibration testing. By contrast, the ZMS statistic does not present this
sensitivity issue and offers the most reliable approach in this context.
Implications for the validation of conditional calibration are discussed.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10055" title="Abstract">arXiv:2402.10055</a> (cross-list from eess.IV) [<a href="/pdf/2402.10055" title="Download PDF">pdf</a>, <a href="/ps/2402.10055" title="Download PostScript">ps</a>, <a href="/format/2402.10055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust semi-automatic vessel tracing in the human retinal image by an  instance segmentation neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+S">Siyi Chen</a>, 
<a href="/search/eess?searchtype=author&query=Kashani%2C+A+H">Amir H. Kashani</a>, 
<a href="/search/eess?searchtype=author&query=Yi%2C+J">Ji Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The morphology and hierarchy of the vascular systems are essential for
perfusion in supporting metabolism. In human retina, one of the most
energy-demanding organs, retinal circulation nourishes the entire inner retina
by an intricate vasculature emerging and remerging at the optic nerve head
(ONH). Thus, tracing the vascular branching from ONH through the vascular tree
can illustrate vascular hierarchy and allow detailed morphological
quantification, and yet remains a challenging task. Here, we presented a novel
approach for a robust semi-automatic vessel tracing algorithm on human fundus
images by an instance segmentation neural network (InSegNN). Distinct from
semantic segmentation, InSegNN separates and labels different vascular trees
individually and therefore enable tracing each tree throughout its branching.
We have built-in three strategies to improve robustness and accuracy with
temporal learning, spatial multi-sampling, and dynamic probability map. We
achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD)
compared to literature, and outperformed baseline U-net. We have demonstrated
tracing individual vessel trees from fundus images, and simultaneously retain
the vessel hierarchy information. InSegNN paves a way for any subsequent
morphological analysis of vascular morphology in relation to retinal diseases.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10060" title="Abstract">arXiv:2402.10060</a> (cross-list from quant-ph) [<a href="/pdf/2402.10060" title="Download PDF">pdf</a>, <a href="/format/2402.10060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Backtracking in Qrisp Applied to Sudoku Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Seidel%2C+R">Raphael Seidel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zander%2C+R">Ren&#xe9; Zander</a>, 
<a href="/search/quant-ph?searchtype=author&query=Petri%C4%8D%2C+M">Matic Petri&#x10d;</a>, 
<a href="/search/quant-ph?searchtype=author&query=Steinmann%2C+N">Niklas Steinmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+D+Q">David Q. Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tcholtchev%2C+N">Nikolay Tcholtchev</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hauswirth%2C+M">Manfred Hauswirth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Programming Languages (cs.PL)

</div>
<p class="mathjax">The quantum backtracking algorithm proposed by Ashley Montanaro raised
considerable interest, as it provides a quantum speed-up for a large class of
classical optimization algorithms. It does not suffer from Barren-Plateaus and
transfers well into the fault-tolerant era, as it requires only a limited
number of arbitrary angle gates. Despite its potential, the algorithm has seen
limited implementation efforts, presumably due to its abstract formulation. In
this work, we provide a detailed instruction on implementing the quantum step
operator for arbitrary backtracking instances. For a single controlled diffuser
of a binary backtracking tree with depth n, our implementation requires only
$6n+14$ CX gates. We detail the process of constructing accept and reject
oracles for Sudoku problems using our interface to quantum backtracking. The
presented code is written using Qrisp, a high-level quantum programming
language, making it executable on most current physical backends and
simulators. Subsequently, we perform several simulator based experiments and
demonstrate solving 4x4 Sudoku instances with up to 9 empty fields. This is, to
the best of our knowledge, the first instance of a compilable implementation of
this generality, marking a significant and exciting step forward in quantum
software engineering.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10071" title="Abstract">arXiv:2402.10071</a> (cross-list from eess.SP) [<a href="/pdf/2402.10071" title="Download PDF">pdf</a>, <a href="/format/2402.10071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhuang%2C+W">Wenhao Zhuang</a>, 
<a href="/search/eess?searchtype=author&query=Mao%2C+Y">Yuyi Mao</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+H">Hengtao He</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+Y">Yao Ge</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+Z">Zhi Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, and 3 tables. Part of this article was submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Orthogonal time frequency space (OTFS) modulation has emerged as a promising
solution to support high-mobility wireless communications, for which,
cost-effective data detectors are critical. Although graph neural network
(GNN)-based data detectors can achieve decent detection accuracy at reasonable
computation cost, they fail to best harness prior information of transmitted
data. To further minimize the data detection error of OTFS systems, this letter
develops an AMP-GNN-based detector, leveraging the approximate message passing
(AMP) algorithm to iteratively improve the symbol estimates of a GNN. Given the
inter-Doppler interference (IDI) symbols incur substantial computational
overhead to the constructed GNN, learning-based IDI approximation is
implemented to sustain low detection complexity. Simulation results demonstrate
a remarkable bit error rate (BER) performance achieved by the proposed
AMP-GNN-based detector compared to existing baselines. Meanwhile, the proposed
IDI approximation scheme avoids a large amount of computations with negligible
BER degradation.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10124" title="Abstract">arXiv:2402.10124</a> (cross-list from math.OC) [<a href="/pdf/2402.10124" title="Download PDF">pdf</a>, <a href="/format/2402.10124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Blob Method for Mean Field Control With Terminal Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Craig%2C+K">Katy Craig</a>, 
<a href="/search/math?searchtype=author&query=Elamvazhuthi%2C+K">Karthik Elamvazhuthi</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+H">Harlin Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In the present work, we develop a novel particle method for a general class
of mean field control problems, with source and terminal constraints. Specific
examples of the problems we consider include the dynamic formulation of the
p-Wasserstein metric, optimal transport around an obstacle, and measure
transport subject to acceleration controls. Unlike existing numerical
approaches, our particle method is meshfree and does not require global
knowledge of an underlying cost function or of the terminal constraint. A key
feature of our approach is a novel way of enforcing the terminal constraint via
a soft, nonlocal approximation, inspired by recent work on blob methods for
diffusion equations. We prove convergence of our particle approximation to
solutions of the continuum mean-field control problem in the sense of
Gamma-convergence. A byproduct of our result is an extension of existing
discrete-to-continuum convergence results for mean field control problems to
more general state and measure costs, as arise when modeling transport around
obstacles, and more general constraint sets, including controllable linear time
invariant systems. Finally, we conclude by implementing our method numerically
and using it to compute solutions the example problems discussed above. We
conduct a detailed numerical investigation of the convergence properties of our
method, as well as its behavior in sampling applications and for approximation
of optimal transport maps.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10127" title="Abstract">arXiv:2402.10127</a> (cross-list from stat.ML) [<a href="/pdf/2402.10127" title="Download PDF">pdf</a>, <a href="/format/2402.10127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear spiked covariance matrices and signal propagation in deep  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+D">Denny Wu</a>, 
<a href="/search/stat?searchtype=author&query=Fan%2C+Z">Zhou Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">Many recent works have studied the eigenvalue spectrum of the Conjugate
Kernel (CK) defined by the nonlinear feature map of a feedforward neural
network. However, existing results only establish weak convergence of the
empirical eigenvalue distribution, and fall short of providing precise
quantitative characterizations of the ''spike'' eigenvalues and eigenvectors
that often capture the low-dimensional signal structure of the learning
problem. In this work, we characterize these signal eigenvalues and
eigenvectors for a nonlinear version of the spiked covariance model, including
the CK as a special case. Using this general result, we give a quantitative
description of how spiked eigenstructure in the input data propagates through
the hidden layers of a neural network with random weights. As a second
application, we study a simple regime of representation learning where the
weight matrix develops a rank-one signal component over training and
characterize the alignment of the target function with the spike eigenvector of
the CK on test data.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10157" title="Abstract">arXiv:2402.10157</a> (cross-list from math.OC) [<a href="/pdf/2402.10157" title="Download PDF">pdf</a>, <a href="/ps/2402.10157" title="Download PostScript">ps</a>, <a href="/format/2402.10157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Stochastic Realization Theory using Functional It&#xf4; Calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Veeravalli%2C+T">Tanya Veeravalli</a>, 
<a href="/search/math?searchtype=author&query=Raginsky%2C+M">Maxim Raginsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages; submitted to MTNS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Probability (math.PR)

</div>
<p class="mathjax">This paper considers the problem of constructing finite-dimensional state
space realizations for stochastic processes that can be represented as the
outputs of a certain type of a causal system driven by a continuous
semimartingale input process. The main assumption is that the output process is
infinitely differentiable, where the notion of differentiability comes from the
functional It\^o calculus introduced by Dupire as a causal (nonanticipative)
counterpart to Malliavin's stochastic calculus of variations. The proposed
approach builds on the ideas of Hijab, who had considered the case of processes
driven by a Brownian motion, and makes contact with the realization theory of
deterministic systems based on formal power series and Chen-Fliess functional
expansions.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10164" title="Abstract">arXiv:2402.10164</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2402.10164" title="Download PDF">pdf</a>, <a href="/format/2402.10164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random features and polynomial rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Aguirre-L%C3%B3pez%2C+F">Fabi&#xe1;n Aguirre-L&#xf3;pez</a>, 
<a href="/search/cond-mat?searchtype=author&query=Franz%2C+S">Silvio Franz</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pastore%2C+M">Mauro Pastore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages + appendix, 4 figures. Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Random features models play a distinguished role in the theory of deep
learning, describing the behavior of neural networks close to their
infinite-width limit. In this work, we present a thorough analysis of the
generalization performance of random features models for generic supervised
learning problems with Gaussian data. Our approach, built with tools from the
statistical mechanics of disordered systems, maps the random features model to
an equivalent polynomial model, and allows us to plot average generalization
curves as functions of the two main control parameters of the problem: the
number of random features $N$ and the size $P$ of the training set, both
assumed to scale as powers in the input dimension $D$. Our results extend the
case of proportional scaling between $N$, $P$ and $D$. They are in accordance
with rigorous bounds known for certain particular learning tasks and are in
quantitative agreement with numerical experiments performed over many order of
magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic
limits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remains
finite.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10188" title="Abstract">arXiv:2402.10188</a> (cross-list from quant-ph) [<a href="/pdf/2402.10188" title="Download PDF">pdf</a>, <a href="/format/2402.10188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trainability Barriers in Low-Depth QAOA Landscapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Rajakumar%2C+J">Joel Rajakumar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Golden%2C+J">John Golden</a>, 
<a href="/search/quant-ph?searchtype=author&query=B%C3%A4rtschi%2C+A">Andreas B&#xe4;rtschi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eidenbenz%2C+S">Stephan Eidenbenz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 21st ACM International Conference on Computing Frontiers CF'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The Quantum Alternating Operator Ansatz (QAOA) is a prominent variational
quantum algorithm for solving combinatorial optimization problems. Its
effectiveness depends on identifying input parameters that yield high-quality
solutions. However, understanding the complexity of training QAOA remains an
under-explored area. Previous results have given analytical performance
guarantees for a small, fixed number of parameters. At the opposite end of the
spectrum, barren plateaus are likely to emerge at $\Omega(n)$ parameters for
$n$ qubits. In this work, we study the difficulty of training in the
intermediate regime, which is the focus of most current numerical studies and
near-term hardware implementations. Through extensive numerical analysis of the
quality and quantity of local minima, we argue that QAOA landscapes can exhibit
a superpolynomial growth in the number of low-quality local minima even when
the number of parameters scales logarithmically with $n$. This means that the
common technique of gradient descent from randomly initialized parameters is
doomed to fail beyond small $n$, and emphasizes the need for good initial
guesses of the optimal parameters.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10204" title="Abstract">arXiv:2402.10204</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.10204" title="Download PDF">pdf</a>, <a href="/format/2402.10204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radio-astronomical Image Reconstruction with Conditional Denoising  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Drozdova%2C+M">Mariia Drozdova</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kinakh%2C+V">Vitaliy Kinakh</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bait%2C+O">Omkar Bait</a>, 
<a href="/search/astro-ph?searchtype=author&query=Taran%2C+O">Olga Taran</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lastufka%2C+E">Erica Lastufka</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dessauges-Zavadsky%2C+M">Miroslava Dessauges-Zavadsky</a>, 
<a href="/search/astro-ph?searchtype=author&query=Holotyak%2C+T">Taras Holotyak</a>, 
<a href="/search/astro-ph?searchtype=author&query=Schaerer%2C+D">Daniel Schaerer</a>, 
<a href="/search/astro-ph?searchtype=author&query=Voloshynovskiy%2C+S">Slava Voloshynovskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In production in Astronomy&amp;Astrophyics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reconstructing sky models from dirty radio images for accurate source
localization and flux estimation is crucial for studying galaxy evolution at
high redshift, especially in deep fields using instruments like the Atacama
Large Millimetre Array (ALMA). With new projects like the Square Kilometre
Array (SKA), there's a growing need for better source extraction methods.
Current techniques, such as CLEAN and PyBDSF, often fail to detect faint
sources, highlighting the need for more accurate methods. This study proposes
using stochastic neural networks to rebuild sky models directly from dirty
images. This method can pinpoint radio sources and measure their fluxes with
related uncertainties, marking a potential improvement in radio source
characterization. We tested this approach on 10164 images simulated with the
CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied
conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models
reconstruction, then used Photutils to determine source coordinates and fluxes,
assessing the model's performance across different water vapor levels. Our
method showed excellence in source localization, achieving more than 90%
completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed
PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in
the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional
DDPMs is a powerful tool for image-to-image translation, yielding accurate and
robust characterisation of radio sources, and outperforming existing
methodologies. While this study underscores its significant potential for
applications in radio astronomy, we also acknowledge certain limitations that
accompany its usage, suggesting directions for further refinement and research.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 16 Feb 24</h3>
<dl>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1311.4566" title="Abstract">arXiv:1311.4566</a> (replaced) [<a href="/pdf/1311.4566" title="Download PDF">pdf</a>, <a href="/ps/1311.4566" title="Download PostScript">ps</a>, <a href="/format/1311.4566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond journals and peer review: towards a more flexible ecosystem for  scholarly communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wood%2C+M">Michael Wood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages. This is another revision of the original version posted in 2013 with a different title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.10699" title="Abstract">arXiv:2003.10699</a> (replaced) [<a href="/pdf/2003.10699" title="Download PDF">pdf</a>, <a href="/format/2003.10699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Human Memory Processes to Model Genre Preferences for  Personalized Music Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowald%2C+D">Dominik Kowald</a>, 
<a href="/search/cs?searchtype=author&query=Lex%2C+E">Elisabeth Lex</a>, 
<a href="/search/cs?searchtype=author&query=Schedl%2C+M">Markus Schedl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dominik Kowald and Elisabeth Lex contributed equally to this work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> HUMANIZE Workshop @ IUI'2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.07796" title="Abstract">arXiv:2006.07796</a> (replaced) [<a href="/pdf/2006.07796" title="Download PDF">pdf</a>, <a href="/format/2006.07796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure by Architecture: Structured Representations without  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leeb%2C+F">Felix Leeb</a>, 
<a href="/search/cs?searchtype=author&query=Lanzillotta%2C+G">Guilia Lanzillotta</a>, 
<a href="/search/cs?searchtype=author&query=Annadani%2C+Y">Yashas Annadani</a>, 
<a href="/search/cs?searchtype=author&query=Besserve%2C+M">Michel Besserve</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Stefan Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.04217" title="Abstract">arXiv:2102.04217</a> (replaced) [<a href="/pdf/2102.04217" title="Download PDF">pdf</a>, <a href="/format/2102.04217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Underground Incentivized Review Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oak%2C+R">Rajvardhan Oak</a>, 
<a href="/search/cs?searchtype=author&query=Shafiq%2C+Z">Zubair Shafiq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted for presentation at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11798" title="Abstract">arXiv:2106.11798</a> (replaced) [<a href="/pdf/2106.11798" title="Download PDF">pdf</a>, <a href="/ps/2106.11798" title="Download PostScript">ps</a>, <a href="/format/2106.11798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The failure of cut-elimination in cyclic proof for first-order logic  with inductive definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oda%2C+Y">Yukihiro Oda</a>, 
<a href="/search/cs?searchtype=author&query=Brotherston%2C+J">James Brotherston</a>, 
<a href="/search/cs?searchtype=author&query=Tatsuta%2C+M">Makoto Tatsuta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Logic and Computation, 2023;, exad068
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.02126" title="Abstract">arXiv:2108.02126</a> (replaced) [<a href="/pdf/2108.02126" title="Download PDF">pdf</a>, <a href="/format/2108.02126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Will Have Order! Optimizing Orders for Fair Reviewer Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Payan%2C+J">Justin Payan</a>, 
<a href="/search/cs?searchtype=author&query=Zick%2C+Y">Yair Zick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 8 figures, extends initial version published at IJCAI 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.07125" title="Abstract">arXiv:2109.07125</a> (replaced) [<a href="/pdf/2109.07125" title="Download PDF">pdf</a>, <a href="/ps/2109.07125" title="Download PostScript">ps</a>, <a href="/format/2109.07125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosability of labeled $\mathfrak{D_p}$ automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kuize Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Raisch%2C+J">Joerg Raisch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.02817" title="Abstract">arXiv:2112.02817</a> (replaced) [<a href="/pdf/2112.02817" title="Download PDF">pdf</a>, <a href="/format/2112.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ED2: Environment Dynamics Decomposition World Models for Continuous  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08295" title="Abstract">arXiv:2201.08295</a> (replaced) [<a href="/pdf/2201.08295" title="Download PDF">pdf</a>, <a href="/format/2201.08295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIVA-DAF: A Deep Learning Framework for Historical Document Image  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%C3%B6gtlin%2C+L">Lars V&#xf6;gtlin</a>, 
<a href="/search/cs?searchtype=author&query=Scius-Bertrand%2C+A">Anna Scius-Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Maergner%2C+P">Paul Maergner</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Andreas Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Ingold%2C+R">Rolf Ingold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.01456" title="Abstract">arXiv:2202.01456</a> (replaced) [<a href="/pdf/2202.01456" title="Download PDF">pdf</a>, <a href="/format/2202.01456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and explainable clustering based on sorting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinye Chen</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCttel%2C+S">Stefan G&#xfc;ttel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.05097" title="Abstract">arXiv:2203.05097</a> (replaced) [<a href="/pdf/2203.05097" title="Download PDF">pdf</a>, <a href="/ps/2203.05097" title="Download PostScript">ps</a>, <a href="/format/2203.05097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for the Interoperability of Cloud Platforms: Towards FAIR  Data in SAFE Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grossman%2C+R+L">Robert L. Grossman</a>, 
<a href="/search/cs?searchtype=author&query=Boyles%2C+R+R">Rebecca R. Boyles</a>, 
<a href="/search/cs?searchtype=author&query=Davis-Dusenbery%2C+B+N">Brandi N. Davis-Dusenbery</a>, 
<a href="/search/cs?searchtype=author&query=Haddock%2C+A">Amanda Haddock</a>, 
<a href="/search/cs?searchtype=author&query=Heath%2C+A+P">Allison P. Heath</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+B+D">Brian D. O&#x27;Connor</a>, 
<a href="/search/cs?searchtype=author&query=Resnick%2C+A+C">Adam C. Resnick</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+D+M">Deanne M. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Ahalt%2C+S">Stan Ahalt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages with 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.09092" title="Abstract">arXiv:2204.09092</a> (replaced) [<a href="/pdf/2204.09092" title="Download PDF">pdf</a>, <a href="/format/2204.09092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indiscriminate Data Poisoning Attacks on Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yiwei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaoliang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TMLR in 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01553" title="Abstract">arXiv:2206.01553</a> (replaced) [<a href="/pdf/2206.01553" title="Download PDF">pdf</a>, <a href="/format/2206.01553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting hyperbolic geometry in networks: why triangles are not enough
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Michielan%2C+R">Riccardo Michielan</a>, 
<a href="/search/physics?searchtype=author&query=Litvak%2C+N">Nelly Litvak</a>, 
<a href="/search/physics?searchtype=author&query=Stegehuis%2C+C">Clara Stegehuis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. E 106, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06944" title="Abstract">arXiv:2207.06944</a> (replaced) [<a href="/pdf/2207.06944" title="Download PDF">pdf</a>, <a href="/ps/2207.06944" title="Download PostScript">ps</a>, <a href="/format/2207.06944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Graph Learning via Sensitivity-Bounded  Personalized PageRank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epasto%2C+A">Alessandro Epasto</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Perozzi%2C+B">Bryan Perozzi</a>, 
<a href="/search/cs?searchtype=author&query=Tsitsulin%2C+A">Anton Tsitsulin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.00963" title="Abstract">arXiv:2208.00963</a> (replaced) [<a href="/pdf/2208.00963" title="Download PDF">pdf</a>, <a href="/format/2208.00963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FrOoDo: Framework for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stieber%2C+J">Jonathan Stieber</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+M">Moritz Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+A">Anirban Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02200" title="Abstract">arXiv:2209.02200</a> (replaced) [<a href="/pdf/2209.02200" title="Download PDF">pdf</a>, <a href="/format/2209.02200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection  in Aerial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhanchao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiang-Gen Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Ran Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 13 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03910" title="Abstract">arXiv:2209.03910</a> (replaced) [<a href="/pdf/2209.03910" title="Download PDF">pdf</a>, <a href="/format/2209.03910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and  Feature-metric Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chidananda%2C+P">Prajwal Chidananda</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+S">Saurabh Nair</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Douglas Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kaehler%2C+A">Adrian Kaehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02042" title="Abstract">arXiv:2210.02042</a> (replaced) [<a href="/pdf/2210.02042" title="Download PDF">pdf</a>, <a href="/format/2210.02042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedMT: Federated Learning with Mixed-type Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jing Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Talhouk%2C+A">Aline Talhouk</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05920" title="Abstract">arXiv:2211.05920</a> (replaced) [<a href="/pdf/2211.05920" title="Download PDF">pdf</a>, <a href="/format/2211.05920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Less is More: On the Value of &quot;Co-training&quot; for Semi-Supervised  Software Defect Predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumder%2C+S">Suvodeep Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+J">Joymallya Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Menzies%2C+T">Tim Menzies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 10 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09022" title="Abstract">arXiv:2211.09022</a> (replaced) [<a href="/pdf/2211.09022" title="Download PDF">pdf</a>, <a href="/format/2211.09022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Efficient Object Detection via Region Proposal Network  Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+N">Nanqing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ericsson%2C+L">Linus Ericsson</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Leonardis%2C+A">Ales Leonardis</a>, 
<a href="/search/cs?searchtype=author&query=McDonagh%2C+S">Steven McDonagh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neurocomputing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14115" title="Abstract">arXiv:2211.14115</a> (replaced) [<a href="/pdf/2211.14115" title="Download PDF">pdf</a>, <a href="/format/2211.14115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Feasibility in Over-the-Air Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Piotrowski%2C+T">Tomasz Piotrowski</a>, 
<a href="/search/stat?searchtype=author&query=Ismayilov%2C+R">Rafail Ismayilov</a>, 
<a href="/search/stat?searchtype=author&query=Frey%2C+M">Matthias Frey</a>, 
<a href="/search/stat?searchtype=author&query=Cavalcante%2C+R+L+G">Renato L.G. Cavalcante</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14307" title="Abstract">arXiv:2211.14307</a> (replaced) [<a href="/pdf/2211.14307" title="Download PDF">pdf</a>, <a href="/format/2211.14307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAEDAY: MAE for few and zero shot AnomalY-Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+E">Eli Schwartz</a>, 
<a href="/search/cs?searchtype=author&query=Arbelle%2C+A">Assaf Arbelle</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Harary%2C+S">Sivan Harary</a>, 
<a href="/search/cs?searchtype=author&query=Scheidegger%2C+F">Florian Scheidegger</a>, 
<a href="/search/cs?searchtype=author&query=Doveh%2C+S">Sivan Doveh</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Computer Vision and Image Understanding, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16234" title="Abstract">arXiv:2211.16234</a> (replaced) [<a href="/pdf/2211.16234" title="Download PDF">pdf</a>, <a href="/format/2211.16234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimCS: Simulation for Domain Incremental Online Continual Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alfarra%2C+M">Motasem Alfarra</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhipeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Matthias M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI Conference on Artificial Intelligence (AAAI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05728" title="Abstract">arXiv:2212.05728</a> (replaced) [<a href="/pdf/2212.05728" title="Download PDF">pdf</a>, <a href="/format/2212.05728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergy and Redundancy Dominated Effects in Time Series via Transfer  Entropy Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%98stergaard%2C+J">Jan &#xd8;stergaard</a>, 
<a href="/search/cs?searchtype=author&query=Boubakani%2C+P">Payam Boubakani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NeurIT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00462" title="Abstract">arXiv:2301.00462</a> (replaced) [<a href="/pdf/2301.00462" title="Download PDF">pdf</a>, <a href="/format/2301.00462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Latent Space Correlation-Aware Autoencoder for Anomaly Detection in  Skewed Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+P">Padmaksha Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02133" title="Abstract">arXiv:2301.02133</a> (replaced) [<a href="/pdf/2301.02133" title="Download PDF">pdf</a>, <a href="/ps/2301.02133" title="Download PostScript">ps</a>, <a href="/format/2301.02133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on highly connected $K_{2,\ell}$-minor free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/math?searchtype=author&query=Pierron%2C+T">Th&#xe9;o Pierron</a>, 
<a href="/search/math?searchtype=author&query=Wesolek%2C+A">Alexandra Wesolek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04943" title="Abstract">arXiv:2301.04943</a> (replaced) [<a href="/pdf/2301.04943" title="Download PDF">pdf</a>, <a href="/format/2301.04943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Nonlinear Optimal Control via System Level Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leeman%2C+A+P">Antoine P. Leeman</a>, 
<a href="/search/math?searchtype=author&query=K%C3%B6hler%2C+J">Johannes K&#xf6;hler</a>, 
<a href="/search/math?searchtype=author&query=Zanelli%2C+A">Andrea Zanelli</a>, 
<a href="/search/math?searchtype=author&query=Bennani%2C+S">Samir Bennani</a>, 
<a href="/search/math?searchtype=author&query=Zeilinger%2C+M+N">Melanie N. Zeilinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE Transactions on Automatic Control (TAC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03811" title="Abstract">arXiv:2302.03811</a> (replaced) [<a href="/pdf/2302.03811" title="Download PDF">pdf</a>, <a href="/format/2302.03811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of Modified Policy Iteration in Risk Sensitive  Exponential Cost Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murthy%2C+Y">Yashaswini Murthy</a>, 
<a href="/search/cs?searchtype=author&query=Moharrami%2C+M">Mehrdad Moharrami</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 3 figures, Under review at Operations Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04685" title="Abstract">arXiv:2302.04685</a> (replaced) [<a href="/pdf/2302.04685" title="Download PDF">pdf</a>, <a href="/format/2302.04685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategies as Resource Terms, and their Categorical Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blondeau-Patissier%2C+L">Lison Blondeau-Patissier</a>, 
<a href="/search/cs?searchtype=author&query=Clairambault%2C+P">Pierre Clairambault</a>, 
<a href="/search/cs?searchtype=author&query=Auclair%2C+L+V">Lionel Vaux Auclair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> extended version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04944" title="Abstract">arXiv:2302.04944</a> (replaced) [<a href="/pdf/2302.04944" title="Download PDF">pdf</a>, <a href="/format/2302.04944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fosong%2C+E">Elliot Fosong</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Arrasy Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Carlucho%2C+I">Ignacio Carlucho</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07998" title="Abstract">arXiv:2302.07998</a> (replaced) [<a href="/pdf/2302.07998" title="Download PDF">pdf</a>, <a href="/ps/2302.07998" title="Download PostScript">ps</a>, <a href="/format/2302.07998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced  Human Activity Recognition in Therapeutic Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadzadeh%2C+M">Mohammad Mohammadzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ghadami%2C+A">Ali Ghadami</a>, 
<a href="/search/cs?searchtype=author&query=Taheri%2C+A">Alireza Taheri</a>, 
<a href="/search/cs?searchtype=author&query=Behzadipour%2C+S">Saeed Behzadipour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08012" title="Abstract">arXiv:2302.08012</a> (replaced) [<a href="/pdf/2302.08012" title="Download PDF">pdf</a>, <a href="/format/2302.08012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equilibrium of Data Markets with Externality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10561" title="Abstract">arXiv:2302.10561</a> (replaced) [<a href="/pdf/2302.10561" title="Download PDF">pdf</a>, <a href="/ps/2302.10561" title="Download PostScript">ps</a>, <a href="/format/2302.10561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-free Optimization and Experimental Validation of RIS-assisted  Wireless Communications under Rich Multipath Fading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+M">Minglei You</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yangyishi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Gan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gros%2C+J+B">Jean Baptiste Gros</a>, 
<a href="/search/cs?searchtype=author&query=Lerosey%2C+G">Geoffroy Lerosey</a>, 
<a href="/search/cs?searchtype=author&query=Nasser%2C+Y">Youssef Nasser</a>, 
<a href="/search/cs?searchtype=author&query=Burton%2C+F">Fraser Burton</a>, 
<a href="/search/cs?searchtype=author&query=Gradoni%2C+G">Gabriele Gradoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Wireless Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10682" title="Abstract">arXiv:2302.10682</a> (replaced) [<a href="/pdf/2302.10682" title="Download PDF">pdf</a>, <a href="/format/2302.10682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation of Splines in Wasserstein Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Justiniano%2C+J">Jorge Justiniano</a>, 
<a href="/search/math?searchtype=author&query=Rumpf%2C+M">Martin Rumpf</a>, 
<a href="/search/math?searchtype=author&query=Erbar%2C+M">Matthias Erbar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01440" title="Abstract">arXiv:2303.01440</a> (replaced) [<a href="/pdf/2303.01440" title="Download PDF">pdf</a>, <a href="/format/2303.01440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programmatic Imitation Learning from Unlabeled and Noisy Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jimmy Xin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Linus Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+K">Kia Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiayi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Holtz%2C+J">Jarrett Holtz</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">Isil Dillig</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03751" title="Abstract">arXiv:2303.03751</a> (replaced) [<a href="/pdf/2303.03751" title="Download PDF">pdf</a>, <a href="/format/2303.03751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zeroth-Order Optimization Meets Human Feedback: Provable Learning via  Ranking Oracles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhiwei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Rybin%2C+D">Dmitry Rybin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+T">Tsung-Hui Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10611" title="Abstract">arXiv:2303.10611</a> (replaced) [<a href="/pdf/2303.10611" title="Download PDF">pdf</a>, <a href="/format/2303.10611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Dual-Domain Undersampled MRI reconstruction: domain-specific  design from the perspective of the receptive field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gao%2C+Z">Ziqi Gao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+S+K">S. Kevin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 IEEE International Symposium on Biomedical Imaging (ISBI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12167" title="Abstract">arXiv:2303.12167</a> (replaced) [<a href="/pdf/2303.12167" title="Download PDF">pdf</a>, <a href="/format/2303.12167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-descent hardware-aware training and deployment for mixed-signal  Neuromorphic processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87akal%2C+U">U&#x11f;urcan &#xc7;akal</a>, 
<a href="/search/cs?searchtype=author&query=Maryada">Maryada</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ulusoy%2C+I">Ilkay Ulusoy</a>, 
<a href="/search/cs?searchtype=author&query=Muir%2C+D+R">Dylan R. Muir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14086" title="Abstract">arXiv:2303.14086</a> (replaced) [<a href="/pdf/2303.14086" title="Download PDF">pdf</a>, <a href="/format/2303.14086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Field Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qi-yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiang-xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shu Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17255" title="Abstract">arXiv:2303.17255</a> (replaced) [<a href="/pdf/2303.17255" title="Download PDF">pdf</a>, <a href="/format/2303.17255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fooling the Image Dehazing Models by First Order Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+J">Jie Gui</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xiaofeng Cong</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chengwei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y+Y">Yuan Yan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James Tin-Yau Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00223" title="Abstract">arXiv:2304.00223</a> (replaced) [<a href="/pdf/2304.00223" title="Download PDF">pdf</a>, <a href="/ps/2304.00223" title="Download PostScript">ps</a>, <a href="/format/2304.00223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Non-Centered Non-Separable Channels and Their  Application in Holographic MIMO Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00891" title="Abstract">arXiv:2304.00891</a> (replaced) [<a href="/pdf/2304.00891" title="Download PDF">pdf</a>, <a href="/ps/2304.00891" title="Download PostScript">ps</a>, <a href="/format/2304.00891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Algorithms for Hierarchical Inference in Deep Learning  applications at the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moothedath%2C+V+N">Vishnu Narayanan Moothedath</a>, 
<a href="/search/cs?searchtype=author&query=Champati%2C+J+P">Jaya Prakash Champati</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+J">James Gross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The original version was submitted to a journal and was later revised. The updated version was accepted in a journal and will be published soon. The 'Journal reference' will be updated as and when the information is available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04315" title="Abstract">arXiv:2304.04315</a> (replaced) [<a href="/pdf/2304.04315" title="Download PDF">pdf</a>, <a href="/format/2304.04315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microseismic source imaging using physics-informed neural networks with  hard constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Huang%2C+X">Xinquan Huang</a>, 
<a href="/search/physics?searchtype=author&query=Alkhalifah%2C+T">Tariq Alkhalifah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Geoscience and Remote Sensing 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10813" title="Abstract">arXiv:2304.10813</a> (replaced) [<a href="/pdf/2304.10813" title="Download PDF">pdf</a>, <a href="/format/2304.10813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tokenization Preference for Human and ML Model: An Annotation Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiraoka%2C+T">Tatsuya Hiraoka</a>, 
<a href="/search/cs?searchtype=author&query=Iwakura%2C+T">Tomoya Iwakura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11005" title="Abstract">arXiv:2304.11005</a> (replaced) [<a href="/pdf/2304.11005" title="Download PDF">pdf</a>, <a href="/format/2304.11005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Correcting Bayesian Optimization through Bayesian Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E">Erik Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th International Conference on Neural Information Processing
  Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13343" title="Abstract">arXiv:2304.13343</a> (replaced) [<a href="/pdf/2304.13343" title="Download PDF">pdf</a>, <a href="/format/2304.13343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Model with Self-Controlled Memory Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinnian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuangzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under preview
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00568" title="Abstract">arXiv:2305.00568</a> (replaced) [<a href="/pdf/2305.00568" title="Download PDF">pdf</a>, <a href="/format/2305.00568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete quadratic model QUBO solution landscapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zaborniak%2C+T">Tristan Zaborniak</a>, 
<a href="/search/quant-ph?searchtype=author&query=Stege%2C+U">Ulrike Stege</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 table, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07358" title="Abstract">arXiv:2305.07358</a> (replaced) [<a href="/pdf/2305.07358" title="Download PDF">pdf</a>, <a href="/format/2305.07358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Versatile and Efficient Visual Knowledge Integration into  Pre-trained Language Models with Cross-Modal Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haochen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10978" title="Abstract">arXiv:2305.10978</a> (replaced) [<a href="/pdf/2305.10978" title="Download PDF">pdf</a>, <a href="/format/2305.10978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Client Selection for Federated Policy Optimization with Environment  Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhijie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S+H">S.H. Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11549" title="Abstract">arXiv:2305.11549</a> (replaced) [<a href="/pdf/2305.11549" title="Download PDF">pdf</a>, <a href="/ps/2305.11549" title="Download PostScript">ps</a>, <a href="/format/2305.11549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Filtering and Source Coding in Distributed Wireless Monitoring  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agheli%2C+P">Pouya Agheli</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Kountouris%2C+M">Marios Kountouris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published in IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12520" title="Abstract">arXiv:2305.12520</a> (replaced) [<a href="/pdf/2305.12520" title="Download PDF">pdf</a>, <a href="/format/2305.12520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armengol-Estap%C3%A9%2C+J">Jordi Armengol-Estap&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+J">Jackson Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Cummins%2C+C">Chris Cummins</a>, 
<a href="/search/cs?searchtype=author&query=O%27Boyle%2C+M+F+P">Michael F.P. O&#x27;Boyle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12716" title="Abstract">arXiv:2305.12716</a> (replaced) [<a href="/pdf/2305.12716" title="Download PDF">pdf</a>, <a href="/format/2305.12716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The CLIP Model is Secretly an Image-to-Prompt Converter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuxuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chunna Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Haoxuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingqiao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, 21 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13036" title="Abstract">arXiv:2305.13036</a> (replaced) [<a href="/pdf/2305.13036" title="Download PDF">pdf</a>, <a href="/format/2305.13036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Structured Components: Towards Adaptive, Interpretable and  Scalable Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jinliang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Renhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Du Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I+W">Ivor W. Tsang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13915" title="Abstract">arXiv:2305.13915</a> (replaced) [<a href="/pdf/2305.13915" title="Download PDF">pdf</a>, <a href="/format/2305.13915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAPR: A Benchmark on Document-Aware Passage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Reimers%2C+N">Nils Reimers</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15244" title="Abstract">arXiv:2305.15244</a> (replaced) [<a href="/pdf/2305.15244" title="Download PDF">pdf</a>, <a href="/format/2305.15244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Lyapunov and Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Layeghi%2C+D">Daniel Layeghi</a>, 
<a href="/search/cs?searchtype=author&query=Tonneau%2C+S">Steve Tonneau</a>, 
<a href="/search/cs?searchtype=author&query=Mistry%2C+M">Michael Mistry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15508" title="Abstract">arXiv:2305.15508</a> (replaced) [<a href="/pdf/2305.15508" title="Download PDF">pdf</a>, <a href="/format/2305.15508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods  for Selective Classification with Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattelan%2C+L+F+P">Lu&#xed;s Felipe P. Cattelan</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+D">Danilo Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16794" title="Abstract">arXiv:2305.16794</a> (replaced) [<a href="/pdf/2305.16794" title="Download PDF">pdf</a>, <a href="/format/2305.16794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Vertical Federated Learning Under Unreliable Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xinchi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Heng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wanru Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W+F">William F. Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gusmao%2C+P+P+B">Pedro P.B. Gusmao</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Generalised extension from our previous work: <a href="/abs/2305.11236">arXiv:2305.11236</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17000" title="Abstract">arXiv:2305.17000</a> (replaced) [<a href="/pdf/2305.17000" title="Download PDF">pdf</a>, <a href="/format/2305.17000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistriBlock: Identifying adversarial audio samples by leveraging  characteristics of the output distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B.%2C+M+P+P">Mat&#xed;as P. Pizarro B.</a>, 
<a href="/search/cs?searchtype=author&query=Kolossa%2C+D">Dorothea Kolossa</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Asja Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18354" title="Abstract">arXiv:2305.18354</a> (replaced) [<a href="/pdf/2305.18354" title="Download PDF">pdf</a>, <a href="/format/2305.18354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and  the Importance of Object-based Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yudong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Vaezipoor%2C+P">Pashootan Vaezipoor</a>, 
<a href="/search/cs?searchtype=author&query=Sanner%2C+S">Scott Sanner</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+E+B">Elias B. Khalil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 15 figures, published in Transactions on Machine Learning Research (TMLR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18829" title="Abstract">arXiv:2305.18829</a> (replaced) [<a href="/pdf/2305.18829" title="Download PDF">pdf</a>, <a href="/format/2305.18829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+C">Chen Min</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dawei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yiming Nie</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bin Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by RAL2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01711" title="Abstract">arXiv:2306.01711</a> (replaced) [<a href="/pdf/2306.01711" title="Download PDF">pdf</a>, <a href="/format/2306.01711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OMNI: Open-endedness via Models of human Notions of Interestingness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+K">Kenneth Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 33 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02285" title="Abstract">arXiv:2306.02285</a> (replaced) [<a href="/pdf/2306.02285" title="Download PDF">pdf</a>, <a href="/format/2306.02285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clarify Confused Nodes via Separated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiajun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chenxuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shanqing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Q">Qi Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoniu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02552" title="Abstract">arXiv:2306.02552</a> (replaced) [<a href="/pdf/2306.02552" title="Download PDF">pdf</a>, <a href="/format/2306.02552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Behavior Simulation with Large Language Model based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingsen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiakai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ruihua Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02939" title="Abstract">arXiv:2306.02939</a> (replaced) [<a href="/pdf/2306.02939" title="Download PDF">pdf</a>, <a href="/format/2306.02939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Stability and Generalization Guarantees of the Decentralized  SGD Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bars%2C+B+L">Batiste Le Bars</a>, 
<a href="/search/cs?searchtype=author&query=Bellet%2C+A">Aur&#xe9;lien Bellet</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+M">Marc Tommasi</a>, 
<a href="/search/cs?searchtype=author&query=Scaman%2C+K">Kevin Scaman</a>, 
<a href="/search/cs?searchtype=author&query=Neglia%2C+G">Giovanni Neglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03117" title="Abstract">arXiv:2306.03117</a> (replaced) [<a href="/pdf/2306.03117" title="Download PDF">pdf</a>, <a href="/format/2306.03117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Str2Str: A Score-based Framework for Zero-shot Protein Conformation  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhong%2C+B">Bozitao Zhong</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024, see <a href="https://openreview.net/forum?id=C4BikKsgmK">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04689" title="Abstract">arXiv:2306.04689</a> (replaced) [<a href="/pdf/2306.04689" title="Download PDF">pdf</a>, <a href="/format/2306.04689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Flow for Robust and Optimal Cosmological Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Dai%2C+B">Biwei Dai</a>, 
<a href="/search/astro-ph?searchtype=author&query=Seljak%2C+U">Uros Seljak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05221" title="Abstract">arXiv:2306.05221</a> (replaced) [<a href="/pdf/2306.05221" title="Download PDF">pdf</a>, <a href="/format/2306.05221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steering No-Regret Learners to Optimal Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Cacciamani%2C+F">Federico Cacciamani</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S+M">Stephen Marcus McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Haupt%2C+A+A">Andreas Alexander Haupt</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Conitzer%2C+V">Vincent Conitzer</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05816" title="Abstract">arXiv:2306.05816</a> (replaced) [<a href="/pdf/2306.05816" title="Download PDF">pdf</a>, <a href="/format/2306.05816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Phishing Sites Using ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koide%2C+T">Takashi Koide</a>, 
<a href="/search/cs?searchtype=author&query=Fukushi%2C+N">Naoki Fukushi</a>, 
<a href="/search/cs?searchtype=author&query=Nakano%2C+H">Hiroki Nakano</a>, 
<a href="/search/cs?searchtype=author&query=Chiba%2C+D">Daiki Chiba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09101" title="Abstract">arXiv:2306.09101</a> (replaced) [<a href="/pdf/2306.09101" title="Download PDF">pdf</a>, <a href="/format/2306.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-aided Wireless Image Transmission with Channel Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haotian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ozfatura%2C+E">Emre Ozfatura</a>, 
<a href="/search/cs?searchtype=author&query=Mikolajczyk%2C+K">Krystian Mikolajczyk</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09739" title="Abstract">arXiv:2306.09739</a> (replaced) [<a href="/pdf/2306.09739" title="Download PDF">pdf</a>, <a href="/format/2306.09739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilized Neural Differential Equations for Learning Dynamics with  Explicit Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=White%2C+A">Alistair White</a>, 
<a href="/search/cs?searchtype=author&query=Kilbertus%2C+N">Niki Kilbertus</a>, 
<a href="/search/cs?searchtype=author&query=Gelbrecht%2C+M">Maximilian Gelbrecht</a>, 
<a href="/search/cs?searchtype=author&query=Boers%2C+N">Niklas Boers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures. Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14291" title="Abstract">arXiv:2306.14291</a> (replaced) [<a href="/pdf/2306.14291" title="Download PDF">pdf</a>, <a href="/format/2306.14291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic  Distance Enhances Open World Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doan%2C+T">Thang Doan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Behpour%2C+S">Sima Behpour</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenbin He</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+L">Liang Gou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Liu Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024 || keywords: Open World Object Detection, Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical Representation Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16958" title="Abstract">arXiv:2306.16958</a> (replaced) [<a href="/pdf/2306.16958" title="Download PDF">pdf</a>, <a href="/ps/2306.16958" title="Download PostScript">ps</a>, <a href="/format/2306.16958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiability of Direct Effects from Summary Causal Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+S">Simon Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Assaad%2C+C+K">Charles K. Assaad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00493" title="Abstract">arXiv:2307.00493</a> (replaced) [<a href="/pdf/2307.00493" title="Download PDF">pdf</a>, <a href="/ps/2307.00493" title="Download PostScript">ps</a>, <a href="/format/2307.00493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence  Time-Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+T">Nhat Thanh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jack Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages (main), 2 pages (appendix), 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02591" title="Abstract">arXiv:2307.02591</a> (replaced) [<a href="/pdf/2307.02591" title="Download PDF">pdf</a>, <a href="/format/2307.02591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant  Behavior Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sunjae Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weisong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Druhl%2C+E">Emily Druhl</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+M+L">Minhee L. Sung</a>, 
<a href="/search/cs?searchtype=author&query=Reisman%2C+J+I">Joel I. Reisman</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjun Li</a>, 
<a href="/search/cs?searchtype=author&query=Kerns%2C+R+D">Robert D. Kerns</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+W">William Becker</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03577" title="Abstract">arXiv:2307.03577</a> (replaced) [<a href="/pdf/2307.03577" title="Download PDF">pdf</a>, <a href="/format/2307.03577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CuTS: Customizable Tabular Synthetic Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vero%2C+M">Mark Vero</a>, 
<a href="/search/cs?searchtype=author&query=Balunovi%C4%87%2C+M">Mislav Balunovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04421" title="Abstract">arXiv:2307.04421</a> (replaced) [<a href="/pdf/2307.04421" title="Download PDF">pdf</a>, <a href="/format/2307.04421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using  Deep Computational Models for Inverse Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/eess?searchtype=author&query=Camps%2C+J">Julia Camps</a>, 
<a href="/search/eess?searchtype=author&query=Zhinuo">Zhinuo</a> (Jenny)
<a href="/search/eess?searchtype=author&query=Wang">Wang</a>, 
<a href="/search/eess?searchtype=author&query=Banerjee%2C+A">Abhirup Banerjee</a>, 
<a href="/search/eess?searchtype=author&query=Beetz%2C+M">Marcel Beetz</a>, 
<a href="/search/eess?searchtype=author&query=Rodriguez%2C+B">Blanca Rodriguez</a>, 
<a href="/search/eess?searchtype=author&query=Grau%2C+V">Vicente Grau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Cardiac digital twins; Inverse inference; Myocardial infarction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06777" title="Abstract">arXiv:2307.06777</a> (replaced) [<a href="/pdf/2307.06777" title="Download PDF">pdf</a>, <a href="/format/2307.06777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciding Conjugacy of a Rational Relation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aiswarya%2C+C">C. Aiswarya</a>, 
<a href="/search/cs?searchtype=author&query=Manuel%2C+A">Amaldev Manuel</a>, 
<a href="/search/cs?searchtype=author&query=Sunny%2C+S">Saina Sunny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07462" title="Abstract">arXiv:2307.07462</a> (replaced) [<a href="/pdf/2307.07462" title="Download PDF">pdf</a>, <a href="/format/2307.07462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Zigzag Vineyard Efficiently Including Expansions and  Contractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+T+K">Tamal K. Dey</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+T">Tao Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Algebraic Topology (math.AT)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09758" title="Abstract">arXiv:2307.09758</a> (replaced) [<a href="/pdf/2307.09758" title="Download PDF">pdf</a>, <a href="/format/2307.09758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray  Report Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nicolson%2C+A">Aaron Nicolson</a>, 
<a href="/search/cs?searchtype=author&query=Dowling%2C+J">Jason Dowling</a>, 
<a href="/search/cs?searchtype=author&query=Koopman%2C+B">Bevan Koopman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10187" title="Abstract">arXiv:2307.10187</a> (replaced) [<a href="/pdf/2307.10187" title="Download PDF">pdf</a>, <a href="/format/2307.10187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Privacy Amplification via Importance Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fay%2C+D">Dominik Fay</a>, 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11114" title="Abstract">arXiv:2307.11114</a> (replaced) [<a href="/pdf/2307.11114" title="Download PDF">pdf</a>, <a href="/ps/2307.11114" title="Download PostScript">ps</a>, <a href="/format/2307.11114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Nature of Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=You%2C+B+J">Barco Jie You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13912" title="Abstract">arXiv:2307.13912</a> (replaced) [<a href="/pdf/2307.13912" title="Download PDF">pdf</a>, <a href="/format/2307.13912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Democratic Values into Social Media AIs via Societal Objective  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chenyan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+S">Michelle S. Lam</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+M+C">Minh Chau Mai</a>, 
<a href="/search/cs?searchtype=author&query=Hancock%2C+J">Jeff Hancock</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+M+S">Michael S. Bernstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted to CSCW 2024 and will be published in Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 163 (April 2024)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the ACM: Human-Computer Interaction, 8, CSCW1,
  Article 163 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15154" title="Abstract">arXiv:2307.15154</a> (replaced) [<a href="/pdf/2307.15154" title="Download PDF">pdf</a>, <a href="/format/2307.15154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A/B Testing and Best-arm Identification for Linear Bandits with  Robustness to Non-stationarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhihan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Camilleri%2C+R">Romain Camilleri</a>, 
<a href="/search/cs?searchtype=author&query=Fazel%2C+M">Maryam Fazel</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03306" title="Abstract">arXiv:2308.03306</a> (replaced) [<a href="/pdf/2308.03306" title="Download PDF">pdf</a>, <a href="/format/2308.03306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Graph Neural Diffusion Networks: Convergence, Generalization,  and Over-Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+G">Guoji Fu</a>, 
<a href="/search/cs?searchtype=author&query=Dupty%2C+M+H">Mohammed Haroon Dupty</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yanfei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L+W">Lee Wee Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03784" title="Abstract">arXiv:2308.03784</a> (replaced) [<a href="/pdf/2308.03784" title="Download PDF">pdf</a>, <a href="/format/2308.03784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Requirements Completeness: Automated Assistance through Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luitel%2C+D">Dipeeka Luitel</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+S">Shabnam Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Sabetzadeh%2C+M">Mehrdad Sabetzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted at the Requirements Engineering Journal (REJ), REFSQ'23 Special Issue. arXiv admin note: text overlap with <a href="/abs/2302.04792">arXiv:2302.04792</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03958" title="Abstract">arXiv:2308.03958</a> (replaced) [<a href="/pdf/2308.03958" title="Download PDF">pdf</a>, <a href="/format/2308.03958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple synthetic data reduces sycophancy in large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jerry Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Da Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifeng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q+V">Quoc V. Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04251" title="Abstract">arXiv:2308.04251</a> (replaced) [<a href="/pdf/2308.04251" title="Download PDF">pdf</a>, <a href="/format/2308.04251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Node-Averaged Complexity of Locally Checkable Problems on Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balliu%2C+A">Alkida Balliu</a>, 
<a href="/search/cs?searchtype=author&query=Brandt%2C+S">Sebastian Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+F">Fabian Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Olivetti%2C+D">Dennis Olivetti</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+G">Gustav Schmid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 68 pages, 10 figures, conference version in 37th International Symposium on Distributed Computing (DISC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04970" title="Abstract">arXiv:2308.04970</a> (replaced) [<a href="/e-print/2308.04970" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitting Concentric Elliptical Shapes Under General Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Al-Sharadqah%2C+A">Ali Al-Sharadqah</a>, 
<a href="/search/stat?searchtype=author&query=Piga%2C+G">Giulano Piga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> it has a serious mistake that needs major revision and it could take several months to fix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06686" title="Abstract">arXiv:2308.06686</a> (replaced) [<a href="/pdf/2308.06686" title="Download PDF">pdf</a>, <a href="/format/2308.06686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchQL: A Programming Framework for Integrity Constraints in Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aaditya Naik</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+A">Adam Stein</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yinjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+M">Mayur Naik</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09565" title="Abstract">arXiv:2308.09565</a> (replaced) [<a href="/pdf/2308.09565" title="Download PDF">pdf</a>, <a href="/format/2308.09565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Role of Layer Normalization in Label-Skewed Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Beitollahi%2C+M">Mahdi Beitollahi</a>, 
<a href="/search/cs?searchtype=author&query=Bie%2C+A">Alex Bie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10044" title="Abstract">arXiv:2308.10044</a> (replaced) [<a href="/pdf/2308.10044" title="Download PDF">pdf</a>, <a href="/format/2308.10044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple counting rule to evaluate directionality of arbitrary rail  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akita%2C+D">Dai Akita</a>, 
<a href="/search/math?searchtype=author&query=Schenz%2C+D+T">Daniel Thorsten Schenz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12969" title="Abstract">arXiv:2308.12969</a> (replaced) [<a href="/pdf/2308.12969" title="Download PDF">pdf</a>, <a href="/format/2308.12969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROAM: Robust and Object-Aware Motion Generation Using Neural Pose  Descriptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wanyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dabral%2C+R">Rishabh Dabral</a>, 
<a href="/search/cs?searchtype=author&query=Leimk%C3%BChler%2C+T">Thomas Leimk&#xfc;hler</a>, 
<a href="/search/cs?searchtype=author&query=Golyanik%2C+V">Vladislav Golyanik</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+M">Marc Habermann</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures; project page: <a href="https://vcai.mpi-inf.mpg.de/projects/ROAM/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on 3D Vision 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13498" title="Abstract">arXiv:2308.13498</a> (replaced) [<a href="/pdf/2308.13498" title="Download PDF">pdf</a>, <a href="/format/2308.13498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Epistemic Uncertainty Estimation in Regression Ensemble Models  Using Pairwise-Distance Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berry%2C+L">Lucas Berry</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14642" title="Abstract">arXiv:2308.14642</a> (replaced) [<a href="/pdf/2308.14642" title="Download PDF">pdf</a>, <a href="/ps/2308.14642" title="Download PostScript">ps</a>, <a href="/format/2308.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Optimal Policy Optimization for Linear Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sherman%2C+U">Uri Sherman</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Alon Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Koren%2C+T">Tomer Koren</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+Y">Yishay Mansour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16017" title="Abstract">arXiv:2308.16017</a> (replaced) [<a href="/pdf/2308.16017" title="Download PDF">pdf</a>, <a href="/ps/2308.16017" title="Download PostScript">ps</a>, <a href="/format/2308.16017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden-Role Games: Equilibrium Concepts and Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carminati%2C+L">Luca Carminati</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00557" title="Abstract">arXiv:2309.00557</a> (replaced) [<a href="/pdf/2309.00557" title="Download PDF">pdf</a>, <a href="/format/2309.00557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concentrated Differential Privacy for Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Azize%2C+A">Achraf Azize</a>, 
<a href="/search/stat?searchtype=author&query=Basu%2C+D">Debabrota Basu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in IEEE SaTML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03447" title="Abstract">arXiv:2309.03447</a> (replaced) [<a href="/pdf/2309.03447" title="Download PDF">pdf</a>, <a href="/format/2309.03447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Broadband Ground Motion Synthesis via Generative Adversarial Neural  Operators: Development and Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Shi%2C+Y">Yaozhong Shi</a>, 
<a href="/search/physics?searchtype=author&query=Lavrentiadis%2C+G">Grigorios Lavrentiadis</a>, 
<a href="/search/physics?searchtype=author&query=Asimaki%2C+D">Domniki Asimaki</a>, 
<a href="/search/physics?searchtype=author&query=Ross%2C+Z+E">Zachary E. Ross</a>, 
<a href="/search/physics?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06651" title="Abstract">arXiv:2309.06651</a> (replaced) [<a href="/pdf/2309.06651" title="Download PDF">pdf</a>, <a href="/format/2309.06651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConR: Contrastive Regularizer for Deep Imbalanced Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keramati%2C+M">Mahsa Keramati</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lili Meng</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+R+D">R. David Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06706" title="Abstract">arXiv:2309.06706</a> (replaced) [<a href="/pdf/2309.06706" title="Download PDF">pdf</a>, <a href="/format/2309.06706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Machine Translation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Shiri%2C+F">Fatemeh Shiri</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08607" title="Abstract">arXiv:2309.08607</a> (replaced) [<a href="/pdf/2309.08607" title="Download PDF">pdf</a>, <a href="/format/2309.08607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitoring of Urban Changes with multi-modal Sentinel 1 and 2 Data in  Mariupol, Ukraine, in 2022/23
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zitzlsberger%2C+G">Georg Zitzlsberger</a>, 
<a href="/search/cs?searchtype=author&query=Podhoranyi%2C+M">Michal Podhoranyi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09505" title="Abstract">arXiv:2309.09505</a> (replaced) [<a href="/pdf/2309.09505" title="Download PDF">pdf</a>, <a href="/format/2309.09505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier-Insensitive Kalman Filtering: Theory and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Truzman%2C+S">Shunit Truzman</a>, 
<a href="/search/eess?searchtype=author&query=Revach%2C+G">Guy Revach</a>, 
<a href="/search/eess?searchtype=author&query=Shlezinger%2C+N">Nir Shlezinger</a>, 
<a href="/search/eess?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09599" title="Abstract">arXiv:2309.09599</a> (replaced) [<a href="/pdf/2309.09599" title="Download PDF">pdf</a>, <a href="/format/2309.09599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential  Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paat%2C+H">Helbert Paat</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Q">Qing Lian</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weilong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICRA 2024. Code: <a href="https://github.com/paathelb/MEDL-U">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10652" title="Abstract">arXiv:2309.10652</a> (replaced) [<a href="/pdf/2309.10652" title="Download PDF">pdf</a>, <a href="/format/2309.10652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear dynamic analysis of shear- and torsion-free rods using  isogeometric discretization, outlier removal and robust time integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thi-Hoa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Roccia%2C+B+A">Bruno A. Roccia</a>, 
<a href="/search/cs?searchtype=author&query=Hiemstra%2C+R+R">Ren&#xe9; R. Hiemstra</a>, 
<a href="/search/cs?searchtype=author&query=Gebhardt%2C+C+G">Cristian G. Gebhardt</a>, 
<a href="/search/cs?searchtype=author&query=Schillinger%2C+D">Dominik Schillinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11040" title="Abstract">arXiv:2309.11040</a> (replaced) [<a href="/pdf/2309.11040" title="Download PDF">pdf</a>, <a href="/format/2309.11040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stein Variational Guided Model Predictive Path Integral Control:  Proposal and Experiments with Fast Maneuvering Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Honda%2C+K">Kohei Honda</a>, 
<a href="/search/cs?searchtype=author&query=Akai%2C+N">Naoki Akai</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+K">Kosuke Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Aoki%2C+M">Mizuho Aoki</a>, 
<a href="/search/cs?searchtype=author&query=Hosogaya%2C+H">Hirotaka Hosogaya</a>, 
<a href="/search/cs?searchtype=author&query=Okuda%2C+H">Hiroyuki Okuda</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Tatsuya Suzuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14053" title="Abstract">arXiv:2309.14053</a> (replaced) [<a href="/pdf/2309.14053" title="Download PDF">pdf</a>, <a href="/format/2309.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting LARS for Large Batch Training Generalization of Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Khoi Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hoa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran-Thanh%2C+L">Long Tran-Thanh</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Nguyen-Hoang Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quoc-Viet Pham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16391" title="Abstract">arXiv:2309.16391</a> (replaced) [<a href="/pdf/2309.16391" title="Download PDF">pdf</a>, <a href="/format/2309.16391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2-Cats: 2D Copula Approximating Transforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Figueiredo%2C+F">Flavio Figueiredo</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+J+G">Jos&#xe9; Geraldo Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J">Jackson Silva</a>, 
<a href="/search/cs?searchtype=author&query=Assun%C3%A7%C3%A3o%2C+R+M">Renato M. Assun&#xe7;&#xe3;o</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00027" title="Abstract">arXiv:2310.00027</a> (replaced) [<a href="/pdf/2310.00027" title="Download PDF">pdf</a>, <a href="/ps/2310.00027" title="Download PostScript">ps</a>, <a href="/format/2310.00027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-Of-Domain Unlabeled Data Improves Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saberi%2C+A+H">Amir Hossein Saberi</a>, 
<a href="/search/stat?searchtype=author&query=Najafi%2C+A">Amir Najafi</a>, 
<a href="/search/stat?searchtype=author&query=Heidari%2C+A">Alireza Heidari</a>, 
<a href="/search/stat?searchtype=author&query=Movasaghinia%2C+M+H">Mohammad Hosein Movasaghinia</a>, 
<a href="/search/stat?searchtype=author&query=Motahari%2C+A">Abolfazl Motahari</a>, 
<a href="/search/stat?searchtype=author&query=Khalaj%2C+B+H">Babak H. Khalaj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024 (Spotlight), 29 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00172" title="Abstract">arXiv:2310.00172</a> (replaced) [<a href="/pdf/2310.00172" title="Download PDF">pdf</a>, <a href="/format/2310.00172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A physics-informed deep learning approach for solving strongly  degenerate parabolic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ambrosio%2C+P">Pasquale Ambrosio</a>, 
<a href="/search/math?searchtype=author&query=Cuomo%2C+S">Salvatore Cuomo</a>, 
<a href="/search/math?searchtype=author&query=De+Rosa%2C+M">Mariapia De Rosa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version improves the previous one
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00301" title="Abstract">arXiv:2310.00301</a> (replaced) [<a href="/pdf/2310.00301" title="Download PDF">pdf</a>, <a href="/format/2310.00301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Hierarchical Environment Design via Generative Trajectory  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dexun Li</a>, 
<a href="/search/cs?searchtype=author&query=Varakantham%2C+P">Pradeep Varakantham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00938" title="Abstract">arXiv:2310.00938</a> (replaced) [<a href="/pdf/2310.00938" title="Download PDF">pdf</a>, <a href="/ps/2310.00938" title="Download PostScript">ps</a>, <a href="/format/2310.00938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An FPRAS for two terminal reliability in directed acyclic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weiming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Heng Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages. v3: improved presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01362" title="Abstract">arXiv:2310.01362</a> (replaced) [<a href="/pdf/2310.01362" title="Download PDF">pdf</a>, <a href="/format/2310.01362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fleet Learning via Policy Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See the code <a href="https://github.com/liruiw/Fleet-Tools">this https URL</a> for more details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02700" title="Abstract">arXiv:2310.02700</a> (replaced) [<a href="/pdf/2310.02700" title="Download PDF">pdf</a>, <a href="/format/2310.02700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights of using Control Theory for minimizing Induced Seismicity in  Underground Reservoirs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gutierrez-Oribio%2C+D">Diego Gutierrez-Oribio</a>, 
<a href="/search/eess?searchtype=author&query=Stefanou%2C+I">Ioannis Stefanou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03224" title="Abstract">arXiv:2310.03224</a> (replaced) [<a href="/pdf/2310.03224" title="Download PDF">pdf</a>, <a href="/ps/2310.03224" title="Download PostScript">ps</a>, <a href="/format/2310.03224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Completion from One-Bit Dither Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eamaz%2C+A">Arian Eamaz</a>, 
<a href="/search/cs?searchtype=author&query=Yeganegi%2C+F">Farhang Yeganegi</a>, 
<a href="/search/cs?searchtype=author&query=Soltanalian%2C+M">Mojtaba Soltanalian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05264" title="Abstract">arXiv:2310.05264</a> (replaced) [<a href="/pdf/2310.05264" title="Download PDF">pdf</a>, <a href="/format/2310.05264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Emergence of Reproducibility and Consistency in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huijie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinfan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minzhe Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Liyue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06379" title="Abstract">arXiv:2310.06379</a> (replaced) [<a href="/pdf/2310.06379" title="Download PDF">pdf</a>, <a href="/format/2310.06379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialization Bias of Fourier Neural Operator: Revisiting the Edge of  Chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koshizuka%2C+T">Takeshi Koshizuka</a>, 
<a href="/search/cs?searchtype=author&query=Fujisawa%2C+M">Masahiro Fujisawa</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+Y">Yusuke Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+I">Issei Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08598" title="Abstract">arXiv:2310.08598</a> (replaced) [<a href="/pdf/2310.08598" title="Download PDF">pdf</a>, <a href="/format/2310.08598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization for Medical Image Analysis: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yoon%2C+J+S">Jee Seok Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Oh%2C+K">Kwanseok Oh</a>, 
<a href="/search/eess?searchtype=author&query=Shin%2C+Y">Yooseung Shin</a>, 
<a href="/search/eess?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>, 
<a href="/search/eess?searchtype=author&query=Suk%2C+H">Heung-Il Suk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09298" title="Abstract">arXiv:2310.09298</a> (replaced) [<a href="/pdf/2310.09298" title="Download PDF">pdf</a>, <a href="/format/2310.09298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency  for Grayscale Image-based Network Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+I">Irfan Khan</a>, 
<a href="/search/cs?searchtype=author&query=Farrukh%2C+Y+A">Yasir Ali Farrukh</a>, 
<a href="/search/cs?searchtype=author&query=Wali%2C+S">Syed Wali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11787" title="Abstract">arXiv:2310.11787</a> (replaced) [<a href="/pdf/2310.11787" title="Download PDF">pdf</a>, <a href="/format/2310.11787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroCUT: A Neural Approach for Robust Graph Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+R">Rishi Shah</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Krishnanshu Jain</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sahil Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Medya%2C+S">Sourav Medya</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12595" title="Abstract">arXiv:2310.12595</a> (replaced) [<a href="/pdf/2310.12595" title="Download PDF">pdf</a>, <a href="/format/2310.12595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning With Hierarchical Models Based on Similarity of Causal  Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wharrie%2C+S">Sophie Wharrie</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13230" title="Abstract">arXiv:2310.13230</a> (replaced) [<a href="/pdf/2310.13230" title="Download PDF">pdf</a>, <a href="/format/2310.13230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Absolute Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weiye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Feihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianhao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changliu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submission to ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16955" title="Abstract">arXiv:2310.16955</a> (replaced) [<a href="/pdf/2310.16955" title="Download PDF">pdf</a>, <a href="/format/2310.16955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break it, Imitate it, Fix it: Robustness by Generating Human-Like  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aradhana Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Balashankar%2C+A">Ananth Balashankar</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Avrahami%2C+T">Thi Avrahami</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17838" title="Abstract">arXiv:2310.17838</a> (replaced) [<a href="/pdf/2310.17838" title="Download PDF">pdf</a>, <a href="/format/2310.17838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Animation Generation and Control on Rigged Models via Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Han Huang</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Torre%2C+F">Fernanda De La Torre</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C+M">Cathy Mengying Fang</a>, 
<a href="/search/cs?searchtype=author&query=Banburski-Fahey%2C+A">Andrzej Banburski-Fahey</a>, 
<a href="/search/cs?searchtype=author&query=Amores%2C+J">Judith Amores</a>, 
<a href="/search/cs?searchtype=author&query=Lanier%2C+J">Jaron Lanier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS Workshop on ML for Creativity and Design 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18166" title="Abstract">arXiv:2310.18166</a> (replaced) [<a href="/pdf/2310.18166" title="Download PDF">pdf</a>, <a href="/format/2310.18166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional Ownership through Fractional Uniqueness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marshall%2C+D">Daniel Marshall</a> (University of Kent), 
<a href="/search/cs?searchtype=author&query=Orchard%2C+D">Dominic Orchard</a> (University of Kent and University of Cambridge)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages + references. In submission (minor revision)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20431" title="Abstract">arXiv:2310.20431</a> (replaced) [<a href="/pdf/2310.20431" title="Download PDF">pdf</a>, <a href="/format/2310.20431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Raising the ClaSS of Streaming Time Series Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ermshaus%2C+A">Arik Ermshaus</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+P">Patrick Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Leser%2C+U">Ulf Leser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20699" title="Abstract">arXiv:2310.20699</a> (replaced) [<a href="/pdf/2310.20699" title="Download PDF">pdf</a>, <a href="/format/2310.20699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Multistate Bennett Acceptance Ratio Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ding%2C+X">Xinqiang Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01591" title="Abstract">arXiv:2311.01591</a> (replaced) [<a href="/pdf/2311.01591" title="Download PDF">pdf</a>, <a href="/format/2311.01591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Fair than Sorry: Adversarial Missing Data Imputation for Fair  GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lina%2C+D+H">Debolina Halder Lina</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Arlei Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02527" title="Abstract">arXiv:2311.02527</a> (replaced) [<a href="/pdf/2311.02527" title="Download PDF">pdf</a>, <a href="/format/2311.02527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Modeling for Soft Pneumatic Actuators via Data-Driven  Parameter Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wu-Te Yang</a>, 
<a href="/search/cs?searchtype=author&query=Stuart%2C+H">Hannah Stuart</a>, 
<a href="/search/cs?searchtype=author&query=Kurkcu%2C+B">Burak Kurkcu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05705" title="Abstract">arXiv:2311.05705</a> (replaced) [<a href="/pdf/2311.05705" title="Download PDF">pdf</a>, <a href="/format/2311.05705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Maximum Utilization in Optimal Time for Learning or  Convergence in the Kolkata Paise Restaurant Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+A">Aniruddha Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Antika Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+B+K">Bikas K. Chakrabarti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures included in manuscript; Accepted for publication in Indian Journal of Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06095" title="Abstract">arXiv:2311.06095</a> (replaced) [<a href="/pdf/2311.06095" title="Download PDF">pdf</a>, <a href="/format/2311.06095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual input stream transformer for vertical drift correction in  eye-tracking reading data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mercier%2C+T+M">Thomas M. Mercier</a>, 
<a href="/search/cs?searchtype=author&query=Budka%2C+M">Marcin Budka</a>, 
<a href="/search/cs?searchtype=author&query=Vasilev%2C+M+R">Martin R. Vasilev</a>, 
<a href="/search/cs?searchtype=author&query=Kirkby%2C+J+A">Julie A. Kirkby</a>, 
<a href="/search/cs?searchtype=author&query=Angele%2C+B">Bernhard Angele</a>, 
<a href="/search/cs?searchtype=author&query=Slattery%2C+T+J">Timothy J. Slattery</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE Transactions on pattern analysis and machine intelligence for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06432" title="Abstract">arXiv:2311.06432</a> (replaced) [<a href="/pdf/2311.06432" title="Download PDF">pdf</a>, <a href="/ps/2311.06432" title="Download PostScript">ps</a>, <a href="/format/2311.06432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Communication: When to Pull Updates?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agheli%2C+P">Pouya Agheli</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>, 
<a href="/search/cs?searchtype=author&query=Kountouris%2C+M">Marios Kountouris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be presented in the IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12356" title="Abstract">arXiv:2311.12356</a> (replaced) [<a href="/pdf/2311.12356" title="Download PDF">pdf</a>, <a href="/format/2311.12356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Linear Projections Loss for Hyperplane-Based Optimization in  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatasubramanian%2C+S">Shyam Venkatasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Aloui%2C+A">Ahmed Aloui</a>, 
<a href="/search/cs?searchtype=author&query=Tarokh%2C+V">Vahid Tarokh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12573" title="Abstract">arXiv:2311.12573</a> (replaced) [<a href="/pdf/2311.12573" title="Download PDF">pdf</a>, <a href="/format/2311.12573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moderating Model Marketplaces: Platform Governance Puzzles for AI  Intermediaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorwa%2C+R">Robert Gorwa</a>, 
<a href="/search/cs?searchtype=author&query=Veale%2C+M">Michael Veale</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> (2024) 16(2) Law Innovation and Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12900" title="Abstract">arXiv:2311.12900</a> (replaced) [<a href="/pdf/2311.12900" title="Download PDF">pdf</a>, <a href="/ps/2311.12900" title="Download PostScript">ps</a>, <a href="/format/2311.12900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing autonomous vehicle acceptance of German residents with and  without visual impairments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacperski%2C+C">Celina Kacperski</a>, 
<a href="/search/cs?searchtype=author&query=Kutzner%2C+F">Florian Kutzner</a>, 
<a href="/search/cs?searchtype=author&query=Vogel%2C+T">Tobias Vogel</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Disability and Rehabilitation: Assistive Technology (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13538" title="Abstract">arXiv:2311.13538</a> (replaced) [<a href="/pdf/2311.13538" title="Download PDF">pdf</a>, <a href="/format/2311.13538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speak Like a Native: Prompting Large Language Models in a Native Style
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13681" title="Abstract">arXiv:2311.13681</a> (replaced) [<a href="/pdf/2311.13681" title="Download PDF">pdf</a>, <a href="/format/2311.13681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compact 3D Gaussian Representation for Radiance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+C">Joo Chan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rho%2C+D">Daniel Rho</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiangyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J+H">Jong Hwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunbyung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="http://maincold2.github.io/c3dgs/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14079" title="Abstract">arXiv:2311.14079</a> (replaced) [<a href="/pdf/2311.14079" title="Download PDF">pdf</a>, <a href="/format/2311.14079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Comparison between Cross-Validation and Mutation-Validation in  Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jinyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hamdan%2C+S">Sami Hamdan</a>, 
<a href="/search/cs?searchtype=author&query=Sasse%2C+L">Leonard Sasse</a>, 
<a href="/search/cs?searchtype=author&query=Morrison%2C+A">Abigail Morrison</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+K+R">Kaustubh R. Patil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15332" title="Abstract">arXiv:2311.15332</a> (replaced) [<a href="/pdf/2311.15332" title="Download PDF">pdf</a>, <a href="/ps/2311.15332" title="Download PostScript">ps</a>, <a href="/format/2311.15332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASI: Accuracy-Stability Index for Evaluating Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Berleant%2C+D">Daniel Berleant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Big Data (BigData),
  Sorrento, Italy, 2023, pp. 4284-4289
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17052" title="Abstract">arXiv:2311.17052</a> (replaced) [<a href="/pdf/2311.17052" title="Download PDF">pdf</a>, <a href="/format/2311.17052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A large-scale particle system with independent jumps and distributed  synchronization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baryshnikov%2C+Y">Yuliy Baryshnikov</a>, 
<a href="/search/math?searchtype=author&query=Stolyar%2C+A">Alexander Stolyar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revision. 29 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17978" title="Abstract">arXiv:2311.17978</a> (replaced) [<a href="/pdf/2311.17978" title="Download PDF">pdf</a>, <a href="/ps/2311.17978" title="Download PostScript">ps</a>, <a href="/format/2311.17978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutArch: An AI-assisted workflow for object detection and automated  recording in archaeological catalogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klein%2C+K">Kevin Klein</a>, 
<a href="/search/cs?searchtype=author&query=Wohde%2C+A">Alyssa Wohde</a>, 
<a href="/search/cs?searchtype=author&query=Gorelik%2C+A+V">Alexander V. Gorelik</a>, 
<a href="/search/cs?searchtype=author&query=Heyd%2C+V">Volker Heyd</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A4mmel%2C+R">Ralf L&#xe4;mmel</a>, 
<a href="/search/cs?searchtype=author&query=Diekmann%2C+Y">Yoan Diekmann</a>, 
<a href="/search/cs?searchtype=author&query=Brami%2C+M">Maxime Brami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18107" title="Abstract">arXiv:2311.18107</a> (replaced) [<a href="/pdf/2311.18107" title="Download PDF">pdf</a>, <a href="/format/2311.18107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic-Geometrical Framework for Object Pose Estimation based on  Mixture Models Avoiding the Correspondence Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoegele%2C+W">Wolfgang Hoegele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18237" title="Abstract">arXiv:2311.18237</a> (replaced) [<a href="/pdf/2311.18237" title="Download PDF">pdf</a>, <a href="/format/2311.18237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Transfer from Vision Foundation Models for Efficient Training  of Small Task-specific Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vemulapalli%2C+R">Raviteja Vemulapalli</a>, 
<a href="/search/cs?searchtype=author&query=Pouransari%2C+H">Hadi Pouransari</a>, 
<a href="/search/cs?searchtype=author&query=Faghri%2C+F">Fartash Faghri</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Farajtabar%2C+M">Mehrdad Farajtabar</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00817" title="Abstract">arXiv:2312.00817</a> (replaced) [<a href="/pdf/2312.00817" title="Download PDF">pdf</a>, <a href="/format/2312.00817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extrapolatable Transformer Pre-training for Ultra Long Time-Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qincheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Buckeridge%2C+D+L">David L. Buckeridge</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03011" title="Abstract">arXiv:2312.03011</a> (replaced) [<a href="/pdf/2312.03011" title="Download PDF">pdf</a>, <a href="/format/2312.03011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructBooth: Instruction-following Personalized Text-to-Image  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chae%2C+D">Daewon Chae</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Nokyung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kimin Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03422" title="Abstract">arXiv:2312.03422</a> (replaced) [<a href="/pdf/2312.03422" title="Download PDF">pdf</a>, <a href="/format/2312.03422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive flexibility function in smart energy systems: A linearized  price-demand mapping approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tohidi%2C+S+S">Seyed Shahabaldin Tohidi</a>, 
<a href="/search/eess?searchtype=author&query=Madsen%2C+H">Henrik Madsen</a>, 
<a href="/search/eess?searchtype=author&query=Tsaousoglou%2C+G">Georgios Tsaousoglou</a>, 
<a href="/search/eess?searchtype=author&query=Ritschel%2C+T+K+S">Tobias K. S. Ritschel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05119" title="Abstract">arXiv:2312.05119</a> (replaced) [<a href="/pdf/2312.05119" title="Download PDF">pdf</a>, <a href="/format/2312.05119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying white matter hyperintensity and brain volumes in  heterogeneous clinical and low-field portable MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laso%2C+P">Pablo Laso</a>, 
<a href="/search/eess?searchtype=author&query=Cerri%2C+S">Stefano Cerri</a>, 
<a href="/search/eess?searchtype=author&query=Sorby-Adams%2C+A">Annabel Sorby-Adams</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+J">Jennifer Guo</a>, 
<a href="/search/eess?searchtype=author&query=Mateen%2C+F">Farrah Mateen</a>, 
<a href="/search/eess?searchtype=author&query=Goebl%2C+P">Philipp Goebl</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jiaming Wu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+P">Peirong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongwei Li</a>, 
<a href="/search/eess?searchtype=author&query=Young%2C+S+I">Sean I. Young</a>, 
<a href="/search/eess?searchtype=author&query=Billot%2C+B">Benjamin Billot</a>, 
<a href="/search/eess?searchtype=author&query=Puonti%2C+O">Oula Puonti</a>, 
<a href="/search/eess?searchtype=author&query=Sze%2C+G">Gordon Sze</a>, 
<a href="/search/eess?searchtype=author&query=Payabavash%2C+S">Sam Payabavash</a>, 
<a href="/search/eess?searchtype=author&query=DeHavenon%2C+A">Adam DeHavenon</a>, 
<a href="/search/eess?searchtype=author&query=Sheth%2C+K+N">Kevin N. Sheth</a>, 
<a href="/search/eess?searchtype=author&query=Rosen%2C+M+S">Matthew S. Rosen</a>, 
<a href="/search/eess?searchtype=author&query=Kirsch%2C+J">John Kirsch</a>, 
<a href="/search/eess?searchtype=author&query=Strisciuglio%2C+N">Nicola Strisciuglio</a>, 
<a href="/search/eess?searchtype=author&query=Wolterink%2C+J+M">Jelmer M. Wolterink</a>, 
<a href="/search/eess?searchtype=author&query=Eshaghi%2C+A">Arman Eshaghi</a>, 
<a href="/search/eess?searchtype=author&query=Barkhof%2C+F">Frederik Barkhof</a>, 
<a href="/search/eess?searchtype=author&query=Kimberly%2C+W+T">W. Taylor Kimberly</a>, 
<a href="/search/eess?searchtype=author&query=Iglesias%2C+J+E">Juan Eugenio Iglesias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06505" title="Abstract">arXiv:2312.06505</a> (replaced) [<a href="/pdf/2312.06505" title="Download PDF">pdf</a>, <a href="/format/2312.06505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounded Question-Answering in Long Egocentric Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+S">Shangzhe Di</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08367" title="Abstract">arXiv:2312.08367</a> (replaced) [<a href="/pdf/2312.08367" title="Download PDF">pdf</a>, <a href="/format/2312.08367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLAP: Efficient Video-Language Alignment via Frame Prompting and  Distilling for Video Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junbang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chun-Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kenan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yu Lou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Ming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08489" title="Abstract">arXiv:2312.08489</a> (replaced) [<a href="/pdf/2312.08489" title="Download PDF">pdf</a>, <a href="/ps/2312.08489" title="Download PostScript">ps</a>, <a href="/format/2312.08489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connectivity Oracles for Predictable Vertex Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bingbing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kosinas%2C+E">Evangelos Kosinas</a>, 
<a href="/search/cs?searchtype=author&query=Polak%2C+A">Adam Polak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08724" title="Abstract">arXiv:2312.08724</a> (replaced) [<a href="/pdf/2312.08724" title="Download PDF">pdf</a>, <a href="/format/2312.08724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Path Recourse for Reinforcement Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Dat Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08901" title="Abstract">arXiv:2312.08901</a> (replaced) [<a href="/pdf/2312.08901" title="Download PDF">pdf</a>, <a href="/format/2312.08901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xijie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+L">Li Lyna Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kwang-Ting Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10594" title="Abstract">arXiv:2312.10594</a> (replaced) [<a href="/pdf/2312.10594" title="Download PDF">pdf</a>, <a href="/ps/2312.10594" title="Download PostScript">ps</a>, <a href="/format/2312.10594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Representation and Learning: Control and Risk  Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhuoyuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Keller%2C+R">Reece Keller</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+X">Xiyu Deng</a>, 
<a href="/search/eess?searchtype=author&query=Hoshino%2C+K">Kenta Hoshino</a>, 
<a href="/search/eess?searchtype=author&query=Tanaka%2C+T">Takashi Tanaka</a>, 
<a href="/search/eess?searchtype=author&query=Nakahira%2C+Y">Yorie Nakahira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the AAAI 24 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10913" title="Abstract">arXiv:2312.10913</a> (replaced) [<a href="/pdf/2312.10913" title="Download PDF">pdf</a>, <a href="/format/2312.10913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GINN-LP: A Growing Interpretable Neural Network for Discovering  Multivariate Laurent Polynomial Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranasinghe%2C+N">Nisal Ranasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Senanayake%2C+D">Damith Senanayake</a>, 
<a href="/search/cs?searchtype=author&query=Seneviratne%2C+S">Sachith Seneviratne</a>, 
<a href="/search/cs?searchtype=author&query=Premaratne%2C+M">Malin Premaratne</a>, 
<a href="/search/cs?searchtype=author&query=Halgamuge%2C+S">Saman Halgamuge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures, Accepted by AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11242" title="Abstract">arXiv:2312.11242</a> (replaced) [<a href="/pdf/2312.11242" title="Download PDF">pdf</a>, <a href="/format/2312.11242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Changyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinnian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+L">Linzheng Chai</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian-Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Di Yin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under preview
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11477" title="Abstract">arXiv:2312.11477</a> (replaced) [<a href="/pdf/2312.11477" title="Download PDF">pdf</a>, <a href="/format/2312.11477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing relaxed configurations in elastic bodies: Mathematical  formulations and numerical methods for cardiac modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Barnafi%2C+N+A">N. A. Barnafi</a>, 
<a href="/search/physics?searchtype=author&query=Regazzoni%2C+F">F. Regazzoni</a>, 
<a href="/search/physics?searchtype=author&query=Riccobelli%2C+D">D. Riccobelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biological Physics (physics.bio-ph)</span>; Soft Condensed Matter (cond-mat.soft); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11560" title="Abstract">arXiv:2312.11560</a> (replaced) [<a href="/pdf/2312.11560" title="Download PDF">pdf</a>, <a href="/format/2312.11560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Emergence: A Study on Proactively Inhibiting the  Monosemantic Neurons of Artificial Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiachuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+S">Shimin Di</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+C+W+W">Charles Wang Wai Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11581" title="Abstract">arXiv:2312.11581</a> (replaced) [<a href="/pdf/2312.11581" title="Download PDF">pdf</a>, <a href="/format/2312.11581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protect Your Score: Contact Tracing With Differential Privacy Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romijnders%2C+R">Rob Romijnders</a>, 
<a href="/search/cs?searchtype=author&query=Louizos%2C+C">Christos Louizos</a>, 
<a href="/search/cs?searchtype=author&query=Asano%2C+Y+M">Yuki M. Asano</a>, 
<a href="/search/cs?searchtype=author&query=Welling%2C+M">Max Welling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14188" title="Abstract">arXiv:2312.14188</a> (replaced) [<a href="/pdf/2312.14188" title="Download PDF">pdf</a>, <a href="/format/2312.14188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Neural Theorem Proving through Data Augmentation and Dynamic  Sampling Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vishwakarma%2C+R">Rahul Vishwakarma</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Subhankar Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14998" title="Abstract">arXiv:2312.14998</a> (replaced) [<a href="/pdf/2312.14998" title="Download PDF">pdf</a>, <a href="/format/2312.14998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic images aid the recognition of human-made art forgeries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ostmeyer%2C+J">Johann Ostmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Schaerf%2C+L">Ludovica Schaerf</a>, 
<a href="/search/cs?searchtype=author&query=Buividovich%2C+P">Pavel Buividovich</a>, 
<a href="/search/cs?searchtype=author&query=Charles%2C+T">Tessa Charles</a>, 
<a href="/search/cs?searchtype=author&query=Postma%2C+E">Eric Postma</a>, 
<a href="/search/cs?searchtype=author&query=Popovici%2C+C">Carina Popovici</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 + 10 pages, 4 + 5 figures, 4 + 10 tables; van Gogh dataset available, DOI: <a href="https://doi.org/10.5281/zenodo.10276928">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15194" title="Abstract">arXiv:2312.15194</a> (replaced) [<a href="/pdf/2312.15194" title="Download PDF">pdf</a>, <a href="/format/2312.15194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PokeMQA: Programmable knowledge editing for Multi-hop Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hengrui Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaixiong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruobing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code is available at <a href="https://github.com/Hengrui-Gu/PokeMQA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17058" title="Abstract">arXiv:2312.17058</a> (replaced) [<a href="/pdf/2312.17058" title="Download PDF">pdf</a>, <a href="/ps/2312.17058" title="Download PostScript">ps</a>, <a href="/format/2312.17058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the optimality of Shapley mechanism for funding public excludable  goods under Sybil strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazorra%2C+B">Bruno Mazorra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17140" title="Abstract">arXiv:2312.17140</a> (replaced) [<a href="/pdf/2312.17140" title="Download PDF">pdf</a>, <a href="/format/2312.17140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Inapproximability of Reconfiguration Problems: PSPACE-Hardness and  some Tight NP-Hardness Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S.%2C+K+C">Karthik C. S.</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01218" title="Abstract">arXiv:2401.01218</a> (replaced) [<a href="/pdf/2401.01218" title="Download PDF">pdf</a>, <a href="/format/2401.01218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Position Debiasing for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The version for ARR submission; 20 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01505" title="Abstract">arXiv:2401.01505</a> (replaced) [<a href="/pdf/2401.01505" title="Download PDF">pdf</a>, <a href="/format/2401.01505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex  and Professional Sports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+A">Andong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Q">Qiuhong Ke</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H">Hossein Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yulan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02384" title="Abstract">arXiv:2401.02384</a> (replaced) [<a href="/pdf/2401.02384" title="Download PDF">pdf</a>, <a href="/format/2401.02384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChartAssisstant: A Universal Chart Multimodal Language Model via  Chart-to-Table Pre-training and Multitask Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanqing Meng</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Quanfeng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated and corrected experimental results, removal of inappropriate experiments, and a more comprehensive experimental setup
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02987" title="Abstract">arXiv:2401.02987</a> (replaced) [<a href="/pdf/2401.02987" title="Download PDF">pdf</a>, <a href="/format/2401.02987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Has Your Pretrained Model Improved? A Multi-head Posterior Based  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aboagye%2C+P">Prince Aboagye</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Saini%2C+U+S">Uday Singh Saini</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+M">Michael Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yujie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06468" title="Abstract">arXiv:2401.06468</a> (replaced) [<a href="/pdf/2401.06468" title="Download PDF">pdf</a>, <a href="/format/2401.06468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Large Language Models for Document-Level Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Minghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+G">George Foster</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress; 20 pages, 16 tables, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06643" title="Abstract">arXiv:2401.06643</a> (replaced) [<a href="/pdf/2401.06643" title="Download PDF">pdf</a>, <a href="/format/2401.06643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of diversity incentives on sample diversity and downstream model  performance in LLM-based text augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cegin%2C+J">Jan Cegin</a>, 
<a href="/search/cs?searchtype=author&query=Pecher%2C+B">Branislav Pecher</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+J">Jakub Simko</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>, 
<a href="/search/cs?searchtype=author&query=Brusilovsky%2C+P">Peter Brusilovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, updated with new experimets - Mistral as downstream task classifier and new method combination (of taboo and hints methods)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06923" title="Abstract">arXiv:2401.06923</a> (replaced) [<a href="/pdf/2401.06923" title="Download PDF">pdf</a>, <a href="/format/2401.06923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimally Supervised Learning using Topological Projections in  Self-Organizing Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Z">Zimeng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ororbia%2C+A">Alexander Ororbia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Desell%2C+T">Travis Desell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07337" title="Abstract">arXiv:2401.07337</a> (replaced) [<a href="/pdf/2401.07337" title="Download PDF">pdf</a>, <a href="/format/2401.07337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Individual and Collective Welfare in Risk Sharing with Many States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Echenique%2C+F">Federico Echenique</a>, 
<a href="/search/econ?searchtype=author&query=Pourbabaee%2C+F">Farzad Pourbabaee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07803" title="Abstract">arXiv:2401.07803</a> (replaced) [<a href="/pdf/2401.07803" title="Download PDF">pdf</a>, <a href="/format/2401.07803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering the Full Potential of Visual Grounding Methods in VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reich%2C+D">Daniel Reich</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+T">Tanja Schultz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07927" title="Abstract">arXiv:2401.07927</a> (replaced) [<a href="/pdf/2401.07927" title="Download PDF">pdf</a>, <a href="/format/2401.07927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are self-explanations from Large Language Models faithful?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madsen%2C+A">Andreas Madsen</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11441" title="Abstract">arXiv:2401.11441</a> (replaced) [<a href="/pdf/2401.11441" title="Download PDF">pdf</a>, <a href="/format/2401.11441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Device Recommender Systems: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruiqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+J">Jing Long</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14126" title="Abstract">arXiv:2401.14126</a> (replaced) [<a href="/pdf/2401.14126" title="Download PDF">pdf</a>, <a href="/ps/2401.14126" title="Download PostScript">ps</a>, <a href="/format/2401.14126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Indexed Linear Logic for Idempotent Intersection Types (Long version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Breuvart%2C+F">Flavien Breuvart</a>, 
<a href="/search/cs?searchtype=author&query=Olimpieri%2C+F">Federico Olimpieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14681" title="Abstract">arXiv:2401.14681</a> (replaced) [<a href="/pdf/2401.14681" title="Download PDF">pdf</a>, <a href="/format/2401.14681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasonTigers@LT-EDI-2024: An Ensemble Approach Towards Detecting  Homophobia and Transphobia in Social Media Comments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Puspo%2C+S+S+C">Sadiya Sayara Chowdhury Puspo</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Emran%2C+A+N+B">Al Nahian Bin Emran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15106" title="Abstract">arXiv:2401.15106</a> (replaced) [<a href="/pdf/2401.15106" title="Download PDF">pdf</a>, <a href="/ps/2401.15106" title="Download PostScript">ps</a>, <a href="/format/2401.15106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision Theoretic Foundations for Experiments Evaluating Human  Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hullman%2C+J">Jessica Hullman</a>, 
<a href="/search/cs?searchtype=author&query=Kale%2C+A">Alex Kale</a>, 
<a href="/search/cs?searchtype=author&query=Hartline%2C+J">Jason Hartline</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15517" title="Abstract">arXiv:2401.15517</a> (replaced) [<a href="/pdf/2401.15517" title="Download PDF">pdf</a>, <a href="/ps/2401.15517" title="Download PostScript">ps</a>, <a href="/format/2401.15517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal Recovery From Product of Two Vandermonde Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapetanovic%2C+D">Dzevdan Kapetanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15814" title="Abstract">arXiv:2401.15814</a> (replaced) [<a href="/pdf/2401.15814" title="Download PDF">pdf</a>, <a href="/format/2401.15814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for  Medication Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weicong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Buntine%2C+W">Wray Buntine</a>, 
<a href="/search/cs?searchtype=author&query=Bingham%2C+G">Gordon Bingham</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15857" title="Abstract">arXiv:2401.15857</a> (replaced) [<a href="/pdf/2401.15857" title="Download PDF">pdf</a>, <a href="/format/2401.15857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opinion Dynamics in Social Multiplex Networks with Mono and  Bi-directional Interactions in the Presence of Leaders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Talebi%2C+A">Amirreza Talebi</a>, 
<a href="/search/eess?searchtype=author&query=Boroujeni%2C+S+P+H">Sayed Pedram Haeri Boroujeni</a>, 
<a href="/search/eess?searchtype=author&query=Razi%2C+A">Abolfazl Razi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16785" title="Abstract">arXiv:2401.16785</a> (replaced) [<a href="/pdf/2401.16785" title="Download PDF">pdf</a>, <a href="/format/2401.16785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HawkEye: Advancing Robust Regression with Bounded, Smooth, and  Insensitive Loss Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M">Mushir Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+M">M. Tanveer</a>, 
<a href="/search/cs?searchtype=author&query=Arshad%2C+M">Mohd. Arshad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17801" title="Abstract">arXiv:2401.17801</a> (replaced) [<a href="/pdf/2401.17801" title="Download PDF">pdf</a>, <a href="/ps/2401.17801" title="Download PostScript">ps</a>, <a href="/format/2401.17801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted-Hamming Metric for Parallel Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bitzer%2C+S">Sebastian Bitzer</a>, 
<a href="/search/cs?searchtype=author&query=Ravagnani%2C+A">Alberto Ravagnani</a>, 
<a href="/search/cs?searchtype=author&query=Weger%2C+V">Violetta Weger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor update with additional citations of weighted-Hamming-metric code constructions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17809" title="Abstract">arXiv:2401.17809</a> (replaced) [<a href="/pdf/2401.17809" title="Download PDF">pdf</a>, <a href="/format/2401.17809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWEA: Changing Factual Knowledge in Large Language Models via Subject  Word Embedding Altering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shasha Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shezheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jie Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weimin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review; Our code will be released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00176" title="Abstract">arXiv:2402.00176</a> (replaced) [<a href="/pdf/2402.00176" title="Download PDF">pdf</a>, <a href="/format/2402.00176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Quantum Machine Learning: An Information-Theoretic  Generalization Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Georgiou%2C+P">Petros Georgiou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jose%2C+S+T">Sharu Theresa Jose</a>, 
<a href="/search/quant-ph?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures. Fixed a typo (wrong inequality sign) in lemma 2 and extended to cover the whole range of values of p. Added reference on inequalities in trace norms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01188" title="Abstract">arXiv:2402.01188</a> (replaced) [<a href="/pdf/2402.01188" title="Download PDF">pdf</a>, <a href="/format/2402.01188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yanfei Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> technical report, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01459" title="Abstract">arXiv:2402.01459</a> (replaced) [<a href="/pdf/2402.01459" title="Download PDF">pdf</a>, <a href="/format/2402.01459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waczy%C5%84ska%2C+J">Joanna Waczy&#x144;ska</a>, 
<a href="/search/cs?searchtype=author&query=Borycki%2C+P">Piotr Borycki</a>, 
<a href="/search/cs?searchtype=author&query=Tadeja%2C+S">S&#x142;awomir Tadeja</a>, 
<a href="/search/cs?searchtype=author&query=Tabor%2C+J">Jacek Tabor</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02009" title="Abstract">arXiv:2402.02009</a> (replaced) [<a href="/pdf/2402.02009" title="Download PDF">pdf</a>, <a href="/format/2402.02009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Multi-Task Learning with Excess Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yifei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shiji Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+H">Hyokun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Belinda Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chilimbi%2C+T">Trishul Chilimbi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02037" title="Abstract">arXiv:2402.02037</a> (replaced) [<a href="/pdf/2402.02037" title="Download PDF">pdf</a>, <a href="/format/2402.02037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EffiBench: Benchmarking the Efficiency of Automatically Generated Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M.Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qing%2C+Y">Yuhao Qing</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Heming Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures, 18 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02172" title="Abstract">arXiv:2402.02172</a> (replaced) [<a href="/pdf/2402.02172" title="Download PDF">pdf</a>, <a href="/format/2402.02172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeAgent: Collaborative Agents for Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Daniel Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kisub Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yewei Song</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Haoye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ezzini%2C+S">Saad Ezzini</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongfeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyande%2C+T+F">Tegawende F. Bissyande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02668" title="Abstract">arXiv:2402.02668</a> (replaced) [<a href="/pdf/2402.02668" title="Download PDF">pdf</a>, <a href="/format/2402.02668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Rateless Set Reconciliation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gilad%2C+Y">Yossi Gilad</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02816" title="Abstract">arXiv:2402.02816</a> (replaced) [<a href="/pdf/2402.02816" title="Download PDF">pdf</a>, <a href="/format/2402.02816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intersectional Two-sided Fairness in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peijie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shaoping Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03877" title="Abstract">arXiv:2402.03877</a> (replaced) [<a href="/pdf/2402.03877" title="Download PDF">pdf</a>, <a href="/format/2402.03877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mouselinos%2C+S">Spyridon Mouselinos</a>, 
<a href="/search/cs?searchtype=author&query=Michalewski%2C+H">Henryk Michalewski</a>, 
<a href="/search/cs?searchtype=author&query=Malinowski%2C+M">Mateusz Malinowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04447" title="Abstract">arXiv:2402.04447</a> (replaced) [<a href="/pdf/2402.04447" title="Download PDF">pdf</a>, <a href="/format/2402.04447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Spectrum Coexistence of Terrestrial Beyond 5G Networks in  Satellite Bands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niloy%2C+T+S+R">Ta Seen Reaz Niloy</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+Z">Zoheb Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+R">Rob Smith</a>, 
<a href="/search/cs?searchtype=author&query=Anapana%2C+V+R">Vikram R. Anapana</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04520" title="Abstract">arXiv:2402.04520</a> (replaced) [<a href="/pdf/2402.04520" title="Download PDF">pdf</a>, <a href="/ps/2402.04520" title="Download PostScript">ps</a>, <a href="/format/2402.04520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computational Limits of Modern Hopfield Models: A Fine-Grained  Complexity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J+Y">Jerry Yao-Chieh Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Thomas Lin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages; v2: fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04655" title="Abstract">arXiv:2402.04655</a> (replaced) [<a href="/pdf/2402.04655" title="Download PDF">pdf</a>, <a href="/format/2402.04655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Vocabulary Calibration for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bob Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04838" title="Abstract">arXiv:2402.04838</a> (replaced) [<a href="/pdf/2402.04838" title="Download PDF">pdf</a>, <a href="/format/2402.04838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuejing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mac+Namee%2C+B">Brian Mac Namee</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Can Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04902" title="Abstract">arXiv:2402.04902</a> (replaced) [<a href="/pdf/2402.04902" title="Download PDF">pdf</a>, <a href="/format/2402.04902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L4Q: Parameter Efficient Quantization-Aware Training on Large Language  Models via LoRA-wise LSQ
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyesung Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yulhwa Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jae-joon Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05067" title="Abstract">arXiv:2402.05067</a> (replaced) [<a href="/pdf/2402.05067" title="Download PDF">pdf</a>, <a href="/format/2402.05067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Modelling with Physics-informed Neural Network: from  Large-scale Dynamics to Small-scale Predictions in Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/physics?searchtype=author&query=Lai%2C+P">Pengyu Lai</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+D">Di Yang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+D">Dewu Yang</a>, 
<a href="/search/physics?searchtype=author&query=Xu%2C+H">Hui Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05273" title="Abstract">arXiv:2402.05273</a> (replaced) [<a href="/pdf/2402.05273" title="Download PDF">pdf</a>, <a href="/format/2402.05273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation  Toolset for Policymakers in Satellite Bands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Niloy%2C+T+R">Ta-seen Reaz Niloy</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+S">Saurav Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Hore%2C+A">Aniruddha Hore</a>, 
<a href="/search/eess?searchtype=author&query=Hassan%2C+Z">Zoheb Hassan</a>, 
<a href="/search/eess?searchtype=author&query=Dietrich%2C+C">Carl Dietrich</a>, 
<a href="/search/eess?searchtype=author&query=Burger%2C+E+W">Eric W. Burger</a>, 
<a href="/search/eess?searchtype=author&query=Reed%2C+J+H">Jeffrey H. Reed</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05515" title="Abstract">arXiv:2402.05515</a> (replaced) [<a href="/pdf/2402.05515" title="Download PDF">pdf</a>, <a href="/format/2402.05515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoisyICL: A Little Noise in Model Parameters Calibrates In-context  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yufeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yoshihiro Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+N">Naoya Inoue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 28 figures, 7 tables (5 pages, 4 figures, 1 table in main body). ACL 2024 under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05619" title="Abstract">arXiv:2402.05619</a> (replaced) [<a href="/pdf/2402.05619" title="Download PDF">pdf</a>, <a href="/format/2402.05619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking Vision and Multi-Agent Communication through Visible Light  Communication using Event Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakagawa%2C+H">Haruyuki Nakagawa</a>, 
<a href="/search/cs?searchtype=author&query=Miyatani%2C+Y">Yoshitaka Miyatani</a>, 
<a href="/search/cs?searchtype=author&query=Kanezaki%2C+A">Asako Kanezaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures, accepted to AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05872" title="Abstract">arXiv:2402.05872</a> (replaced) [<a href="/pdf/2402.05872" title="Download PDF">pdf</a>, <a href="/format/2402.05872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You&#x27;ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for  Semantic and Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ewen%2C+P">Parker Ewen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anran Li</a>, 
<a href="/search/cs?searchtype=author&query=Bagali%2C+A">Anup Bagali</a>, 
<a href="/search/cs?searchtype=author&query=Gunjal%2C+G">Gitesh Gunjal</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+R">Ram Vasudevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05902" title="Abstract">arXiv:2402.05902</a> (replaced) [<a href="/pdf/2402.05902" title="Download PDF">pdf</a>, <a href="/ps/2402.05902" title="Download PostScript">ps</a>, <a href="/format/2402.05902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClickSAM: Fine-tuning Segment Anything Model using click prompts for  ultrasound image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Aimee Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+G">Grace Fei</a>, 
<a href="/search/cs?searchtype=author&query=Pasupuleti%2C+H">Hemanth Pasupuleti</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, SPIE Medical Imaging Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06015" title="Abstract">arXiv:2402.06015</a> (replaced) [<a href="/pdf/2402.06015" title="Download PDF">pdf</a>, <a href="/format/2402.06015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Karamolegkou%2C+A">Antonia Karamolegkou</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06366" title="Abstract">arXiv:2402.06366</a> (replaced) [<a href="/pdf/2402.06366" title="Download PDF">pdf</a>, <a href="/format/2402.06366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAT-based Learning of Computation Tree Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pommellet%2C+A">Adrien Pommellet</a>, 
<a href="/search/cs?searchtype=author&query=Stan%2C+D">Daniel Stan</a>, 
<a href="/search/cs?searchtype=author&query=Scatton%2C+S">Simon Scatton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCAR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06461" title="Abstract">arXiv:2402.06461</a> (replaced) [<a href="/pdf/2402.06461" title="Download PDF">pdf</a>, <a href="/format/2402.06461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Flow Straightening for Generative Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jongmin Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juho Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06649" title="Abstract">arXiv:2402.06649</a> (replaced) [<a href="/pdf/2402.06649" title="Download PDF">pdf</a>, <a href="/ps/2402.06649" title="Download PostScript">ps</a>, <a href="/format/2402.06649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replacing CAPTCHA with XNO micropayments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiruvayipati%2C+S">Sujanavan Tiruvayipati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06664" title="Abstract">arXiv:2402.06664</a> (replaced) [<a href="/pdf/2402.06664" title="Download PDF">pdf</a>, <a href="/format/2402.06664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Agents can Autonomously Hack Websites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Richard Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bindu%2C+R">Rohan Bindu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Akul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Q">Qiusi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06808" title="Abstract">arXiv:2402.06808</a> (replaced) [<a href="/pdf/2402.06808" title="Download PDF">pdf</a>, <a href="/format/2402.06808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explain Variance of Prediction in Variational Time Series Models for  Clinical Deterioration Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+J">Jaideep Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07157" title="Abstract">arXiv:2402.07157</a> (replaced) [<a href="/pdf/2402.07157" title="Download PDF">pdf</a>, <a href="/format/2402.07157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xidong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Ziyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koushik%2C+G+A">Girish A. Koushik</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07180" title="Abstract">arXiv:2402.07180</a> (replaced) [<a href="/pdf/2402.07180" title="Download PDF">pdf</a>, <a href="/format/2402.07180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAGNETO: Edge AI for Human Activity Recognition -- Privacy and  Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+J">Jingwei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Arvanitakis%2C+G">George Arvanitakis</a>, 
<a href="/search/cs?searchtype=author&query=Ndhlovu%2C+M">Mthandazo Ndhlovu</a>, 
<a href="/search/cs?searchtype=author&query=Hacid%2C+H">Hakim Hacid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EDBT 2024 (demo track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07221" title="Abstract">arXiv:2402.07221</a> (replaced) [<a href="/pdf/2402.07221" title="Download PDF">pdf</a>, <a href="/format/2402.07221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Reasons that Agents Act: Intention and Instrumental Goals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ward%2C+F+R">Francis Rhys Ward</a>, 
<a href="/search/cs?searchtype=author&query=MacDermott%2C+M">Matt MacDermott</a>, 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>, 
<a href="/search/cs?searchtype=author&query=Everitt%2C+T">Tom Everitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAMAS24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07354" title="Abstract">arXiv:2402.07354</a> (replaced) [<a href="/pdf/2402.07354" title="Download PDF">pdf</a>, <a href="/format/2402.07354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using  diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ren%2C+T">Tianyi Ren</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+A">Abhishek Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Rivera%2C+J+H">Juampablo Heras Rivera</a>, 
<a href="/search/eess?searchtype=author&query=Rebala%2C+H">Harshitha Rebala</a>, 
<a href="/search/eess?searchtype=author&query=Honey%2C+E">Ethan Honey</a>, 
<a href="/search/eess?searchtype=author&query=Chopra%2C+A">Agamdeep Chopra</a>, 
<a href="/search/eess?searchtype=author&query=Ruzevick%2C+J">Jacob Ruzevick</a>, 
<a href="/search/eess?searchtype=author&query=Kurt%2C+M">Mehmet Kurt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07456" title="Abstract">arXiv:2402.07456</a> (replaced) [<a href="/pdf/2402.07456" title="Download PDF">pdf</a>, <a href="/format/2402.07456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zichen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zhenmin Weng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhoumianze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://os-copilot.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07660" title="Abstract">arXiv:2402.07660</a> (replaced) [<a href="/pdf/2402.07660" title="Download PDF">pdf</a>, <a href="/ps/2402.07660" title="Download PostScript">ps</a>, <a href="/format/2402.07660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R&#xe9;nyi Resolvability, Noise Stability, and Anti-contractivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages. Exponential convergence for the R\'enyi divergence $D_q(P_{Y}^{\otimes n}\|Q_{Y^n})$ with $q\ge 1$ was added. Also some minor revisions were made
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07738" title="Abstract">arXiv:2402.07738</a> (replaced) [<a href="/pdf/2402.07738" title="Download PDF">pdf</a>, <a href="/format/2402.07738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Link Predictor By In-Context Learning on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kaiwen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhichun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07834" title="Abstract">arXiv:2402.07834</a> (replaced) [<a href="/pdf/2402.07834" title="Download PDF">pdf</a>, <a href="/format/2402.07834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing across Temporal Domains with Koopman Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qiuhao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gezheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+R">Ruizhi Pu</a>, 
<a href="/search/cs?searchtype=author&query=Shui%2C+C">Changjian Shui</a>, 
<a href="/search/cs?searchtype=author&query=Gagne%2C+C">Christian Gagne</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shichun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C+X">Charles X. Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, Accepted by AAAI 2024. arXiv admin note: text overlap with <a href="/abs/2206.00047">arXiv:2206.00047</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07876" title="Abstract">arXiv:2402.07876</a> (replaced) [<a href="/pdf/2402.07876" title="Download PDF">pdf</a>, <a href="/format/2402.07876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Improvement using Language Feedback Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+V">Victor Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+D">Dipendra Misra</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre C&#xf4;t&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08090" title="Abstract">arXiv:2402.08090</a> (replaced) [<a href="/pdf/2402.08090" title="Download PDF">pdf</a>, <a href="/format/2402.08090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Neural Contracting Dynamics: Extended Linearization and Global  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaffe%2C+S">Sean Jaffe</a>, 
<a href="/search/cs?searchtype=author&query=Davydov%2C+A">Alexander Davydov</a>, 
<a href="/search/cs?searchtype=author&query=Lapsekili%2C+D">Deniz Lapsekili</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ambuj Singh</a>, 
<a href="/search/cs?searchtype=author&query=Bullo%2C+F">Francesco Bullo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures. Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08093" title="Abstract">arXiv:2402.08093</a> (replaced) [<a href="/pdf/2402.08093" title="Download PDF">pdf</a>, <a href="/format/2402.08093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BASE TTS: Lessons from building a billion-parameter Text-to-Speech model  on 100K hours of data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%81ajszczak%2C+M">Mateusz &#x141;ajszczak</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%A1mbara%2C+G">Guillermo C&#xe1;mbara</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Beyhan%2C+F">Fatih Beyhan</a>, 
<a href="/search/cs?searchtype=author&query=van+Korlaar%2C+A">Arent van Korlaar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Joly%2C+A">Arnaud Joly</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Cortinas%2C+%C3%81">&#xc1;lvaro Mart&#xed;n-Cortinas</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A">Ammar Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Michalski%2C+A">Adam Michalski</a>, 
<a href="/search/cs?searchtype=author&query=Moinet%2C+A">Alexis Moinet</a>, 
<a href="/search/cs?searchtype=author&query=Karlapati%2C+S">Sri Karlapati</a>, 
<a href="/search/cs?searchtype=author&query=Muszy%C5%84ska%2C+E">Ewa Muszy&#x144;ska</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haohan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Putrycz%2C+B">Bartosz Putrycz</a>, 
<a href="/search/cs?searchtype=author&query=Gambino%2C+S+L">Soledad L&#xf3;pez Gambino</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K">Kayeon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Sokolova%2C+E">Elena Sokolova</a>, 
<a href="/search/cs?searchtype=author&query=Drugman%2C+T">Thomas Drugman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1.1 (fixed typos)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08172" title="Abstract">arXiv:2402.08172</a> (replaced) [<a href="/pdf/2402.08172" title="Download PDF">pdf</a>, <a href="/format/2402.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Projection-Based Time-Segmented Reduced Order Model for  Fluid-Structure Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Q">Qijia Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiquan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Pengtao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoping Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08341" title="Abstract">arXiv:2402.08341</a> (replaced) [<a href="/pdf/2402.08341" title="Download PDF">pdf</a>, <a href="/format/2402.08341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting Personality Traits in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hilliard%2C+A">Airlie Hilliard</a>, 
<a href="/search/cs?searchtype=author&query=Munoz%2C+C">Cristian Munoz</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zekun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Koshiyama%2C+A+S">Adriano Soares Koshiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript submitted to ACM Facct. Authors One and Two contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08436" title="Abstract">arXiv:2402.08436</a> (replaced) [<a href="/pdf/2402.08436" title="Download PDF">pdf</a>, <a href="/ps/2402.08436" title="Download PostScript">ps</a>, <a href="/format/2402.08436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The current state of security -- Insights from the German software  industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Langstrof%2C+T">Timo Langstrof</a>, 
<a href="/search/cs?searchtype=author&query=Sabau%2C+A+R">Alex R. Sabau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08563" title="Abstract">arXiv:2402.08563</a> (replaced) [<a href="/pdf/2402.08563" title="Download PDF">pdf</a>, <a href="/format/2402.08563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Restoration Tackles Forward and Inverse Problems for  the Laplace Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Amartya Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Stadt%2C+M+M">Melissa M. Stadt</a>, 
<a href="/search/cs?searchtype=author&query=Podina%2C+L">Lena Podina</a>, 
<a href="/search/cs?searchtype=author&query=Kohandel%2C+M">Mohammad Kohandel</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08595" title="Abstract">arXiv:2402.08595</a> (replaced) [<a href="/pdf/2402.08595" title="Download PDF">pdf</a>, <a href="/format/2402.08595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homomorphism Counts for Graph Neural Networks: All About That Basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+E">Emily Jin</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+I+I">Ismail Ilkan Ceylan</a>, 
<a href="/search/cs?searchtype=author&query=Lanzinger%2C+M">Matthias Lanzinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08638" title="Abstract">arXiv:2402.08638</a> (replaced) [<a href="/pdf/2402.08638" title="Download PDF">pdf</a>, <a href="/format/2402.08638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ousidhoum%2C+N">Nedjma Ousidhoum</a>, 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+S+H">Shamsuddeen Hassan Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mohamed Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Abdulmumin%2C+I">Idris Abdulmumin</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I+S">Ibrahim Said Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+S">Sanchit Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+V">Vladimir Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Ayele%2C+A+A">Abinew Ali Ayele</a>, 
<a href="/search/cs?searchtype=author&query=Baswani%2C+P">Pavan Baswani</a>, 
<a href="/search/cs?searchtype=author&query=Beloucif%2C+M">Meriem Beloucif</a>, 
<a href="/search/cs?searchtype=author&query=Biemann%2C+C">Chris Biemann</a>, 
<a href="/search/cs?searchtype=author&query=Bourhim%2C+S">Sofia Bourhim</a>, 
<a href="/search/cs?searchtype=author&query=De+Kock%2C+C">Christine De Kock</a>, 
<a href="/search/cs?searchtype=author&query=Dekebo%2C+G+S">Genet Shanko Dekebo</a>, 
<a href="/search/cs?searchtype=author&query=Hourrane%2C+O">Oumaima Hourrane</a>, 
<a href="/search/cs?searchtype=author&query=Kanumolu%2C+G">Gopichand Kanumolu</a>, 
<a href="/search/cs?searchtype=author&query=Madasu%2C+L">Lokesh Madasu</a>, 
<a href="/search/cs?searchtype=author&query=Rutunda%2C+S">Samuel Rutunda</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+M">Manish Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Solorio%2C+T">Thamar Solorio</a>, 
<a href="/search/cs?searchtype=author&query=Surange%2C+N">Nirmal Surange</a>, 
<a href="/search/cs?searchtype=author&query=Tilaye%2C+H+G">Hailegnaw Getaneh Tilaye</a>, 
<a href="/search/cs?searchtype=author&query=Vishnubhotla%2C+K">Krishnapriya Vishnubhotla</a>, 
<a href="/search/cs?searchtype=author&query=Winata%2C+G">Genta Winata</a>, 
<a href="/search/cs?searchtype=author&query=Yimam%2C+S+M">Seid Muhie Yimam</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08656" title="Abstract">arXiv:2402.08656</a> (replaced) [<a href="/pdf/2402.08656" title="Download PDF">pdf</a>, <a href="/format/2402.08656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroBench: An Open-Source Benchmark Framework for the Standardization  of Methodology in Brainwave-based Authentication Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaurasia%2C+A+K">Avinash Kumar Chaurasia</a>, 
<a href="/search/cs?searchtype=author&query=Fallahi%2C+M">Matin Fallahi</a>, 
<a href="/search/cs?searchtype=author&query=Strufe%2C+T">Thorsten Strufe</a>, 
<a href="/search/cs?searchtype=author&query=Terh%C3%B6rst%2C+P">Philipp Terh&#xf6;rst</a>, 
<a href="/search/cs?searchtype=author&query=Cabarcos%2C+P+A">Patricia Arias Cabarcos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 Figures, 3 tables, Submitted to the Journal of Information Security and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08711" title="Abstract">arXiv:2402.08711</a> (replaced) [<a href="/pdf/2402.08711" title="Download PDF">pdf</a>, <a href="/ps/2402.08711" title="Download PostScript">ps</a>, <a href="/format/2402.08711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correction to &quot;Wasserstein distance estimates for the distributions of  numerical approximations to ergodic stochastic differential equations&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Paulin%2C+D">Daniel Paulin</a>, 
<a href="/search/stat?searchtype=author&query=Whalley%2C+P+A">Peter A. Whalley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08777" title="Abstract">arXiv:2402.08777</a> (replaced) [<a href="/pdf/2402.08777" title="Download PDF">pdf</a>, <a href="/format/2402.08777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+Z">Zhihan Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+W">Weimin Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ho%2C+H">Harrison Ho</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Shi%2C+L">Lizhen Shi</a>, 
<a href="/search/q-bio?searchtype=author&query=Davuluri%2C+R+V">Ramana V Davuluri</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Z">Zhong Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+H">Han Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08851" title="Abstract">arXiv:2402.08851</a> (replaced) [<a href="/pdf/2402.08851" title="Download PDF">pdf</a>, <a href="/ps/2402.08851" title="Download PostScript">ps</a>, <a href="/format/2402.08851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,  Pareto-Optimality, and Efficient Computability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tr%C3%B6bst%2C+T">Thorben Tr&#xf6;bst</a>, 
<a href="/search/cs?searchtype=author&query=Vazirani%2C+V+V">Vijay V. Vazirani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08991" title="Abstract">arXiv:2402.08991</a> (replaced) [<a href="/pdf/2402.08991" title="Download PDF">pdf</a>, <a href="/ps/2402.08991" title="Download PostScript">ps</a>, <a href="/format/2402.08991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Model-Based Reinforcement Learning Against Adversarial  Corruption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09015" title="Abstract">arXiv:2402.09015</a> (replaced) [<a href="/pdf/2402.09015" title="Download PDF">pdf</a>, <a href="/format/2402.09015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arabzadeh%2C+N">Negar Arabzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kiseleva%2C+J">Julia Kiseleva</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Dibia%2C+V">Victor Dibia</a>, 
<a href="/search/cs?searchtype=author&query=Fourney%2C+A">Adam Fourney</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+C">Charles Clarke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09051" title="Abstract">arXiv:2402.09051</a> (replaced) [<a href="/pdf/2402.09051" title="Download PDF">pdf</a>, <a href="/format/2402.09051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09094" title="Abstract">arXiv:2402.09094</a> (replaced) [<a href="/pdf/2402.09094" title="Download PDF">pdf</a>, <a href="/format/2402.09094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unity is Strength: Enhancing Precision in Reentrancy Vulnerability  Detection of Smart Contract Analysis Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zexu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiachi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+P">Peilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weizhe Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09132" title="Abstract">arXiv:2402.09132</a> (replaced) [<a href="/pdf/2402.09132" title="Download PDF">pdf</a>, <a href="/format/2402.09132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Adversarial Capabilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+M+H">Minh Hieu Le</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09179" title="Abstract">arXiv:2402.09179</a> (replaced) [<a href="/pdf/2402.09179" title="Download PDF">pdf</a>, <a href="/format/2402.09179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model  Customization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+R">Rui Wen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenbo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09193" title="Abstract">arXiv:2402.09193</a> (replaced) [<a href="/pdf/2402.09193" title="Download PDF">pdf</a>, <a href="/format/2402.09193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Ir)rationality and Cognitive Biases in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macmillan-Scott%2C+O">Olivia Macmillan-Scott</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09205" title="Abstract">arXiv:2402.09205</a> (replaced) [<a href="/pdf/2402.09205" title="Download PDF">pdf</a>, <a href="/format/2402.09205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tell Me More! Towards Implicit User Intention Understanding of Language  Model Driven Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingxiang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhong Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jia Deng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 tables, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09232" title="Abstract">arXiv:2402.09232</a> (replaced) [<a href="/pdf/2402.09232" title="Download PDF">pdf</a>, <a href="/ps/2402.09232" title="Download PostScript">ps</a>, <a href="/format/2402.09232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterated Straight-Line Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navarro%2C+G">Gonzalo Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Urbina%2C+C">Cristian Urbina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version of the article includes the proofs omitted from LATIN24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09234" title="Abstract">arXiv:2402.09234</a> (replaced) [<a href="/pdf/2402.09234" title="Download PDF">pdf</a>, <a href="/format/2402.09234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash  Simulations Using Graph Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kneifl%2C+J">Jonas Kneifl</a>, 
<a href="/search/cs?searchtype=author&query=Fehr%2C+J">J&#xf6;rg Fehr</a>, 
<a href="/search/cs?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09288" title="Abstract">arXiv:2402.09288</a> (replaced) [<a href="/pdf/2402.09288" title="Download PDF">pdf</a>, <a href="/format/2402.09288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EcoVal: An Efficient Data Valuation Framework for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarun%2C+A+K">Ayush K Tarun</a>, 
<a href="/search/cs?searchtype=author&query=Chundawat%2C+V+S">Vikram S Chundawat</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+M">Murari Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H+M">Hong Ming Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bowei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09338" title="Abstract">arXiv:2402.09338</a> (replaced) [<a href="/pdf/2402.09338" title="Download PDF">pdf</a>, <a href="/format/2402.09338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks Asymptotic Behaviours for the Resolution of Inverse  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Del+Debbio%2C+L">Luigi Del Debbio</a>, 
<a href="/search/physics?searchtype=author&query=Naviglio%2C+M">Manuel Naviglio</a>, 
<a href="/search/physics?searchtype=author&query=Tarantelli%2C+F">Francesco Tarantelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Artificial Intelligence (cs.AI); High Energy Physics - Lattice (hep-lat); High Energy Physics - Theory (hep-th)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09345" title="Abstract">arXiv:2402.09345</a> (replaced) [<a href="/pdf/2402.09345" title="Download PDF">pdf</a>, <a href="/format/2402.09345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Reward Hacking via Information-Theoretic Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yuchun Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Rong Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09379" title="Abstract">arXiv:2402.09379</a> (replaced) [<a href="/pdf/2402.09379" title="Download PDF">pdf</a>, <a href="/format/2402.09379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed-sparsity matrix approximation from matrix-vector products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amsel%2C+N">Noah Amsel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tyler Chen</a>, 
<a href="/search/cs?searchtype=author&query=Keles%2C+F+D">Feyza Duman Keles</a>, 
<a href="/search/cs?searchtype=author&query=Halikias%2C+D">Diana Halikias</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Cameron Musco</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Christopher Musco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> correctly alphabetize authors, fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item367">Cross-lists</a></li>
<li><a href="#item451">Replacements</a></li>
</ul>
<small>[ total of 692 entries:  <b>1-692</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
